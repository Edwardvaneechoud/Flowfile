{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"quickstart.html","title":"Quick Start Guide","text":"Get Started with Flowfile in 5 Minutes"},{"location":"quickstart.html#installation","title":"Installation","text":"Recommended quickstart: Install from PyPI <pre><code>pip install flowfile</code></pre> <p>This installs everything you need - the Python API, visual editor, and all services.</p>"},{"location":"quickstart.html#alternative-installation-methods","title":"Alternative Installation Methods","text":"Desktop Application (Pre-built Installer) <p>Download the latest installer for your platform: - Windows: Flowfile-Setup.exe - macOS: Flowfile.dmg </p> <p>Note: You may see security warnings since the installer isn't signed. On Windows, click \"More info\" \u2192 \"Run anyway\". On macOS, right-click \u2192 \"Open\" \u2192 confirm.</p> Development Setup (From Source) <pre><code># Clone repository\ngit clone https://github.com/edwardvaneechoud/Flowfile.git\ncd Flowfile\n\n# Install with Poetry\npoetry install\n\n# Start services\npoetry run flowfile_worker  # Terminal 1 (port 63579)\npoetry run flowfile_core    # Terminal 2 (port 63578)\n\n# Start frontend\ncd flowfile_frontend\nnpm install\nnpm run dev:web  # Terminal 3 (port 8080)\n</code></pre>"},{"location":"quickstart.html#choose-your-path","title":"Choose Your Path","text":"Non-Technical Users <p>Perfect for: Analysts, business users, Excel power users</p> <p>No coding required!</p> <ul> <li>\u2705 Drag and drop interface</li> <li>\u2705 Visual data preview</li> <li>\u2705 Export to Excel/CSV</li> <li>\u2705 Built-in transformations</li> </ul> Start Visual Tutorial \u2192 Technical Users <p>Perfect for: Developers, data scientists, engineers</p> <p>Full programmatic control!</p> <ul> <li>\u2705 Polars-compatible API</li> <li>\u2705 Cloud storage integration</li> <li>\u2705 Version control friendly</li> <li>\u2705 Complex dynamic logic</li> </ul> Start Python Tutorial \u2192"},{"location":"quickstart.html#non-technical-quickstart","title":"Quick Start for Non-Technical Users","text":"Goal: Clean and analyze sales data without writing any code"},{"location":"quickstart.html#step-1-start-flowfile-and-create-a-flow","title":"Step 1: Start Flowfile, and create a Flow","text":"<p>Open your terminal (Command Prompt on Windows, Terminal on Mac) and type:</p> <p><pre><code>flowfile run ui\n</code></pre> Your browser should automatically open to the Flowfile UI.</p> <p>If the browser does not open automatically</p> <p>If the browser does not open automatically, you can manually navigate to http://127.0.0.1:63578/ui#/main/designer in your web browser.</p> <p>Creating your First Flow:</p> <ol> <li>Click \"Create\" to create a new data pipeline</li> <li>Click \"Create New File Here\"</li> <li>Name your flow (e.g., \"Sales Data Analysis\")</li> <li>Click on \"Settings\" in the top right to configure your flow</li> <li>Set the Execution mode to \"Development\"</li> </ol> <p>Your should see now an empty flow: New clean flow interface</p>"},{"location":"quickstart.html#step-2-load-your-data","title":"Step 2: Load Your Data","text":"<p>Loading a CSV or Excel file:</p> <ol> <li>Find the \"Read Data\" node in the left panel under \"Input\"</li> <li>Drag it onto the canvas (center area)</li> <li>Click the node to open settings on the right</li> <li>Click \"Browse\" and select your file</li> <li>Configure options (if needed):<ul> <li>For CSV: Check \"Has Headers\" if your file has column names</li> <li>For Excel: Select the sheet name</li> </ul> </li> <li>Click \"Run\" (top toolbar) to load the data</li> <li>Click the node to preview your data in the bottom panel</li> </ol>"},{"location":"quickstart.html#step-3-clean-your-data","title":"Step 3: Clean Your Data","text":"<p>Let's remove duplicate records and filter for high-value transactions:</p> Remove Duplicates <ol> <li>Drag \"Drop Duplicates\" node from Transform section</li> <li>Connect it to your Read Data node</li> <li>Select columns to check for duplicates</li> <li>Click Run</li> </ol> Filter Data <ol> <li>Drag \"Filter Data\" node from Transform section</li> <li>Connect it to Drop Duplicates node</li> <li>Enter formula: <code>[Quantity] &gt; 7</code></li> <li>Click Run</li> </ol>"},{"location":"quickstart.html#step-4-analyze-your-data","title":"Step 4: Analyze Your Data","text":"<p>Create a summary by city:</p> <ol> <li>Add a Group By node from the Aggregate section</li> <li>Connect it to your Filter node</li> <li>Configure the aggregation:</li> <li>Group by: <code>city</code></li> <li>Aggregations:<ul> <li><code>gross income</code> \u2192 Sum \u2192 Name it <code>total_sales</code></li> <li><code>gross income</code> \u2192 Average \u2192 Name it <code>avg_sale</code></li> <li><code>gross income</code> \u2192 Count \u2192 Name it <code>number_of_sales</code></li> </ul> </li> <li>Click Run to see your summary</li> </ol> Data after group by <p></p>"},{"location":"quickstart.html#step-5-save-your-results","title":"Step 5: Save Your Results","text":"<p>Export your cleaned data:</p> <ol> <li>Add a \"Write Data\" node from Output section</li> <li>Connect it to your final transformation</li> <li>Choose format:<ul> <li>Excel: Best for sharing with colleagues</li> <li>CSV: Best for Excel/Google Sheets</li> <li>Parquet: Best for large datasets</li> </ul> </li> <li>Set file path (e.g., <code>cleaned_sales.xlsx</code>)</li> <li>Click Run to save</li> </ol>"},{"location":"quickstart.html#heres-what-your-complete-flow-should-look-like","title":"Here's what your complete flow should look like:","text":""},{"location":"quickstart.html#congratulations","title":"Congratulations!","text":"<p>You've just built your first data pipeline! You can: - Save this flow using File \u2192 Save (creates a <code>.flowfile</code>) - Share it with colleagues who can run it without any setup - Schedule it to run automatically (coming soon) - Export as Python code if you want to see what's happening behind the scenes</p>"},{"location":"quickstart.html#pro-tips-for-non-technical-users","title":"Pro Tips for Non-Technical Users:","text":"<ul> <li>Use descriptions: Right-click nodes and add descriptions to document your work</li> <li>Preview often: Click nodes after running to see data at each step</li> <li>Start small: Test with a sample of your data first</li> <li>Save versions: Save different versions of your flow as you build</li> </ul>"},{"location":"quickstart.html#next-steps","title":"Next Steps","text":"<p>Complete Visual Guide Learn All Nodes Connect to Databases</p>"},{"location":"quickstart.html#technical-quickstart","title":"Quick Start for Technical Users","text":"Goal: Build a production-ready ETL pipeline with cloud integration"},{"location":"quickstart.html#step-1-install-and-import","title":"Step 1: Install and Import","text":"<pre><code>pip install flowfile\n</code></pre> <pre><code>import flowfile as ff\nfrom flowfile import col, when, lit\nimport polars as pl  # Flowfile returns Polars DataFrames\n</code></pre>"},{"location":"quickstart.html#step-2-build-a-real-world-etl-pipeline","title":"Step 2: Build a Real-World ETL Pipeline","text":"<p>Let's build a production pipeline that reads from S3, transforms data, and writes results:</p> <pre><code># Configure S3 connection (one-time setup)\nfrom pydantic import SecretStr\n\nimport flowfile as ff\n\nff.create_cloud_storage_connection_if_not_exists(\n    ff.FullCloudStorageConnection(\n        connection_name=\"production-data\",\n        storage_type=\"s3\",\n        auth_method=\"access_key\",\n        aws_region=\"us-east-1\",\n        aws_access_key_id=\"AKIAIOSFODNN7EXAMPLE\",\n        aws_secret_access_key=SecretStr(\"wJalrXUtnFEMI/K7MDENG\")\n    )\n)\n</code></pre>"},{"location":"quickstart.html#step-3-extract-and-transform","title":"Step 3: Extract and Transform","text":"<pre><code># Build the pipeline (lazy evaluation - no data loaded yet!)\nimport flowfile as ff\npipeline = (\n    # Extract: Read partitioned parquet files from S3\n    ff.scan_parquet_from_cloud_storage(\n        \"s3://data-lake/sales/year=2024/month=*\", \n        connection_name=\"production-data\",\n        description=\"Load Q1-Q4 2024 sales data\"\n    )\n\n    # Transform: Clean and enrich\n    .filter(\n        (ff.col(\"status\") == \"completed\") &amp; \n        (ff.col(\"amount\") &gt; 0),\n        description=\"Keep only valid completed transactions\"\n    )\n\n    # Add calculated fields\n    .with_columns([\n        # Business logic\n        (ff.col(\"amount\") * ff.col(\"quantity\")).alias(\"line_total\"),\n        (ff.col(\"amount\") * ff.col(\"quantity\") * 0.1).alias(\"tax\"),\n\n        # Date features for analytics\n        ff.col(\"order_date\").dt.quarter().alias(\"quarter\"),\n        ff.col(\"order_date\").dt.day_of_week().alias(\"day_of_week\"),\n\n        # Customer segmentation\n        ff.when(ff.col(\"customer_lifetime_value\") &gt; 10000)\n            .then(ff.lit(\"VIP\"))\n            .when(ff.col(\"customer_lifetime_value\") &gt; 1000)\n            .then(ff.lit(\"Regular\"))\n            .otherwise(ff.lit(\"New\"))\n            .alias(\"customer_segment\"),\n\n        # Region mapping\n        ff.when(ff.col(\"state\").is_in([\"CA\", \"OR\", \"WA\"]))\n            .then(ff.lit(\"West\"))\n            .when(ff.col(\"state\").is_in([\"NY\", \"NJ\", \"PA\"]))\n            .then(ff.lit(\"Northeast\"))\n            .when(ff.col(\"state\").is_in([\"TX\", \"FL\", \"GA\"]))\n            .then(ff.lit(\"South\"))\n            .otherwise(ff.lit(\"Midwest\"))\n            .alias(\"region\")\n    ], description=\"Add business metrics and segments\")\n\n    # Complex aggregation\n    .group_by([\"region\", \"quarter\", \"customer_segment\"])\n    .agg([\n        # Revenue metrics\n        ff.col(\"line_total\").sum().alias(\"total_revenue\"),\n        ff.col(\"tax\").sum().alias(\"total_tax\"),\n\n        # Order metrics\n        ff.col(\"order_id\").n_unique().alias(\"unique_orders\"),\n        ff.col(\"customer_id\").n_unique().alias(\"unique_customers\"),\n\n        # Performance metrics\n        ff.col(\"line_total\").mean().round(2).alias(\"avg_order_value\"),\n        ff.col(\"quantity\").sum().alias(\"units_sold\"),\n\n        # Statistical metrics\n        ff.col(\"line_total\").std().round(2).alias(\"revenue_std\"),\n        ff.col(\"line_total\").quantile(0.5).alias(\"median_order_value\")\n    ])\n\n    # Final cleanup\n    .sort([\"region\", \"quarter\", \"total_revenue\"], descending=[False, False, True])\n    .filter(ff.col(\"total_revenue\") &gt; 1000)  # Remove noise\n)\n\n# Check the execution plan (no data processed yet!)\nprint(pipeline.explain())  # Shows optimized Polars query plan\n</code></pre>"},{"location":"quickstart.html#step-4-load-and-monitor","title":"Step 4: Load and Monitor","text":"<pre><code># Option 1: Write to cloud storage\npipeline.write_parquet_to_cloud_storage(\n    \"s3://data-warehouse/aggregated/sales_summary_2024.parquet\",\n    connection_name=\"production-data\",\n    compression=\"snappy\",\n    description=\"Save aggregated results for BI tools\"\n)\n\n# Option 2: Write to Delta Lake for versioning\npipeline.write_delta(\n    \"s3://data-warehouse/delta/sales_summary\",\n    connection_name=\"production-data\",\n    write_mode=\"append\",  # or \"overwrite\"\n    description=\"Append to Delta table\"\n)\n\n# Option 3: Collect for analysis\ndf_result = pipeline.collect()  # NOW it executes everything!\nprint(f\"Processed {len(df_result):,} aggregated records\")\nprint(df_result.head())\n</code></pre>"},{"location":"quickstart.html#step-5-advanced-features","title":"Step 5: Advanced Features","text":""},{"location":"quickstart.html#visualize-your-pipeline","title":"Visualize Your Pipeline","text":"<pre><code># Open in visual editor\nff.open_graph_in_editor(pipeline.flow_graph)\n\n# This shows your entire pipeline\n# as a visual flow diagram!\n</code></pre>  Visual overview of pipeline"},{"location":"quickstart.html#export-as-pure-python","title":"Export as Pure Python","text":"<pre><code># Generate standalone code\ncode = pipeline.flow_graph.generate_code()\n\n# Deploy without Flowfile dependency!\n# Uses only Polars\n</code></pre>"},{"location":"quickstart.html#step-6-production-patterns","title":"Step 6: Production Patterns","text":"<p>Pattern 1: Data Quality Checks</p> <pre><code>from datetime import datetime\nimport flowfile as ff\n\ndef data_quality_pipeline(df: ff.FlowFrame) -&gt; ff.FlowFrame:\n    \"\"\"Reusable data quality check pipeline\"\"\"\n\n    # Record initial count\n    initial_count = df.select(ff.col(\"*\").count().alias(\"count\"))\n\n    # Apply quality filters\n    clean_df = (\n        df\n        # Remove nulls in critical fields\n        .drop_nulls(subset=[\"order_id\", \"customer_id\", \"amount\"])\n\n        # Validate data ranges\n        .filter(\n            (ff.col(\"amount\").is_between(0, 1000000)) &amp;\n            (ff.col(\"quantity\") &gt; 0) &amp;\n            (ff.col(\"order_date\") &lt;= datetime.now())\n        )\n\n        # Remove duplicates\n        .unique(subset=[\"order_id\"], keep=\"first\")\n    )\n\n    # Log quality metrics\n    final_count = clean_df.select(ff.col(\"*\").count().alias(\"count\"))\n    print(f\"Initial count: {initial_count.collect()[0]['count']}\")\n    print(f\"Final count after quality checks: {final_count.collect()[0]['count']}\")\n    return clean_df\n</code></pre> <p>Pattern 2: Incremental Processing</p> <pre><code># Read only new data since last run\nfrom datetime import datetime\nimport flowfile as ff\nlast_processed = datetime(2024, 10, 1)\n\nincremental_pipeline = (\n    ff.scan_parquet_from_cloud_storage(\n        \"s3://data-lake/events/\",\n        connection_name=\"production-data\"\n    )\n    .filter(ff.col(\"event_timestamp\") &gt; last_processed)\n    .group_by(ff.col(\"event_timestamp\").dt.date().alias(\"date\"))\n    .agg([\n        ff.col(\"event_id\").count().alias(\"event_count\"),\n        ff.col(\"user_id\").n_unique().alias(\"unique_users\")\n    ])\n)\n\n# Process and append to existing data\nincremental_pipeline.write_delta(\n    \"s3://data-warehouse/delta/daily_metrics\",\n    connection_name=\"production-data\",\n    write_mode=\"append\"\n)\n</code></pre> <p>Pattern 3: Multi-Source Join</p> <pre><code># Combine data from multiple sources\nimport flowfile as ff\n\ncustomers = ff.scan_parquet_from_cloud_storage(\n    \"s3://data-lake/customers/\",\n    connection_name=\"production-data\"\n)\n\norders = ff.scan_csv_from_cloud_storage(\n    \"s3://raw-data/orders/\",\n    connection_name=\"production-data\",\n    delimiter=\"|\",\n    has_header=True\n)\n\nproducts = ff.read_parquet(\"local_products.parquet\")\n\n# Complex multi-join pipeline\nenriched_orders = (\n    orders\n    .join(customers, on=\"customer_id\", how=\"left\")\n    .join(products, on=\"product_id\", how=\"left\")\n    .with_columns([\n        # Handle missing values from left joins\n        ff.col(\"customer_segment\").fill_null(\"Unknown\"),\n        ff.col(\"product_category\").fill_null(\"Other\"),\n\n        # Calculate metrics\n        (ff.col(\"unit_price\") * ff.col(\"quantity\") * \n         (ff.lit(1) - ff.col(\"discount_rate\").fill_null(0))).alias(\"net_revenue\")\n    ])\n)\n\n# Materialize results\nresults = enriched_orders.collect()\n</code></pre>"},{"location":"quickstart.html#next-steps-for-technical-users","title":"Next Steps for Technical Users","text":"Complete API Reference Architecture Deep Dive Core Internals Polars Documentation"},{"location":"quickstart.html#why-flowfile","title":"\ud83c\udf1f Why Flowfile?","text":"\u26a1 Performance <p>Built on Polars - Uses the speed of Polars</p> \ud83d\udd04 Dual Interface <p>Same pipeline works in both visual and code. Switch anytime, no lock-in.</p> \ud83d\udce6 Export to Production <p>Generate pure Python/Polars code. Deploy anywhere without Flowfile.</p> \u2601\ufe0f Cloud Support <p>Direct S3/cloud storage support, no need for expensive clusters to analyse your data</p>"},{"location":"quickstart.html#troubleshooting","title":"Troubleshooting","text":"Installation Issues <pre><code># If pip install fails, try:\npip install --upgrade pip\npip install flowfile\n\n# For M1/M2 Macs:\npip install flowfile --no-binary :all:\n\n# Behind corporate proxy:\npip install --proxy http://proxy.company.com:8080 flowfile\n</code></pre> Port Already in Use <pre><code># Find what's using port 63578\nlsof -i :63578  # Mac/Linux\nnetstat -ano | findstr :63578  # Windows\n\n# Kill the process or use different port:\nFLOWFILE_PORT=8080 flowfile run ui\n</code></pre>"},{"location":"quickstart.html#get-help","title":"Get Help","text":"Documentation Full Documentation Discussions GitHub Discussions Issues GitHub Issues Ready to Transform Your Data? <p>Join thousands of users building data pipelines with Flowfile</p> Start Visual (No Code) \u2192 Start Coding (Python) \u2192"},{"location":"for-developers/index.html","title":"Flowfile: For Developers","text":"<p>Welcome to the developer documentation for Flowfile. This is the home for anyone who wants to contribute to the platform or understand its internal architecture.</p> <p>Looking to use the Python API?</p> <p>If you want to use Flowfile's Python API to build data pipelines, check out the Python API User Guide. This developer section focuses on Flowfile's internal architecture and design philosophy.</p>"},{"location":"for-developers/index.html#the-core-philosophy-code-and-ui-are-the-same-thing","title":"The Core Philosophy: Code and UI are the Same Thing","text":"<p>Flowfile is built on an architecture where the Python API and the visual editor are two interfaces to the exact same underlying objects: the <code>FlowGraph</code> and its <code>FlowNodes</code>.</p> <p>When you write <code>df.filter(...)</code>, you programmatically construct a <code>FlowNode</code> and attach it to the <code>FlowGraph</code>. When a user drags a \"Filter\" node in the UI, they create the identical object. This dual interface philosophy means your work is never locked into one paradigm.</p> <ul> <li><code>FlowGraph</code>: The central orchestrator that holds the complete definition of your pipeline\u2014every node, setting, and connection.</li> <li><code>FlowNode</code>: An individual, executable step in your pipeline that wraps settings and logic.</li> <li><code>FlowDataEngine</code>: A smart wrapper around a Polars <code>LazyFrame</code> that carries the data and its schema between nodes.</li> </ul> <p>Learn more about this in our Dual Interface Philosophy guide.</p>"},{"location":"for-developers/index.html#getting-started-with-development","title":"Getting Started with Development","text":""},{"location":"for-developers/index.html#1-prerequisites","title":"1. Prerequisites","text":"<p>To contribute to Flowfile, you should be familiar with:</p> <ul> <li>Required Knowledge: Python 3.10+, and a basic familiarity with Polars or Pandas.</li> <li>Helpful Knowledge: Experience with Polars LazyFrames, Directed Acyclic Graphs (DAGs), and Pydantic.</li> </ul>"},{"location":"for-developers/index.html#2-set-up-your-environment","title":"2. Set Up Your Environment","text":"<p>Before diving in, clone the repository and install the dependencies using Poetry:</p> <pre><code># For development/contributing\ngit clone [https://github.com/edwardvaneechoud/Flowfile](https://github.com/edwardvaneechoud/Flowfile)\ncd Flowfile\npoetry install\n</code></pre>"},{"location":"for-developers/index.html#3-see-it-in-action-a-quick-example","title":"3. See It in Action: A Quick Example","text":"<p>The following code builds a data pipeline using the Python API. This same pipeline can be generated visually in the UI.</p> <pre><code>import flowfile as ff\nfrom flowfile import col\n\n# Create a FlowFrame from a local CSV\ndf = ff.read_csv(\"sales_data.csv\", description=\"Load raw sales data\")\n\n# Build a transformation pipeline with a familiar, chainable API\nprocessed_sales = (\n    df.filter(col(\"amount\") &gt; 100, description=\"Filter for significant sales\")\n    .with_columns(\n        (col(\"quantity\") * col(\"price\")).alias(\"total_revenue\")\n    )\n    .group_by(\"region\", description=\"Aggregate sales by region\")\n    .agg(\n        col(\"total_revenue\").sum()\n    )\n)\n\n# Get your results as a Polars DataFrame\nresults_df = processed_sales.collect()\nprint(results_df)\n</code></pre>"},{"location":"for-developers/index.html#documentation-guides","title":"Documentation Guides","text":"<ul> <li>Core Architecture: A deep dive into how <code>FlowGraph</code>, <code>FlowNode</code>, and <code>FlowDataEngine</code> work together.</li> <li>Technical Architecture: An overview of the system design, including the three-service architecture and performance optimizations.</li> <li>Python API Reference: The complete, auto-generated API reference for all core classes and methods.</li> <li>Visual UI Integration: Learn how to launch and control the visual editor from Python.</li> </ul>"},{"location":"for-developers/index.html#contributing-to-flowfile","title":"Contributing to Flowfile","text":"<p>We welcome contributions! Adding a new node requires changes across the stack:</p> <ul> <li>Backend: You'll need to define Pydantic setting models, implement the transformation logic in the <code>FlowDataEngine</code>, and register the new node in the <code>FlowGraph</code>.</li> <li>Frontend: Currently, you must also manually create a Vue component for the node's configuration form in the visual editor.</li> </ul> <p>For a more detailed breakdown, please read the Contributing section in our Design Philosophy guide.</p>"},{"location":"for-developers/architecture.html","title":"Technical Architecture","text":"<p>Flowfile's architecture integrates visual design with high-performance data processing through three interconnected services and utilizes Polars' lazy evaluation. This design provides real-time feedback, processes large datasets efficiently, and maintains UI responsiveness during intensive computations. On this page, the three-service architecture, key technical features such as real-time schema prediction and efficient data exchange, and the role of Polars' lazy evaluation are explained.</p> <p></p>"},{"location":"for-developers/architecture.html#core-components","title":"Core Components","text":""},{"location":"for-developers/architecture.html#three-service-architecture","title":"Three-Service Architecture","text":"<p>Designer (Electron + Vue) The visual interface where data pipelines are build through drag-and-drop operations. It communicates with the Core service to provide real-time feedback and displays data previews.</p> <p>Core Service (FastAPI) The orchestration engine that manages workflows, predicts schemas, and coordinates execution. It maintains the Directed Acyclic Graph (DAG) structure and handles all UI interactions and overall flow logic.</p> <p>Worker Service (FastAPI) Handles heavy data computations in isolated processes. It executes Polars transformations, materializes data, and manages data caching using Apache Arrow IPC format, preventing large datasets from overwhelming the Core service.</p>"},{"location":"for-developers/architecture.html#key-technical-features","title":"Key Technical Features","text":""},{"location":"for-developers/architecture.html#real-time-schema-prediction","title":"Real-time Schema Prediction","text":"<p>When you add or configure a node, Flowfile immediately shows how your data structure will change \u2014 without executing any transformations. This continuous feedback significantly reduces user errors and increases predictability. This happens through:</p> <ul> <li>Schema Callbacks: Custom functions, defined per node type, that calculate output schemas based on node settings and input schemas.</li> <li>Lazy Evaluation: Leveraging Polars' ability to determine the schema of a planned transformation (<code>LazyFrame</code>) without processing the full dataset.</li> </ul> View Schema Prediction Python Example <pre><code># Example: Schema prediction for a Group By operation\ndef schema_callback():\n    output_columns = [(c.old_name, c.new_name, c.output_type) for c in group_by_settings.groupby_input.agg_cols]\n    depends_on = node.node_inputs.main_inputs[0]\n    input_schema_dict: Dict[str, str] = {s.name: s.data_type for s in depends_on.schema}\n    output_schema = []\n    for old_name, new_name, data_type in output_columns:\n        data_type = input_schema_dict[old_name] if data_type is None else data_type\n        output_schema.append(FlowfileColumn.from_input(data_type=data_type, column_name=new_name))\n    return output_schema\n</code></pre>"},{"location":"for-developers/architecture.html#the-directed-acyclic-graph-dag-the-foundation-of-workflows","title":"The Directed Acyclic Graph (DAG): The Foundation of Workflows","text":"<p>As you add and connect nodes, Flowfile builds a Directed Acyclic Graph (DAG) where:</p> <ul> <li>Nodes represent data operations (read file, filter, join, write to database, etc.).</li> <li>Edges represent the flow of data between operations.</li> </ul> <p>The DAG is managed by the <code>EtlGraph</code> class in the Core service, which orchestrates the entire workflow:</p> View EtlGraph Python Implementation <pre><code>class EtlGraph:\n    \"\"\"\n    Manages the ETL workflow as a DAG. Stores nodes, dependencies,\n    and settings, and handles the execution order.\n    \"\"\"\n    uuid: str\n    _node_db: Dict[Union[str, int], NodeStep]  # Internal storage for all node steps\n    _flow_starts: List[NodeStep]               # Nodes that initiate data flow (e.g., readers)\n    _node_ids: List[Union[str, int]]           # Tracking node identifiers\n    flow_settings: schemas.FlowSettings        # Global configuration for the flow\n\n    def add_node_step(self, node_id: Union[int, str], function: Callable,\n                      node_type: str, **kwargs) -&gt; None:\n        \"\"\"Adds a new processing node (NodeStep) to the graph.\"\"\"\n        node_step = NodeStep(node_id=node_id, function=function, node_type=node_type, **kwargs)\n        self._node_db[node_id] = node_step\n        # Additional logic to manage dependencies and flow starts...\n\n    def run_graph(self) -&gt; RunInformation:\n        \"\"\"Executes the entire flow in the correct topological order.\"\"\"\n        execution_order = self.topological_sort() # Determine correct sequence\n        run_info = RunInformation()\n        for node in execution_order:\n            # Execute node based on mode (Development/Performance)\n            node_results = node.execute_node() # Simplified representation\n            run_info.add_result(node.node_id, node_results)\n        return run_info\n\n    def topological_sort(self) -&gt; List[NodeStep]:\n        \"\"\"Determines the correct order to execute nodes based on dependencies.\"\"\"\n        # Standard DAG topological sort algorithm...\n        pass\n</code></pre> <p>Each <code>NodeStep</code> in the graph encapsulates information about its dependencies, transformation logic, and output schema. This structure allows Flowfile to determine execution order, track data lineage, optimize performance, and provide schema predictions throughout the pipeline.</p>"},{"location":"for-developers/architecture.html#execution-modes","title":"Execution Modes","text":"<p>By clicking on settings \u2192 execution modes you can set how the flow will be executed the next time you run the flow.</p> <p></p> <p>Flowfile offers two execution modes tailored for different needs:</p> Feature Development Mode Performance Mode Purpose Interactive debugging, step inspection Optimized execution for production/speed Execution Executes node-by-node Builds full plan, executes minimally Data Caching Caches intermediate results per step Minimal caching (only if specified/needed) Preview Data Available for all nodes Only for final/cached nodes Memory Usage Potentially higher Generally lower Speed Moderate Faster for complex flows <p>Development Mode In Development mode, each node's transformation is triggered sequentially within the Worker service. Its intermediate result is typically serialized using Apache Arrow IPC format and cached to disk. This allows you to inspect the data at each step in the Designer via small samples fetched from the cache.</p> <p>Performance Mode In Performance mode, Flowfile fully embraces Polars' lazy evaluation. The Core service constructs the entire Polars execution plan based on the DAG. This plan (<code>LazyFrame</code>) is passed to the Worker service. The Worker only materializes (executes <code>.collect()</code> or <code>.sink_*()</code>) the plan when an output node (like writing to a file) requires the final result, or if a node is explicitly configured to cache its results (<code>node.cache_results</code>). This minimizes computation and memory usage by avoiding unnecessary intermediate materializations.</p> View Performance Mode Python Example <pre><code># Execution logic in Performance Mode (simplified)\ndef execute_performance_mode(self, node: NodeStep, is_output_node: bool):\n    \"\"\"Handles execution in performance mode, leveraging lazy evaluation.\"\"\"\n    if is_output_node or node.cache_results:\n        # If result is needed (output or caching), trigger execution in Worker\n        external_df_fetcher = ExternalDfFetcher(\n            lf=node.get_resulting_data().data_frame, # Pass the LazyFrame plan\n            file_ref=node.hash, # Unique reference for caching\n            wait_on_completion=False # Usually async\n        )\n        # Worker executes .collect() or .sink_*() and caches if needed\n        result = external_df_fetcher.get_result() # May return LazyFrame or trigger compute\n        return result # Or potentially just confirmation if sinking\n    else:\n        # If not output/cached, just pass the LazyFrame plan along\n        # No computation happens here for intermediate nodes\n        return node.get_resulting_data().data_frame\n</code></pre> <p>Crucially, all actual data processing and materialization of Polars DataFrames/LazyFrames happens in the Worker service. This separation prevents large datasets from overwhelming the Core service, ensuring the UI remains responsive.</p>"},{"location":"for-developers/architecture.html#efficient-data-exchange","title":"Efficient Data Exchange","text":"<p>Flowfile uses Apache Arrow IPC format for efficient inter-process communication between the Core and Worker services:</p> <ol> <li>Worker Processing &amp; Serialization: When the Worker needs to materialize data (either for intermediate caching in Development mode or final results), it computes the Polars DataFrame. The resulting DataFrame is serialized into the efficient Arrow IPC binary format.</li> <li>Disk Caching: This serialized data is saved to a temporary file on disk. This file acts as a cache, identified by a unique hash (<code>file_ref</code>). The Worker informs the Core that the result is ready at this <code>file_ref</code>.</li> <li>Core Fetching: If the Core (or subsequently, another Worker task) needs this data, it uses the <code>file_ref</code> to access the cached Arrow file directly. This avoids sending large datasets over network sockets between processes.</li> <li>UI Sampling: For UI previews, the Core requests a small sample (e.g., the first 100 rows) from the Worker. The Worker reads just the sample from the Arrow IPC file and sends only that lightweight data back to the Core, which forwards it to the Designer.</li> </ol> <p>This ensures responsiveness, memory isolation, and efficiency.</p> <p>Here\u2019s how the Core might offload computation to the Worker, and how the Worker manages the separate process execution:</p> View Core-Side Python Example <pre><code># Core side - Initiating remote execution in the Worker (simplified)\ndef execute_remote(self, performance_mode: bool = False) -&gt; None:\n    \"\"\"Offloads the execution of a node's LazyFrame to the Worker service.\"\"\"\n    # Create a fetcher instance to manage communication with the Worker\n    external_df_fetcher = ExternalDfFetcher(\n        lf=self.get_resulting_data().data_frame, # The Polars LazyFrame plan\n        file_ref=self.hash,                      # Unique identifier for the result/cache\n        wait_on_completion=False,                # Operate asynchronously\n    )\n\n    # Store the fetcher to potentially retrieve results later\n    self._fetch_cached_df = external_df_fetcher\n\n    # Request the Worker to start processing (this returns quickly)\n    # The actual computation happens asynchronously in the Worker\n    external_df_fetcher.start_processing_in_worker() # Hypothetical method name\n\n    # For UI updates, request a sample separately\n    self.store_example_data_generator(external_df_fetcher) # Fetches sample async\n</code></pre> View Worker-Side Python Example <pre><code># Worker side - Managing computation in a separate process (simplified)\ndef start_process(\n    polars_serializable_object: bytes, # Serialized LazyFrame plan\n    task_id: str,\n    file_ref: str, # Path for cached output (Arrow IPC file)\n    # ... other args like operation type\n) -&gt; None:\n    \"\"\"Launches a separate OS process to handle the heavy computation.\"\"\"\n    # Use multiprocessing context for safety\n    mp_context = multiprocessing.get_context('spawn') # or 'fork' depending on OS/needs\n\n    # Shared memory/queue for progress tracking and results/errors\n    progress = mp_context.Value('i', 0) # Shared integer for progress %\n    error_message = mp_context.Array('c', 1024) # Shared buffer for error messages\n    queue = mp_context.Queue(maxsize=1) # For potentially passing back results (or file ref)\n\n    # Define the target function and arguments for the new process\n    process = mp_context.Process(\n        target=process_task, # The function that runs Polars .collect()/.sink()\n        kwargs={\n            'polars_serializable_object': polars_serializable_object,\n            'progress': progress,\n            'error_message': error_message,\n            'queue': queue,\n            'file_path': file_ref, # Where to save the Arrow IPC output\n            # ... other necessary kwargs\n        }\n    )\n    process.start() # Launch the independent process\n\n    # Monitor the task (e.g., update status in a database, check progress)\n    handle_task(task_id, process, progress, error_message, queue)\n</code></pre>"},{"location":"for-developers/architecture.html#the-power-of-lazy-evaluation","title":"The Power of Lazy Evaluation","text":"<p>By building on Polars' lazy evaluation, Flowfile achieves:</p> <ul> <li>Memory Efficiency: Data is loaded and processed only when necessary, often streaming through operations without loading entire datasets into memory at once. This allows processing datasets larger than RAM.</li> <li>Query Optimization: Polars analyzes the entire execution plan and can reorder, combine, or eliminate operations for maximum efficiency.</li> <li>Parallel Execution: Polars automatically parallelizes operations across all available CPU cores during execution.</li> <li>Predicate Pushdown: Filters and selections are applied as early as possible in the plan, often directly at the data source level (like during file reading), minimizing the amount of data that needs to be processed downstream.</li> </ul>"},{"location":"for-developers/architecture.html#summary","title":"Summary","text":"<p>This design enables Flowfile to:</p> <ul> <li>Provide instant feedback without processing data.</li> <li>Handle datasets of any size efficiently.</li> <li>Keep the UI responsive during heavy computations.</li> <li>Scale from simple transformations to complex ETL pipelines.</li> </ul> <p>For a deep dive into the implementation details, see the full technical article.</p>"},{"location":"for-developers/creating-custom-nodes.html","title":"Creating Custom Nodes","text":"<p>Build your own data transformation nodes with custom UI components and processing logic.</p> <p>Visual Alternative</p> <p>You can also create custom nodes visually using the Node Designer without writing Python files directly.</p> <p>Beta Feature</p> <p>Custom nodes are currently in beta. Some features like changing the icon are still in development.</p>"},{"location":"for-developers/creating-custom-nodes.html#what-are-custom-nodes","title":"What Are Custom Nodes?","text":"<p>Custom nodes let you extend Flowfile with your own data transformations that appear as native nodes in the visual editor. Each custom node includes:</p> <ul> <li>Custom UI - Automatically generated settings panels with dropdowns, inputs, and toggles</li> <li>Data Processing - Polars-based transformation logic</li> <li>Visual Integration - Appears seamlessly in the node palette</li> </ul>"},{"location":"for-developers/creating-custom-nodes.html#quick-start","title":"Quick Start","text":""},{"location":"for-developers/creating-custom-nodes.html#1-create-your-first-node","title":"1. Create Your First Node","text":"<p>Create a new Python file in your custom nodes directory:</p> <pre><code>~/.flowfile/user_defined_nodes/my_first_node.py\n</code></pre> <p>Custom Node Location</p> <p>The <code>~/.flowfile/user_defined_nodes/</code> directory is automatically created when you first run Flowfile. Place all your custom nodes here.</p> <p>Here's a simple example that adds a greeting column:</p> <pre><code>import polars as pl\nfrom flowfile_core.flowfile.node_designer import (\n    CustomNodeBase,\n    Section,\n    NodeSettings,\n    TextInput,\n    SingleSelect,\n    ColumnSelector,\n    Types\n)\n\nclass GreetingSettings(NodeSettings):\n    main_config: Section = Section(\n        title=\"Greeting Configuration\",\n        description=\"Configure how to greet your data\",\n        name_column=ColumnSelector(\n            label=\"Name Column\",\n            data_types=Types.String,\n            required=True\n        ),\n        greeting_style=SingleSelect(\n            label=\"Greeting Style\",\n            options=[\n                (\"formal\", \"Formal (Hello, Mr/Ms)\"),\n                (\"casual\", \"Casual (Hey there!)\"),\n                (\"enthusiastic\", \"Enthusiastic (OMG HI!!!)\"),\n            ],\n            default=\"casual\"\n        ),\n        custom_message=TextInput(\n            label=\"Custom Message\",\n            default=\"Nice to meet you!\",\n            placeholder=\"Enter your custom greeting...\"\n        )\n    )\n\nclass GreetingNode(CustomNodeBase):\n    node_name: str = \"Greeting Generator\"\n    node_category: str = \"Text Processing\"\n    title: str = \"Add Personal Greetings\"\n    intro: str = \"Transform names into personalized greetings\"\n\n    settings_schema: GreetingSettings = GreetingSettings()\n\n    def process(self, input_df: pl.LazyFrame) -&gt; pl.LazyFrame:\n        # Get values from the UI\n        name_col = self.settings_schema.main_config.name_column.value\n        style = self.settings_schema.main_config.greeting_style.value\n        custom = self.settings_schema.main_config.custom_message.value\n\n        # Define greeting logic\n        if style == \"formal\":\n            greeting_expr = pl.concat_str([\n                pl.lit(\"Hello, \"), \n                pl.col(name_col), \n                pl.lit(f\". {custom}\")\n            ])\n        elif style == \"casual\":\n            greeting_expr = pl.concat_str([\n                pl.lit(\"Hey \"), \n                pl.col(name_col), \n                pl.lit(f\"! {custom}\")\n            ])\n        else:  # enthusiastic\n            greeting_expr = pl.concat_str([\n                pl.lit(\"OMG HI \"), \n                pl.col(name_col).str.to_uppercase(), \n                pl.lit(f\"!!! {custom} \ud83c\udf89\")\n            ])\n\n        return input_df.with_columns([\n            greeting_expr.alias(\"greeting\")\n        ])\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#2-use-your-node","title":"2. Use Your Node","text":"<ol> <li>Restart Flowfile to load your new node</li> <li>Open the visual editor</li> <li>Find your node in the \"Text Processing\" category</li> <li>Drag it onto the canvas</li> <li>Configure the settings in the right panel</li> <li>Run your flow and see the results!</li> </ol>  Visual overview of the result!"},{"location":"for-developers/creating-custom-nodes.html#understanding-the-architecture","title":"Understanding the Architecture","text":""},{"location":"for-developers/creating-custom-nodes.html#node-structure","title":"Node Structure","text":"<p>Every custom node has three main parts:</p> <pre><code>class MyCustomNode(CustomNodeBase):\n    # 1. Metadata - How the node appears in Flowfile\n    node_name: str = \"My Amazing Node\"\n    node_category: str = \"Data Enhancement\"\n    title: str = \"Add Personal Greetings\"\n    intro: str = \"Transform names into personalized greetings\"\n\n    # 2. Settings Schema - The UI configuration\n    settings_schema: MySettings = MySettings()\n\n    # 3. Processing Logic - What the node actually does\n    def process(self, input_df: pl.LazyFrame) -&gt; pl.LazyFrame:\n        # Your transformation logic here\n        return modified_df\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#3-process","title":"3. Process","text":"<p>Only one input supported for now</p> <p>Currently, custom nodes only support a single input DataFrame. Support for multiple inputs is planned for future releases.</p> <p>The process method is the engine of your node. This is where you write your Polars code to transform the data.</p> <ul> <li> <p>Input: The method receives the incoming data as a Polars LazyFrame. If your node has multiple inputs, they will be passed as separate arguments (e.g., process(self, df1)).</p> </li> <li> <p>Accessing Settings: Inside this method, you can get the current values from your UI components using self.settings_schema...value. <li> <p>Output: The method must return a single Polars LazyFrame, which becomes the output of your node.</p> </li>"},{"location":"for-developers/creating-custom-nodes.html#settings-architecture","title":"Settings Architecture","text":"<p>Settings are organized in sections for clean UI organization:</p> <pre><code>class MyNodeSettings(NodeSettings):\n    # Each section becomes a collapsible panel in the UI\n    basic_config: Section = Section(\n        title=\"Basic Settings\",\n        description=\"Core functionality options\",\n        # Components go here as keyword arguments\n        input_column=ColumnSelector(...),\n        operation_type=SingleSelect(...)\n    )\n\n    advanced_options: Section = Section(\n        title=\"Advanced Options\",\n        description=\"Fine-tune behavior\",\n        enable_caching=ToggleSwitch(...),\n        max_iterations=NumericInput(...)\n    )\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#available-ui-components","title":"Available UI Components","text":""},{"location":"for-developers/creating-custom-nodes.html#text-input","title":"Text Input","text":"<p>For capturing string values:</p> <pre><code>text_field = TextInput(\n    label=\"Enter a value\",\n    default=\"Default text\",\n    placeholder=\"Hint text here...\"\n)\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#numeric-input","title":"Numeric Input","text":"<p>For numbers with optional validation:</p> <pre><code>number_field = NumericInput(\n    label=\"Count\",\n    default=10,\n    min_value=1,\n    max_value=100\n)\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#single-select","title":"Single Select","text":"<p>Dropdown for one choice:</p> <pre><code>choice_field = SingleSelect(\n    label=\"Choose option\",\n    options=[\n        (\"value1\", \"Display Name 1\"),\n        (\"value2\", \"Display Name 2\"),\n        (\"simple\", \"Simple String Option\")\n    ],\n    default=\"value1\"\n)\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#multi-select","title":"Multi Select","text":"<p>For selecting multiple options:</p> <pre><code>multi_field = MultiSelect(\n    label=\"Select multiple\",\n    options=[\n        (\"opt1\", \"Option 1\"),\n        (\"opt2\", \"Option 2\"),\n        (\"opt3\", \"Option 3\")\n    ],\n    default=[\"opt1\", \"opt2\"]\n)\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#toggle-switch","title":"Toggle Switch","text":"<p>Boolean on/off control:</p> <pre><code>toggle_field = ToggleSwitch(\n    label=\"Enable feature\",\n    default=True,\n    description=\"Turn this on to enable the feature\"\n)\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#column-selector","title":"Column Selector","text":"<p>Smart column picker with type filtering:</p> <pre><code># Select any column\nany_column = ColumnSelector(\n    label=\"Pick a column\",\n    data_types=Types.All\n)\n\n# Select only numeric columns\nnumeric_column = ColumnSelector(\n    label=\"Numeric column only\",\n    data_types=Types.Numeric,\n    required=True\n)\n\n# Select multiple string columns\ntext_columns = ColumnSelector(\n    label=\"Text columns\",\n    data_types=[Types.String, Types.Categorical],\n    multiple=True\n)\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#secret-selector","title":"Secret Selector","text":"<p>Access stored secrets (API keys, credentials, tokens) securely:</p> <pre><code>from flowfile_core.flowfile.node_designer import SecretSelector\n\nclass MyNode(CustomNodeBase):\n    settings_schema = NodeSettings(\n        components=[\n            SecretSelector(name=\"api_key\", label=\"API Key\"),\n        ]\n    )\n</code></pre> <p>The SecretSelector displays a dropdown of available secrets configured by the user. Secrets are stored securely and retrieved at runtime. This is useful for nodes that connect to external APIs or services.</p>"},{"location":"for-developers/creating-custom-nodes.html#dynamic-column-options","title":"Dynamic Column Options","text":"<p>Use <code>IncomingColumns</code> for dropdowns that populate with input columns:</p> <pre><code>column_dropdown = SingleSelect(\n    label=\"Choose input column\",\n    options=IncomingColumns  # Automatically filled with column names\n)\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#type-filtering-in-column-selector","title":"Type Filtering in Column Selector","text":"<p>The <code>Types</code> object provides convenient type filtering:</p> <pre><code>from flowfile_core.flowfile.node_designer import Types\n\n# Type groups\nTypes.Numeric    # All numeric types\nTypes.String     # String and categorical\nTypes.Date       # Date, datetime, time\nTypes.Boolean    # Boolean columns\nTypes.All        # All column types\n\n# Specific types\nTypes.Int64      # 64-bit integers\nTypes.Float      # Float64\nTypes.Decimal    # Decimal types\n\n# Mix and match\ndata_types=[Types.Numeric, Types.Date]  # Numbers and dates only\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#real-world-examples","title":"Real-World Examples","text":""},{"location":"for-developers/creating-custom-nodes.html#data-quality-node","title":"Data Quality Node","text":"<pre><code>class DataQualityNode(CustomNodeBase):\n    node_name: str = \"Data Quality Checker\"\n    node_category: str = \"Data Validation\"\n\n    settings_schema: DataQualitySettings = DataQualitySettings(\n        validation_rules=Section(\n            title=\"Validation Rules\",\n            columns_to_check=ColumnSelector(\n                label=\"Columns to Validate\",\n                data_types=Types.All,\n                multiple=True\n            ),\n            null_threshold=NumericInput(\n                label=\"Max Null Percentage\",\n                default=5.0,\n                min_value=0,\n                max_value=100\n            ),\n            add_summary=ToggleSwitch(\n                label=\"Add Quality Summary\",\n                default=True\n            )\n        )\n    )\n\n    def process(self, input_df: pl.LazyFrame) -&gt; pl.LazyFrame:\n        columns = self.settings_schema.validation_rules.columns_to_check.value\n        threshold = self.settings_schema.validation_rules.null_threshold.value\n\n        # Calculate quality metrics\n        quality_checks = []\n        for col in columns:\n            null_pct = (input_df[col].is_null().sum() / len(input_df)) * 100\n            quality_checks.append({\n                \"column\": col,\n                \"null_percentage\": null_pct,\n                \"quality_flag\": \"PASS\" if null_pct &lt;= threshold else \"FAIL\"\n            })\n\n        # Add quality flags to original data\n        result_df = input_df\n        for check in quality_checks:\n            if check[\"quality_flag\"] == \"FAIL\":\n                result_df = result_df.with_columns([\n                    pl.col(check[\"column\"]).is_null().alias(f\"{check['column']}_has_issues\")\n                ])\n\n        return result_df\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#text-processing-node","title":"Text Processing Node","text":"<pre><code>class TextCleanerNode(CustomNodeBase):\n    node_name: str = \"Text Cleaner\"\n    node_category: str = \"Text Processing\"\n\n    settings_schema: TextCleanerSettings = TextCleanerSettings(\n        cleaning_options=Section(\n            title=\"Cleaning Options\",\n            text_column=ColumnSelector(\n                label=\"Text Column\",\n                data_types=Types.String,\n                required=True\n            ),\n            operations=MultiSelect(\n                label=\"Cleaning Operations\",\n                options=[\n                    (\"lowercase\", \"Convert to lowercase\"),\n                    (\"remove_punctuation\", \"Remove punctuation\"),\n                    (\"remove_extra_spaces\", \"Remove extra spaces\"),\n                    (\"remove_numbers\", \"Remove numbers\"),\n                    (\"trim\", \"Trim whitespace\")\n                ],\n                default=[\"lowercase\", \"trim\"]\n            ),\n            output_column=TextInput(\n                label=\"Output Column Name\",\n                default=\"cleaned_text\"\n            )\n        )\n    )\n\n    def process(self, input_df: pl.LazyFrame) -&gt; pl.LazyFrame:\n        text_col = self.settings_schema.cleaning_options.text_column.value\n        operations = self.settings_schema.cleaning_options.operations.value\n        output_col = self.settings_schema.cleaning_options.output_column.value\n\n        # Start with original text\n        expr = pl.col(text_col)\n\n        # Apply selected operations\n        if \"lowercase\" in operations:\n            expr = expr.str.to_lowercase()\n        if \"remove_punctuation\" in operations:\n            expr = expr.str.replace_all(r\"[^\\w\\s]\", \"\")\n        if \"remove_extra_spaces\" in operations:\n            expr = expr.str.replace_all(r\"\\s+\", \" \")\n        if \"remove_numbers\" in operations:\n            expr = expr.str.replace_all(r\"\\d+\", \"\")\n        if \"trim\" in operations:\n            expr = expr.str.strip_chars()\n\n        return input_df.with_columns([expr.alias(output_col)])\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#best-practices","title":"Best Practices","text":""},{"location":"for-developers/creating-custom-nodes.html#1-performance","title":"1. Performance","text":"<p>Try to use Polars expressions and lazy evaluation to keep your nodes efficient.  A collect will be executed in the core process and can cause issues when using remote compute.</p>"},{"location":"for-developers/creating-custom-nodes.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"for-developers/creating-custom-nodes.html#node-doesnt-appear","title":"Node Doesn't Appear","text":"<ol> <li>Check the file is in <code>~/.flowfile/user_defined_nodes/</code></li> <li>Restart Flowfile completely</li> <li>Check for Python syntax errors in terminal</li> <li>Ensure your class inherits from <code>CustomNodeBase</code></li> </ol>"},{"location":"for-developers/creating-custom-nodes.html#settings-dont-work","title":"Settings Don't Work","text":"<ol> <li>Verify <code>settings_schema</code> is properly assigned</li> <li>Check component imports</li> <li>Ensure section structure is correct</li> <li>Use <code>.value</code> to access component values in <code>process()</code></li> </ol>"},{"location":"for-developers/creating-custom-nodes.html#processing-errors","title":"Processing Errors","text":"<ol> <li>Check input DataFrame exists and has expected columns</li> <li>Use <code>print()</code> or logging for debugging</li> <li>Handle null values and edge cases</li> <li>Ensure return type is <code>pl.LazyFrame</code></li> </ol>"},{"location":"for-developers/creating-custom-nodes.html#coming-soon","title":"Coming Soon","text":"<p>The following features are planned for future releases:</p> <ul> <li>Node Templates - Quick-start templates for common patterns</li> <li>Custom Icons - Upload custom icons for your nodes</li> <li>Node Categories - Create your own node categories</li> <li>Testing Framework - Built-in testing for custom nodes</li> <li>Node Publishing - Share nodes with the community</li> </ul> <p>Ready to build? Start with the Custom Node Tutorial for a step-by-step walkthrough!</p>"},{"location":"for-developers/custom-node-tutorial.html","title":"Custom Node Tutorial: Build an Emoji Generator","text":"<p>Learn to create custom nodes by building a fun emoji generator that transforms boring data into expressive datasets with visual flair.</p> <p>What You'll Build</p> <p>By the end of this tutorial, you'll have created a fully functional \"Emoji Generator\" node that: - Takes numeric data and converts it into mood-based emojis - Provides 7 different emoji themes (performance, temperature, money, etc.) - Includes intensity controls and random sparkle effects - Features a sophisticated multi-section UI</p>"},{"location":"for-developers/custom-node-tutorial.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Flowfile installed and working (<code>pip install flowfile</code>)</li> <li>Basic understanding of Python</li> <li>Familiarity with Polars DataFrames (helpful but not required)</li> </ul>"},{"location":"for-developers/custom-node-tutorial.html#step-1-set-up-your-development-environment","title":"Step 1: Set Up Your Development Environment","text":"<p>First, locate your custom nodes directory:</p> <pre><code># Check if the directory exists\nls ~/.flowfile/user_defined_nodes/\n\n# If it doesn't exist, start Flowfile once to create it\nflowfile run ui\n</code></pre> <p>Create your node file:</p> <pre><code>touch ~/.flowfile/user_defined_nodes/emoji_generator.py\n</code></pre>"},{"location":"for-developers/custom-node-tutorial.html#step-2-import-required-components","title":"Step 2: Import Required Components","text":"<p>Start by importing all the components you'll need:</p> <pre><code>import polars as pl\nimport random\nfrom typing import List\n\nfrom flowfile_core.flowfile.node_designer import (\n    CustomNodeBase,\n    Section, \n    NodeSettings,\n    TextInput,\n    NumericInput,\n    SingleSelect,\n    ToggleSwitch,\n    ColumnSelector,\n    MultiSelect,\n    Types\n)\n</code></pre>"},{"location":"for-developers/custom-node-tutorial.html#step-3-design-your-settings-schema","title":"Step 3: Design Your Settings Schema","text":"<p>We'll create a two-section UI: one for mood detection and one for styling options.</p>"},{"location":"for-developers/custom-node-tutorial.html#create-the-first-section","title":"Create the First Section","text":"<pre><code>class EmojiMoodSection(Section):\n    source_column: ColumnSelector = ColumnSelector(\n        label=\"Analyze This Column\",\n        multiple=False,\n        required=True,\n        data_types=Types.Numeric  # Only show numeric columns\n    )\n\n    mood_type: SingleSelect = SingleSelect(\n        label=\"Emoji Mood Logic\",\n        options=[\n            (\"performance\", \"\ud83d\udcc8 Performance Based (High = \ud83d\ude0e, Low = \ud83d\ude30)\"),\n            (\"temperature\", \"\ud83c\udf21\ufe0f Temperature (Hot = \ud83d\udd25, Cold = \ud83e\uddca)\"),\n            (\"money\", \"\ud83d\udcb0 Money Mode (Rich = \ud83e\udd11, Poor = \ud83d\ude22)\"),\n            (\"energy\", \"\u26a1 Energy Level (High = \ud83d\ude80, Low = \ud83d\udd0b)\"),\n            (\"love\", \"\u2764\ufe0f Love Meter (High = \ud83d\ude0d, Low = \ud83d\udc94)\"),\n            (\"chaos\", \"\ud83c\udfb2 Pure Chaos (Random emojis!)\"),\n            (\"pizza\", \"\ud83c\udf55 Pizza Scale (Everything becomes pizza)\")\n        ],\n        default=\"performance\"\n    )\n\n    threshold_value: NumericInput = NumericInput(\n        label=\"Mood Threshold\",\n        default=50.0,\n        min_value=0,\n        max_value=100\n    )\n\n    emoji_column_name: TextInput = TextInput(\n        label=\"New Emoji Column Name\",\n        default=\"mood_emoji\",\n        placeholder=\"Name your emoji column...\"\n    )\n</code></pre>"},{"location":"for-developers/custom-node-tutorial.html#create-the-second-section","title":"Create the Second Section","text":"<pre><code>class EmojiStyleSection(Section):\n    emoji_intensity: SingleSelect = SingleSelect(\n        label=\"Emoji Intensity\",\n        options=[\n            (\"subtle\", \"\ud83d\ude10 Subtle (One emoji)\"),\n            (\"normal\", \"\ud83d\ude0a Normal (1-2 emojis)\"),\n            (\"extra\", \"\ud83e\udd29 Extra (2-3 emojis)\"),\n            (\"maximum\", \"\ud83e\udd2f\ud83c\udf89\ud83d\ude80 MAXIMUM OVERDRIVE\")\n        ],\n        default=\"normal\"\n    )\n\n    add_random_sparkle: ToggleSwitch = ToggleSwitch(\n        label=\"Add Random Sparkles \u2728\",\n        default=True,\n        description=\"Randomly sprinkle \u2728 for extra pizzazz\"\n    )\n\n    emoji_categories: MultiSelect = MultiSelect(\n        label=\"Allowed Emoji Categories\",\n        options=[\n            (\"faces\", \"\ud83d\ude00 Faces &amp; Emotions\"),\n            (\"animals\", \"\ud83e\udd84 Animals\"),\n            (\"food\", \"\ud83c\udf54 Food &amp; Drink\"),\n            (\"nature\", \"\ud83c\udf08 Nature\"),\n            (\"objects\", \"\ud83c\udfae Objects\"),\n            (\"symbols\", \"\ud83d\udcaf Symbols\"),\n            (\"flags\", \"\ud83c\udff4\u200d\u2620\ufe0f Flags\")\n        ],\n        default=[\"faces\", \"animals\", \"food\"]\n    )\n</code></pre>"},{"location":"for-developers/custom-node-tutorial.html#combine-sections-into-settings","title":"Combine Sections into Settings","text":"<pre><code>class EmojiSettings(NodeSettings):\n    mood_config: EmojiMoodSection = EmojiMoodSection(\n        title=\"Mood Detection \ud83d\ude0a\",\n        description=\"Configure how to detect the vibe of your data\"\n    )\n\n    style_options: EmojiStyleSection = EmojiStyleSection(\n        title=\"Emoji Style \ud83c\udfa8\",\n        description=\"Fine-tune your emoji experience\"\n    )\n</code></pre>"},{"location":"for-developers/custom-node-tutorial.html#step-4-create-the-main-node-class","title":"Step 4: Create the Main Node Class","text":"<pre><code>class EmojiGenerator(CustomNodeBase):\n    # Node metadata - how it appears in Flowfile\n    node_name: str = \"Emoji Generator \ud83c\udf89\"\n    node_category: str = \"Fun Stuff\"  # This creates a new category\n    node_group: str = \"custom\"\n    title: str = \"Emoji Generator\"\n    intro: str = \"Transform boring data into fun emoji-filled datasets! \ud83d\ude80\"\n\n    # I/O configuration\n    number_of_inputs: int = 1\n    number_of_outputs: int = 1\n\n    # Link to our settings schema\n    settings_schema: EmojiSettings = EmojiSettings()\n\n    def process(self, input_df: pl.DataFrame) -&gt; pl.DataFrame:\n        # We'll implement this in the next step\n        pass\n</code></pre>"},{"location":"for-developers/custom-node-tutorial.html#step-5-implement-the-processing-logic","title":"Step 5: Implement the Processing Logic","text":"<p>Now for the fun part - the actual emoji generation logic:</p> <pre><code>def process(self, input_df: pl.DataFrame) -&gt; pl.DataFrame:\n    # Get settings values from the UI\n    column_name = self.settings_schema.mood_config.source_column.value\n    mood_type = self.settings_schema.mood_config.mood_type.value\n    threshold = self.settings_schema.mood_config.threshold_value.value\n    emoji_col_name = self.settings_schema.mood_config.emoji_column_name.value\n    intensity = self.settings_schema.style_options.emoji_intensity.value\n    add_sparkle = self.settings_schema.style_options.add_random_sparkle.value\n\n    # Define emoji sets for different moods\n    emoji_sets = {\n        \"performance\": {\n            \"high\": [\"\ud83d\ude0e\", \"\ud83d\udcaa\", \"\ud83c\udfc6\", \"\ud83d\udc51\", \"\ud83c\udf1f\", \"\ud83d\udcaf\", \"\ud83d\udd25\"],\n            \"low\": [\"\ud83d\ude30\", \"\ud83d\ude13\", \"\ud83d\udcc9\", \"\ud83d\ude22\", \"\ud83d\udc94\", \"\ud83c\udd98\", \"\ud83d\ude35\"]\n        },\n        \"temperature\": {\n            \"high\": [\"\ud83d\udd25\", \"\ud83c\udf0b\", \"\u2600\ufe0f\", \"\ud83e\udd75\", \"\ud83c\udf21\ufe0f\", \"\u2668\ufe0f\", \"\ud83c\udfd6\ufe0f\"],\n            \"low\": [\"\ud83e\uddca\", \"\u2744\ufe0f\", \"\u26c4\", \"\ud83e\udd76\", \"\ud83c\udf28\ufe0f\", \"\ud83c\udfd4\ufe0f\", \"\ud83d\udc27\"]\n        },\n        \"money\": {\n            \"high\": [\"\ud83e\udd11\", \"\ud83d\udcb0\", \"\ud83d\udc8e\", \"\ud83c\udfe6\", \"\ud83d\udcb3\", \"\ud83e\ude99\", \"\ud83d\udcc8\"],\n            \"low\": [\"\ud83d\ude22\", \"\ud83d\udcb8\", \"\ud83d\udcc9\", \"\ud83c\udfda\ufe0f\", \"\ud83d\ude2d\", \"\ud83e\udd7a\", \"\ud83d\udcca\"]\n        },\n        \"energy\": {\n            \"high\": [\"\ud83d\ude80\", \"\u26a1\", \"\ud83d\udca5\", \"\ud83c\udfaf\", \"\ud83c\udfc3\", \"\ud83e\udd38\", \"\ud83c\udfaa\"],\n            \"low\": [\"\ud83d\udd0b\", \"\ud83d\ude34\", \"\ud83d\udecc\", \"\ud83d\udc0c\", \"\ud83e\udd71\", \"\ud83d\ude2a\", \"\ud83d\udca4\"]\n        },\n        \"love\": {\n            \"high\": [\"\ud83d\ude0d\", \"\u2764\ufe0f\", \"\ud83d\udc95\", \"\ud83e\udd70\", \"\ud83d\udc98\", \"\ud83d\udc9d\", \"\ud83d\udc68\u200d\u2764\ufe0f\u200d\ud83d\udc68\"],\n            \"low\": [\"\ud83d\udc94\", \"\ud83d\ude22\", \"\ud83d\ude2d\", \"\ud83e\udd40\", \"\ud83d\ude14\", \"\ud83d\udc80\", \"\ud83d\udda4\"]\n        },\n        \"chaos\": {\n            \"high\": [\"\ud83e\udd96\", \"\ud83c\udfb8\", \"\ud83d\ude81\", \"\ud83c\udfaa\", \"\ud83e\udd9c\", \"\ud83c\udfad\", \"\ud83c\udff4\u200d\u2620\ufe0f\"],\n            \"low\": [\"\ud83e\udd54\", \"\ud83e\udde6\", \"\ud83d\udcce\", \"\ud83e\uddb7\", \"\ud83e\uddf2\", \"\ud83d\udd0c\", \"\ud83e\udea3\"]\n        },\n        \"pizza\": {\n            \"high\": [\"\ud83c\udf55\", \"\ud83c\udf55\ud83c\udf55\", \"\ud83c\udf55\ud83d\udd25\", \"\ud83c\udf55\ud83d\ude0d\", \"\ud83c\udf55\ud83c\udf89\", \"\ud83c\udf55\ud83d\udcaf\", \"\ud83c\udf55\ud83d\udc51\"],\n            \"low\": [\"\ud83c\udf55\", \"\ud83c\udf55\ud83d\ude22\", \"\ud83c\udf55\ud83d\udc94\", \"\ud83c\udf55\ud83d\ude2d\", \"\ud83c\udf55\ud83e\udd7a\", \"\ud83c\udf55\ud83d\ude14\", \"\ud83c\udf55\"]\n        }\n    }\n\n    # Helper function to get emoji based on value\n    def get_emoji(value, mood_type, threshold, intensity):\n        if value is None:\n            return \"\u2753\"\n\n        emoji_list = emoji_sets.get(mood_type, emoji_sets[\"performance\"])\n\n        if mood_type == \"chaos\":\n            # Random emoji from both lists\n            all_emojis = emoji_list[\"high\"] + emoji_list[\"low\"]\n            base_emoji = random.choice(all_emojis)\n        elif mood_type == \"pizza\":\n            # Everything is pizza\n            base_emoji = \"\ud83c\udf55\"\n        else:\n            # Use threshold to determine high/low\n            if value &gt;= threshold:\n                base_emoji = random.choice(emoji_list[\"high\"])\n            else:\n                base_emoji = random.choice(emoji_list[\"low\"])\n\n        # Add intensity\n        if intensity == \"subtle\":\n            result = base_emoji\n        elif intensity == \"normal\":\n            result = base_emoji\n            if random.random() &gt; 0.5:\n                result += random.choice([\"\", \"\u2728\", \"\"])\n        elif intensity == \"extra\":\n            extras = [\"\u2728\", \"\ud83d\udcab\", \"\u2b50\", \"\"]\n            result = base_emoji + random.choice(extras) + random.choice(extras)\n        else:  # maximum\n            chaos_emojis = [\"\ud83c\udf89\", \"\ud83d\ude80\", \"\ud83d\udca5\", \"\ud83c\udf08\", \"\u2728\", \"\ud83d\udd25\", \"\ud83d\udcaf\", \"\u26a1\"]\n            result = base_emoji + \"\".join(random.choices(chaos_emojis, k=3))\n\n        # Add random sparkle\n        if add_sparkle and random.random() &gt; 0.7:\n            result += \"\u2728\"\n\n        return result\n\n    # Create emoji column using map_elements\n    emoji_expr = (\n        pl.col(column_name)\n        .map_elements(\n            lambda x: get_emoji(x, mood_type, threshold, intensity),\n            return_dtype=pl.String\n        )\n        .alias(emoji_col_name)\n    )\n\n    # Add bonus columns based on intensity\n    if intensity == \"maximum\":\n        # Add extra fun columns in maximum mode\n        return input_df.with_columns([\n            emoji_expr,\n            pl.lit(\"\ud83c\udf89 PARTY MODE \ud83c\udf89\").alias(\"vibe_check\"),\n            pl.col(column_name).map_elements(\n                lambda x: \"\ud83d\udd25\" * min(int((x or 0) / 20), 5) if x else \"\ud83d\udca4\",\n                return_dtype=pl.String\n            ).alias(\"fire_meter\")\n        ])\n    else:\n        return input_df.with_columns([emoji_expr])\n</code></pre>"},{"location":"for-developers/custom-node-tutorial.html#step-6-test-your-node","title":"Step 6: Test Your Node","text":"<p>Save your file and test it:</p> <ol> <li> <p>Restart Flowfile completely:    <pre><code># Stop any running Flowfile processes\n# Then start again\nflowfile run ui\n</code></pre></p> </li> <li> <p>Create a test flow:</p> </li> <li>Create a new flow</li> <li> <p>Add a \"Manual Input\" node with some test data:     <pre><code>[\n  {\n    \"name\": \"bob\",\n    \"value\": \"21\"\n  },\n  {\n    \"name\": \"magret\",\n    \"value\": \"62.1\"\n  },\n  {\n    \"name\": \"fish\",\n    \"value\": \"1.2\"\n  },\n  {\n    \"name\": \"dog\",\n    \"value\": \"20\"\n  }\n]\n</code></pre></p> </li> <li> <p>Look for your \"Emoji Generator \ud83c\udf89\" in the \"User defined operations\" category</p> </li> <li>Connect it to your manual input</li> <li>Configure the settings and run!</li> </ol>  Visual overview of the result!  <p></p>"},{"location":"for-developers/custom-node-tutorial.html#performance-tips","title":"Performance Tips","text":"<ol> <li>Use Polars expressions instead of Python loops when possible</li> <li>Avoid collecting DataFrames in the middle of processing</li> <li>Handle large datasets by checking input size and warning users</li> <li>Cache expensive operations like random number generation</li> <li>Use lazy evaluation - let Polars optimize the query plan</li> </ol>"},{"location":"for-developers/custom-node-tutorial.html#complete-working-example","title":"Complete Working Example","text":"<p>Here's the complete, working emoji generator node:</p> <pre><code>import polars as pl\nimport random\nfrom typing import List\n\nfrom flowfile_core.flowfile.node_designer import (\n    CustomNodeBase,\n    Section,\n    NodeSettings,\n    TextInput,\n    NumericInput,\n    SingleSelect,\n    ToggleSwitch,\n    ColumnSelector,\n    MultiSelect,\n    Types\n)\n\n\nclass EmojiMoodSection(Section):\n    source_column: ColumnSelector = ColumnSelector(\n        label=\"Analyze This Column\",\n        multiple=False,\n        required=True,\n        data_types=Types.Numeric  # Only show numeric columns\n    )\n\n    mood_type: SingleSelect = SingleSelect(\n        label=\"Emoji Mood Logic\",\n        options=[\n            (\"performance\", \"\ud83d\udcc8 Performance Based (High = \ud83d\ude0e, Low = \ud83d\ude30)\"),\n            (\"temperature\", \"\ud83c\udf21\ufe0f Temperature (Hot = \ud83d\udd25, Cold = \ud83e\uddca)\"),\n            (\"money\", \"\ud83d\udcb0 Money Mode (Rich = \ud83e\udd11, Poor = \ud83d\ude22)\"),\n            (\"energy\", \"\u26a1 Energy Level (High = \ud83d\ude80, Low = \ud83d\udd0b)\"),\n            (\"love\", \"\u2764\ufe0f Love Meter (High = \ud83d\ude0d, Low = \ud83d\udc94)\"),\n            (\"chaos\", \"\ud83c\udfb2 Pure Chaos (Random emojis!)\"),\n            (\"pizza\", \"\ud83c\udf55 Pizza Scale (Everything becomes pizza)\")\n        ],\n        default=\"performance\"\n    )\n\n    threshold_value: NumericInput = NumericInput(\n        label=\"Mood Threshold\",\n        default=50.0,\n        min_value=0,\n        max_value=100\n    )\n\n    emoji_column_name: TextInput = TextInput(\n        label=\"New Emoji Column Name\",\n        default=\"mood_emoji\",\n        placeholder=\"Name your emoji column...\"\n    )\n\n\nclass EmojiStyleSection(Section):\n    emoji_intensity: SingleSelect = SingleSelect(\n        label=\"Emoji Intensity\",\n        options=[\n            (\"subtle\", \"\ud83d\ude10 Subtle (One emoji)\"),\n            (\"normal\", \"\ud83d\ude0a Normal (1-2 emojis)\"),\n            (\"extra\", \"\ud83e\udd29 Extra (2-3 emojis)\"),\n            (\"maximum\", \"\ud83e\udd2f\ud83c\udf89\ud83d\ude80 MAXIMUM OVERDRIVE\")\n        ],\n        default=\"normal\"\n    )\n\n    add_random_sparkle: ToggleSwitch = ToggleSwitch(\n        label=\"Add Random Sparkles \u2728\",\n        default=True,\n        description=\"Randomly sprinkle \u2728 for extra pizzazz\"\n    )\n\n    emoji_categories: MultiSelect = MultiSelect(\n        label=\"Allowed Emoji Categories\",\n        options=[\n            (\"faces\", \"\ud83d\ude00 Faces &amp; Emotions\"),\n            (\"animals\", \"\ud83e\udd84 Animals\"),\n            (\"food\", \"\ud83c\udf54 Food &amp; Drink\"),\n            (\"nature\", \"\ud83c\udf08 Nature\"),\n            (\"objects\", \"\ud83c\udfae Objects\"),\n            (\"symbols\", \"\ud83d\udcaf Symbols\"),\n            (\"flags\", \"\ud83c\udff4\u200d\u2620\ufe0f Flags\")\n        ],\n        default=[\"faces\", \"animals\", \"food\"]\n    )\n\n\nclass EmojiSettings(NodeSettings):\n    mood_config: EmojiMoodSection = EmojiMoodSection(\n        title=\"Mood Detection \ud83d\ude0a\",\n        description=\"Configure how to detect the vibe of your data\"\n    )\n\n    style_options: EmojiStyleSection = EmojiStyleSection(\n        title=\"Emoji Style \ud83c\udfa8\",\n        description=\"Fine-tune your emoji experience\"\n    )\n\n\nclass EmojiGenerator(CustomNodeBase):\n    # Node metadata - how it appears in Flowfile\n    node_name: str = \"Emoji Generator \ud83c\udf89\"\n    node_category: str = \"Fun Stuff\"  # This creates a new category\n    node_group: str = \"custom\"\n    title: str = \"Emoji Generator\"\n    intro: str = \"Transform boring data into fun emoji-filled datasets! \ud83d\ude80\"\n\n    # I/O configuration\n    number_of_inputs: int = 1\n    number_of_outputs: int = 1\n\n    # Link to our settings schema\n    settings_schema: EmojiSettings = EmojiSettings()\n\n    def process(self, input_df: pl.DataFrame) -&gt; pl.DataFrame:\n        # Get settings values from the UI\n        # Note how we can just access these from the settings that we have defined\n        column_name = self.settings_schema.mood_config.source_column.value\n        mood_type = self.settings_schema.mood_config.mood_type.value\n        threshold = self.settings_schema.mood_config.threshold_value.value\n        emoji_col_name = self.settings_schema.mood_config.emoji_column_name.value\n        intensity = self.settings_schema.style_options.emoji_intensity.value\n        add_sparkle = self.settings_schema.style_options.add_random_sparkle.value\n\n        # Define emoji sets for different moods\n        emoji_sets = {\n            \"performance\": {\n                \"high\": [\"\ud83d\ude0e\", \"\ud83d\udcaa\", \"\ud83c\udfc6\", \"\ud83d\udc51\", \"\ud83c\udf1f\", \"\ud83d\udcaf\", \"\ud83d\udd25\"],\n                \"low\": [\"\ud83d\ude30\", \"\ud83d\ude13\", \"\ud83d\udcc9\", \"\ud83d\ude22\", \"\ud83d\udc94\", \"\ud83c\udd98\", \"\ud83d\ude35\"]\n            },\n            \"temperature\": {\n                \"high\": [\"\ud83d\udd25\", \"\ud83c\udf0b\", \"\u2600\ufe0f\", \"\ud83e\udd75\", \"\ud83c\udf21\ufe0f\", \"\u2668\ufe0f\", \"\ud83c\udfd6\ufe0f\"],\n                \"low\": [\"\ud83e\uddca\", \"\u2744\ufe0f\", \"\u26c4\", \"\ud83e\udd76\", \"\ud83c\udf28\ufe0f\", \"\ud83c\udfd4\ufe0f\", \"\ud83d\udc27\"]\n            },\n            \"money\": {\n                \"high\": [\"\ud83e\udd11\", \"\ud83d\udcb0\", \"\ud83d\udc8e\", \"\ud83c\udfe6\", \"\ud83d\udcb3\", \"\ud83e\ude99\", \"\ud83d\udcc8\"],\n                \"low\": [\"\ud83d\ude22\", \"\ud83d\udcb8\", \"\ud83d\udcc9\", \"\ud83c\udfda\ufe0f\", \"\ud83d\ude2d\", \"\ud83e\udd7a\", \"\ud83d\udcca\"]\n            },\n            \"energy\": {\n                \"high\": [\"\ud83d\ude80\", \"\u26a1\", \"\ud83d\udca5\", \"\ud83c\udfaf\", \"\ud83c\udfc3\", \"\ud83e\udd38\", \"\ud83c\udfaa\"],\n                \"low\": [\"\ud83d\udd0b\", \"\ud83d\ude34\", \"\ud83d\udecc\", \"\ud83d\udc0c\", \"\ud83e\udd71\", \"\ud83d\ude2a\", \"\ud83d\udca4\"]\n            },\n            \"love\": {\n                \"high\": [\"\ud83d\ude0d\", \"\u2764\ufe0f\", \"\ud83d\udc95\", \"\ud83e\udd70\", \"\ud83d\udc98\", \"\ud83d\udc9d\", \"\ud83d\udc68\u200d\u2764\ufe0f\u200d\ud83d\udc68\"],\n                \"low\": [\"\ud83d\udc94\", \"\ud83d\ude22\", \"\ud83d\ude2d\", \"\ud83e\udd40\", \"\ud83d\ude14\", \"\ud83d\udc80\", \"\ud83d\udda4\"]\n            },\n            \"chaos\": {\n                \"high\": [\"\ud83e\udd96\", \"\ud83c\udfb8\", \"\ud83d\ude81\", \"\ud83c\udfaa\", \"\ud83e\udd9c\", \"\ud83c\udfad\", \"\ud83c\udff4\u200d\u2620\ufe0f\"],\n                \"low\": [\"\ud83e\udd54\", \"\ud83e\udde6\", \"\ud83d\udcce\", \"\ud83e\uddb7\", \"\ud83e\uddf2\", \"\ud83d\udd0c\", \"\ud83e\udea3\"]\n            },\n            \"pizza\": {\n                \"high\": [\"\ud83c\udf55\", \"\ud83c\udf55\ud83c\udf55\", \"\ud83c\udf55\ud83d\udd25\", \"\ud83c\udf55\ud83d\ude0d\", \"\ud83c\udf55\ud83c\udf89\", \"\ud83c\udf55\ud83d\udcaf\", \"\ud83c\udf55\ud83d\udc51\"],\n                \"low\": [\"\ud83c\udf55\", \"\ud83c\udf55\ud83d\ude22\", \"\ud83c\udf55\ud83d\udc94\", \"\ud83c\udf55\ud83d\ude2d\", \"\ud83c\udf55\ud83e\udd7a\", \"\ud83c\udf55\ud83d\ude14\", \"\ud83c\udf55\"]\n            }\n        }\n\n        # Helper function to get emoji based on value\n        def get_emoji(value, mood_type, threshold, intensity):\n            if value is None:\n                return \"\u2753\"\n\n            emoji_list = emoji_sets.get(mood_type, emoji_sets[\"performance\"])\n\n            if mood_type == \"chaos\":\n                # Random emoji from both lists\n                all_emojis = emoji_list[\"high\"] + emoji_list[\"low\"]\n                base_emoji = random.choice(all_emojis)\n            elif mood_type == \"pizza\":\n                # Everything is pizza\n                base_emoji = \"\ud83c\udf55\"\n            else:\n                # Use threshold to determine high/low\n                if value &gt;= threshold:\n                    base_emoji = random.choice(emoji_list[\"high\"])\n                else:\n                    base_emoji = random.choice(emoji_list[\"low\"])\n\n            # Add intensity\n            if intensity == \"subtle\":\n                result = base_emoji\n            elif intensity == \"normal\":\n                result = base_emoji\n                if random.random() &gt; 0.5:\n                    result += random.choice([\"\", \"\u2728\", \"\"])\n            elif intensity == \"extra\":\n                extras = [\"\u2728\", \"\ud83d\udcab\", \"\u2b50\", \"\"]\n                result = base_emoji + random.choice(extras) + random.choice(extras)\n            else:  # maximum\n                chaos_emojis = [\"\ud83c\udf89\", \"\ud83d\ude80\", \"\ud83d\udca5\", \"\ud83c\udf08\", \"\u2728\", \"\ud83d\udd25\", \"\ud83d\udcaf\", \"\u26a1\"]\n                result = base_emoji + \"\".join(random.choices(chaos_emojis, k=3))\n\n            # Add random sparkle\n            if add_sparkle and random.random() &gt; 0.7:\n                result += \"\u2728\"\n\n            return result\n\n        # Create emoji column using map_elements\n        emoji_expr = (\n            pl.col(column_name)\n            .map_elements(\n                lambda x: get_emoji(x, mood_type, threshold, intensity),\n                return_dtype=pl.String\n            )\n            .alias(emoji_col_name)\n        )\n\n        # Add bonus columns based on intensity\n        if intensity == \"maximum\":\n            # Add extra fun columns in maximum mode\n            return input_df.with_columns([\n                emoji_expr,\n                pl.lit(\"\ud83c\udf89 PARTY MODE \ud83c\udf89\").alias(\"vibe_check\"),\n                pl.col(column_name).map_elements(\n                    lambda x: \"\ud83d\udd25\" * min(int((x or 0) / 20), 5) if x else \"\ud83d\udca4\",\n                    return_dtype=pl.String\n                ).alias(\"fire_meter\")\n            ])\n        else:\n            return input_df.with_columns([emoji_expr])\n</code></pre> <p>Save this as <code>~/.flowfile/user_defined_nodes/emoji_generator.py</code>, restart Flowfile, and enjoy your new emoji-powered data transformations!</p>"},{"location":"for-developers/custom-node-tutorial.html#congratulations","title":"Congratulations! \ud83c\udf89","text":"<p>You've successfully created a fully functional custom node </p> <ul> <li>\u2705 Multi-section UI with 6 different component types</li> <li>\u2705 Complex processing logic with multiple mood themes</li> <li>\u2705 Advanced features like intensity control and random effects</li> <li>\u2705 Professional documentation and structure</li> </ul>"},{"location":"for-developers/design-philosophy.html","title":"The Architecture Story: How Flowfile Bridges Code and Visual Worlds","text":"<p>\ud83d\udccb TL;DR - Key Takeaways</p> <p>Key Points</p> <ul> <li>Two ways to build pipelines: Write Python code or use drag-and-drop UI - both create the same thing</li> <li>Settings-based design: Every transformation is just a configuration object (Pydantic model)</li> <li>Clear separation: Graph structure, settings, and execution are handled separately</li> <li>Happy accident: Started as a UI project, ended up with an architecture that works great for both UI and code</li> </ul> <p>Navigation</p> <ul> <li>This page: Architecture overview and design decisions</li> <li>Core Developer Guide: Technical implementation details</li> <li>Python API: How to use Flowfile in your projects</li> </ul> <p>\ud83d\udc65 Who Should Read This?</p> <p>Target Audience</p> <ul> <li>Contributors who want to understand the codebase</li> <li>Users curious about how things work internally</li> <li>Developers building similar dual-interface tools</li> <li>Anyone interested in bridging UI and code approaches</li> </ul>"},{"location":"for-developers/design-philosophy.html#the-problem-we-solved","title":"The Problem We Solved","text":"<p>Most data tools force you to choose: either use a visual interface (easy but limited) or write code (powerful but complex). We wanted both in the same tool.</p> <p>The challenge: How do you make a visual drag-and-drop interface that creates the exact same pipelines as writing code?</p> <p>The platform started with a clean, settings-based backend where every transformation is a declarative configuration object. This design is perfect for a UI. But developers don't think in configuration objects\u2014they think in code:</p> <pre><code># How developers want to write data code\ndf.filter(col(\"price\") &gt; 100).group_by(\"region\").sum()\n</code></pre> <p>The breakthrough came from realizing that the Polars API would be able to convert to our settings object and therefore creating the same settings object that the UI creates. Both interfaces become different ways to build the same underlying configuration, giving developers the expressiveness they want while maintaining the structured settings the UI needs.</p>"},{"location":"for-developers/design-philosophy.html#the-result","title":"The result","text":"<p>How It Actually Happened</p> <p>This wasn't some grand plan. I started building a drag-and-drop UI and needed a clean way to configure nodes. Settings objects made sense for the UI. But the development of Flowfile has never been a planned approach, it was just about building what sounded fun. Later, when looking at other projects, I realized I could just have the API methods create the same settings objects, well that is fun. Suddenly there were two equivalent interfaces almost by accident,. Since Polars does the actual data processing, our settings just configure what Polars should do. This turned out to be an easy abstraction layer that showed it's potential from the start.</p> <p>The result is a Python API that constructs the exact same configuration objects as the visual editor:</p> <ul> <li>The Python API <code>df.filter(...)</code>  translates directly to a <code>NodeFilter</code> object</li> <li>The Visual Editor creates an identical <code>NodeFilter</code> object through clicks and drags</li> </ul> <p>Both interfaces are different ways to build the same Directed Acyclic Graph (DAG), providing the experience of a code-native API combined with the accessibility of a visual editor.</p>"},{"location":"for-developers/design-philosophy.html#one-pipeline-two-ways","title":"One Pipeline, Two Ways","text":"<p>Let's build the same pipeline using both approaches to see how they produce identical results.</p>"},{"location":"for-developers/design-philosophy.html#sample-data","title":"Sample Data","text":"<pre><code>import flowfile as ff\n\nraw_data = [\n    {\"id\": 1, \"region\": \"North\", \"quantity\": 10, \"price\": 150},\n    {\"id\": 2, \"region\": \"South\", \"quantity\": 5, \"price\": 300},\n    {\"id\": 3, \"region\": \"East\", \"quantity\": 8, \"price\": 200},\n    {\"id\": 4, \"region\": \"West\", \"quantity\": 12, \"price\": 100},\n    {\"id\": 5, \"region\": \"North\", \"quantity\": 20, \"price\": 250},\n    {\"id\": 6, \"region\": \"South\", \"quantity\": 15, \"price\": 400},\n    {\"id\": 7, \"region\": \"East\", \"quantity\": 18, \"price\": 350},\n    {\"id\": 8, \"region\": \"West\", \"quantity\": 25, \"price\": 500},\n]\n</code></pre>"},{"location":"for-developers/design-philosophy.html#method-1-the-flowfile-api-developer-experience","title":"Method 1: The Flowfile API (Developer Experience)","text":"<p>Code: <pre><code>import flowfile as ff\nfrom flowfile_core.flowfile.flow_graph import FlowGraph\n\ngraph: FlowGraph = ff.create_flow_graph()\n\n# Create pipeline with fluent API\ndf_1 = ff.FlowFrame(raw_data, flow_graph=graph)\n\ndf_2 = df_1.with_columns(\n    flowfile_formulas=['[quantity] * [price]'], \n    output_column_names=[\"total\"]\n)\n\ndf_3 = df_2.filter(flowfile_formula=\"[total]&gt;1500\")\n\ndf_4 = df_3.group_by(['region']).agg([\n    ff.col(\"total\").sum().alias(\"total_revenue\"),\n    ff.col(\"total\").mean().alias(\"avg_transaction\"),\n])\n</code></pre></p> Inspecting the graph <p>Graph Introspection: <pre><code># Access all nodes that were created in the graph\nprint(graph._node_db)\n# {1: Node id: 1 (manual_input), \n#  3: Node id: 3 (formula), \n#  4: Node id: 4 (filter), \n#  5: Node id: 5 (group_by)}\n\n# Find the starting node(s) of the graph\nprint(graph._flow_starts)\n# [Node id: 1 (manual_input)]\n\n# From every node, access the next node that depends on it\nprint(graph.get_node(1).leads_to_nodes)\n# [Node id: 3 (formula)]\n\n# The other way around works too\nprint(graph.get_node(3).node_inputs)\n# NodeStepInputs(Left Input: None, Right Input: None, \n#                Main Inputs: [Node id: 1 (manual_input)])\n\n# Access the settings and type of any node\nprint(graph.get_node(4).setting_input)\nprint(graph.get_node(4).node_type)\n</code></pre></p>"},{"location":"for-developers/design-philosophy.html#method-2-direct-graph-construction-what-happens-internally","title":"Method 2: Direct Graph Construction (What Happens Internally)","text":"<p>Code: <pre><code>from flowfile_core.schemas import node_interface, transformation_settings, RawData\nfrom flowfile_core.flowfile.flow_graph import add_connection\n\nflow = ff.create_flow_graph()\n\n# Node 1: Manual input\nnode_manual_input = node_interface.NodeManualInput(\n    flow_id=flow.flow_id, \n    node_id=1,\n    raw_data_format=RawData.from_pylist(raw_data)\n)\nflow.add_manual_input(node_manual_input)\n\n# Node 2: Add formula for total\nformula_node = node_interface.NodeFormula(\n    flow_id=1,\n    node_id=2,\n    function=transformation_settings.FunctionInput(\n        field=transformation_settings.FieldInput(\n            name=\"total\", \n            data_type=\"Double\"\n        ),\n        function=\"[quantity] * [price]\"\n    )\n)\nflow.add_formula(formula_node)\nadd_connection(flow, \n    node_interface.NodeConnection.create_from_simple_input(1, 2))\n\n# Node 3: Filter high value transactions\nfilter_node = node_interface.NodeFilter(\n    flow_id=1,\n    node_id=3,\n    filter_input=transformation_settings.FilterInput(\n        filter_type=\"advanced\",\n        advanced_filter=\"[total]&gt;1500\"\n    )\n)\nflow.add_filter(filter_node)\nadd_connection(flow, \n    node_interface.NodeConnection.create_from_simple_input(2, 3))\n\n# Node 4: Group by region\ngroup_by_node = node_interface.NodeGroupBy(\n    flow_id=1,\n    node_id=4,\n    groupby_input=transformation_settings.GroupByInput(\n        agg_cols=[\n            transformation_settings.AggColl(\"region\", \"groupby\"),\n            transformation_settings.AggColl(\"total\", \"sum\", \"total_revenue\"),\n            transformation_settings.AggColl(\"total\", \"mean\", \"avg_transaction\")\n        ]\n    )\n)\nflow.add_group_by(group_by_node)\nadd_connection(flow, \n    node_interface.NodeConnection.create_from_simple_input(3, 4))\n</code></pre></p> <p>Schema Inspection: <pre><code># Check the schema at any node\nprint([s.get_minimal_field_info() for s in flow.get_node(4).schema])\n# [MinimalFieldInfo(name='region', data_type='String'), \n#  MinimalFieldInfo(name='total_revenue', data_type='Float64'), \n#  MinimalFieldInfo(name='avg_transaction', data_type='Float64')]\n</code></pre></p> Both methods produce the exact same Polars execution plan: <p>This is the polars query plan generated by both methods:</p> <pre><code>```\nAGGREGATE[maintain_order: false]\n  [col(\"total\").sum().alias(\"total_revenue\"), \n   col(\"total\").mean().alias(\"avg_transaction\")] BY [col(\"region\")]\n  FROM\n  FILTER [(col(\"total\")) &gt; (1500)]\n  FROM\n  WITH_COLUMNS:\n  [[(col(\"quantity\")) * (col(\"price\"))].alias(\"total\")]\n  DF [\"id\", \"region\", \"quantity\", \"price\"]; PROJECT 3/4 COLUMNS\n```\n</code></pre>"},{"location":"for-developers/design-philosophy.html#core-architecture","title":"Core Architecture","text":""},{"location":"for-developers/design-philosophy.html#three-fundamental-concepts","title":"Three Fundamental Concepts","text":""},{"location":"for-developers/design-philosophy.html#1-the-dag-is-everything","title":"1. The DAG is Everything","text":"<p>Every Flowfile pipeline is a Directed Acyclic Graph where. This is captured in the  FlowGraph</p> <ul> <li>Nodes are transformations (filter, join, group_by, etc.)</li> <li>Edges represent data flow between nodes</li> <li>Settings are Pydantic models configuring each transformation</li> </ul>"},{"location":"for-developers/design-philosophy.html#2-settings-drive-everything","title":"2. Settings Drive Everything","text":"<p>Every node is composed of two parts: the Node class (a Pydantic BaseModel) that holds metadata and the Settings (often dataclasses) that configure the transformation:</p> <p>Read more about Nodes and the transformations</p> <pre><code># The Node: metadata and graph position\nclass NodeGroupBy(NodeSingleInput):\n    groupby_input: transform_schema.GroupByInput = None\n\nclass NodeSingleInput(NodeBase):\n    depending_on_id: Optional[int] = -1  # Parent node reference\n\nclass NodeBase(BaseModel):\n    flow_id: int\n    node_id: int\n    cache_results: Optional[bool] = False\n    pos_x: Optional[float] = 0\n    pos_y: Optional[float] = 0\n    description: Optional[str] = None\n    # ... graph metadata ...\n\n# The Settings: transformation configuration (dataclass)\n@dataclass\nclass GroupByInput:\n    \"\"\"Defines how to perform the group by operation\"\"\"\n    agg_cols: List[AggColl]\n\n@dataclass\nclass AggColl:\n    \"\"\"Single aggregation operation\"\"\"\n    old_name: str      # Column to aggregate\n    agg: str          # Aggregation function ('sum', 'mean', etc.)\n    new_name: Optional[str]  # Output column name\n    output_type: Optional[str] = None\n</code></pre> <p>Settings Power The Backend</p> <p>This dual structure\u2014Nodes for graph metadata, Settings for transformation logic\u2014drives the backend:</p> <ul> <li>\ud83d\udd27 Code generation (method signatures match settings)</li> <li>\ud83d\udcbe Serialization (graphs can be saved/loaded)</li> <li>\ud83d\udd2e Schema prediction (output types are inferred from AggColl)</li> <li>\ud83c\udfa8 UI structure (defines what the frontend needs to collect, though forms are manually built)</li> </ul>"},{"location":"for-developers/design-philosophy.html#3-execution-is-everything","title":"3. Execution is Everything","text":"<p>The <code>FlowDataEngine</code> orchestrates everything about execution. While the DAG defines structure and settings define configuration, FlowDataEngine is the runtime brain that makes it all happen.</p> <p>FlowDataEngine handles: - Compute location (worker service vs local execution) - Caching strategy (when to materialize, where to store) - Schema caching (avoiding redundant schema calculations) - Lazy vs eager evaluation (performance vs debugging modes) - Data movement (passing LazyFrames between transformations)</p> <p>This separation is powerful: the DAG remains a pure specification, settings stay declarative, and FlowDataEngine owns all execution concerns. It wraps a Polars LazyFrame/DataFrame but is really the execution orchestrator\u2014deciding where, when, and how transformations run.</p>"},{"location":"for-developers/design-philosophy.html#understanding-flownode","title":"Understanding FlowNode","text":"<p>The <code>FlowNode</code> class is the heart of each transformation in the graph. Each node encapsulates everything needed for a single transformation step:</p> <p>Core FlowNode Components</p> <p>Essential State:</p> <ul> <li><code>_function</code>: The closure containing the transformation logic</li> <li><code>leads_to_nodes</code>: List of downstream nodes that depend on this one</li> <li><code>node_information</code>: Metadata (id, type, position, connections)</li> <li><code>_hash</code>: Unique identifier based on settings and parent hashes</li> </ul> <p>Runtime State:</p> <ul> <li><code>results</code>: Holds the resulting data, errors, and example data paths</li> <li><code>node_stats</code>: Tracks execution status (has_run, is_canceled, etc.)</li> <li><code>node_settings</code>: Runtime settings (cache_results, streamable, etc.)</li> <li><code>state_needs_reset</code>: Flag indicating if the node needs recalculation</li> </ul> <p>Schema Information:</p> <ul> <li><code>node_schema</code>: Input/output columns and predicted schemas</li> <li><code>schema_callback</code>: Function to calculate schema without execution</li> </ul> <p>The beauty is that FlowNode doesn't know about specific transformations\u2014it just orchestrates the execution of its <code>_function</code> closure with the right inputs and manages the resulting state.</p>"},{"location":"for-developers/design-philosophy.html#flowfile-the-use-of-closures","title":"Flowfile: The Use of Closures","text":"<p>When a method like <code>.filter()</code> is called, no data is actually filtered. Instead, a <code>FlowNode</code> is created containing a function\u2014a closure that remembers its settings.</p> <p>Visual: How Closures Build the Execution Chain <pre><code>graph LR\n    subgraph \"Node 1: manual_input\"\n        direction TB\n        settings1(\"&lt;b&gt;Settings&lt;/b&gt;&lt;br/&gt;raw_data = [...]\")\n        func1(\"&lt;b&gt;_func()&lt;/b&gt;&lt;br/&gt;&lt;i&gt;closure&lt;/i&gt;\")\n        settings1 -.-&gt; |remembered by| func1\n    end\n\n    subgraph \"Node 2: with_columns&lt;br/&gt;(formula)\"\n        direction TB\n        settings2(\"&lt;b&gt;Settings&lt;/b&gt;&lt;br/&gt;formula = '[q] * [p]'\")\n        func2(\"&lt;b&gt;_func(fl)&lt;/b&gt;&lt;br/&gt;&lt;i&gt;closure&lt;/i&gt;\")\n        settings2 -.-&gt; |remembered by| func2\n    end\n\n    subgraph \"Node 3: filter\"\n        direction TB\n        settings3(\"&lt;b&gt;Settings&lt;/b&gt;&lt;br/&gt;filter = '[total] &gt; 1500'\")\n        func3(\"&lt;b&gt;_func(fl)&lt;/b&gt;&lt;br/&gt;&lt;i&gt;closure&lt;/i&gt;\")\n        settings3 -.-&gt; |remembered by| func3\n    end\n\n    subgraph \"Node 4: group_by\"\n        direction TB\n        settings4(\"&lt;b&gt;Settings&lt;/b&gt;&lt;br/&gt;agg = sum(total)\")\n        func4(\"&lt;b&gt;_func(fl)&lt;/b&gt;&lt;br/&gt;&lt;i&gt;closure&lt;/i&gt;\")\n        settings4 -.-&gt; |remembered by| func4\n    end\n\n    Result([Schema / Data])\n\n    func1 ==&gt; |FlowDataEngine| func2\n    func2 ==&gt; |FlowDataEngine| func3\n    func3 ==&gt; |FlowDataEngine| func4\n    func4 ==&gt; |Final FlowDataEngine&lt;br/&gt;with full LazyFrame plan| Result</code></pre></p> <p>Each <code>_func</code> is a closure that wraps around the previous one, building up a chain. The beauty is that Polars can track the schema through this entire chain without executing any data transformations\u2014it just builds the query plan!</p>"},{"location":"for-developers/design-philosophy.html#the-closure-pattern-in-practice","title":"The Closure Pattern in Practice","text":"<p>Here's how closures are actually created in FlowGraph:</p> <pre><code># From the FlowGraph implementation\ndef add_group_by(self, group_by_settings: input_schema.NodeGroupBy):\n    # The closure: captures group_by_settings\n    def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n        return fl.do_group_by(group_by_settings.groupby_input, False)\n\n    self.add_node_step(\n        node_id=group_by_settings.node_id,\n        function=_func,  # This closure remembers group_by_settings!\n        node_type='group_by',\n        setting_input=group_by_settings,\n        input_node_ids=[group_by_settings.depending_on_id]\n    )\n\ndef add_union(self, union_settings: input_schema.NodeUnion):\n    # Another closure: captures union_settings\n    def _func(*flowfile_tables: FlowDataEngine):\n        dfs = [flt.data_frame for flt in flowfile_tables]\n        return FlowDataEngine(pl.concat(dfs, how='diagonal_relaxed'))\n\n    self.add_node_step(\n        node_id=union_settings.node_id,\n        function=_func,  # This closure has everything it needs\n        node_type='union',\n        setting_input=union_settings,\n        input_node_ids=union_settings.depending_on_ids\n    )\n</code></pre> <p>Each <code>_func</code> is a closure that captures its specific settings. When these functions are composed during execution, they form a chain:</p> <pre><code># Conceptual composition of the closures\nresult = group_by._func(\n    filter._func(\n        formula._func(\n            manual_input._func()\n        )\n    )\n)\n\n# Result is a FlowDataEngine with a LazyFrame that knows its schema\nprint(result.data_frame.collect_schema())\n# Schema([('region', String), ('total_revenue', Float64), ('avg_transaction', Float64)])\n</code></pre> <p>Why This Works</p> <ol> <li>Each <code>_func</code> is a closure containing the node's settings</li> <li>Functions only need FlowDataEngine as input (or multiple for joins/unions)</li> <li>LazyFrame tracks schema changes through the entire chain</li> <li>No data is processed\u2014Polars just builds the query plan</li> </ol> <p>The result: instant schema feedback without running expensive computations!</p>"},{"location":"for-developers/design-philosophy.html#fallback-schema-callbacks","title":"Fallback: Schema Callbacks","text":"<p>For nodes that can't infer schemas automatically (external data sources), each FlowNode can have a <code>schema_callback</code>:</p> <pre><code>def schema_callback(settings, input_schema):\n    \"\"\"Pure function: settings + input schema \u2192 output schema\"\"\"\n    # Calculate output schema without data\n    return new_schema\n</code></pre>"},{"location":"for-developers/design-philosophy.html#execution-methods","title":"Execution Methods","text":"<p>Flowfile offers flexible execution strategies depending on your needs:</p>"},{"location":"for-developers/design-philosophy.html#available-execution-methods","title":"\ud83d\ude80 Available Execution Methods","text":""},{"location":"for-developers/design-philosophy.html#performance-mode","title":"Performance Mode","text":"<p>When to use: Production pipelines, large datasets</p> <pre><code># Get the final result efficiently\nresult = flow.get_node(final_node_id).get_resulting_data()\n</code></pre> <p>Characteristics:</p> <ul> <li>\u26a1 Pull-based execution from the final node</li> <li>\ud83c\udfaf Polars optimizes the entire pipeline</li> <li>\ud83d\udca8 Data flows once through optimized plan</li> <li>\ud83d\udeab No intermediate materialization</li> </ul>"},{"location":"for-developers/design-philosophy.html#development-mode","title":"Development Mode","text":"<p>When to use: Debugging, inspection, incremental development</p> <pre><code># Execute with caching enabled\nimport flowfile as ff\n\nflow = ff.create_flow_graph()\nflow.flow_settings.execution_mode = \"Development\"\n\n# Add transformations here\nflow.run_graph()\n\n# Inspect intermediate results\nnode_3_result = flow.get_node(3).results.get_example_data()\n\nflow.get_node(3).needs_run(performance_mode=False) # False\n</code></pre> <p>Characteristics: - \ud83d\udcdd Push-based execution in topological order - \ud83d\udcbe Each node's output written to disk - \ud83d\udd0d Inspect any intermediate result - \ud83d\udd04 When re-running the flow, only the steps that have changed(directly and indirectly) will run</p>"},{"location":"for-developers/design-philosophy.html#explain-plan","title":"Explain Plan","text":"<p>When to use: Optimization, understanding deeply the execution plan.</p> <p>!!! warning This feature uses directly the Polars implementation, when the full flow cannot be fully converted to Polars, it will show partial executions.</p> <pre><code># See what Polars will actually do\nplan = flow.get_node(node_id).get_resulting_data().data_frame.explain()\nprint(plan)\n</code></pre> <p>Characteristics: - \ud83d\udcca Shows optimized query plan - \ud83d\udd0d Understand Polars optimizations - \ud83d\udcc8 Identify performance bottlenecks - \ud83c\udfaf No actual execution</p>"},{"location":"for-developers/design-philosophy.html#system-architecture","title":"System Architecture","text":""},{"location":"for-developers/design-philosophy.html#service-architecture","title":"Service Architecture","text":"<pre><code>graph LR\n    subgraph \"Frontend\"\n        A[Designer&lt;br/&gt;Vue/Electron]\n    end\n\n    subgraph \"Backend\"\n        B[Core Service&lt;br/&gt;FastAPI]\n        C[Worker Service&lt;br/&gt;FastAPI]\n    end\n\n    subgraph \"Storage\"\n        D[Arrow IPC&lt;br/&gt;Cache]\n    end\n\n    A &lt;--&gt;|Settings/Schema| B\n    B &lt;--&gt;|Execution| C\n    C &lt;--&gt;|Data| D</code></pre> <p>Service Responsibilities</p> <p>Designer: - Visual graph building interface - Node configuration forms (manually implemented) - Real-time schema feedback</p> <p>Core: - DAG management - Execution orchestration - Schema prediction</p> <p>Worker: - Polars transformations - Data caching (Arrow IPC) - Isolated from Core for scalability</p>"},{"location":"for-developers/design-philosophy.html#project-structure","title":"Project Structure","text":"<pre><code>flowfile/\n\u251c\u2500\u2500 flowfile_core/\n\u2502   \u251c\u2500\u2500 nodes/              # Node implementations\n\u2502   \u251c\u2500\u2500 schemas/            # Pydantic models\n\u2502   \u2514\u2500\u2500 flowfile/          # Graph management\n\u251c\u2500\u2500 flowfile_worker/\n\u2502   \u251c\u2500\u2500 execution/         # Polars execution\n\u2502   \u2514\u2500\u2500 cache/            # Arrow IPC caching\n\u2514\u2500\u2500 flowfile_frontend/\n    \u251c\u2500\u2500 components/       # Vue components\n    \u2514\u2500\u2500 electron/        # Desktop app\n</code></pre>"},{"location":"for-developers/design-philosophy.html#contributing","title":"Contributing","text":"<p>Current State of Node Development</p> <p>While the backend architecture elegantly uses settings-driven nodes, adding new nodes requires work across multiple layers. The frontend currently requires manual implementation for each node type\u2014the visual editor doesn't automatically generate forms from Pydantic schemas yet.</p> <p>However, there are also opportunities for more focused contributions! Integration with databases and cloud services is needed\u2014these are smaller, more targeted tasks since the core structure is already in place. There's a lot of active development happening, so it's an exciting time to contribute!</p>"},{"location":"for-developers/design-philosophy.html#adding-a-new-node-the-full-picture","title":"Adding a New Node: The Full Picture","text":"<p>Adding a node isn't as simple as defining settings and a function. Here's what's actually required:</p>"},{"location":"for-developers/design-philosophy.html#backend-requirements","title":"Backend Requirements","text":"<ol> <li>Define the Pydantic settings model in <code>schemas/</code></li> <li>Implement the transformation method on <code>FlowDataEngine</code></li> <li>Add the node method to <code>FlowGraph</code> (e.g., <code>add_custom_transform()</code>)</li> <li>Create the closure function that captures settings</li> <li>Define schema callbacks for predicting output schemas</li> <li>Register the node in the node registry</li> </ol> <p>Example of what's really needed in FlowGraph:</p> <pre><code>def add_custom_transform(self, transform_settings: input_schema.NodeCustomTransform):\n    # Create the closure that captures settings\n    def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n        return fl.do_custom_transform(transform_settings.transform_input)\n\n    # Register with the graph\n    self.add_node_step(\n        node_id=transform_settings.node_id,\n        function=_func,\n        node_type='custom_transform',\n        setting_input=transform_settings,\n        input_node_ids=[transform_settings.depending_on_id]\n    )\n\n    # Don't forget schema prediction!\n    node = self.get_node(transform_settings.node_id)\n    # ... schema callback setup ...\n</code></pre>"},{"location":"for-developers/design-philosophy.html#frontend-requirements","title":"Frontend Requirements","text":"<p>Currently, you'll need to:</p> <ol> <li>Create a new Vue component for the node's configuration form</li> <li>Handle the visual representation in the graph editor</li> <li>Map the UI inputs to the backend settings structure</li> <li>Add the node type to the visual editor's palette</li> </ol> <p>This manual process ensures full control over the UI/UX but requires significant development effort.</p>"},{"location":"for-developers/design-philosophy.html#future-vision","title":"Future Vision","text":"<p>The goal is to eventually auto-generate UI from Pydantic schemas, which would complete the settings-driven architecture. This would make adding new nodes closer to just defining the backend settings and transformation logic, with the UI automatically following.</p> <p>The beauty of Flowfile's architecture\u2014discovered through the organic evolution from a UI-first approach\u2014is that even though adding nodes requires work across multiple layers today, the settings-based design provides a clear contract between visual and code interfaces. </p> <p>I hope you enjoyed learning about Flowfile's architecture and found the dual-interface approach as exciting as I do! If you have questions, ideas, or want to contribute, ] feel free to reach out via GitHub or check our Core Developer Guide. Happy building!</p>"},{"location":"for-developers/docker-deployment.html","title":"Docker Deployment Guide","text":"<p>Deploy Flowfile using Docker Compose for development and production environments.</p>"},{"location":"for-developers/docker-deployment.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose installed</li> <li>Basic understanding of Docker concepts</li> </ul>"},{"location":"for-developers/docker-deployment.html#quick-start","title":"Quick Start","text":""},{"location":"for-developers/docker-deployment.html#option-a-interactive-setup-recommended","title":"Option A: Interactive Setup (Recommended)","text":"<ol> <li> <p>Start the services: <pre><code>docker compose up -d\n</code></pre></p> </li> <li> <p>Open Flowfile: http://localhost:8080</p> </li> <li> <p>Follow the Setup Wizard:</p> </li> <li>Click \"Generate Master Key\"</li> <li>Copy the generated key</li> <li>Add to your <code>.env</code> file: <code>FLOWFILE_MASTER_KEY=&lt;your-key&gt;</code></li> <li> <p>Restart: <code>docker compose restart</code></p> </li> <li> <p>Log in with default credentials (<code>admin</code> / <code>changeme</code>)</p> </li> </ol>"},{"location":"for-developers/docker-deployment.html#option-b-pre-configured-setup","title":"Option B: Pre-configured Setup","text":"<p>For automated deployments:</p>"},{"location":"for-developers/docker-deployment.html#step-1-generate-the-master-key","title":"Step 1: Generate the Master Key","text":"<pre><code>python -c \"from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())\"\n</code></pre> <p>Copy the output for the next step.</p>"},{"location":"for-developers/docker-deployment.html#step-2-configure-environment","title":"Step 2: Configure Environment","text":"<pre><code>cp .env.example .env\n</code></pre> <p>Edit <code>.env</code>:</p> <pre><code>FLOWFILE_MASTER_KEY=&lt;your-generated-key&gt;\nFLOWFILE_ADMIN_USER=admin\nFLOWFILE_ADMIN_PASSWORD=YourSecurePassword123!\nJWT_SECRET_KEY=your-secure-jwt-secret-at-least-32-chars\n</code></pre>"},{"location":"for-developers/docker-deployment.html#step-3-start-services","title":"Step 3: Start Services","text":"<pre><code>docker compose up -d\n</code></pre> <p>Access at: http://localhost:8080</p>"},{"location":"for-developers/docker-deployment.html#security-architecture","title":"Security Architecture","text":"Component Purpose Configuration Master Key Encrypts user secrets at rest <code>FLOWFILE_MASTER_KEY</code> env var User Secrets API keys, passwords, tokens Encrypted in database JWT Secret Signs authentication tokens <code>JWT_SECRET_KEY</code> env var User Password Authenticates users Hashed in database"},{"location":"for-developers/docker-deployment.html#how-they-work-together","title":"How They Work Together","text":"<ol> <li>User logs in with username/password</li> <li>Server issues a JWT token (signed with <code>JWT_SECRET_KEY</code>)</li> <li>User creates secrets (e.g., \"my_api_key\" = \"sk-xxx\")</li> <li>Secret value is encrypted using the master key before storage</li> <li>At runtime, secrets are decrypted with the master key for use in flows</li> </ol>"},{"location":"for-developers/docker-deployment.html#production-checklist","title":"Production Checklist","text":"<ul> <li>[ ] Generate a unique <code>FLOWFILE_MASTER_KEY</code></li> <li>[ ] Set a strong <code>FLOWFILE_ADMIN_PASSWORD</code></li> <li>[ ] Generate secure <code>JWT_SECRET_KEY</code> with <code>openssl rand -hex 32</code></li> <li>[ ] Never commit <code>.env</code> to version control</li> <li>[ ] Back up <code>.env</code> securely (losing master key = losing all encrypted secrets)</li> <li>[ ] Set up HTTPS (reverse proxy with nginx/traefik)</li> <li>[ ] Configure firewall rules</li> </ul>"},{"location":"for-developers/docker-deployment.html#docker-compose-services","title":"Docker Compose Services","text":"<pre><code>services:\n  flowfile-frontend:  # Web UI on port 8080\n  flowfile-core:      # API server on port 63578\n  flowfile-worker:    # Background job processor on port 63579\n</code></pre> <p>All services share: - The master key via <code>FLOWFILE_MASTER_KEY</code> environment variable - User data volume (<code>./flowfile_data</code>) - Internal network for communication</p>"},{"location":"for-developers/docker-deployment.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"for-developers/docker-deployment.html#setup-wizard-keeps-appearing","title":"Setup wizard keeps appearing","text":"<p>The master key is not configured. Add <code>FLOWFILE_MASTER_KEY</code> to your <code>.env</code> file and restart.</p>"},{"location":"for-developers/docker-deployment.html#invalid-master-key-format","title":"Invalid master key format","text":"<p>The master key must be a valid Fernet key. Generate a new one:</p> <pre><code>python -c \"from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())\"\n</code></pre> <p>Warning: Changing the key makes existing encrypted secrets unreadable.</p>"},{"location":"for-developers/docker-deployment.html#container-fails-to-start","title":"Container fails to start","text":"<p>Check the logs:</p> <pre><code>docker compose logs flowfile-core\ndocker compose logs flowfile-worker\n</code></pre>"},{"location":"for-developers/docker-deployment.html#volumes","title":"Volumes","text":"Volume Purpose <code>./flowfile_data</code> User uploads, files <code>./saved_flows</code> Saved flow definitions <code>flowfile-internal-storage</code> Internal application data"},{"location":"for-developers/docker-deployment.html#scaling-considerations","title":"Scaling Considerations","text":"<p>For high-availability deployments: - Use external PostgreSQL instead of SQLite - Deploy multiple worker instances - Use Redis for distributed task queue - Place services behind a load balancer</p>"},{"location":"for-developers/flowfile-core.html","title":"Flowfile Core: A Developer's Guide","text":"<p>Welcome! This guide is for developers who want to understand, use, and contribute to <code>flowfile-core</code>. We'll dive into the architecture, see how data flows, and learn how to build powerful data pipelines.</p> <p>Looking for the API docs?</p> <ul> <li>Python API Reference for users: If you want to USE Flowfile</li> <li>Design Philosophy: If you want to understand WHY Flowfile works this way</li> <li>This page: If you want to understand HOW Flowfile works internally</li> </ul> <p>New to Flowfile?</p> <p>If you're looking for the high-level Python API, start with the Python API Overview. This guide dives into the internal architecture.</p> <p>Ready? Let's build something!</p>"},{"location":"for-developers/flowfile-core.html#the-core-architecture","title":"The Core Architecture","text":"<p>At its heart, <code>flowfile-core</code> is composed of three main objects:</p> <ol> <li><code>FlowGraph</code>: The central orchestrator. It holds your pipeline, manages the nodes, and controls the execution flow.</li> <li><code>FlowNode</code>: An individual step in your pipeline. It's a wrapper around your settings and logic, making it an executable part of the graph.</li> <li><code>FlowDataEngine</code>: The data itself, which flows between nodes. It's a smart wrapper around a Polars LazyFrame, carrying both the data and its schema.</li> </ol> <p>Let's see these in action.</p>"},{"location":"for-developers/flowfile-core.html#1-the-flowgraph-your-pipeline-orchestrator","title":"1. The FlowGraph: Your Pipeline Orchestrator","text":"<p>Everything starts with the <code>FlowGraph</code>. Think of it as the canvas for your data pipeline.</p> <p>Let's create one:</p> <pre><code>from flowfile_core.flowfile.flow_graph import FlowGraph\nfrom flowfile_core.schemas.schemas import FlowSettings\n\n# Initialize the graph with some basic settings\ngraph = FlowGraph(\n    flow_settings=FlowSettings(\n        flow_id=1,\n        name=\"My First Pipeline\"\n    )\n)\n\nprint(graph)\n</code></pre> Output of <code>print(graph)</code> <pre><code>FlowGraph(\nNodes: {}                                #&lt;-- An empty dictionary. No nodes yet!\n\nSettings:                                #&lt;-- The FlowSettings object you provided.\n  -flow_id: 1                            #&lt;-- A unique ID for this flow.\n  -description: None                     #&lt;-- An optional description.\n  -save_location: None                   #&lt;-- Where the flow definition is saved.\n  -name: My First Pipeline               #&lt;-- The name of our flow.\n  -path:                                 #&lt;-- Path to the flow file.\n  -execution_mode: Development           #&lt;-- 'Development' for debugging or 'Performance' for speed.\n  -execution_location: local             #&lt;-- Where the flow runs ('local' or 'remote').\n  -auto_save: False                      #&lt;-- Auto-save changes (feature in development).\n  -modified_on: None                     #&lt;-- Last modified timestamp (feature in development).\n  -show_detailed_progress: True          #&lt;-- If True, shows detailed logs in the UI.\n  -is_running: False                     #&lt;-- Is the flow currently running?\n  -is_canceled: False                    #&lt;-- Was a cancellation requested?\n)\n</code></pre> <pre><code>print(graph.run_graph())\n# flow_id=1 start_time=datetime.datetime(...) end_time=datetime.datetime(...) success=True nodes_completed=0 number_of_nodes=0 node_step_result=[]\n</code></pre> <p>It runs successfully but does nothing, as expected. The FlowGraph's job is to:</p> <ul> <li>Contain all the nodes.</li> <li>Manage the connections between them.</li> <li>Calculate the optimal execution order.</li> <li>Orchestrate the entire run lifecycle.</li> </ul> <p>Let's give it a node to manage.</p>"},{"location":"for-developers/flowfile-core.html#2-adding-a-node-where-settings-come-to-life","title":"2. Adding a Node: Where Settings Come to Life","text":"<p>You don't add raw functions or data directly to the graph. Instead, you provide settings objects (which are just Pydantic models). The graph then transforms these settings into executable <code>FlowNodes</code>.</p> <p>Watch this:</p> <p><pre><code>from flowfile_core.schemas import input_schema\n\n# 1. Define your data using a settings object.\n# This is just a Pydantic model holding configuration.\nmanual_input_settings = input_schema.NodeManualInput(\n    flow_id=1,\n    node_id=1,\n    raw_data_format=input_schema.RawData.from_pylist([\n        {\"name\": \"Alice\", \"age\": 30},\n        {\"name\": \"Bob\", \"age\": 25}\n    ])\n)\n\n# 2. Add the settings to the graph.\ngraph.add_manual_input(manual_input_settings)\n</code></pre> So, what did the graph just do? It didn't just store our settings. It created a FlowNode.</p> <pre><code># Let's retrieve the object the graph created\nnode = graph.get_node(1)\n\nprint(type(node))\n# &lt;class 'flowfile_core.flowfile.flow_node.flow_node.FlowNode'&gt;\n</code></pre> <p>The <code>FlowNode</code> is the wrapper that makes your settings operational. It holds your original settings but also adds the machinery needed for execution.</p> Peek inside the <code>FlowNode</code> <pre><code># The FlowNode keeps your original settings\nprint(node.setting_input == manual_input_settings)\n# True\n\n# But it also contains the execution logic...\nprint(f\"Has a function to run: {node._function is not None}\")\n# Has a function to run: True\n\n# ...state tracking...\nprint(f\"Can track its state: {hasattr(node, 'node_stats')}\")\n# Can track its state: True\n\n# ...connections to other nodes...\nprint(f\"Can connect to other nodes: {hasattr(node, 'leads_to_nodes')}\")\n# Can connect to other nodes: True\n\n# ...and a place to store its results.\nprint(f\"Has a place for results: {hasattr(node, 'results')}\")\n# Has a place for results: True\n</code></pre> <p>This separation is key: Settings define what to do, and the FlowNode figures out how to do it within the graph.</p>"},{"location":"for-developers/flowfile-core.html#3-connections-the-key-to-a-flowing-pipeline","title":"3. Connections: The Key to a Flowing Pipeline","text":"<pre><code>from flowfile_core.schemas.transform_schema import FilterInput\n\n# 1. Define settings for a filter node\nfilter_settings = input_schema.NodeFilter(\n    flow_id=1,\n    node_id=2,\n    filter_input=FilterInput(\n        filter_type=\"advanced\",\n        advanced_filter=\"[age] &gt; 28\" # Polars expression syntax\n    )\n)\ngraph.add_filter(filter_settings)\n\n# 2. Run the graph\nresult = graph.run_graph()\nprint(f\"Nodes executed: {result.nodes_completed}/{len(graph.nodes)}\")\n# Nodes executed: 1/2\n</code></pre> <p>Only one node ran! Why? The graph is smart; it knows the filter node has no input, thus will never succeed running.</p> Why the filter node was skipped <pre><code>filter_node = graph.get_node(2)\n\n# The graph checks if a node is a \"start\" node (has no inputs)\nprint(f\"Is filter_node a start node? {filter_node.is_start}\")\n# Is filter_node a start node? False\n\n# It sees that the filter node is missing an input connection\nprint(f\"Does filter_node have an input? {filter_node.has_input}\")\n# Does filter_node have an input? False\n\n# The graph's execution plan only includes nodes it can reach from a start node\nprint(f\"Graph start nodes: {graph._flow_starts}\")\n# Graph start nodes: [Node id: 1 (manual_input)]\n</code></pre> <p>The execution engine works like this:</p> <ol> <li>It identifies all start nodes (like our manual input).</li> <li>It builds an execution plan by following the connections from those start nodes.</li> <li>Any node not connected to this flow is ignored.</li> </ol> <p>Let's fix that by adding a connection.</p> <pre><code>from flowfile_core.flowfile.flow_graph import add_connection\n# Create a connection object\nconnection = input_schema.NodeConnection.create_from_simple_input(\n    from_id=1, # From our manual input node\n    to_id=2,   # To our filter node\n    input_type=\"main\"\n)\n\n# Add it to the graph\nadd_connection(graph, connection)\n\n# Let's check what changed\nprint(f\"Node 1 now leads to: {graph.get_node(1).leads_to_nodes}\")\n# Node 1 now leads to: [Node id: 2 (filter)]\n\nprint(f\"Node 2 now receives from: {graph.get_node(2).node_inputs.main_inputs}\")\n# Node 2 now receives from: [Node id: 1 (manual_input)]\n</code></pre> <p>Now that they are connected, let's run the graph again.</p> <pre><code>result = graph.run_graph()\n\n# The graph determines the correct execution order\nprint(\"Execution Order:\")\nfor node_result in result.node_step_result:\n    print(f\"  - Node {node_result.node_id} ran successfully: {node_result.success}\")\n# Execution Order:\n#   - Node 1 ran successfully: True\n#   - Node 2 ran successfully: True\n</code></pre> <p>Success! Both nodes executed. The connection allowed data to flow from the input to the filter.</p>"},{"location":"for-developers/flowfile-core.html#4-the-flowdataengine-the-data-carrier","title":"4. The FlowDataEngine: The Data Carrier","text":"<p>When data moves from one node to another, it's bundled up in a FlowDataEngine object. This isn't just raw data; it's an enhanced wrapper around a Polars LazyFrame.</p> <pre><code># Let's inspect the data after the run\nnode1 = graph.get_node(1)\nnode2 = graph.get_node(2)\n\n# Get the resulting data from each node\ndata_engine1 = node1.get_resulting_data()\ndata_engine2 = node2.get_resulting_data()\n\nprint(f\"Type of object passed between nodes: {type(data_engine1)}\")\n# Type of object passed between nodes: &lt;class 'flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine'&gt;\n\n# Let's see the transformation\nprint(f\"Rows from Node 1 (Input): {len(data_engine1.collect())}\")\n# Rows from Node 1 (Input): 2\n\nprint(f\"Rows from Node 2 (Filter): {len(data_engine2.collect())}\")\n# Rows from Node 2 (Filter): 1  &lt;-- Success! Bob (age 25) was filtered out.\n</code></pre> <p>The <code>FlowDataEngine</code> is the boundary between <code>flowfile-core</code> and Polars. It: * Carries the data (as a LazyFrame). * Maintains schema information. * Tracks metadata like record counts. * Manages lazy vs. eager execution.</p>"},{"location":"for-developers/flowfile-core.html#5-the-hash-system-smart-change-detection","title":"5. The Hash System: Smart Change Detection","text":"<p>How does the graph know when to re-run a node? Every <code>FlowNode</code> has a unique hash based on its configuration and its inputs.</p> <pre><code>node2 = graph.get_node(2)\noriginal_hash = node2.hash\nprint(f\"Original hash of filter node: {original_hash[:10]}...\")\n# Original hash of filter node: ...\n\n# Now, let's change the filter's settings\nnode2.setting_input.filter_input.advanced_filter = \"[age] &gt; 20\"\n\n# The node instantly knows it's been changed\nprint(f\"Settings changed, needs reset: {node2.needs_reset()}\")\n# Settings changed, needs reset: True\n\n# Resetting recalculates the hash\nnode2.reset()\nnew_hash = node2.hash\nprint(f\"New hash of filter node: {new_hash[:10]}...\")\n# New hash of filter node: ...\n\nprint(f\"Hash changed: {original_hash != new_hash}\")\n# Hash changed: True\n</code></pre> <p>This hash is calculated from:</p> <ul> <li>The node's own settings.</li> <li>The hashes of all its direct parent nodes.</li> </ul> <p>This creates a chain of dependency. If you change a node, <code>flowfile-core</code> knows that it and all downstream nodes need to be re-run, while upstream nodes can use their cached results. This is crucial for efficiency.</p>"},{"location":"for-developers/flowfile-core.html#6-schema-prediction-see-the-future","title":"6. Schema Prediction: See the Future","text":"<p>One of the most powerful features for interactive UI is schema prediction. A node can predict its output schema without processing any data.</p> <p>Let's add a \"formula\" node to create a new column.</p> <pre><code>from flowfile_core.schemas.transform_schema import FunctionInput, FieldInput\n\n# 1. Add a formula node to double the age\nformula_settings = input_schema.NodeFormula(\n    flow_id=1,\n    node_id=3,\n    function=FunctionInput(\n        field=FieldInput(name=\"age_doubled\", data_type=\"Int64\"),\n        function=\"[age] * 2\" # Polars expression\n    )\n)\ngraph.add_formula(formula_settings)\n\n# 2. Connect the filter node to our new formula node\nadd_connection(graph, input_schema.NodeConnection.create_from_simple_input(2, 3))\n\n# 3. Predict the schema\nformula_node = graph.get_node(3)\npredicted_schema = formula_node.get_predicted_schema()\n\nprint(\"Predicted columns for Node 3:\")\nfor col in predicted_schema:\n    print(f\"  - {col.column_name} (Type: {col.data_type})\")\n\n# This works even though the node has not run yet!\nprint(f\"\\nHas the formula node run? {formula_node.node_stats.has_run_with_current_setup}\")\n</code></pre> Output of Schema Prediction <pre><code>Predicted columns for Node 3:\n  - name (Type: String)\n  - age (Type: Int64)\n  - city (Type: String)\n  - age_doubled (Type: Int64)\n</code></pre> <p>How does this work? The node simply:</p> <ol> <li>Asks its parent node(s) for their output schema.</li> <li>Applies its own transformation logic to that schema (not the data).</li> <li>Returns the resulting new schema.</li> </ol> <p>This allows a UI to show you how your data will be transformed in real-time, as you build the pipeline.</p>"},{"location":"for-developers/flowfile-core.html#the-complete-picture-a-summary","title":"The Complete Picture: A Summary","text":"<p>Let's recap the entire lifecycle:</p> <ul> <li>You provide Settings: You define steps using simple Pydantic models (<code>NodeManualInput</code>, <code>NodeFilter</code>, etc.).</li> <li>Graph Creates FlowNodes: The <code>FlowGraph</code> takes your settings and wraps them in <code>FlowNode</code> objects, adding execution logic, state, and connection points.</li> <li>You Connect Nodes: You create <code>NodeConnection</code> objects. This builds the pipeline topology, which the graph uses to determine the execution order.</li> <li>You Run the Graph: When <code>graph.run_graph()</code> is called:<ul> <li>An execution plan is created via topological sort.</li> <li>Execution starts from the \"start nodes\".</li> <li>Each node receives a <code>FlowDataEngine</code> from its parent.</li> <li>It applies its transformation logic.</li> <li>It returns a new <code>FlowDataEngine</code> to its children.</li> </ul> </li> <li>Results Flow Through: The data, wrapped in the <code>FlowDataEngine</code>, moves down the pipeline, getting transformed at each step.</li> </ul> <p>This architecture provides a powerful combination of flexibility, introspection, and performance, bridging the gap between a visual, no-code interface and a powerful, code-driven engine.</p>"},{"location":"for-developers/flowfile-core.html#execution-strategy-how-nodes-decide-where-to-run","title":"Execution Strategy: How Nodes Decide Where to Run","text":"<p>When a node executes, it doesn't just \"run.\" It goes through a decision pipeline that determines whether to run and how to run. This is handled by the <code>NodeExecutor</code> class (<code>flowfile_core.flowfile.flow_node.executor</code>).</p> <p>The behavior depends on two settings visible in the frontend:</p> <ul> <li>Execution mode \u2014 Development or Performance (set per flow)</li> <li>Execution location \u2014 local or remote (derived from global settings)</li> </ul>"},{"location":"for-developers/flowfile-core.html#execution-modes-from-the-frontend","title":"Execution Modes from the Frontend","text":"<p>In the UI, users choose between Development and Performance mode. Here's what each mode does when running remotely:</p>"},{"location":"for-developers/flowfile-core.html#development-mode-remote","title":"Development Mode (remote)","text":"<p>Development mode is the interactive, debugging-friendly mode. It produces preview data (100-row samples) so the UI can show what each node outputs.</p> Node type What happens Narrow transforms (select, sample, union) Computation runs locally, only a 100-row sample is sent to the remote worker for preview (<code>LOCAL_WITH_SAMPLING</code>) Wide transforms (sort, record_count) Entire computation runs on the remote worker (<code>REMOTE</code>) Everything else (filter, join, group_by, input nodes) Entire computation runs on the remote worker (<code>REMOTE</code>) Any node with <code>cache_results</code> enabled Always fully remote regardless of transform type (<code>REMOTE</code>)"},{"location":"for-developers/flowfile-core.html#performance-mode-remote","title":"Performance Mode (remote)","text":"<p>Performance mode skips preview data generation and only runs nodes when strictly necessary \u2014 output nodes, cache rebuilds, or nodes that have never run. Once a node has run, it is skipped on subsequent executions. No sampling takes place.</p>"},{"location":"for-developers/flowfile-core.html#local-mode","title":"Local Mode","text":"<p>When <code>run_location = \"local\"</code> (e.g. no worker available), all nodes use FULL_LOCAL \u2014 everything runs in-process regardless of execution mode.</p>"},{"location":"for-developers/flowfile-core.html#the-three-strategies","title":"The Three Strategies","text":"<p>Under the hood, the mode and location map to one of three execution strategies:</p> Strategy What happens When used FULL_LOCAL Everything runs in-process <code>run_location = \"local\"</code> LOCAL_WITH_SAMPLING Computation runs locally, remote worker generates a 100-row sample for UI preview Development mode + narrow transforms REMOTE Entire computation is offloaded to the remote worker Development mode + wide/other transforms, or <code>cache_results</code> enabled"},{"location":"for-developers/flowfile-core.html#how-the-strategy-is-determined","title":"How the Strategy Is Determined","text":"<p>The decision lives in <code>_determine_strategy</code> and follows this priority:</p> <pre><code>1. local            \u2192 FULL_LOCAL\n2. cache_results    \u2192 REMOTE\n3. narrow transform \u2192 LOCAL_WITH_SAMPLING\n4. everything else  \u2192 REMOTE\n</code></pre> <p>Why narrow transforms get LOCAL_WITH_SAMPLING: Narrow transforms like <code>select</code> only operate on columns \u2014 dropping, renaming, filtering. They don't reshape or aggregate data, so they're cheap to compute locally. The only thing sent to the remote worker is a request to materialize 100 sample rows for the UI preview (<code>ExternalSampler</code> \u2192 <code>POST /store_sample/</code>).</p> <p>Why cache overrides to REMOTE: When <code>cache_results</code> is enabled on a node, the full result must be materialized and written to disk by the remote worker. A local computation with sampling wouldn't produce the cached artifact the downstream nodes expect.</p>"},{"location":"for-developers/flowfile-core.html#the-local_with_sampling-flow","title":"The LOCAL_WITH_SAMPLING Flow","text":"<p>This is the path taken by narrow transforms in Development mode:</p> <pre><code>LOCAL_WITH_SAMPLING (e.g. select node, Development mode):\n\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502   Local process   \u2502              \u2502  Remote worker   \u2502\n  \u2502                   \u2502              \u2502                  \u2502\n  \u2502 1. Compute full   \u2502              \u2502                  \u2502\n  \u2502    result locally  \u2502\u2500\u2500serialize\u2500\u2500\u25b6\u2502 2. Materialize   \u2502\n  \u2502    (LazyFrame)     \u2502  LazyFrame   \u2502    100 rows only \u2502\n  \u2502                   \u2502              \u2502    (POST /store_  \u2502\n  \u2502 3. Store sample   \u2502\u25c0\u2500\u2500\u2500\u2500done\u2500\u2500\u2500\u2500\u2500\u2502     sample/)     \u2502\n  \u2502    for UI preview  \u2502              \u2502                  \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Compare this to REMOTE, used by wide transforms and other node types:</p> <pre><code>REMOTE (e.g. sort node, join node):\n\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502   Local process   \u2502              \u2502  Remote worker   \u2502\n  \u2502                   \u2502              \u2502                  \u2502\n  \u2502 1. Serialize the  \u2502\u2500\u2500serialize\u2500\u2500\u25b6\u2502 2. Compute full  \u2502\n  \u2502    LazyFrame plan  \u2502  LazyFrame   \u2502    result        \u2502\n  \u2502                   \u2502              \u2502 3. Write to disk  \u2502\n  \u2502 4. Fetch result   \u2502\u25c0\u2500\u2500\u2500\u2500done\u2500\u2500\u2500\u2500\u2500\u2502 4. Generate      \u2502\n  \u2502    + sample data   \u2502              \u2502    sample data   \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"for-developers/flowfile-core.html#the-decision-pipeline","title":"The Decision Pipeline","text":"<p>Before choosing a strategy, the executor decides whether to run at all:</p> <pre><code>output node?                    \u2192 RUN\nforced refresh (reset_cache)?   \u2192 RUN\nnever ran before?               \u2192 RUN (with strategy)\nsource file changed? (read)     \u2192 RUN (with strategy)\ncache_results + cache present?  \u2192 SKIP\ncache_results + cache missing?  \u2192 RUN REMOTE (rebuild cache)\nalready ran?                    \u2192 SKIP\n</code></pre> <p>If a node has already run (<code>has_run_with_current_setup = True</code>) and nothing invalidated it, it skips. Results from the previous execution are still available in memory. This applies to both Development and Performance mode.</p>"},{"location":"for-developers/flowfile-core.html#node-classification","title":"Node Classification","text":"<p>The strategy routing depends on how nodes are classified in the node store:</p> Node <code>transform_type</code> Development + remote Performance + remote select narrow LOCAL_WITH_SAMPLING skip if already ran sample narrow LOCAL_WITH_SAMPLING skip if already ran union narrow LOCAL_WITH_SAMPLING skip if already ran sort wide REMOTE skip if already ran record_count wide REMOTE skip if already ran filter other REMOTE skip if already ran join other REMOTE skip if already ran group_by other REMOTE skip if already ran manual_input other REMOTE skip if already ran <p>Narrow transforms are defined with <code>transform_type=\"narrow\"</code> in <code>configs/node_store/nodes.py</code>.</p>"},{"location":"for-developers/flowfile-core.html#the-fastapi-service-your-api-layer","title":"The FastAPI Service: Your API Layer","text":"<p>While <code>FlowGraph</code>, <code>FlowNode</code>, and <code>FlowDataEngine</code> power the core pipeline logic, the FastAPI service is what makes it accessible from the outside world.</p> <p>Think of it as the control panel for your pipelines:</p> <ul> <li>HTTP interface \u2013 Wraps the core Python objects in a REST API so UIs (like Flowfile\u2019s) or other systems can create, run, and inspect flows via standard web requests.</li> <li>State management \u2013 Keeps track of all active <code>FlowGraph</code> sessions. When the UI triggers a change, it\u2019s really calling one of these endpoints, which updates the in-memory graph.</li> <li>Security \u2013 Handles authentication and authorization so only the right users can access or modify flows.</li> <li>Data previews \u2013 When you view a node\u2019s output in the UI, the API calls <code>.get_resulting_data()</code> on the corresponding <code>FlowNode</code> and returns a sample to the client.</li> </ul> <p>In short: FastAPI turns the in-memory power of <code>flowfile-core</code> into a secure, interactive web service, enabling rich, real-time applications to be built on top of your pipelines.</p>"},{"location":"for-developers/kernel-architecture.html","title":"Kernel Architecture","text":"<p>The kernel system provides isolated Python code execution inside Docker containers. This page explains the internal architecture, component interactions, and key design decisions.</p> <p>Looking for the user guide?</p> <p>See Kernel Execution for the user-facing documentation on how to write code and use the <code>flowfile</code> API inside kernels.</p>"},{"location":"for-developers/kernel-architecture.html#overview","title":"Overview","text":"<p>The kernel system consists of two main components:</p> <ol> <li>Kernel Manager (<code>flowfile_core/flowfile_core/kernel/</code>) \u2014 Orchestrates Docker container lifecycle and proxies execution requests from the Core API</li> <li>Kernel Runtime (<code>kernel_runtime/</code>) \u2014 A FastAPI application that runs inside each Docker container, executes user code, and manages artifacts</li> </ol> <pre><code>graph LR\n    Frontend[\"Frontend&lt;br/&gt;(Vue/Electron)\"]\n    Core[\"Core API&lt;br/&gt;(port 63578)\"]\n    Manager[\"Kernel Manager&lt;br/&gt;(Docker API)\"]\n    K1[\"Kernel Container&lt;br/&gt;(port 9999)\"]\n    K2[\"Kernel Container&lt;br/&gt;(port 9999)\"]\n    Shared[\"Shared Volume&lt;br/&gt;(parquet I/O, artifacts)\"]\n\n    Frontend --&gt;|JWT auth| Core\n    Core --&gt; Manager\n    Manager --&gt;|docker run / stop / rm| K1\n    Manager --&gt;|docker run / stop / rm| K2\n    Core --&gt;|HTTP ExecuteRequest| K1\n    Core --&gt;|HTTP ExecuteRequest| K2\n    K1 --&gt;|log callback| Core\n    K2 --&gt;|log callback| Core\n    K1 --- Shared\n    K2 --- Shared\n    Core --- Shared</code></pre>"},{"location":"for-developers/kernel-architecture.html#kernel-manager","title":"Kernel Manager","text":"<p>Location: <code>flowfile_core/flowfile_core/kernel/manager.py</code></p> <p>The <code>KernelManager</code> is a singleton that runs inside the Core service. It manages all kernel containers for all users.</p>"},{"location":"for-developers/kernel-architecture.html#container-lifecycle","title":"Container Lifecycle","text":"Operation What Happens Create Allocates a <code>KernelInfo</code> record, persists config to the database Start Verifies the <code>flowfile-kernel</code> Docker image exists, runs <code>docker.containers.run()</code>, polls <code>/health</code> until ready (120s timeout) Execute Serializes inputs to parquet, sends <code>ExecuteRequest</code> via HTTP, tracks kernel state Stop Stops and removes the Docker container Delete Stops if running, removes from in-memory registry and database Shutdown Called on Core shutdown \u2014 stops all running kernel containers"},{"location":"for-developers/kernel-architecture.html#port-allocation","title":"Port Allocation","text":"<ul> <li>Local mode: Allocates ports from the range 19000\u201319999. Each kernel gets a unique host port mapped to container port 9999.</li> <li>Docker-in-Docker mode: Skips port allocation entirely. Containers communicate via container names on the shared Docker network (<code>flowfile-network</code>).</li> </ul>"},{"location":"for-developers/kernel-architecture.html#docker-in-docker-support","title":"Docker-in-Docker Support","text":"<p>When the Core service itself runs inside Docker (e.g., via <code>docker compose</code>), the manager auto-detects:</p> <ul> <li>The Docker network to attach kernel containers to</li> <li>The named volume covering the shared storage path</li> <li>Whether to use container names or <code>localhost:port</code> for communication</li> </ul> <p>This is configured automatically \u2014 no manual setup required beyond mounting the Docker socket.</p>"},{"location":"for-developers/kernel-architecture.html#environment-variables-passed-to-kernels","title":"Environment Variables Passed to Kernels","text":"<pre><code>KERNEL_PACKAGES=\"{space-separated packages}\"\nFLOWFILE_CORE_URL=\"{core service URL}\"\nFLOWFILE_INTERNAL_TOKEN=\"{auth token}\"\nFLOWFILE_KERNEL_ID=\"{kernel_id}\"\nFLOWFILE_HOST_SHARED_DIR=\"{host path}\"          # local mode only\nFLOWFILE_KERNEL_SHARED_DIR=\"{container path}\"\nPERSISTENCE_ENABLED=\"true|false\"\nPERSISTENCE_PATH=\"{path to artifact storage}\"\nRECOVERY_MODE=\"lazy|eager|clear\"\n</code></pre>"},{"location":"for-developers/kernel-architecture.html#kernel-runtime","title":"Kernel Runtime","text":"<p>Location: <code>kernel_runtime/kernel_runtime/main.py</code></p> <p>The runtime is a FastAPI application (port 9999) that runs inside each Docker container. It executes user code and manages artifacts.</p>"},{"location":"for-developers/kernel-architecture.html#execution-flow","title":"Execution Flow","text":"<p>When the Core sends an <code>ExecuteRequest</code>:</p> <ol> <li>Namespace creation \u2014 Creates or reuses a persistent Python namespace for the flow (Jupyter-style cell execution)</li> <li>Artifact cleanup \u2014 Clears artifacts from previous executions of the same node</li> <li>Context setup \u2014 Sets up the <code>flowfile</code> API context (paths, artifact store, auth token)</li> <li>Code execution \u2014 Runs user code in a worker thread via <code>asyncio.to_thread()</code></li> <li>Output capture \u2014 Collects stdout, stderr, display outputs, and artifact metadata</li> <li>Response \u2014 Returns <code>ExecuteResult</code> with all outputs back to Core</li> </ol> View execution sequence diagram <pre><code>sequenceDiagram\n    participant Core as Core API\n    participant Manager as Kernel Manager\n    participant Runtime as Kernel Runtime\n    participant Code as User Code\n\n    Core-&gt;&gt;Manager: execute(kernel_id, code, inputs)\n    Manager-&gt;&gt;Manager: Write inputs to parquet\n    Manager-&gt;&gt;Runtime: POST /execute (ExecuteRequest)\n    Runtime-&gt;&gt;Runtime: Get/create namespace for flow\n    Runtime-&gt;&gt;Runtime: Clear previous node artifacts\n    Runtime-&gt;&gt;Code: exec(code, namespace)\n    Code-&gt;&gt;Runtime: flowfile.read_input()\n    Runtime--&gt;&gt;Code: pl.LazyFrame\n    Code-&gt;&gt;Code: Transform data\n    Code-&gt;&gt;Runtime: flowfile.publish_output(df)\n    Code-&gt;&gt;Runtime: flowfile.display(fig)\n    Code-&gt;&gt;Core: flowfile.log(\"message\") [HTTP callback]\n    Runtime--&gt;&gt;Manager: ExecuteResult\n    Manager--&gt;&gt;Core: ExecuteResult</code></pre>"},{"location":"for-developers/kernel-architecture.html#thread-based-execution","title":"Thread-based Execution","text":"<p>User code runs in a dedicated thread so the FastAPI event loop stays responsive. The thread ID is tracked to enable cancellation:</p> <ul> <li>HTTP interrupt: <code>POST /interrupt</code> injects <code>KeyboardInterrupt</code> via <code>PyThreadState_SetAsyncExc()</code></li> <li>Signal fallback: Docker <code>kill -SIGUSR1</code> for blocking C extensions</li> </ul>"},{"location":"for-developers/kernel-architecture.html#persistent-namespaces","title":"Persistent Namespaces","text":"<p>Each flow gets its own Python namespace dictionary (like a Jupyter kernel). Variables defined in one execution are available in subsequent executions of the same flow. Namespaces are stored in an LRU cache (default: 20 flows) to bound memory.</p>"},{"location":"for-developers/kernel-architecture.html#artifact-system","title":"Artifact System","text":""},{"location":"for-developers/kernel-architecture.html#in-memory-store","title":"In-Memory Store","text":"<p>Location: <code>kernel_runtime/kernel_runtime/artifact_store.py</code></p> <p>The <code>ArtifactStore</code> is a thread-safe, flow-scoped, in-memory store with optional disk persistence:</p> <ul> <li>Flow isolation \u2014 Artifacts are keyed by <code>(flow_id, name)</code>, preventing cross-flow conflicts</li> <li>Lazy loading \u2014 Disk-persisted artifacts can be loaded into memory on first access</li> <li>LRU eviction \u2014 Prevents unbounded memory growth for lazy-indexed artifacts</li> <li>Per-key locks \u2014 Lazy loading uses per-key locks to avoid blocking the global lock during I/O</li> </ul>"},{"location":"for-developers/kernel-architecture.html#disk-persistence","title":"Disk Persistence","text":"<p>Location: <code>kernel_runtime/kernel_runtime/artifact_persistence.py</code></p> <p>When persistence is enabled, artifacts are written to disk using cloudpickle:</p> <pre><code>{persistence_path}/{flow_id}/{artifact_name}/\n  \u251c\u2500\u2500 data.artifact     # cloudpickle-serialized object\n  \u2514\u2500\u2500 meta.json         # JSON metadata + SHA-256 checksum\n</code></pre> <p>SHA-256 checksums validate data integrity on load. Path components are sanitized to prevent directory traversal.</p>"},{"location":"for-developers/kernel-architecture.html#serialization","title":"Serialization","text":"<p>Location: <code>kernel_runtime/kernel_runtime/serialization.py</code></p> <p>Format is auto-detected based on object type:</p> Object Type Format Rationale Polars / Pandas DataFrame Parquet Efficient columnar storage scikit-learn, NumPy, SciPy, XGBoost, LightGBM, CatBoost Joblib Optimized for ML objects Everything else Cloudpickle Handles closures and dynamic classes <p>A pre-serialization check (<code>check_pickleable</code>) validates that objects can be serialized before committing to API calls, providing clear error messages for common issues (lambdas, local classes, open file handles).</p>"},{"location":"for-developers/kernel-architecture.html#flowfile-client-api","title":"Flowfile Client API","text":"<p>Location: <code>kernel_runtime/kernel_runtime/flowfile_client.py</code></p> <p>This module provides the <code>flowfile</code> namespace available to user code. It uses <code>contextvars</code> for thread-safe execution context management.</p>"},{"location":"for-developers/kernel-architecture.html#path-translation","title":"Path Translation","text":"<p>In local Docker mode, the Core API returns paths using the host filesystem. The client translates these to the container's <code>/shared</code> mount:</p> <pre><code>Host:      /Users/you/.flowfile/shared/data/input.parquet\nContainer: /shared/data/input.parquet\n</code></pre> <p>In Docker-in-Docker mode, paths are identical (same named volume).</p>"},{"location":"for-developers/kernel-architecture.html#global-artifact-flow","title":"Global Artifact Flow","text":"<p>Publishing to the global catalog follows a three-step protocol:</p> <ol> <li>Prepare \u2014 Kernel calls <code>POST /artifacts/prepare-upload</code> to get a staging path (shared filesystem) or presigned URL (S3)</li> <li>Serialize \u2014 Object is written directly to the staging location</li> <li>Finalize \u2014 Kernel calls <code>POST /artifacts/finalize</code> with checksum and size; Core moves the file to permanent storage</li> </ol>"},{"location":"for-developers/kernel-architecture.html#core-side-components","title":"Core-side Components","text":""},{"location":"for-developers/kernel-architecture.html#api-routes","title":"API Routes","text":"<p>Location: <code>flowfile_core/flowfile_core/kernel/routes.py</code></p> <p>All endpoints require JWT authentication and enforce user ownership:</p> Endpoint Description <code>GET /kernels/</code> List user's kernels <code>POST /kernels/</code> Create a kernel <code>POST /kernels/{id}/start</code> Start kernel container <code>POST /kernels/{id}/stop</code> Stop kernel container <code>DELETE /kernels/{id}</code> Delete kernel <code>POST /kernels/{id}/execute</code> Execute code <code>POST /kernels/{id}/execute_cell</code> Execute in interactive mode <code>POST /kernels/{id}/interrupt</code> Cancel execution <code>GET /kernels/{id}/artifacts</code> List artifacts <code>POST /kernels/{id}/clear</code> Clear all artifacts <code>GET /kernels/{id}/display_outputs</code> Get display outputs <code>GET /kernels/{id}/memory</code> Get memory usage <code>GET /kernels/docker-status</code> Check Docker availability"},{"location":"for-developers/kernel-architecture.html#database-persistence","title":"Database Persistence","text":"<p>Location: <code>flowfile_core/flowfile_core/kernel/persistence.py</code></p> <p>Kernel configurations (id, name, packages, CPU, memory, GPU) are persisted to the SQLAlchemy database. Runtime state (container ID, port, process state) is ephemeral and reconstructed at startup by reclaiming running containers.</p>"},{"location":"for-developers/kernel-architecture.html#data-models","title":"Data Models","text":"<p>Location: <code>flowfile_core/flowfile_core/kernel/models.py</code></p> <p>Key models:</p> <ul> <li><code>KernelConfig</code> \u2014 Input for creating a kernel</li> <li><code>KernelInfo</code> \u2014 Full kernel state including runtime info</li> <li><code>KernelState</code> \u2014 Enum: <code>STOPPED</code>, <code>STARTING</code>, <code>IDLE</code>, <code>EXECUTING</code>, <code>ERROR</code></li> <li><code>ExecuteRequest</code> / <code>ExecuteResult</code> \u2014 Code execution request and response</li> <li><code>DisplayOutput</code> \u2014 Rendered visualization (mime_type + data)</li> </ul>"},{"location":"for-developers/kernel-architecture.html#security-model","title":"Security Model","text":""},{"location":"for-developers/kernel-architecture.html#authentication","title":"Authentication","text":"Boundary Mechanism Frontend \u2192 Core JWT tokens (user login) Core \u2192 Kernel Internal token (<code>X-Internal-Token</code> header) Kernel \u2192 Core (callbacks) Same internal token <p>The internal token is passed per-request in the <code>ExecuteRequest</code> and also set as an environment variable in the container for fallback.</p>"},{"location":"for-developers/kernel-architecture.html#sandboxing","title":"Sandboxing","text":"<ul> <li>Process isolation \u2014 Each kernel runs in its own Docker container</li> <li>Resource limits \u2014 Memory (<code>mem_limit</code>) and CPU (<code>nano_cpus</code>) are enforced by Docker</li> <li>Filesystem isolation \u2014 Only the shared volume is mounted; the host filesystem is not accessible</li> <li>User ownership \u2014 Each kernel is owned by the user who created it; other users cannot access it</li> </ul>"},{"location":"for-developers/kernel-architecture.html#trust-boundaries","title":"Trust Boundaries","text":"<p>Artifact serialization uses pickle/cloudpickle. This is acceptable because:</p> <ul> <li>Artifacts are only written by user code running inside the kernel</li> <li>Users already have arbitrary code execution privileges</li> <li>No untrusted external data flows into the artifact store</li> </ul>"},{"location":"for-developers/kernel-architecture.html#docker-setup","title":"Docker Setup","text":""},{"location":"for-developers/kernel-architecture.html#building-the-kernel-image","title":"Building the Kernel Image","text":"<pre><code># Via docker compose\ndocker compose build flowfile-kernel\n\n# Or directly\ndocker build -t flowfile-kernel -f kernel_runtime/Dockerfile kernel_runtime/\n</code></pre> <p>The image is based on <code>python:3.12-slim</code> and includes:</p> <ul> <li>Data stack: Polars, PyArrow, NumPy</li> <li>ML stack: scikit-learn, Joblib</li> <li>Serialization: cloudpickle</li> <li>Server: FastAPI + Uvicorn</li> </ul>"},{"location":"for-developers/kernel-architecture.html#docker-compose-integration","title":"Docker Compose Integration","text":"<p>The <code>docker-compose.yml</code> includes a build-only service for the kernel image:</p> <pre><code>flowfile-kernel:\n  build:\n    context: kernel_runtime\n    dockerfile: Dockerfile\n  image: flowfile-kernel\n  entrypoint: [\"true\"]\n  restart: \"no\"\n  profiles:\n    - kernel\n</code></pre> <p>This service is not started by <code>docker compose up</code> \u2014 it only builds the image. The Core service creates kernel containers dynamically via the Docker API.</p> <p>Docker Socket</p> <p>The Core service requires access to the Docker socket (<code>/var/run/docker.sock</code>) to manage kernel containers. In production, consider using a Docker socket proxy (e.g., <code>tecnativa/docker-socket-proxy</code>) to restrict API access.</p>"},{"location":"for-developers/kernel-architecture.html#network-configuration","title":"Network Configuration","text":"<p>The Docker network uses a fixed name (<code>flowfile-network</code>) so that dynamically created kernel containers can join it:</p> <pre><code>networks:\n  flowfile-network:\n    driver: bridge\n    name: flowfile-network\n</code></pre>"},{"location":"for-developers/kernel-architecture.html#testing","title":"Testing","text":""},{"location":"for-developers/kernel-architecture.html#unit-tests","title":"Unit Tests","text":"<pre><code># Kernel runtime unit tests\npip install -e \"kernel_runtime/[test]\"\npython -m pytest kernel_runtime/tests -v\n</code></pre>"},{"location":"for-developers/kernel-architecture.html#integration-tests-docker-required","title":"Integration Tests (Docker Required)","text":"<pre><code># Build the kernel image first\ndocker build -t flowfile-kernel -f kernel_runtime/Dockerfile kernel_runtime/\n\n# Run kernel integration tests\npoetry run pytest flowfile_core/tests -m kernel -v\n</code></pre> <p>The <code>kernel</code> pytest marker identifies tests that require Docker. These tests are skipped in environments without Docker and run in a separate CI job.</p>"},{"location":"for-developers/kernel-architecture.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Kernel Execution \u2014 User guide for writing kernel code</li> <li>Architecture \u2014 Overall Flowfile architecture</li> <li>Docker Deployment \u2014 Docker compose reference</li> </ul>"},{"location":"for-developers/python-api-reference.html","title":"Flowfile Core API Reference","text":"<p>This section provides a detailed API reference for the core Python objects, data models, and API routes in <code>flowfile-core</code>. The documentation is generated directly from the source code docstrings.</p>"},{"location":"for-developers/python-api-reference.html#core-components","title":"Core Components","text":"<p>This section covers the fundamental classes that manage the state and execution of data pipelines. These are the main \"verbs\" of the library.</p>"},{"location":"for-developers/python-api-reference.html#flowgraph","title":"FlowGraph","text":"<p>The <code>FlowGraph</code> is the central object that orchestrates the execution of data transformations. It is built incrementally as you chain operations. This DAG (Directed Acyclic Graph) represents the entire pipeline.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph","title":"<code>flowfile_core.flowfile.flow_graph.FlowGraph</code>","text":"<p>A class representing a Directed Acyclic Graph (DAG) for data processing pipelines.</p> <p>It manages nodes, connections, and the execution of the entire flow.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes a new FlowGraph instance.</p> <code>__repr__</code> <p>Provides the official string representation of the FlowGraph instance.</p> <code>add_cloud_storage_reader</code> <p>Adds a cloud storage read node to the flow graph.</p> <code>add_cloud_storage_writer</code> <p>Adds a node to write data to a cloud storage provider.</p> <code>add_cross_join</code> <p>Adds a cross join node to the graph.</p> <code>add_database_reader</code> <p>Adds a node to read data from a database.</p> <code>add_database_writer</code> <p>Adds a node to write data to a database.</p> <code>add_datasource</code> <p>Adds a data source node to the graph.</p> <code>add_dependency_on_polars_lazy_frame</code> <p>Adds a special node that directly injects a Polars LazyFrame into the graph.</p> <code>add_explore_data</code> <p>Adds a specialized node for data exploration and visualization.</p> <code>add_external_source</code> <p>Adds a node for a custom external data source.</p> <code>add_filter</code> <p>Adds a filter node to the graph.</p> <code>add_formula</code> <p>Adds a node that applies a formula to create or modify a column.</p> <code>add_fuzzy_match</code> <p>Adds a fuzzy matching node to join data on approximate string matches.</p> <code>add_graph_solver</code> <p>Adds a node that solves graph-like problems within the data.</p> <code>add_group_by</code> <p>Adds a group-by aggregation node to the graph.</p> <code>add_include_cols</code> <p>Adds columns to both the input and output column lists.</p> <code>add_initial_node_analysis</code> <p>Adds a data exploration/analysis node based on a node promise.</p> <code>add_join</code> <p>Adds a join node to combine two data streams based on key columns.</p> <code>add_manual_input</code> <p>Adds a node for manual data entry.</p> <code>add_node_promise</code> <p>Adds a placeholder node to the graph that is not yet fully configured.</p> <code>add_node_step</code> <p>The core method for adding or updating a node in the graph.</p> <code>add_node_to_starting_list</code> <p>Adds a node to the list of starting nodes for the flow if not already present.</p> <code>add_output</code> <p>Adds an output node to write the final data to a destination.</p> <code>add_pivot</code> <p>Adds a pivot node to the graph.</p> <code>add_polars_code</code> <p>Adds a node that executes custom Polars code.</p> <code>add_python_script</code> <p>Adds a node that executes Python code on a kernel container.</p> <code>add_read</code> <p>Adds a node to read data from a local file (e.g., CSV, Parquet, Excel).</p> <code>add_record_count</code> <p>Adds a filter node to the graph.</p> <code>add_record_id</code> <p>Adds a node to create a new column with a unique ID for each record.</p> <code>add_sample</code> <p>Adds a node to take a random or top-N sample of the data.</p> <code>add_select</code> <p>Adds a node to select, rename, reorder, or drop columns.</p> <code>add_sort</code> <p>Adds a node to sort the data based on one or more columns.</p> <code>add_sql_source</code> <p>Adds a node that reads data from a SQL source.</p> <code>add_text_to_rows</code> <p>Adds a node that splits cell values into multiple rows.</p> <code>add_union</code> <p>Adds a union node to combine multiple data streams.</p> <code>add_unique</code> <p>Adds a node to find and remove duplicate rows.</p> <code>add_unpivot</code> <p>Adds an unpivot node to the graph.</p> <code>add_user_defined_node</code> <p>Adds a user-defined custom node to the graph.</p> <code>apply_layout</code> <p>Calculates and applies a layered layout to all nodes in the graph.</p> <code>cancel</code> <p>Cancels an ongoing graph execution.</p> <code>capture_history_if_changed</code> <p>Capture history only if the flow state actually changed.</p> <code>capture_history_snapshot</code> <p>Capture the current state before a change for undo support.</p> <code>close_flow</code> <p>Performs cleanup operations, such as clearing node caches.</p> <code>copy_node</code> <p>Creates a copy of an existing node.</p> <code>delete_node</code> <p>Deletes a node from the graph and updates all its connections.</p> <code>generate_code</code> <p>Generates code for the flow graph.</p> <code>get_frontend_data</code> <p>Formats the graph structure into a JSON-like dictionary for a specific legacy frontend.</p> <code>get_history_state</code> <p>Get the current state of the history system.</p> <code>get_implicit_starter_nodes</code> <p>Finds nodes that can act as starting points but are not explicitly defined as such.</p> <code>get_node</code> <p>Retrieves a node from the graph by its ID.</p> <code>get_node_data</code> <p>Retrieves all data needed to render a node in the UI.</p> <code>get_node_storage</code> <p>Serializes the entire graph's state into a storable format.</p> <code>get_nodes_overview</code> <p>Gets a list of dictionary representations for all nodes in the graph.</p> <code>get_run_info</code> <p>Gets a summary of the most recent graph execution.</p> <code>get_vue_flow_input</code> <p>Formats the graph's nodes and edges into a schema suitable for the VueFlow frontend.</p> <code>print_tree</code> <p>Print flow_graph as a visual tree structure, showing the DAG relationships with ASCII art.</p> <code>redo</code> <p>Redo the last undone action.</p> <code>remove_from_output_cols</code> <p>Removes specified columns from the list of expected output columns.</p> <code>reset</code> <p>Forces a deep reset on all nodes in the graph.</p> <code>restore_from_snapshot</code> <p>Clear current state and rebuild from a snapshot.</p> <code>run_graph</code> <p>Executes the entire data flow graph from start to finish.</p> <code>save_flow</code> <p>Saves the current state of the flow graph to a file.</p> <code>trigger_fetch_node</code> <p>Executes a specific node in the graph by its ID.</p> <code>undo</code> <p>Undo the last action by restoring to the previous state.</p> <p>Attributes:</p> Name Type Description <code>execution_location</code> <code>ExecutionLocationsLiteral</code> <p>Gets the current execution location.</p> <code>execution_mode</code> <code>ExecutionModeLiteral</code> <p>Gets the current execution mode ('Development' or 'Performance').</p> <code>flow_id</code> <code>int</code> <p>Gets the unique identifier of the flow.</p> <code>graph_has_functions</code> <code>bool</code> <p>Checks if the graph has any nodes.</p> <code>graph_has_input_data</code> <code>bool</code> <p>Checks if the graph has an initial input data source.</p> <code>node_connections</code> <code>list[tuple[int, int]]</code> <p>Computes and returns a list of all connections in the graph.</p> <code>nodes</code> <code>list[FlowNode]</code> <p>Gets a list of all FlowNode objects in the graph.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>class FlowGraph:\n    \"\"\"A class representing a Directed Acyclic Graph (DAG) for data processing pipelines.\n\n    It manages nodes, connections, and the execution of the entire flow.\n    \"\"\"\n\n    uuid: str\n    depends_on: dict[\n        int,\n        Union[\n            ParquetFile,\n            FlowDataEngine,\n            \"FlowGraph\",\n            pl.DataFrame,\n        ],\n    ]\n    _flow_id: int\n    _input_data: Union[ParquetFile, FlowDataEngine, \"FlowGraph\"]\n    _input_cols: list[str]\n    _output_cols: list[str]\n    _node_db: dict[str | int, FlowNode]\n    _node_ids: list[str | int]\n    _results: FlowDataEngine | None = None\n    cache_results: bool = False\n    schema: list[FlowfileColumn] | None = None\n    has_over_row_function: bool = False\n    _flow_starts: list[int | str] = None\n    latest_run_info: RunInformation | None = None\n    start_datetime: datetime = None\n    end_datetime: datetime = None\n    _flow_settings: schemas.FlowSettings = None\n    flow_logger: FlowLogger\n\n    def __init__(\n        self,\n        flow_settings: schemas.FlowSettings | schemas.FlowGraphConfig,\n        name: str = None,\n        input_cols: list[str] = None,\n        output_cols: list[str] = None,\n        path_ref: str = None,\n        input_flow: Union[ParquetFile, FlowDataEngine, \"FlowGraph\"] = None,\n        cache_results: bool = False,\n    ):\n        \"\"\"Initializes a new FlowGraph instance.\n\n        Args:\n            flow_settings: The configuration settings for the flow.\n            name: The name of the flow.\n            input_cols: A list of input column names.\n            output_cols: A list of output column names.\n            path_ref: An optional path to an initial data source.\n            input_flow: An optional existing data object to start the flow with.\n            cache_results: A global flag to enable or disable result caching.\n        \"\"\"\n        if isinstance(flow_settings, schemas.FlowGraphConfig):\n            flow_settings = schemas.FlowSettings.from_flow_settings_input(flow_settings)\n\n        self._flow_settings = flow_settings\n        self.uuid = str(uuid1())\n        self.start_datetime = None\n        self.end_datetime = None\n        self.latest_run_info = None\n        self._flow_id = flow_settings.flow_id\n        self.flow_logger = FlowLogger(flow_settings.flow_id)\n        self._flow_starts: list[FlowNode] = []\n        self._results = None\n        self.schema = None\n        self.has_over_row_function = False\n        self._input_cols = [] if input_cols is None else input_cols\n        self._output_cols = [] if output_cols is None else output_cols\n        self._node_ids = []\n        self._node_db = {}\n        self.cache_results = cache_results\n        self.__name__ = name if name else \"flow_\" + str(id(self))\n        self.depends_on = {}\n        self.artifact_context = ArtifactContext()\n\n        # Initialize history manager for undo/redo support\n        from flowfile_core.flowfile.history_manager import HistoryManager\n        from flowfile_core.schemas.history_schema import HistoryConfig\n\n        history_config = HistoryConfig(enabled=flow_settings.track_history)\n        self._history_manager = HistoryManager(config=history_config)\n\n        if path_ref is not None:\n            self.add_datasource(input_schema.NodeDatasource(file_path=path_ref))\n        elif input_flow is not None:\n            self.add_datasource(input_file=input_flow)\n\n    @property\n    def flow_settings(self) -&gt; schemas.FlowSettings:\n        return self._flow_settings\n\n    @flow_settings.setter\n    def flow_settings(self, flow_settings: schemas.FlowSettings):\n        if (self._flow_settings.execution_location != flow_settings.execution_location) or (\n            self._flow_settings.execution_mode != flow_settings.execution_mode\n        ):\n            self.reset()\n        self._flow_settings = flow_settings\n\n    # ==================== History Management Methods ====================\n\n    def capture_history_snapshot(\n        self,\n        action_type: HistoryActionType,\n        description: str,\n        node_id: int = None,\n    ) -&gt; bool:\n        \"\"\"Capture the current state before a change for undo support.\n\n        Args:\n            action_type: The type of action being performed.\n            description: Human-readable description of the action.\n            node_id: Optional ID of the affected node.\n\n        Returns:\n            True if snapshot was captured, False if skipped.\n        \"\"\"\n        return self._history_manager.capture_snapshot(self, action_type, description, node_id)\n\n    def capture_history_if_changed(\n        self,\n        pre_snapshot: schemas.FlowfileData,\n        action_type: HistoryActionType,\n        description: str,\n        node_id: int = None,\n    ) -&gt; bool:\n        \"\"\"Capture history only if the flow state actually changed.\n\n        Use this for settings updates where the change might be a no-op.\n        Call this AFTER the change is applied.\n\n        Args:\n            pre_snapshot: The FlowfileData captured BEFORE the change.\n            action_type: The type of action that was performed.\n            description: Human-readable description of the action.\n            node_id: Optional ID of the affected node.\n\n        Returns:\n            True if a change was detected and snapshot was captured.\n        \"\"\"\n        return self._history_manager.capture_if_changed(self, pre_snapshot, action_type, description, node_id)\n\n    def undo(self) -&gt; UndoRedoResult:\n        \"\"\"Undo the last action by restoring to the previous state.\n\n        Returns:\n            UndoRedoResult indicating success or failure.\n        \"\"\"\n        return self._history_manager.undo(self)\n\n    def redo(self) -&gt; UndoRedoResult:\n        \"\"\"Redo the last undone action.\n\n        Returns:\n            UndoRedoResult indicating success or failure.\n        \"\"\"\n        return self._history_manager.redo(self)\n\n    def get_history_state(self) -&gt; HistoryState:\n        \"\"\"Get the current state of the history system.\n\n        Returns:\n            HistoryState with information about available undo/redo operations.\n        \"\"\"\n        return self._history_manager.get_state()\n\n    def _execute_with_history(\n        self,\n        operation: Callable[[], Any],\n        action_type: HistoryActionType,\n        description: str,\n        node_id: int = None,\n    ) -&gt; Any:\n        \"\"\"Execute an operation with automatic history capture.\n\n        This helper captures the state before the operation, executes it,\n        and records history only if the state actually changed.\n\n        Args:\n            operation: A callable that performs the actual operation.\n            action_type: The type of action being performed.\n            description: Human-readable description of the action.\n            node_id: Optional ID of the affected node.\n\n        Returns:\n            The result of the operation (if any).\n        \"\"\"\n        # Skip history capture if tracking is disabled for this flow\n        if not self.flow_settings.track_history:\n            return operation()\n\n        pre_snapshot = self.get_flowfile_data()\n        result = operation()\n        self._history_manager.capture_if_changed(self, pre_snapshot, action_type, description, node_id)\n        return result\n\n    def restore_from_snapshot(self, snapshot: schemas.FlowfileData) -&gt; None:\n        \"\"\"Clear current state and rebuild from a snapshot.\n\n        This method is used internally by undo/redo to restore a previous state.\n\n        Args:\n            snapshot: The FlowfileData snapshot to restore from.\n        \"\"\"\n        from flowfile_core.flowfile.manage.io_flowfile import (\n            _flowfile_data_to_flow_information,\n            determine_insertion_order,\n        )\n\n        # Preserve the current flow_id and source_registration_id\n        original_flow_id = self._flow_id\n        original_source_registration_id = self._flow_settings.source_registration_id\n\n        # Convert snapshot to FlowInformation\n        flow_info = _flowfile_data_to_flow_information(snapshot)\n\n        # Clear current state\n        self._node_db.clear()\n        self._node_ids.clear()\n        self._flow_starts.clear()\n        self._results = None\n\n        # Restore flow settings (preserve original flow_id and source_registration_id)\n        self._flow_settings = flow_info.flow_settings\n        self._flow_settings.flow_id = original_flow_id\n        self._flow_id = original_flow_id\n        if self._flow_settings.source_registration_id is None:\n            self._flow_settings.source_registration_id = original_source_registration_id\n        self.__name__ = flow_info.flow_name or self.__name__\n\n        # Determine node insertion order\n        ingestion_order = determine_insertion_order(flow_info)\n\n        # First pass: Create all nodes as promises\n        for node_id in ingestion_order:\n            node_info = flow_info.data[node_id]\n            node_promise = input_schema.NodePromise(\n                flow_id=original_flow_id,\n                node_id=node_info.id,\n                pos_x=node_info.x_position or 0,\n                pos_y=node_info.y_position or 0,\n                node_type=node_info.type,\n            )\n            if hasattr(node_info.setting_input, \"cache_results\"):\n                node_promise.cache_results = node_info.setting_input.cache_results\n            self.add_node_promise(node_promise)\n\n        # Second pass: Apply settings using add_&lt;node_type&gt; methods\n        for node_id in ingestion_order:\n            node_info = flow_info.data[node_id]\n            if node_info.is_setup and node_info.setting_input is not None:\n                # Update flow_id in setting_input\n                if hasattr(node_info.setting_input, \"flow_id\"):\n                    node_info.setting_input.flow_id = original_flow_id\n\n                if hasattr(node_info.setting_input, \"is_user_defined\") and node_info.setting_input.is_user_defined:\n                    if node_info.type in CUSTOM_NODE_STORE:\n                        user_defined_node_class = CUSTOM_NODE_STORE[node_info.type]\n                        self.add_user_defined_node(\n                            custom_node=user_defined_node_class.from_settings(node_info.setting_input.settings),\n                            user_defined_node_settings=node_info.setting_input,\n                        )\n                else:\n                    add_method = getattr(self, \"add_\" + node_info.type, None)\n                    if add_method:\n                        add_method(node_info.setting_input)\n\n        # Third pass: Restore connections\n        for node_id in ingestion_order:\n            node_info = flow_info.data[node_id]\n            from_node = self.get_node(node_id)\n            if from_node is None:\n                continue\n\n            for output_node_id in node_info.outputs or []:\n                to_node = self.get_node(output_node_id)\n                if to_node is None:\n                    continue\n\n                output_node_info = flow_info.data.get(output_node_id)\n                if output_node_info is None:\n                    continue\n\n                # Determine connection type\n                is_left_input = (output_node_info.left_input_id == node_id) and (\n                    to_node.left_input is None or to_node.left_input.node_id != node_id\n                )\n                is_right_input = (output_node_info.right_input_id == node_id) and (\n                    to_node.right_input is None or to_node.right_input.node_id != node_id\n                )\n                is_main_input = node_id in (output_node_info.input_ids or [])\n\n                if is_left_input:\n                    insert_type = \"left\"\n                elif is_right_input:\n                    insert_type = \"right\"\n                elif is_main_input:\n                    insert_type = \"main\"\n                else:\n                    continue\n\n                to_node.add_node_connection(from_node, insert_type)\n\n        logger.info(f\"Restored flow from snapshot with {len(self._node_db)} nodes\")\n\n    # ==================== End History Management Methods ====================\n\n    def add_node_to_starting_list(self, node: FlowNode) -&gt; None:\n        \"\"\"Adds a node to the list of starting nodes for the flow if not already present.\n\n        Args:\n            node: The FlowNode to add as a starting node.\n        \"\"\"\n        if node.node_id not in {self_node.node_id for self_node in self._flow_starts}:\n            self._flow_starts.append(node)\n\n    def add_node_promise(self, node_promise: input_schema.NodePromise, track_history: bool = True):\n        \"\"\"Adds a placeholder node to the graph that is not yet fully configured.\n\n        Useful for building the graph structure before all settings are available.\n        Automatically captures history for undo/redo support.\n\n        Args:\n            node_promise: A promise object containing basic node information.\n            track_history: Whether to track this change in history (default True).\n        \"\"\"\n\n        def _do_add():\n            def placeholder(n: FlowNode = None):\n                if n is None:\n                    return FlowDataEngine()\n                return n\n\n            self.add_node_step(\n                node_id=node_promise.node_id,\n                node_type=node_promise.node_type,\n                function=placeholder,\n                setting_input=node_promise,\n            )\n            if node_promise.is_user_defined:\n                node_needs_settings: bool\n                custom_node = CUSTOM_NODE_STORE.get(node_promise.node_type)\n                if custom_node is None:\n                    raise Exception(f\"Custom node type '{node_promise.node_type}' not found in registry.\")\n                settings_schema = custom_node.model_fields[\"settings_schema\"].default\n                node_needs_settings = settings_schema is not None and not settings_schema.is_empty()\n                if not node_needs_settings:\n                    user_defined_node_settings = input_schema.UserDefinedNode(settings={}, **node_promise.model_dump())\n                    initialized_model = custom_node()\n                    self.add_user_defined_node(\n                        custom_node=initialized_model, user_defined_node_settings=user_defined_node_settings\n                    )\n\n        if track_history:\n            self._execute_with_history(\n                _do_add,\n                HistoryActionType.ADD_NODE,\n                f\"Add {node_promise.node_type} node\",\n                node_id=node_promise.node_id,\n            )\n        else:\n            _do_add()\n\n    def apply_layout(self, y_spacing: int = 150, x_spacing: int = 200, initial_y: int = 100):\n        \"\"\"Calculates and applies a layered layout to all nodes in the graph.\n\n        This updates their x and y positions for UI rendering.\n\n        Args:\n            y_spacing: The vertical spacing between layers.\n            x_spacing: The horizontal spacing between nodes in the same layer.\n            initial_y: The initial y-position for the first layer.\n        \"\"\"\n        self.flow_logger.info(\"Applying layered layout...\")\n        start_time = time()\n        try:\n            # Calculate new positions for all nodes\n            new_positions = calculate_layered_layout(\n                self, y_spacing=y_spacing, x_spacing=x_spacing, initial_y=initial_y\n            )\n\n            if not new_positions:\n                self.flow_logger.warning(\"Layout calculation returned no positions.\")\n                return\n\n            # Apply the new positions to the setting_input of each node\n            updated_count = 0\n            for node_id, (pos_x, pos_y) in new_positions.items():\n                node = self.get_node(node_id)\n                if node and hasattr(node, \"setting_input\"):\n                    setting = node.setting_input\n                    if hasattr(setting, \"pos_x\") and hasattr(setting, \"pos_y\"):\n                        setting.pos_x = pos_x\n                        setting.pos_y = pos_y\n                        updated_count += 1\n                    else:\n                        self.flow_logger.warning(\n                            f\"Node {node_id} setting_input ({type(setting)}) lacks pos_x/pos_y attributes.\"\n                        )\n                elif node:\n                    self.flow_logger.warning(f\"Node {node_id} lacks setting_input attribute.\")\n                # else: Node not found, already warned by calculate_layered_layout\n\n            end_time = time()\n            self.flow_logger.info(\n                f\"Layout applied to {updated_count}/{len(self.nodes)} nodes in {end_time - start_time:.2f} seconds.\"\n            )\n\n        except Exception as e:\n            self.flow_logger.error(f\"Error applying layout: {e}\")\n            raise  # Optional: re-raise the exception\n\n    @property\n    def flow_id(self) -&gt; int:\n        \"\"\"Gets the unique identifier of the flow.\"\"\"\n        return self._flow_id\n\n    @flow_id.setter\n    def flow_id(self, new_id: int):\n        \"\"\"Sets the unique identifier for the flow and updates all child nodes.\n\n        Args:\n            new_id: The new flow ID.\n        \"\"\"\n        self._flow_id = new_id\n        for node in self.nodes:\n            if hasattr(node.setting_input, \"flow_id\"):\n                node.setting_input.flow_id = new_id\n        self.flow_settings.flow_id = new_id\n\n    def __repr__(self):\n        \"\"\"Provides the official string representation of the FlowGraph instance.\"\"\"\n        settings_str = \"  -\" + \"\\n  -\".join(f\"{k}: {v}\" for k, v in self.flow_settings)\n        return f\"FlowGraph(\\nNodes: {self._node_db}\\n\\nSettings:\\n{settings_str}\"\n\n    def print_tree(self):\n        \"\"\"Print flow_graph as a visual tree structure, showing the DAG relationships with ASCII art.\"\"\"\n        if not self._node_db:\n            self.flow_logger.info(\"Empty flow graph\")\n            return\n\n        # Build node information\n        node_info = build_node_info(self.nodes)\n\n        # Calculate depths for all nodes\n        for node_id in node_info:\n            calculate_depth(node_id, node_info)\n\n        # Group nodes by depth\n        depth_groups, max_depth = group_nodes_by_depth(node_info)\n\n        # Sort nodes within each depth group\n        for depth in depth_groups:\n            depth_groups[depth].sort()\n\n        # Create the main flow visualization\n        lines = [\"=\" * 80, \"Flow Graph Visualization\", \"=\" * 80, \"\"]\n\n        # Track which nodes connect to what\n        merge_points = define_node_connections(node_info)\n\n        # Build the flow paths\n\n        # Find the maximum label length for each depth level\n        max_label_length = {}\n        for depth in range(max_depth + 1):\n            if depth in depth_groups:\n                max_len = max(len(node_info[nid].label) for nid in depth_groups[depth])\n                max_label_length[depth] = max_len\n\n        # Draw the paths\n        drawn_nodes = set()\n        merge_drawn = set()\n\n        # Group paths by their merge points\n        paths_by_merge = {}\n        standalone_paths = []\n\n        # Build flow paths\n        paths = build_flow_paths(node_info, self._flow_starts, merge_points)\n\n        # Define paths to merge and standalone paths\n        for path in paths:\n            if len(path) &gt; 1 and path[-1] in merge_points and len(merge_points[path[-1]]) &gt; 1:\n                merge_id = path[-1]\n                if merge_id not in paths_by_merge:\n                    paths_by_merge[merge_id] = []\n                paths_by_merge[merge_id].append(path)\n            else:\n                standalone_paths.append(path)\n\n        # Draw merged paths\n        draw_merged_paths(node_info, merge_points, paths_by_merge, merge_drawn, drawn_nodes, lines)\n\n        # Draw standlone paths\n        draw_standalone_paths(drawn_nodes, standalone_paths, lines, node_info)\n\n        # Add undrawn nodes\n        add_un_drawn_nodes(drawn_nodes, node_info, lines)\n\n        try:\n            execution_plan = compute_execution_plan(\n                nodes=self.nodes, flow_starts=self._flow_starts + self.get_implicit_starter_nodes()\n            )\n            ordered_nodes = execution_plan.all_nodes\n            if ordered_nodes:\n                for i, node in enumerate(ordered_nodes, 1):\n                    lines.append(f\"  {i:3d}. {node_info[node.node_id].label}\")\n        except Exception as e:\n            lines.append(f\"  Could not determine execution order: {e}\")\n\n        # Print everything\n        output = \"\\n\".join(lines)\n\n        print(output)\n\n    def get_nodes_overview(self):\n        \"\"\"Gets a list of dictionary representations for all nodes in the graph.\"\"\"\n        output = []\n        for v in self._node_db.values():\n            output.append(v.get_repr())\n        return output\n\n    def remove_from_output_cols(self, columns: list[str]):\n        \"\"\"Removes specified columns from the list of expected output columns.\n\n        Args:\n            columns: A list of column names to remove.\n        \"\"\"\n        cols = set(columns)\n        self._output_cols = [c for c in self._output_cols if c not in cols]\n\n    def get_node(self, node_id: int | str = None) -&gt; FlowNode | None:\n        \"\"\"Retrieves a node from the graph by its ID.\n\n        Args:\n            node_id: The ID of the node to retrieve. If None, retrieves the last added node.\n\n        Returns:\n            The FlowNode object, or None if not found.\n        \"\"\"\n        if node_id is None:\n            node_id = self._node_ids[-1]\n        node = self._node_db.get(node_id)\n        if node is not None:\n            return node\n\n    def add_user_defined_node(\n        self, *, custom_node: CustomNodeBase, user_defined_node_settings: input_schema.UserDefinedNode\n    ):\n        \"\"\"Adds a user-defined custom node to the graph.\n\n        When the custom node has a ``kernel_id`` set, the process code is sent\n        to the kernel for execution instead of running locally.  This enables\n        custom nodes to use external packages installed on the kernel.\n\n        Args:\n            custom_node: The custom node instance to add.\n            user_defined_node_settings: The settings for the user-defined node.\n        \"\"\"\n        # Enforce kernel selection when executing a kernel-required custom node\n        if custom_node.requires_kernel and not user_defined_node_settings.kernel_id:\n            raise ValueError(\"Kernel selection is required to execute this custom node.\")\n\n        # Propagate kernel_id from the schema settings if present\n        kernel_id = user_defined_node_settings.kernel_id or custom_node.kernel_id\n        output_names = user_defined_node_settings.output_names or custom_node.output_names\n\n        if kernel_id:\n            _func = self._make_kernel_user_defined_func(\n                custom_node=custom_node,\n                user_defined_node_settings=user_defined_node_settings,\n                kernel_id=kernel_id,\n                output_names=output_names,\n            )\n        else:\n            _func = self._make_local_user_defined_func(\n                custom_node=custom_node,\n                user_defined_node_settings=user_defined_node_settings,\n            )\n\n        self.add_node_step(\n            node_id=user_defined_node_settings.node_id,\n            function=_func,\n            setting_input=user_defined_node_settings,\n            input_node_ids=user_defined_node_settings.depending_on_ids,\n            node_type=custom_node.item,\n        )\n        if custom_node.number_of_inputs == 0:\n            node = self.get_node(user_defined_node_settings.node_id)\n            self.add_node_to_starting_list(node)\n\n    def _make_local_user_defined_func(\n        self, *, custom_node: CustomNodeBase, user_defined_node_settings: input_schema.UserDefinedNode\n    ) -&gt; Callable:\n        \"\"\"Create the execution function for a locally-executed custom node.\"\"\"\n\n        def _func(*flow_data_engine: FlowDataEngine) -&gt; FlowDataEngine | None:\n            user_id = user_defined_node_settings.user_id\n            if user_id is not None:\n                custom_node.set_execution_context(user_id)\n                if custom_node.settings_schema:\n                    custom_node.settings_schema.set_secret_context(user_id, custom_node.accessed_secrets)\n\n            output = custom_node.process(*(fde.data_frame for fde in flow_data_engine))\n\n            accessed_secrets = custom_node.get_accessed_secrets()\n            if accessed_secrets:\n                logger.info(f\"Node '{user_defined_node_settings.node_id}' accessed secrets: {accessed_secrets}\")\n            if isinstance(output, (pl.LazyFrame, pl.DataFrame)):\n                return FlowDataEngine(output)\n            return None\n\n        return _func\n\n    def _make_kernel_user_defined_func(\n        self,\n        *,\n        custom_node: CustomNodeBase,\n        user_defined_node_settings: input_schema.UserDefinedNode,\n        kernel_id: str,\n        output_names: list[str],\n    ) -&gt; Callable:\n        \"\"\"Create the execution function for a kernel-executed custom node.\n\n        Follows the same pattern as ``add_python_script``: writes inputs to\n        shared parquet files, generates the kernel code from the custom node's\n        process method, executes on the kernel, and reads back the outputs.\n        \"\"\"\n\n        def _func(*flow_data_engine: FlowDataEngine) -&gt; FlowDataEngine | None:\n            manager = get_kernel_manager()\n            node_id = user_defined_node_settings.node_id\n            flow_id = self.flow_id\n            node_logger = self.flow_logger.get_node_logger(node_id)\n\n            # Compute available artifacts\n            upstream_ids = self._get_upstream_node_ids(node_id)\n            self.artifact_context.compute_available(\n                node_id=node_id,\n                kernel_id=kernel_id,\n                upstream_node_ids=upstream_ids,\n            )\n\n            # Prepare shared directories\n            shared_base = manager.shared_volume_path\n            input_dir = os.path.join(shared_base, str(flow_id), str(node_id), \"inputs\")\n            output_dir = os.path.join(shared_base, str(flow_id), str(node_id), \"outputs\")\n            os.makedirs(input_dir, exist_ok=True)\n            os.makedirs(output_dir, exist_ok=True)\n\n            # Write inputs to parquet\n            input_paths = write_inputs_to_parquet(flow_data_engine, manager, input_dir, flow_id, node_id)\n\n            # Generate the kernel code from the custom node's process method\n            code = custom_node.generate_kernel_code()\n\n            # Build request and execute on the kernel\n            request = build_execute_request(\n                node_id=node_id,\n                code=code,\n                input_paths=input_paths,\n                output_dir=output_dir,\n                flow_id=flow_id,\n                manager=manager,\n                source_registration_id=self._flow_settings.source_registration_id,\n            )\n\n            node = self.get_node(node_id)\n            cancel_event = threading.Event()\n            if node is not None:\n                node._kernel_cancel_context = (kernel_id, manager)\n                node._kernel_cancel_event = cancel_event\n            try:\n                result = manager.execute_sync(\n                    kernel_id, request, self.flow_logger, cancel_event=cancel_event\n                )\n            finally:\n                if node is not None:\n                    node._kernel_cancel_context = None\n                    node._kernel_cancel_event = None\n\n            # Forward stdout/stderr\n            forward_kernel_logs(result, node_logger)\n\n            if not result.success:\n                raise RuntimeError(f\"Kernel execution failed: {result.error}\")\n\n            # Record artifacts\n            if result.artifacts_published:\n                self.artifact_context.record_published(\n                    node_id=node_id,\n                    kernel_id=kernel_id,\n                    artifacts=[{\"name\": n} for n in result.artifacts_published],\n                )\n            if result.artifacts_deleted:\n                self.artifact_context.record_deleted(\n                    node_id=node_id,\n                    kernel_id=kernel_id,\n                    artifact_names=result.artifacts_deleted,\n                )\n\n            # Read outputs and populate named outputs on the FlowNode\n            node = self.get_node(node_id)\n            primary_result: FlowDataEngine | None = None\n            for i, name in enumerate(output_names):\n                output_path = os.path.join(output_dir, f\"{name}.parquet\")\n                if os.path.exists(output_path):\n                    fde = FlowDataEngine(pl.scan_parquet(output_path))\n                    handle = f\"output-{i}\"\n                    if node is not None:\n                        node._named_outputs[handle] = fde\n                    if i == 0:\n                        primary_result = fde\n\n            if primary_result is not None:\n                return primary_result\n\n            # No output published \u2014 pass through first input\n            return flow_data_engine[0] if flow_data_engine else FlowDataEngine(pl.LazyFrame())\n\n        return _func\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_pivot(self, pivot_settings: input_schema.NodePivot):\n        \"\"\"Adds a pivot node to the graph.\n\n        Args:\n            pivot_settings: The settings for the pivot operation.\n        \"\"\"\n\n        def _func(fl: FlowDataEngine):\n            return fl.do_pivot(pivot_settings.pivot_input, self.flow_logger.get_node_logger(pivot_settings.node_id))\n\n        self.add_node_step(\n            node_id=pivot_settings.node_id,\n            function=_func,\n            node_type=\"pivot\",\n            setting_input=pivot_settings,\n            input_node_ids=[pivot_settings.depending_on_id],\n        )\n\n        node = self.get_node(pivot_settings.node_id)\n\n        def schema_callback():\n            input_data = node.singular_main_input.get_resulting_data()  # get from the previous step the data\n            input_data.lazy = True  # ensure the dataset is lazy\n            input_lf = input_data.data_frame  # get the lazy frame\n            return pre_calculate_pivot_schema(input_data.schema, pivot_settings.pivot_input, input_lf=input_lf)\n\n        node.schema_callback = schema_callback\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_unpivot(self, unpivot_settings: input_schema.NodeUnpivot):\n        \"\"\"Adds an unpivot node to the graph.\n\n        Args:\n            unpivot_settings: The settings for the unpivot operation.\n        \"\"\"\n\n        def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n            return fl.unpivot(unpivot_settings.unpivot_input)\n\n        self.add_node_step(\n            node_id=unpivot_settings.node_id,\n            function=_func,\n            node_type=\"unpivot\",\n            setting_input=unpivot_settings,\n            input_node_ids=[unpivot_settings.depending_on_id],\n        )\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_union(self, union_settings: input_schema.NodeUnion):\n        \"\"\"Adds a union node to combine multiple data streams.\n\n        Args:\n            union_settings: The settings for the union operation.\n        \"\"\"\n\n        def _func(*flowfile_tables: FlowDataEngine):\n            dfs: list[pl.LazyFrame] | list[pl.DataFrame] = [flt.data_frame for flt in flowfile_tables]\n            return FlowDataEngine(pl.concat(dfs, how=\"diagonal_relaxed\"))\n\n        self.add_node_step(\n            node_id=union_settings.node_id,\n            function=_func,\n            node_type=\"union\",\n            setting_input=union_settings,\n            input_node_ids=union_settings.depending_on_ids,\n        )\n\n    def add_initial_node_analysis(self, node_promise: input_schema.NodePromise, track_history: bool = True):\n        \"\"\"Adds a data exploration/analysis node based on a node promise.\n\n        Automatically captures history for undo/redo support.\n\n        Args:\n            node_promise: The promise representing the node to be analyzed.\n            track_history: Whether to track this change in history (default True).\n        \"\"\"\n\n        def _do_add():\n            node_analysis = create_graphic_walker_node_from_node_promise(node_promise)\n            self.add_explore_data(node_analysis)\n\n        if track_history:\n            self._execute_with_history(\n                _do_add,\n                HistoryActionType.ADD_NODE,\n                f\"Add {node_promise.node_type} node\",\n                node_id=node_promise.node_id,\n            )\n        else:\n            _do_add()\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_explore_data(self, node_analysis: input_schema.NodeExploreData):\n        \"\"\"Adds a specialized node for data exploration and visualization.\n\n        Args:\n            node_analysis: The settings for the data exploration node.\n        \"\"\"\n        sample_size: int = 10000\n\n        def analysis_preparation(flowfile_table: FlowDataEngine):\n            if flowfile_table.number_of_records &lt;= 0:\n                number_of_records = flowfile_table.get_number_of_records(calculate_in_worker_process=True)\n            else:\n                number_of_records = flowfile_table.number_of_records\n            if number_of_records &gt; sample_size:\n                flowfile_table = flowfile_table.get_sample(sample_size, random=True)\n            external_sampler = ExternalDfFetcher(\n                lf=flowfile_table.data_frame,\n                file_ref=\"__gf_walker\" + node.hash,\n                wait_on_completion=True,\n                node_id=node.node_id,\n                flow_id=self.flow_id,\n            )\n            node.results.analysis_data_generator = get_read_top_n(\n                external_sampler.status.file_ref, n=min(sample_size, number_of_records)\n            )\n            return flowfile_table\n\n        def schema_callback():\n            node = self.get_node(node_analysis.node_id)\n            if len(node.all_inputs) == 1:\n                input_node = node.all_inputs[0]\n                return input_node.schema\n            else:\n                return [FlowfileColumn.from_input(\"col_1\", \"na\")]\n\n        self.add_node_step(\n            node_id=node_analysis.node_id,\n            node_type=\"explore_data\",\n            function=analysis_preparation,\n            setting_input=node_analysis,\n            schema_callback=schema_callback,\n        )\n        node = self.get_node(node_analysis.node_id)\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_group_by(self, group_by_settings: input_schema.NodeGroupBy):\n        \"\"\"Adds a group-by aggregation node to the graph.\n\n        Args:\n            group_by_settings: The settings for the group-by operation.\n        \"\"\"\n\n        def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n            return fl.do_group_by(group_by_settings.groupby_input, False)\n\n        self.add_node_step(\n            node_id=group_by_settings.node_id,\n            function=_func,\n            node_type=\"group_by\",\n            setting_input=group_by_settings,\n            input_node_ids=[group_by_settings.depending_on_id],\n        )\n\n        node = self.get_node(group_by_settings.node_id)\n\n        def schema_callback():\n            output_columns = [(c.old_name, c.new_name, c.output_type) for c in group_by_settings.groupby_input.agg_cols]\n            depends_on = node.node_inputs.main_inputs[0]\n            input_schema_dict: dict[str, str] = {s.name: s.data_type for s in depends_on.schema}\n            output_schema = []\n            for old_name, new_name, data_type in output_columns:\n                data_type = input_schema_dict[old_name] if data_type is None else data_type\n                output_schema.append(FlowfileColumn.from_input(data_type=data_type, column_name=new_name))\n            return output_schema\n\n        node.schema_callback = schema_callback\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_filter(self, filter_settings: input_schema.NodeFilter):\n        \"\"\"Adds a filter node to the graph.\n\n        Args:\n            filter_settings: The settings for the filter operation.\n        \"\"\"\n\n        def _func(fl: FlowDataEngine):\n            is_advanced = filter_settings.filter_input.is_advanced()\n\n            if is_advanced:\n                predicate = filter_settings.filter_input.advanced_filter\n                return fl.do_filter(predicate)\n            else:\n                basic_filter = filter_settings.filter_input.basic_filter\n                if basic_filter is None:\n                    logger.warning(\"Basic filter is None, returning unfiltered data\")\n                    return fl\n\n                try:\n                    field_data_type = fl.get_schema_column(basic_filter.field).generic_datatype()\n                except Exception:\n                    field_data_type = None\n\n                expression = build_filter_expression(basic_filter, field_data_type)\n                filter_settings.filter_input.advanced_filter = expression\n                return fl.do_filter(expression)\n\n        self.add_node_step(\n            filter_settings.node_id,\n            _func,\n            node_type=\"filter\",\n            renew_schema=False,\n            setting_input=filter_settings,\n            input_node_ids=[filter_settings.depending_on_id],\n        )\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_record_count(self, node_number_of_records: input_schema.NodeRecordCount):\n        \"\"\"Adds a filter node to the graph.\n\n        Args:\n            node_number_of_records: The settings for the record count operation.\n        \"\"\"\n\n        def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n            return fl.get_record_count()\n\n        self.add_node_step(\n            node_id=node_number_of_records.node_id,\n            function=_func,\n            node_type=\"record_count\",\n            setting_input=node_number_of_records,\n            input_node_ids=[node_number_of_records.depending_on_id],\n        )\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_polars_code(self, node_polars_code: input_schema.NodePolarsCode):\n        \"\"\"Adds a node that executes custom Polars code.\n\n        Args:\n            node_polars_code: The settings for the Polars code node.\n        \"\"\"\n\n        def _func(*flowfile_tables: FlowDataEngine) -&gt; FlowDataEngine:\n            return execute_polars_code(*flowfile_tables, code=node_polars_code.polars_code_input.polars_code)\n\n        self.add_node_step(\n            node_id=node_polars_code.node_id,\n            function=_func,\n            node_type=\"polars_code\",\n            setting_input=node_polars_code,\n            input_node_ids=node_polars_code.depending_on_ids,\n        )\n\n        try:\n            polars_code_parser.validate_code(node_polars_code.polars_code_input.polars_code)\n        except Exception as e:\n            node = self.get_node(node_id=node_polars_code.node_id)\n            node.results.errors = str(e)\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_python_script(self, node_python_script: input_schema.NodePythonScript):\n        \"\"\"Adds a node that executes Python code on a kernel container.\"\"\"\n\n        def _func(*flowfile_tables: FlowDataEngine) -&gt; FlowDataEngine:\n            kernel_id = node_python_script.python_script_input.kernel_id\n            code = node_python_script.python_script_input.code\n\n            if not kernel_id:\n                raise ValueError(\"No kernel selected for python_script node\")\n\n            manager = get_kernel_manager()\n            node_id = node_python_script.node_id\n            flow_id = self.flow_id\n            node_logger = self.flow_logger.get_node_logger(node_id)\n\n            # 1. Make upstream artifacts visible to the kernel\n            self.artifact_context.compute_available(\n                node_id=node_id,\n                kernel_id=kernel_id,\n                upstream_node_ids=self._get_upstream_node_ids(node_id),\n            )\n\n            # 2. Write input tables to the shared volume\n            shared_base = manager.shared_volume_path\n            input_dir = os.path.join(shared_base, str(flow_id), str(node_id), \"inputs\")\n            output_dir = os.path.join(shared_base, str(flow_id), str(node_id), \"outputs\")\n            os.makedirs(input_dir, exist_ok=True)\n            os.makedirs(output_dir, exist_ok=True)\n            self.flow_logger.info(f\"Prepared shared directories for kernel execution: {input_dir}, {output_dir}\")\n\n            input_paths = write_inputs_to_parquet(flowfile_tables, manager, input_dir, flow_id, node_id)\n\n            # 3. Build request and execute on the kernel\n            request = build_execute_request(\n                node_id=node_id,\n                code=code,\n                input_paths=input_paths,\n                output_dir=output_dir,\n                flow_id=flow_id,\n                manager=manager,\n                source_registration_id=self._flow_settings.source_registration_id,\n            )\n\n            node = self.get_node(node_id)\n            cancel_event = threading.Event()\n            node._kernel_cancel_context = (kernel_id, manager)\n            node._kernel_cancel_event = cancel_event\n            try:\n                result = manager.execute_sync(\n                    kernel_id, request, self.flow_logger, cancel_event=cancel_event\n                )\n            finally:\n                node._kernel_cancel_context = None\n                node._kernel_cancel_event = None\n\n            # 4. Forward kernel stdout/stderr and check for errors\n            forward_kernel_logs(result, node_logger)\n            if not result.success:\n                raise RuntimeError(f\"Kernel execution failed: {result.error}\")\n\n            # 5. Record artifact changes\n            if result.artifacts_published:\n                self.artifact_context.record_published(\n                    node_id=node_id,\n                    kernel_id=kernel_id,\n                    artifacts=[{\"name\": n} for n in result.artifacts_published],\n                )\n            if result.artifacts_deleted:\n                self.artifact_context.record_deleted(\n                    node_id=node_id,\n                    kernel_id=kernel_id,\n                    artifact_names=result.artifacts_deleted,\n                )\n\n            # 6. Read output parquet or pass through first input\n            output_path = os.path.join(output_dir, \"main.parquet\")\n            if os.path.exists(output_path):\n                return FlowDataEngine(pl.scan_parquet(output_path))\n            return flowfile_tables[0] if flowfile_tables else FlowDataEngine(pl.LazyFrame())\n\n        def schema_callback():\n            \"\"\"Best-effort schema prediction for python_script nodes.\n\n            Returns the input node(s) schema as a reasonable default\n            (most python_script nodes transform and pass through).\n            If nothing is available, returns [] \u2014 never raises.\n            \"\"\"\n            try:\n                node = self.get_node(node_python_script.node_id)\n                if node is None:\n                    return []\n\n                main_inputs = node.node_inputs.main_inputs\n                if main_inputs:\n                    first_input = main_inputs[0]\n                    input_node_schema = first_input.schema\n                    if input_node_schema:\n                        return input_node_schema\n                return []\n            except Exception:\n                return []\n\n        self.add_node_step(\n            node_id=node_python_script.node_id,\n            function=_func,\n            node_type=\"python_script\",\n            setting_input=node_python_script,\n            input_node_ids=node_python_script.depending_on_ids,\n            schema_callback=schema_callback,\n        )\n\n    def add_dependency_on_polars_lazy_frame(self, lazy_frame: pl.LazyFrame, node_id: int):\n        \"\"\"Adds a special node that directly injects a Polars LazyFrame into the graph.\n\n        Note: This is intended for backend use and will not work in the UI editor.\n\n        Args:\n            lazy_frame: The Polars LazyFrame to inject.\n            node_id: The ID for the new node.\n        \"\"\"\n\n        def _func():\n            return FlowDataEngine(lazy_frame)\n\n        node_promise = input_schema.NodePromise(\n            flow_id=self.flow_id, node_id=node_id, node_type=\"polars_lazy_frame\", is_setup=True\n        )\n        self.add_node_step(\n            node_id=node_promise.node_id, node_type=node_promise.node_type, function=_func, setting_input=node_promise\n        )\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_unique(self, unique_settings: input_schema.NodeUnique):\n        \"\"\"Adds a node to find and remove duplicate rows.\n\n        Args:\n            unique_settings: The settings for the unique operation.\n        \"\"\"\n\n        def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n            return fl.make_unique(unique_settings.unique_input)\n\n        self.add_node_step(\n            node_id=unique_settings.node_id,\n            function=_func,\n            input_columns=[],\n            node_type=\"unique\",\n            setting_input=unique_settings,\n            input_node_ids=[unique_settings.depending_on_id],\n        )\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_graph_solver(self, graph_solver_settings: input_schema.NodeGraphSolver):\n        \"\"\"Adds a node that solves graph-like problems within the data.\n\n        This node can be used for operations like finding network paths,\n        calculating connected components, or performing other graph algorithms\n        on relational data that represents nodes and edges.\n\n        Args:\n            graph_solver_settings: The settings object defining the graph inputs\n                and the specific algorithm to apply.\n        \"\"\"\n\n        def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n            return fl.solve_graph(graph_solver_settings.graph_solver_input)\n\n        self.add_node_step(\n            node_id=graph_solver_settings.node_id,\n            function=_func,\n            node_type=\"graph_solver\",\n            setting_input=graph_solver_settings,\n            input_node_ids=[graph_solver_settings.depending_on_id],\n        )\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_formula(self, function_settings: input_schema.NodeFormula):\n        \"\"\"Adds a node that applies a formula to create or modify a column.\n\n        Args:\n            function_settings: The settings for the formula operation.\n        \"\"\"\n\n        error = \"\"\n        if function_settings.function.field.data_type not in (None, transform_schema.AUTO_DATA_TYPE):\n            output_type = cast_str_to_polars_type(function_settings.function.field.data_type)\n        else:\n            output_type = None\n        if output_type not in (None, transform_schema.AUTO_DATA_TYPE):\n            new_col = [\n                FlowfileColumn.from_input(column_name=function_settings.function.field.name, data_type=str(output_type))\n            ]\n        else:\n            new_col = [FlowfileColumn.from_input(function_settings.function.field.name, \"String\")]\n\n        def _func(fl: FlowDataEngine):\n            return fl.apply_sql_formula(\n                func=function_settings.function.function,\n                col_name=function_settings.function.field.name,\n                output_data_type=output_type,\n            )\n\n        self.add_node_step(\n            function_settings.node_id,\n            _func,\n            output_schema=new_col,\n            node_type=\"formula\",\n            renew_schema=False,\n            setting_input=function_settings,\n            input_node_ids=[function_settings.depending_on_id],\n        )\n        if error != \"\":\n            node = self.get_node(function_settings.node_id)\n            node.results.errors = error\n            return False, error\n        else:\n            return True, \"\"\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_cross_join(self, cross_join_settings: input_schema.NodeCrossJoin) -&gt; \"FlowGraph\":\n        \"\"\"Adds a cross join node to the graph.\n\n        Args:\n            cross_join_settings: The settings for the cross join operation.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n\n        def _func(main: FlowDataEngine, right: FlowDataEngine) -&gt; FlowDataEngine:\n            for left_select in cross_join_settings.cross_join_input.left_select.renames:\n                left_select.is_available = True if left_select.old_name in main.schema else False\n            for right_select in cross_join_settings.cross_join_input.right_select.renames:\n                right_select.is_available = True if right_select.old_name in right.schema else False\n            return main.do_cross_join(\n                cross_join_input=cross_join_settings.cross_join_input,\n                auto_generate_selection=cross_join_settings.auto_generate_selection,\n                verify_integrity=False,\n                other=right,\n            )\n\n        self.add_node_step(\n            node_id=cross_join_settings.node_id,\n            function=_func,\n            input_columns=[],\n            node_type=\"cross_join\",\n            setting_input=cross_join_settings,\n            input_node_ids=cross_join_settings.depending_on_ids,\n        )\n        return self\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_join(self, join_settings: input_schema.NodeJoin) -&gt; \"FlowGraph\":\n        \"\"\"Adds a join node to combine two data streams based on key columns.\n\n        Args:\n            join_settings: The settings for the join operation.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n\n        def _func(main: FlowDataEngine, right: FlowDataEngine) -&gt; FlowDataEngine:\n            for left_select in join_settings.join_input.left_select.renames:\n                left_select.is_available = True if left_select.old_name in main.schema else False\n            for right_select in join_settings.join_input.right_select.renames:\n                right_select.is_available = True if right_select.old_name in right.schema else False\n            return main.join(\n                join_input=join_settings.join_input,\n                auto_generate_selection=join_settings.auto_generate_selection,\n                verify_integrity=False,\n                other=right,\n            )\n\n        self.add_node_step(\n            node_id=join_settings.node_id,\n            function=_func,\n            input_columns=[],\n            node_type=\"join\",\n            setting_input=join_settings,\n            input_node_ids=join_settings.depending_on_ids,\n        )\n        return self\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_fuzzy_match(self, fuzzy_settings: input_schema.NodeFuzzyMatch) -&gt; \"FlowGraph\":\n        \"\"\"Adds a fuzzy matching node to join data on approximate string matches.\n\n        Args:\n            fuzzy_settings: The settings for the fuzzy match operation.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n\n        def _func(main: FlowDataEngine, right: FlowDataEngine) -&gt; FlowDataEngine:\n            node = self.get_node(node_id=fuzzy_settings.node_id)\n            if self.execution_location == \"local\":\n                return main.fuzzy_join(\n                    fuzzy_match_input=deepcopy(fuzzy_settings.join_input),\n                    other=right,\n                    node_logger=self.flow_logger.get_node_logger(fuzzy_settings.node_id),\n                )\n\n            f = main.start_fuzzy_join(\n                fuzzy_match_input=deepcopy(fuzzy_settings.join_input),\n                other=right,\n                file_ref=node.hash,\n                flow_id=self.flow_id,\n                node_id=fuzzy_settings.node_id,\n            )\n            logger.info(\"Started the fuzzy match action\")\n            node._fetch_cached_df = f  # Add to the node so it can be cancelled and fetch later if needed\n            return FlowDataEngine(f.get_result())\n\n        def schema_callback():\n            fm_input_copy = FuzzyMatchInputManager(\n                fuzzy_settings.join_input\n            )  # Deepcopy create an unique object per func\n            node = self.get_node(node_id=fuzzy_settings.node_id)\n            return calculate_fuzzy_match_schema(\n                fm_input_copy,\n                left_schema=node.node_inputs.main_inputs[0].schema,\n                right_schema=node.node_inputs.right_input.schema,\n            )\n\n        self.add_node_step(\n            node_id=fuzzy_settings.node_id,\n            function=_func,\n            input_columns=[],\n            node_type=\"fuzzy_match\",\n            setting_input=fuzzy_settings,\n            input_node_ids=fuzzy_settings.depending_on_ids,\n            schema_callback=schema_callback,\n        )\n\n        return self\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_text_to_rows(self, node_text_to_rows: input_schema.NodeTextToRows) -&gt; \"FlowGraph\":\n        \"\"\"Adds a node that splits cell values into multiple rows.\n\n        This is useful for un-nesting data where a single field contains multiple\n        values separated by a delimiter.\n\n        Args:\n            node_text_to_rows: The settings object that specifies the column to split\n                and the delimiter to use.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n\n        def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n            return table.split(node_text_to_rows.text_to_rows_input)\n\n        self.add_node_step(\n            node_id=node_text_to_rows.node_id,\n            function=_func,\n            node_type=\"text_to_rows\",\n            setting_input=node_text_to_rows,\n            input_node_ids=[node_text_to_rows.depending_on_id],\n        )\n        return self\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_sort(self, sort_settings: input_schema.NodeSort) -&gt; \"FlowGraph\":\n        \"\"\"Adds a node to sort the data based on one or more columns.\n\n        Args:\n            sort_settings: The settings for the sort operation.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n\n        def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n            return table.do_sort(sort_settings.sort_input)\n\n        self.add_node_step(\n            node_id=sort_settings.node_id,\n            function=_func,\n            node_type=\"sort\",\n            setting_input=sort_settings,\n            input_node_ids=[sort_settings.depending_on_id],\n        )\n        return self\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_sample(self, sample_settings: input_schema.NodeSample) -&gt; \"FlowGraph\":\n        \"\"\"Adds a node to take a random or top-N sample of the data.\n\n        Args:\n            sample_settings: The settings object specifying the size of the sample.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n\n        def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n            return table.get_sample(sample_settings.sample_size)\n\n        self.add_node_step(\n            node_id=sample_settings.node_id,\n            function=_func,\n            node_type=\"sample\",\n            setting_input=sample_settings,\n            input_node_ids=[sample_settings.depending_on_id],\n        )\n        return self\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_record_id(self, record_id_settings: input_schema.NodeRecordId) -&gt; \"FlowGraph\":\n        \"\"\"Adds a node to create a new column with a unique ID for each record.\n\n        Args:\n            record_id_settings: The settings object specifying the name of the\n                new record ID column.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n\n        def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n            return table.add_record_id(record_id_settings.record_id_input)\n\n        self.add_node_step(\n            node_id=record_id_settings.node_id,\n            function=_func,\n            node_type=\"record_id\",\n            setting_input=record_id_settings,\n            input_node_ids=[record_id_settings.depending_on_id],\n        )\n        return self\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_select(self, select_settings: input_schema.NodeSelect) -&gt; \"FlowGraph\":\n        \"\"\"Adds a node to select, rename, reorder, or drop columns.\n\n        Args:\n            select_settings: The settings for the select operation.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n\n        select_cols = select_settings.select_input\n        drop_cols = tuple(s.old_name for s in select_settings.select_input)\n\n        def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n            input_cols = set(f.name for f in table.schema)\n            ids_to_remove = []\n            for i, select_col in enumerate(select_cols):\n                if select_col.data_type is None:\n                    select_col.data_type = table.get_schema_column(select_col.old_name).data_type\n                if select_col.old_name not in input_cols:\n                    select_col.is_available = False\n                    if not select_col.keep:\n                        ids_to_remove.append(i)\n                else:\n                    select_col.is_available = True\n            ids_to_remove.reverse()\n            for i in ids_to_remove:\n                v = select_cols.pop(i)\n                del v\n            return table.do_select(\n                select_inputs=transform_schema.SelectInputs(select_cols), keep_missing=select_settings.keep_missing\n            )\n\n        self.add_node_step(\n            node_id=select_settings.node_id,\n            function=_func,\n            input_columns=[],\n            node_type=\"select\",\n            drop_columns=list(drop_cols),\n            setting_input=select_settings,\n            input_node_ids=[select_settings.depending_on_id],\n        )\n        return self\n\n    @property\n    def graph_has_functions(self) -&gt; bool:\n        \"\"\"Checks if the graph has any nodes.\"\"\"\n        return len(self._node_ids) &gt; 0\n\n    def delete_node(self, node_id: int | str):\n        \"\"\"Deletes a node from the graph and updates all its connections.\n\n        Args:\n            node_id: The ID of the node to delete.\n\n        Raises:\n            Exception: If the node with the given ID does not exist.\n        \"\"\"\n        logger.info(f\"Starting deletion of node with ID: {node_id}\")\n\n        node = self._node_db.get(node_id)\n        if node:\n            logger.info(f\"Found node: {node_id}, processing deletion\")\n\n            lead_to_steps: list[FlowNode] = node.leads_to_nodes\n            logger.debug(f\"Node {node_id} leads to {len(lead_to_steps)} other nodes\")\n\n            if len(lead_to_steps) &gt; 0:\n                for lead_to_step in lead_to_steps:\n                    logger.debug(f\"Deleting input node {node_id} from dependent node {lead_to_step}\")\n                    lead_to_step.delete_input_node(node_id, complete=True)\n\n            if not node.is_start:\n                depends_on: list[FlowNode] = node.node_inputs.get_all_inputs()\n                logger.debug(f\"Node {node_id} depends on {len(depends_on)} other nodes\")\n\n                for depend_on in depends_on:\n                    logger.debug(f\"Removing lead_to reference {node_id} from node {depend_on}\")\n                    depend_on.delete_lead_to_node(node_id)\n\n            self._node_db.pop(node_id)\n            logger.debug(f\"Successfully removed node {node_id} from node_db\")\n            del node\n            logger.info(\"Node object deleted\")\n        else:\n            logger.error(f\"Failed to find node with id {node_id}\")\n            raise Exception(f\"Node with id {node_id} does not exist\")\n\n    @property\n    def graph_has_input_data(self) -&gt; bool:\n        \"\"\"Checks if the graph has an initial input data source.\"\"\"\n        return self._input_data is not None\n\n    def add_node_step(\n        self,\n        node_id: int | str,\n        function: Callable,\n        input_columns: list[str] = None,\n        output_schema: list[FlowfileColumn] = None,\n        node_type: str = None,\n        drop_columns: list[str] = None,\n        renew_schema: bool = True,\n        setting_input: Any = None,\n        cache_results: bool = None,\n        schema_callback: Callable = None,\n        input_node_ids: list[int] = None,\n    ) -&gt; FlowNode:\n        \"\"\"The core method for adding or updating a node in the graph.\n\n        Args:\n            node_id: The unique ID for the node.\n            function: The core processing function for the node.\n            input_columns: A list of input column names required by the function.\n            output_schema: A predefined schema for the node's output.\n            node_type: A string identifying the type of node (e.g., 'filter', 'join').\n            drop_columns: A list of columns to be dropped after the function executes.\n            renew_schema: If True, the schema is recalculated after execution.\n            setting_input: A configuration object containing settings for the node.\n            cache_results: If True, the node's results are cached for future runs.\n            schema_callback: A function that dynamically calculates the output schema.\n            input_node_ids: A list of IDs for the nodes that this node depends on.\n\n        Returns:\n            The created or updated FlowNode object.\n        \"\"\"\n        # Wrap schema_callback with output_field_config support\n        # If the node has output_field_config enabled, use it for schema prediction\n        output_field_config = getattr(setting_input, \"output_field_config\", None) if setting_input else None\n\n        logger.info(\n            f\"add_node_step: node_id={node_id}, node_type={node_type}, \"\n            f\"has_setting_input={setting_input is not None}, \"\n            f\"has_output_field_config={output_field_config is not None}, \"\n            f\"config_enabled={output_field_config.enabled if output_field_config else False}, \"\n            f\"has_schema_callback={schema_callback is not None}\"\n        )\n\n        # IMPORTANT: Always create wrapped callback if output_field_config exists (even if enabled=False)\n        # This ensures nodes like PolarsCode get a schema callback when output_field_config is defined\n        if output_field_config:\n            if output_field_config.enabled:\n                logger.info(\n                    f\"add_node_step: Creating/wrapping schema_callback for node {node_id} with output_field_config \"\n                    f\"(validation_mode={output_field_config.validation_mode_behavior}, {len(output_field_config.fields)} fields, \"\n                    f\"base_callback={'present' if schema_callback else 'None'})\"\n                )\n            else:\n                logger.debug(f\"add_node_step: output_field_config present for node {node_id} but disabled\")\n\n            # Even if schema_callback is None, create a wrapped one for output_field_config\n            schema_callback = create_schema_callback_with_output_config(schema_callback, output_field_config)\n            logger.info(\n                f\"add_node_step: schema_callback {'created' if schema_callback else 'failed'} for node {node_id}\"\n            )\n\n        existing_node = self.get_node(node_id)\n        if existing_node is not None:\n            if existing_node.node_type != node_type:\n                self.delete_node(existing_node.node_id)\n                existing_node = None\n        if existing_node:\n            input_nodes = existing_node.all_inputs\n        elif input_node_ids is not None:\n            input_nodes = [self.get_node(node_id) for node_id in input_node_ids]\n        else:\n            input_nodes = None\n        if isinstance(input_columns, str):\n            input_columns = [input_columns]\n        if (\n            input_nodes is not None\n            or function.__name__ in (\"placeholder\", \"analysis_preparation\")\n            or node_type in (\"cloud_storage_reader\", \"polars_lazy_frame\", \"input_data\")\n        ):\n            if not existing_node:\n                node = FlowNode(\n                    node_id=node_id,\n                    function=function,\n                    output_schema=output_schema,\n                    input_columns=input_columns,\n                    drop_columns=drop_columns,\n                    renew_schema=renew_schema,\n                    setting_input=setting_input,\n                    node_type=node_type,\n                    name=function.__name__,\n                    schema_callback=schema_callback,\n                    parent_uuid=self.uuid,\n                )\n            else:\n                existing_node.update_node(\n                    function=function,\n                    output_schema=output_schema,\n                    input_columns=input_columns,\n                    drop_columns=drop_columns,\n                    setting_input=setting_input,\n                    schema_callback=schema_callback,\n                )\n                node = existing_node\n        else:\n            raise Exception(\"No data initialized\")\n        self._node_db[node_id] = node\n        self._node_ids.append(node_id)\n        return node\n\n    def add_include_cols(self, include_columns: list[str]):\n        \"\"\"Adds columns to both the input and output column lists.\n\n        Args:\n            include_columns: A list of column names to include.\n        \"\"\"\n        for column in include_columns:\n            if column not in self._input_cols:\n                self._input_cols.append(column)\n            if column not in self._output_cols:\n                self._output_cols.append(column)\n        return self\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_output(self, output_file: input_schema.NodeOutput):\n        \"\"\"Adds an output node to write the final data to a destination.\n\n        Args:\n            output_file: The settings for the output file.\n        \"\"\"\n\n        def _func(df: FlowDataEngine):\n            execute_remote = self.execution_location != \"local\"\n            df.output(\n                output_fs=output_file.output_settings,\n                flow_id=self.flow_id,\n                node_id=output_file.node_id,\n                execute_remote=execute_remote,\n            )\n            return df\n\n        def schema_callback():\n            input_node: FlowNode = self.get_node(output_file.node_id).node_inputs.main_inputs[0]\n\n            return input_node.schema\n\n        input_node_id = output_file.depending_on_id if hasattr(output_file, \"depending_on_id\") else None\n        self.add_node_step(\n            node_id=output_file.node_id,\n            function=_func,\n            input_columns=[],\n            node_type=\"output\",\n            setting_input=output_file,\n            schema_callback=schema_callback,\n            input_node_ids=[input_node_id],\n        )\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_database_writer(self, node_database_writer: input_schema.NodeDatabaseWriter):\n        \"\"\"Adds a node to write data to a database.\n\n        Args:\n            node_database_writer: The settings for the database writer node.\n        \"\"\"\n\n        node_type = \"database_writer\"\n        database_settings: input_schema.DatabaseWriteSettings = node_database_writer.database_write_settings\n        database_connection: input_schema.DatabaseConnection | input_schema.FullDatabaseConnection | None\n        if database_settings.connection_mode == \"inline\":\n            database_connection: input_schema.DatabaseConnection = database_settings.database_connection\n            encrypted_password = get_encrypted_secret(\n                current_user_id=node_database_writer.user_id, secret_name=database_connection.password_ref\n            )\n            if encrypted_password is None:\n                raise HTTPException(status_code=400, detail=\"Password not found\")\n        else:\n            database_reference_settings = get_local_database_connection(\n                database_settings.database_connection_name, node_database_writer.user_id\n            )\n            encrypted_password = database_reference_settings.password.get_secret_value()\n\n        def _func(df: FlowDataEngine):\n            df.lazy = True\n            database_external_write_settings = (\n                sql_models.DatabaseExternalWriteSettings.create_from_from_node_database_writer(\n                    node_database_writer=node_database_writer,\n                    password=encrypted_password,\n                    table_name=(\n                        database_settings.schema_name + \".\" + database_settings.table_name\n                        if database_settings.schema_name\n                        else database_settings.table_name\n                    ),\n                    database_reference_settings=(\n                        database_reference_settings if database_settings.connection_mode == \"reference\" else None\n                    ),\n                    lf=df.data_frame,\n                )\n            )\n            external_database_writer = ExternalDatabaseWriter(\n                database_external_write_settings, wait_on_completion=False\n            )\n            node._fetch_cached_df = external_database_writer\n            external_database_writer.get_result()\n            return df\n\n        def schema_callback():\n            input_node: FlowNode = self.get_node(node_database_writer.node_id).node_inputs.main_inputs[0]\n            return input_node.schema\n\n        self.add_node_step(\n            node_id=node_database_writer.node_id,\n            function=_func,\n            input_columns=[],\n            node_type=node_type,\n            setting_input=node_database_writer,\n            schema_callback=schema_callback,\n        )\n        node = self.get_node(node_database_writer.node_id)\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_database_reader(self, node_database_reader: input_schema.NodeDatabaseReader):\n        \"\"\"Adds a node to read data from a database.\n\n        Args:\n            node_database_reader: The settings for the database reader node.\n        \"\"\"\n\n        logger.info(\"Adding database reader\")\n        node_type = \"database_reader\"\n        database_settings: input_schema.DatabaseSettings = node_database_reader.database_settings\n        database_connection: input_schema.DatabaseConnection | input_schema.FullDatabaseConnection | None\n        if database_settings.connection_mode == \"inline\":\n            database_connection: input_schema.DatabaseConnection = database_settings.database_connection\n            encrypted_password = get_encrypted_secret(\n                current_user_id=node_database_reader.user_id, secret_name=database_connection.password_ref\n            )\n            if encrypted_password is None:\n                raise HTTPException(status_code=400, detail=\"Password not found\")\n        else:\n            database_reference_settings = get_local_database_connection(\n                database_settings.database_connection_name, node_database_reader.user_id\n            )\n            database_connection = database_reference_settings\n            encrypted_password = database_reference_settings.password.get_secret_value()\n\n        def _func():\n            sql_source = BaseSqlSource(\n                query=None if database_settings.query_mode == \"table\" else database_settings.query,\n                table_name=database_settings.table_name,\n                schema_name=database_settings.schema_name,\n                fields=node_database_reader.fields,\n            )\n            database_external_read_settings = (\n                sql_models.DatabaseExternalReadSettings.create_from_from_node_database_reader(\n                    node_database_reader=node_database_reader,\n                    password=encrypted_password,\n                    query=sql_source.query,\n                    database_reference_settings=(\n                        database_reference_settings if database_settings.connection_mode == \"reference\" else None\n                    ),\n                )\n            )\n\n            external_database_fetcher = ExternalDatabaseFetcher(\n                database_external_read_settings, wait_on_completion=False\n            )\n            node._fetch_cached_df = external_database_fetcher\n            fl = FlowDataEngine(external_database_fetcher.get_result())\n            node_database_reader.fields = [c.get_minimal_field_info() for c in fl.schema]\n            return fl\n\n        def schema_callback():\n            sql_source = SqlSource(\n                connection_string=sql_utils.construct_sql_uri(\n                    database_type=database_connection.database_type,\n                    host=database_connection.host,\n                    port=database_connection.port,\n                    database=database_connection.database,\n                    username=database_connection.username,\n                    password=decrypt_secret(encrypted_password),\n                ),\n                query=None if database_settings.query_mode == \"table\" else database_settings.query,\n                table_name=database_settings.table_name,\n                schema_name=database_settings.schema_name,\n                fields=node_database_reader.fields,\n            )\n            return sql_source.get_schema()\n\n        node = self.get_node(node_database_reader.node_id)\n        if node:\n            node.node_type = node_type\n            node.name = node_type\n            node.function = _func\n            node.setting_input = node_database_reader\n            node.node_settings.cache_results = node_database_reader.cache_results\n            self.add_node_to_starting_list(node)\n            node.schema_callback = schema_callback\n        else:\n            node = FlowNode(\n                node_database_reader.node_id,\n                function=_func,\n                setting_input=node_database_reader,\n                name=node_type,\n                node_type=node_type,\n                parent_uuid=self.uuid,\n                schema_callback=schema_callback,\n            )\n            self._node_db[node_database_reader.node_id] = node\n            self.add_node_to_starting_list(node)\n            self._node_ids.append(node_database_reader.node_id)\n\n    def add_sql_source(self, external_source_input: input_schema.NodeExternalSource):\n        \"\"\"Adds a node that reads data from a SQL source.\n\n        This is a convenience alias for `add_external_source`.\n\n        Args:\n            external_source_input: The settings for the external SQL source node.\n        \"\"\"\n        logger.info(\"Adding sql source\")\n        self.add_external_source(external_source_input)\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_cloud_storage_writer(self, node_cloud_storage_writer: input_schema.NodeCloudStorageWriter) -&gt; None:\n        \"\"\"Adds a node to write data to a cloud storage provider.\n\n        Args:\n            node_cloud_storage_writer: The settings for the cloud storage writer node.\n        \"\"\"\n\n        node_type = \"cloud_storage_writer\"\n\n        def _func(df: FlowDataEngine):\n            df.lazy = True\n            execute_remote = self.execution_location != \"local\"\n            cloud_connection_settings = get_cloud_connection_settings(\n                connection_name=node_cloud_storage_writer.cloud_storage_settings.connection_name,\n                user_id=node_cloud_storage_writer.user_id,\n                auth_mode=node_cloud_storage_writer.cloud_storage_settings.auth_mode,\n            )\n            full_cloud_storage_connection = FullCloudStorageConnection(\n                storage_type=cloud_connection_settings.storage_type,\n                auth_method=cloud_connection_settings.auth_method,\n                aws_allow_unsafe_html=cloud_connection_settings.aws_allow_unsafe_html,\n                **CloudStorageReader.get_storage_options(cloud_connection_settings),\n            )\n            if execute_remote:\n                settings = get_cloud_storage_write_settings_worker_interface(\n                    write_settings=node_cloud_storage_writer.cloud_storage_settings,\n                    connection=full_cloud_storage_connection,\n                    lf=df.data_frame,\n                    user_id=node_cloud_storage_writer.user_id,\n                    flowfile_node_id=node_cloud_storage_writer.node_id,\n                    flowfile_flow_id=self.flow_id,\n                )\n                external_database_writer = ExternalCloudWriter(settings, wait_on_completion=False)\n                node._fetch_cached_df = external_database_writer\n                external_database_writer.get_result()\n            else:\n                cloud_storage_write_settings_internal = CloudStorageWriteSettingsInternal(\n                    connection=full_cloud_storage_connection,\n                    write_settings=node_cloud_storage_writer.cloud_storage_settings,\n                )\n                df.to_cloud_storage_obj(cloud_storage_write_settings_internal)\n            return df\n\n        def schema_callback():\n            logger.info(\"Starting to run the schema callback for cloud storage writer\")\n            if self.get_node(node_cloud_storage_writer.node_id).is_correct:\n                return self.get_node(node_cloud_storage_writer.node_id).node_inputs.main_inputs[0].schema\n            else:\n                return [FlowfileColumn.from_input(column_name=\"__error__\", data_type=\"String\")]\n\n        self.add_node_step(\n            node_id=node_cloud_storage_writer.node_id,\n            function=_func,\n            input_columns=[],\n            node_type=node_type,\n            setting_input=node_cloud_storage_writer,\n            schema_callback=schema_callback,\n            input_node_ids=[node_cloud_storage_writer.depending_on_id],\n        )\n\n        node = self.get_node(node_cloud_storage_writer.node_id)\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_cloud_storage_reader(self, node_cloud_storage_reader: input_schema.NodeCloudStorageReader) -&gt; None:\n        \"\"\"Adds a cloud storage read node to the flow graph.\n\n        Args:\n            node_cloud_storage_reader: The settings for the cloud storage read node.\n        \"\"\"\n        node_type = \"cloud_storage_reader\"\n        logger.info(\"Adding cloud storage reader\")\n        cloud_storage_read_settings = node_cloud_storage_reader.cloud_storage_settings\n\n        def _func():\n            logger.info(\"Starting to run the schema callback for cloud storage reader\")\n            self.flow_logger.info(\"Starting to run the schema callback for cloud storage reader\")\n            settings = CloudStorageReadSettingsInternal(\n                read_settings=cloud_storage_read_settings,\n                connection=get_cloud_connection_settings(\n                    connection_name=cloud_storage_read_settings.connection_name,\n                    user_id=node_cloud_storage_reader.user_id,\n                    auth_mode=cloud_storage_read_settings.auth_mode,\n                ),\n            )\n            fl = FlowDataEngine.from_cloud_storage_obj(settings)\n            return fl\n\n        node = self.add_node_step(\n            node_id=node_cloud_storage_reader.node_id,\n            function=_func,\n            cache_results=node_cloud_storage_reader.cache_results,\n            setting_input=node_cloud_storage_reader,\n            node_type=node_type,\n        )\n        self.add_node_to_starting_list(node)\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_external_source(self, external_source_input: input_schema.NodeExternalSource):\n        \"\"\"Adds a node for a custom external data source.\n\n        Args:\n            external_source_input: The settings for the external source node.\n        \"\"\"\n\n        node_type = \"external_source\"\n        external_source_script = getattr(external_sources.custom_external_sources, external_source_input.identifier)\n        source_settings = getattr(\n            input_schema, snake_case_to_camel_case(external_source_input.identifier)\n        ).model_validate(external_source_input.source_settings)\n        if hasattr(external_source_script, \"initial_getter\"):\n            initial_getter = external_source_script.initial_getter(source_settings)\n        else:\n            initial_getter = None\n        data_getter = external_source_script.getter(source_settings)\n        external_source = data_source_factory(\n            source_type=\"custom\",\n            data_getter=data_getter,\n            initial_data_getter=initial_getter,\n            orientation=external_source_input.source_settings.orientation,\n            schema=None,\n        )\n\n        def _func():\n            logger.info(\"Calling external source\")\n            fl = FlowDataEngine.create_from_external_source(external_source=external_source)\n            external_source_input.source_settings.fields = [c.get_minimal_field_info() for c in fl.schema]\n            return fl\n\n        node = self.get_node(external_source_input.node_id)\n        if node:\n            node.node_type = node_type\n            node.name = node_type\n            node.function = _func\n            node.setting_input = external_source_input\n            node.node_settings.cache_results = external_source_input.cache_results\n            self.add_node_to_starting_list(node)\n\n        else:\n            node = FlowNode(\n                external_source_input.node_id,\n                function=_func,\n                setting_input=external_source_input,\n                name=node_type,\n                node_type=node_type,\n                parent_uuid=self.uuid,\n            )\n            self._node_db[external_source_input.node_id] = node\n            self.add_node_to_starting_list(node)\n            self._node_ids.append(external_source_input.node_id)\n        if external_source_input.source_settings.fields and len(external_source_input.source_settings.fields) &gt; 0:\n            logger.info(\"Using provided schema in the node\")\n\n            def schema_callback():\n                return [\n                    FlowfileColumn.from_input(f.name, f.data_type) for f in external_source_input.source_settings.fields\n                ]\n\n            node.schema_callback = schema_callback\n        else:\n            logger.warning(\"Removing schema\")\n            node._schema_callback = None\n        self.add_node_step(\n            node_id=external_source_input.node_id,\n            function=_func,\n            input_columns=[],\n            node_type=node_type,\n            setting_input=external_source_input,\n        )\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_read(self, input_file: input_schema.NodeRead):\n        \"\"\"Adds a node to read data from a local file (e.g., CSV, Parquet, Excel).\n\n        Args:\n            input_file: The settings for the read operation.\n        \"\"\"\n        if (\n            input_file.received_file.file_type in (\"xlsx\", \"excel\")\n            and input_file.received_file.table_settings.sheet_name == \"\"\n        ):\n            sheet_name = fastexcel.read_excel(input_file.received_file.path).sheet_names[0]\n            input_file.received_file.table_settings.sheet_name = sheet_name\n\n        received_file = input_file.received_file\n        input_file.received_file.set_absolute_filepath()\n\n        def _func():\n            input_file.received_file.set_absolute_filepath()\n            if input_file.received_file.file_type == \"parquet\":\n                input_data = FlowDataEngine.create_from_path(input_file.received_file)\n            elif (\n                input_file.received_file.file_type == \"csv\"\n                and \"utf\" in input_file.received_file.table_settings.encoding\n            ):\n                input_data = FlowDataEngine.create_from_path(input_file.received_file)\n            else:\n                input_data = FlowDataEngine.create_from_path_worker(\n                    input_file.received_file, node_id=input_file.node_id, flow_id=self.flow_id\n                )\n            input_data.name = input_file.received_file.name\n            return input_data\n\n        node = self.get_node(input_file.node_id)\n        schema_callback = None\n        if node:\n            start_hash = node.hash\n            node.node_type = \"read\"\n            node.name = \"read\"\n            node.function = _func\n            node.setting_input = input_file\n            self.add_node_to_starting_list(node)\n\n            if start_hash != node.hash:\n                logger.info(\"Hash changed, updating schema\")\n                if len(received_file.fields) &gt; 0:\n                    # If the file has fields defined, we can use them to create the schema\n                    def schema_callback():\n                        return [FlowfileColumn.from_input(f.name, f.data_type) for f in received_file.fields]\n\n                elif input_file.received_file.file_type in (\"csv\", \"json\", \"parquet\"):\n                    # everything that can be scanned by polars\n                    def schema_callback():\n                        input_data = FlowDataEngine.create_from_path(input_file.received_file)\n                        return input_data.schema\n\n                elif input_file.received_file.file_type in (\"xlsx\", \"excel\"):\n                    # If the file is an Excel file, we need to use the openpyxl engine to read the schema\n                    schema_callback = get_xlsx_schema_callback(\n                        engine=\"openpyxl\",\n                        file_path=received_file.file_path,\n                        sheet_name=received_file.table_settings.sheet_name,\n                        start_row=received_file.table_settings.start_row,\n                        end_row=received_file.table_settings.end_row,\n                        start_column=received_file.table_settings.start_column,\n                        end_column=received_file.table_settings.end_column,\n                        has_headers=received_file.table_settings.has_headers,\n                    )\n                else:\n                    schema_callback = None\n        else:\n            node = FlowNode(\n                input_file.node_id,\n                function=_func,\n                setting_input=input_file,\n                name=\"read\",\n                node_type=\"read\",\n                parent_uuid=self.uuid,\n            )\n            self._node_db[input_file.node_id] = node\n            self.add_node_to_starting_list(node)\n            self._node_ids.append(input_file.node_id)\n\n        if schema_callback is not None:\n            node.schema_callback = schema_callback\n            node.user_provided_schema_callback = schema_callback\n        return self\n\n    @with_history_capture(HistoryActionType.UPDATE_SETTINGS)\n    def add_datasource(self, input_file: input_schema.NodeDatasource | input_schema.NodeManualInput) -&gt; \"FlowGraph\":\n        \"\"\"Adds a data source node to the graph.\n\n        This method serves as a factory for creating starting nodes, handling both\n        file-based sources and direct manual data entry.\n\n        Args:\n            input_file: The configuration object for the data source.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n        if isinstance(input_file, input_schema.NodeManualInput):\n            input_data = FlowDataEngine(input_file.raw_data_format)\n            ref = \"manual_input\"\n        else:\n            input_data = FlowDataEngine(path_ref=input_file.file_ref)\n            ref = \"datasource\"\n        node = self.get_node(input_file.node_id)\n        if node:\n            node.node_type = ref\n            node.name = ref\n            node.function = input_data\n            node.setting_input = input_file\n            self.add_node_to_starting_list(node)\n\n        else:\n            input_data.collect()\n            node = FlowNode(\n                input_file.node_id,\n                function=input_data,\n                setting_input=input_file,\n                name=ref,\n                node_type=ref,\n                parent_uuid=self.uuid,\n            )\n            self._node_db[input_file.node_id] = node\n            self.add_node_to_starting_list(node)\n            self._node_ids.append(input_file.node_id)\n        return self\n\n    def add_manual_input(self, input_file: input_schema.NodeManualInput):\n        \"\"\"Adds a node for manual data entry.\n\n        This is a convenience alias for `add_datasource`.\n\n        Args:\n            input_file: The settings and data for the manual input node.\n        \"\"\"\n        self.add_datasource(input_file)\n\n    @property\n    def nodes(self) -&gt; list[FlowNode]:\n        \"\"\"Gets a list of all FlowNode objects in the graph.\"\"\"\n\n        return list(self._node_db.values())\n\n    @property\n    def execution_mode(self) -&gt; schemas.ExecutionModeLiteral:\n        \"\"\"Gets the current execution mode ('Development' or 'Performance').\"\"\"\n        return self.flow_settings.execution_mode\n\n    def get_implicit_starter_nodes(self) -&gt; list[FlowNode]:\n        \"\"\"Finds nodes that can act as starting points but are not explicitly defined as such.\n\n        Some nodes, like the Polars Code node, can function without an input. This\n        method identifies such nodes if they have no incoming connections.\n\n        Returns:\n            A list of `FlowNode` objects that are implicit starting nodes.\n        \"\"\"\n        starting_node_ids = [node.node_id for node in self._flow_starts]\n        implicit_starting_nodes = []\n        for node in self.nodes:\n            if node.node_template.can_be_start and not node.has_input and node.node_id not in starting_node_ids:\n                implicit_starting_nodes.append(node)\n        return implicit_starting_nodes\n\n    @execution_mode.setter\n    def execution_mode(self, mode: schemas.ExecutionModeLiteral):\n        \"\"\"Sets the execution mode for the flow.\n\n        Args:\n            mode: The execution mode to set.\n        \"\"\"\n        self.flow_settings.execution_mode = mode\n\n    @property\n    def execution_location(self) -&gt; schemas.ExecutionLocationsLiteral:\n        \"\"\"Gets the current execution location.\"\"\"\n        return self.flow_settings.execution_location\n\n    @execution_location.setter\n    def execution_location(self, execution_location: schemas.ExecutionLocationsLiteral):\n        \"\"\"Sets the execution location for the flow.\n\n        Args:\n            execution_location: The execution location to set.\n        \"\"\"\n        if self.flow_settings.execution_location != execution_location:\n            self.reset()\n        self.flow_settings.execution_location = execution_location\n\n    def validate_if_node_can_be_fetched(self, node_id: int) -&gt; None:\n        flow_node = self._node_db.get(node_id)\n        if not flow_node:\n            raise Exception(\"Node not found found\")\n        execution_plan = compute_execution_plan(\n            nodes=self.nodes, flow_starts=self._flow_starts + self.get_implicit_starter_nodes()\n        )\n        if flow_node.node_id in [skip_node.node_id for skip_node in execution_plan.skip_nodes]:\n            raise Exception(\"Node can not be executed because it does not have it's inputs\")\n\n    def create_initial_run_information(self, number_of_nodes: int, run_type: Literal[\"fetch_one\", \"full_run\"]):\n        return RunInformation(\n            flow_id=self.flow_id,\n            start_time=datetime.datetime.now(),\n            end_time=None,\n            success=None,\n            number_of_nodes=number_of_nodes,\n            node_step_result=[],\n            run_type=run_type,\n        )\n\n    def create_empty_run_information(self) -&gt; RunInformation:\n        return RunInformation(\n            flow_id=self.flow_id,\n            start_time=None,\n            end_time=None,\n            success=None,\n            number_of_nodes=0,\n            node_step_result=[],\n            run_type=\"init\",\n        )\n\n    def trigger_fetch_node(self, node_id: int) -&gt; RunInformation | None:\n        \"\"\"Executes a specific node in the graph by its ID.\"\"\"\n        if self.flow_settings.is_running:\n            raise Exception(\"Flow is already running\")\n        flow_node = self.get_node(node_id)\n        self.flow_settings.is_running = True\n        self.flow_settings.is_canceled = False\n        self.flow_logger.clear_log_file()\n        self.latest_run_info = self.create_initial_run_information(1, \"fetch_one\")\n        node_logger = self.flow_logger.get_node_logger(flow_node.node_id)\n        node_result = NodeResult(node_id=flow_node.node_id, node_name=flow_node.name)\n        logger.info(f\"Starting to run: node {flow_node.node_id}, start time: {node_result.start_timestamp}\")\n        try:\n            self.latest_run_info.node_step_result.append(node_result)\n            flow_node.execute_node(\n                run_location=self.flow_settings.execution_location,\n                performance_mode=False,\n                node_logger=node_logger,\n                optimize_for_downstream=False,\n                reset_cache=True,\n            )\n            node_result.error = str(flow_node.results.errors)\n            if self.flow_settings.is_canceled:\n                node_result.success = None\n                node_result.success = None\n                node_result.is_running = False\n            node_result.success = flow_node.results.errors is None\n            node_result.end_timestamp = time()\n            node_result.run_time = int(node_result.end_timestamp - node_result.start_timestamp)\n            node_result.is_running = False\n            self.latest_run_info.nodes_completed += 1\n            self.latest_run_info.end_time = datetime.datetime.now()\n            self.flow_settings.is_running = False\n            return self.get_run_info()\n        except Exception as e:\n            node_result.error = \"Node did not run\"\n            node_result.success = False\n            node_result.end_timestamp = time()\n            node_result.run_time = int(node_result.end_timestamp - node_result.start_timestamp)\n            node_result.is_running = False\n            node_logger.error(f\"Error in node {flow_node.node_id}: {e}\")\n        finally:\n            self.flow_settings.is_running = False\n\n    # ------------------------------------------------------------------\n    # Artifact helpers\n    # ------------------------------------------------------------------\n\n    def _get_upstream_node_ids(self, node_id: int) -&gt; list[int]:\n        \"\"\"Get all upstream node IDs (direct and transitive) for *node_id*.\n\n        Traverses the ``all_inputs`` links recursively and returns a\n        deduplicated list in breadth-first order.\n        \"\"\"\n        node = self.get_node(node_id)\n        if node is None:\n            return []\n\n        visited: set[int] = set()\n        result: list[int] = []\n        queue = list(node.all_inputs)\n        while queue:\n            current = queue.pop(0)\n            cid = current.node_id\n            if cid in visited:\n                continue\n            visited.add(cid)\n            result.append(cid)\n            queue.extend(current.all_inputs)\n        return result\n\n    def _get_required_kernel_ids(self) -&gt; set[str]:\n        \"\"\"Return the set of kernel IDs used by ``python_script`` nodes.\"\"\"\n        kernel_ids: set[str] = set()\n        for node in self.nodes:\n            if node.node_type == \"python_script\" and node.setting_input is not None:\n                kid = getattr(\n                    getattr(node.setting_input, \"python_script_input\", None),\n                    \"kernel_id\",\n                    None,\n                )\n                if kid:\n                    kernel_ids.add(kid)\n        return kernel_ids\n\n    def _compute_rerun_python_script_node_ids(\n        self,\n        plan_skip_ids: set[str | int],\n    ) -&gt; set[int]:\n        \"\"\"Return node IDs for ``python_script`` nodes that will re-execute.\n\n        A python_script node will re-execute (and thus needs its old\n        artifacts cleared) when:\n\n        * It is NOT in the execution-plan skip set, **and**\n        * Its execution state indicates it has NOT already run with the\n          current setup (i.e. its cache is stale or it never ran).\n        \"\"\"\n        rerun: set[int] = set()\n        for node in self.nodes:\n            if node.node_type != \"python_script\":\n                continue\n            if node.node_id in plan_skip_ids:\n                continue\n            if not node._execution_state.has_run_with_current_setup:\n                rerun.add(node.node_id)\n        return rerun\n\n    def _group_rerun_nodes_by_kernel(\n        self,\n        rerun_node_ids: set[int],\n    ) -&gt; dict[str, set[int]]:\n        \"\"\"Group *rerun_node_ids* by their kernel ID.\n\n        Returns a mapping ``kernel_id \u2192 {node_id, \u2026}``.\n        \"\"\"\n        kernel_nodes: dict[str, set[int]] = {}\n        for node in self.nodes:\n            if node.node_id not in rerun_node_ids:\n                continue\n            if node.node_type == \"python_script\" and node.setting_input is not None:\n                kid = getattr(\n                    getattr(node.setting_input, \"python_script_input\", None),\n                    \"kernel_id\",\n                    None,\n                )\n                if kid:\n                    kernel_nodes.setdefault(kid, set()).add(node.node_id)\n        return kernel_nodes\n\n    def _execute_single_node(\n        self,\n        node: FlowNode,\n        performance_mode: bool,\n        run_info_lock: threading.Lock,\n    ) -&gt; tuple[NodeResult, FlowNode]:\n        \"\"\"Executes a single node, records its result, and returns both.\n\n        Thread-safe: uses run_info_lock when mutating shared run information.\n\n        Args:\n            node: The node to execute.\n            performance_mode: Whether to run in performance mode.\n            run_info_lock: Lock protecting shared RunInformation state.\n\n        Returns:\n            A (NodeResult, FlowNode) tuple for post-stage failure propagation.\n        \"\"\"\n        node_logger = self.flow_logger.get_node_logger(node.node_id)\n        node_result = NodeResult(node_id=node.node_id, node_name=node.name)\n\n        with run_info_lock:\n            self.latest_run_info.node_step_result.append(node_result)\n\n        logger.info(f\"Starting to run: node {node.node_id}, start time: {node_result.start_timestamp}\")\n        node.execute_node(\n            run_location=self.flow_settings.execution_location,\n            performance_mode=performance_mode,\n            node_logger=node_logger,\n        )\n        try:\n            node_result.error = str(node.results.errors)\n            if self.flow_settings.is_canceled:\n                node_result.success = None\n                node_result.is_running = False\n                return node_result, node\n            node_result.success = node.results.errors is None\n            node_result.end_timestamp = time()\n            node_result.run_time = int(node_result.end_timestamp - node_result.start_timestamp)\n            node_result.is_running = False\n        except Exception as e:\n            node_result.error = \"Node did not run\"\n            node_result.success = False\n            node_result.end_timestamp = time()\n            node_result.run_time = int(node_result.end_timestamp - node_result.start_timestamp)\n            node_result.is_running = False\n            node_logger.error(f\"Error in node {node.node_id}: {e}\")\n\n        node_logger.info(f\"Completed node with success: {node_result.success}\")\n        with run_info_lock:\n            self.latest_run_info.nodes_completed += 1\n\n        return node_result, node\n\n    def run_graph(self) -&gt; RunInformation | None:\n        \"\"\"Executes the entire data flow graph from start to finish.\n\n        Independent nodes within the same execution stage are run in parallel\n        using threads. Stages are processed sequentially so that all dependencies\n        are satisfied before a stage begins.\n\n        Returns:\n            A RunInformation object summarizing the execution results.\n\n        Raises:\n            Exception: If the flow is already running.\n        \"\"\"\n        if self.flow_settings.is_running:\n            raise Exception(\"Flow is already running\")\n        try:\n            self.flow_settings.is_running = True\n            self.flow_settings.is_canceled = False\n            self.flow_logger.clear_log_file()\n            self.flow_logger.info(\"Starting to run flowfile flow...\")\n\n            execution_plan = compute_execution_plan(\n                nodes=self.nodes, flow_starts=self._flow_starts + self.get_implicit_starter_nodes()\n            )\n\n            # Selectively clear artifacts only for nodes that will re-run.\n            # Nodes that are up-to-date keep their artifacts in both the\n            # metadata tracker AND the kernel's in-memory store so that\n            # downstream nodes can still read them.\n            plan_skip_ids: set[str | int] = {n.node_id for n in execution_plan.skip_nodes}\n            rerun_node_ids = self._compute_rerun_python_script_node_ids(plan_skip_ids)\n\n            # Expand re-run set: if a re-running node previously deleted\n            # artifacts, the original producer nodes must also re-run so\n            # those artifacts are available again in the kernel store.\n            while True:\n                deleted_producers = self.artifact_context.get_producer_nodes_for_deletions(\n                    rerun_node_ids,\n                )\n                new_ids = deleted_producers - rerun_node_ids\n                if not new_ids:\n                    break\n                rerun_node_ids |= new_ids\n\n            # Force producer nodes (added due to artifact deletions) to\n            # actually re-execute by marking their execution state stale.\n            for nid in rerun_node_ids:\n                node = self.get_node(nid)\n                if node is not None and node._execution_state.has_run_with_current_setup:\n                    node._execution_state.has_run_with_current_setup = False\n\n            # Also purge stale metadata for nodes not in this graph\n            # (e.g. injected externally or left over from removed nodes).\n            graph_node_ids = set(self._node_db.keys())\n            stale_node_ids = {nid for nid in self.artifact_context._node_states if nid not in graph_node_ids}\n            nodes_to_clear = rerun_node_ids | stale_node_ids\n            if nodes_to_clear:\n                self.artifact_context.clear_nodes(nodes_to_clear)\n\n            if rerun_node_ids:\n                # Clear the actual kernel-side artifacts for re-running nodes\n                kernel_node_map = self._group_rerun_nodes_by_kernel(rerun_node_ids)\n                for kid, node_ids_for_kernel in kernel_node_map.items():\n                    try:\n                        manager = get_kernel_manager()\n                        manager.clear_node_artifacts_sync(\n                            kid, list(node_ids_for_kernel), flow_id=self.flow_id, flow_logger=self.flow_logger\n                        )\n                    except Exception:\n                        logger.debug(\n                            \"Could not clear node artifacts for kernel '%s', nodes %s\",\n                            kid,\n                            sorted(node_ids_for_kernel),\n                        )\n\n            self.latest_run_info = self.create_initial_run_information(execution_plan.node_count, \"full_run\")\n\n            skip_node_message(self.flow_logger, execution_plan.skip_nodes)\n            execution_order_message(self.flow_logger, execution_plan.stages)\n            performance_mode = self.flow_settings.execution_mode == \"Performance\"\n\n            run_info_lock = threading.Lock()\n            skip_node_ids: set[str | int] = plan_skip_ids\n\n            for stage in execution_plan.stages:\n                if self.flow_settings.is_canceled:\n                    self.flow_logger.info(\"Flow canceled\")\n                    break\n\n                nodes_to_run = [n for n in stage.nodes if n.node_id not in skip_node_ids]\n\n                for skipped in stage.nodes:\n                    if skipped.node_id in skip_node_ids:\n                        node_logger = self.flow_logger.get_node_logger(skipped.node_id)\n                        node_logger.info(f\"Skipping node {skipped.node_id}\")\n\n                if not nodes_to_run:\n                    continue\n\n                is_local = self.flow_settings.execution_location == \"local\"\n                max_workers = 1 if is_local else self.flow_settings.max_parallel_workers\n                if len(nodes_to_run) == 1 or max_workers == 1:\n                    # Single node or parallelism disabled \u2014 run sequentially\n                    stage_results = [\n                        self._execute_single_node(node, performance_mode, run_info_lock) for node in nodes_to_run\n                    ]\n                else:\n                    # Multiple independent nodes \u2014 run in parallel\n                    stage_results: list[tuple[NodeResult, FlowNode]] = []\n                    workers = min(max_workers, len(nodes_to_run))\n                    with ThreadPoolExecutor(max_workers=workers) as executor:\n                        futures = {\n                            executor.submit(self._execute_single_node, node, performance_mode, run_info_lock): node\n                            for node in nodes_to_run\n                        }\n                        for future in as_completed(futures):\n                            stage_results.append(future.result())\n\n                # After the stage completes, propagate failures to downstream nodes\n                for node_result, node in stage_results:\n                    if not node_result.success:\n                        for dep in node.get_all_dependent_nodes():\n                            skip_node_ids.add(dep.node_id)\n\n            self.latest_run_info.end_time = datetime.datetime.now()\n            self.flow_logger.info(\"Flow completed!\")\n            self.end_datetime = datetime.datetime.now()\n            self.flow_settings.is_running = False\n            if self.flow_settings.is_canceled:\n                self.flow_logger.info(\"Flow canceled\")\n            return self.get_run_info()\n        except Exception as e:\n            raise e\n        finally:\n            self.flow_settings.is_running = False\n\n    def get_run_info(self) -&gt; RunInformation:\n        \"\"\"Gets a summary of the most recent graph execution.\n\n        Returns:\n            A RunInformation object with details about the last run.\n        \"\"\"\n        is_running = self.flow_settings.is_running\n        if self.latest_run_info is None:\n            return self.create_empty_run_information()\n\n        elif not is_running and self.latest_run_info.success is not None:\n            return self.latest_run_info\n\n        run_info = self.latest_run_info\n        if not is_running:\n            run_info.success = all(nr.success for nr in run_info.node_step_result)\n        return run_info\n\n    @property\n    def node_connections(self) -&gt; list[tuple[int, int]]:\n        \"\"\"Computes and returns a list of all connections in the graph.\n\n        Returns:\n            A list of tuples, where each tuple is a (source_id, target_id) pair.\n        \"\"\"\n        connections = set()\n        for node in self.nodes:\n            outgoing_connections = [(node.node_id, ltn.node_id) for ltn in node.leads_to_nodes]\n            incoming_connections = [(don.node_id, node.node_id) for don in node.all_inputs]\n            node_connections = [\n                c for c in outgoing_connections + incoming_connections if (c[0] is not None and c[1] is not None)\n            ]\n            for node_connection in node_connections:\n                if node_connection not in connections:\n                    connections.add(node_connection)\n        return list(connections)\n\n    def get_node_data(self, node_id: int, include_example: bool = True) -&gt; NodeData:\n        \"\"\"Retrieves all data needed to render a node in the UI.\n\n        Args:\n            node_id: The ID of the node.\n            include_example: Whether to include data samples in the result.\n\n        Returns:\n            A NodeData object, or None if the node is not found.\n        \"\"\"\n        node = self._node_db[node_id]\n        return node.get_node_data(flow_id=self.flow_id, include_example=include_example)\n\n    def get_flowfile_data(self) -&gt; schemas.FlowfileData:\n        start_node_ids = {v.node_id for v in self._flow_starts}\n\n        nodes = []\n        for node in self.nodes:\n            node_info = node.get_node_information()\n            flowfile_node = schemas.FlowfileNode(\n                id=node_info.id,\n                type=node_info.type,\n                is_start_node=node.node_id in start_node_ids,\n                description=node_info.description,\n                node_reference=node_info.node_reference,\n                x_position=int(node_info.x_position),\n                y_position=int(node_info.y_position),\n                left_input_id=node_info.left_input_id,\n                right_input_id=node_info.right_input_id,\n                input_ids=node_info.input_ids,\n                outputs=node_info.outputs,\n                setting_input=node_info.setting_input,\n            )\n            nodes.append(flowfile_node)\n\n        settings = schemas.FlowfileSettings(\n            description=self.flow_settings.description,\n            execution_mode=self.flow_settings.execution_mode,\n            execution_location=self.flow_settings.execution_location,\n            auto_save=self.flow_settings.auto_save,\n            show_detailed_progress=self.flow_settings.show_detailed_progress,\n            max_parallel_workers=self.flow_settings.max_parallel_workers,\n            source_registration_id=self.flow_settings.source_registration_id,\n        )\n        return schemas.FlowfileData(\n            flowfile_version=__version__,\n            flowfile_id=self.flow_id,\n            flowfile_name=self.__name__,\n            flowfile_settings=settings,\n            nodes=nodes,\n        )\n\n    def get_node_storage(self) -&gt; schemas.FlowInformation:\n        \"\"\"Serializes the entire graph's state into a storable format.\n\n        Returns:\n            A FlowInformation object representing the complete graph.\n        \"\"\"\n        node_information = {\n            node.node_id: node.get_node_information() for node in self.nodes if node.is_setup and node.is_correct\n        }\n\n        return schemas.FlowInformation(\n            flow_id=self.flow_id,\n            flow_name=self.__name__,\n            flow_settings=self.flow_settings,\n            data=node_information,\n            node_starts=[v.node_id for v in self._flow_starts],\n            node_connections=self.node_connections,\n        )\n\n    def cancel(self):\n        \"\"\"Cancels an ongoing graph execution.\"\"\"\n\n        if not self.flow_settings.is_running:\n            return\n        self.flow_settings.is_canceled = True\n        for node in self.nodes:\n            node.cancel()\n\n    def close_flow(self):\n        \"\"\"Performs cleanup operations, such as clearing node caches.\"\"\"\n\n        for node in self.nodes:\n            node.remove_cache()\n\n    def _handle_flow_renaming(self, new_name: str, new_path: Path):\n        \"\"\"\n        Handle the rename of a flow when it is being saved.\n        \"\"\"\n        if (\n            self.flow_settings\n            and self.flow_settings.path\n            and Path(self.flow_settings.path).absolute() != new_path.absolute()\n        ):\n            self.__name__ = new_name\n            self.flow_settings.save_location = str(new_path.absolute())\n            self.flow_settings.name = new_name\n        if self.flow_settings and not self.flow_settings.save_location:\n            self.flow_settings.save_location = str(new_path.absolute())\n            self.__name__ = new_name\n            self.flow_settings.name = new_name\n\n    def save_flow(self, flow_path: str):\n        \"\"\"Saves the current state of the flow graph to a file.\n\n        Supports multiple formats based on file extension:\n        - .yaml / .yml: New YAML format\n        - .json: JSON format\n\n        Args:\n            flow_path: The path where the flow file will be saved.\n        \"\"\"\n        logger.info(\"Saving flow to %s\", flow_path)\n        path = Path(flow_path)\n        os.makedirs(path.parent, exist_ok=True)\n        suffix = path.suffix.lower()\n        new_flow_name = path.name.replace(suffix, \"\")\n        self._handle_flow_renaming(new_flow_name, path)\n        self.flow_settings.modified_on = datetime.datetime.now().timestamp()\n        try:\n            if suffix == \".flowfile\":\n                raise DeprecationWarning(\n                    \"The .flowfile format is deprecated. Please use .yaml or .json formats.\\n\\n\"\n                    \"Or stay on v0.4.1 if you still need .flowfile support.\\n\\n\"\n                )\n            elif suffix in (\".yaml\", \".yml\"):\n                flowfile_data = self.get_flowfile_data()\n                data = flowfile_data.model_dump(mode=\"json\")\n                with open(flow_path, \"w\", encoding=\"utf-8\") as f:\n                    yaml.dump(data, f, default_flow_style=False, sort_keys=False, allow_unicode=True)\n            elif suffix == \".json\":\n                flowfile_data = self.get_flowfile_data()\n                data = flowfile_data.model_dump(mode=\"json\")\n                with open(flow_path, \"w\", encoding=\"utf-8\") as f:\n                    json.dump(data, f, indent=2, ensure_ascii=False)\n\n            else:\n                flowfile_data = self.get_flowfile_data()\n                logger.warning(f\"Unknown file extension {suffix}. Defaulting to YAML format.\")\n                data = flowfile_data.model_dump(mode=\"json\")\n                with open(flow_path, \"w\", encoding=\"utf-8\") as f:\n                    yaml.dump(data, f, default_flow_style=False, sort_keys=False, allow_unicode=True)\n\n        except Exception as e:\n            logger.error(f\"Error saving flow: {e}\")\n            raise\n\n        self.flow_settings.path = flow_path\n\n    def get_frontend_data(self) -&gt; dict:\n        \"\"\"Formats the graph structure into a JSON-like dictionary for a specific legacy frontend.\n\n        This method transforms the graph's state into a format compatible with the\n        Drawflow.js library.\n\n        Returns:\n            A dictionary representing the graph in Drawflow format.\n        \"\"\"\n        result = {\"Home\": {\"data\": {}}}\n        flow_info: schemas.FlowInformation = self.get_node_storage()\n\n        for node_id, node_info in flow_info.data.items():\n            if node_info.is_setup:\n                try:\n                    pos_x = node_info.data.pos_x\n                    pos_y = node_info.data.pos_y\n                    # Basic node structure\n                    result[\"Home\"][\"data\"][str(node_id)] = {\n                        \"id\": node_info.id,\n                        \"name\": node_info.type,\n                        \"data\": {},  # Additional data can go here\n                        \"class\": node_info.type,\n                        \"html\": node_info.type,\n                        \"typenode\": \"vue\",\n                        \"inputs\": {},\n                        \"outputs\": {},\n                        \"pos_x\": pos_x,\n                        \"pos_y\": pos_y,\n                    }\n                except Exception as e:\n                    logger.error(e)\n            # Add outputs to the node based on `outputs` in your backend data\n            if node_info.outputs:\n                outputs = {o: 0 for o in node_info.outputs}\n                for o in node_info.outputs:\n                    outputs[o] += 1\n                connections = []\n                for output_node_id, n_connections in outputs.items():\n                    leading_to_node = self.get_node(output_node_id)\n                    input_types = leading_to_node.get_input_type(node_info.id)\n                    for input_type in input_types:\n                        if input_type == \"main\":\n                            input_frontend_id = \"input_1\"\n                        elif input_type == \"right\":\n                            input_frontend_id = \"input_2\"\n                        elif input_type == \"left\":\n                            input_frontend_id = \"input_3\"\n                        else:\n                            input_frontend_id = \"input_1\"\n                        connection = {\"node\": str(output_node_id), \"input\": input_frontend_id}\n                        connections.append(connection)\n\n                result[\"Home\"][\"data\"][str(node_id)][\"outputs\"][\"output_1\"] = {\"connections\": connections}\n            else:\n                result[\"Home\"][\"data\"][str(node_id)][\"outputs\"] = {\"output_1\": {\"connections\": []}}\n\n            # Add input to the node based on `depending_on_id` in your backend data\n            if (\n                node_info.left_input_id is not None\n                or node_info.right_input_id is not None\n                or node_info.input_ids is not None\n            ):\n                main_inputs = node_info.main_input_ids\n                result[\"Home\"][\"data\"][str(node_id)][\"inputs\"][\"input_1\"] = {\n                    \"connections\": [{\"node\": str(main_node_id), \"input\": \"output_1\"} for main_node_id in main_inputs]\n                }\n                if node_info.right_input_id is not None:\n                    result[\"Home\"][\"data\"][str(node_id)][\"inputs\"][\"input_2\"] = {\n                        \"connections\": [{\"node\": str(node_info.right_input_id), \"input\": \"output_1\"}]\n                    }\n                if node_info.left_input_id is not None:\n                    result[\"Home\"][\"data\"][str(node_id)][\"inputs\"][\"input_3\"] = {\n                        \"connections\": [{\"node\": str(node_info.left_input_id), \"input\": \"output_1\"}]\n                    }\n        return result\n\n    def get_vue_flow_input(self) -&gt; schemas.VueFlowInput:\n        \"\"\"Formats the graph's nodes and edges into a schema suitable for the VueFlow frontend.\n\n        Returns:\n            A VueFlowInput object.\n        \"\"\"\n        edges: list[schemas.NodeEdge] = []\n        nodes: list[schemas.NodeInput] = []\n        for node in self.nodes:\n            nodes.append(node.get_node_input())\n            edges.extend(node.get_edge_input())\n        return schemas.VueFlowInput(node_edges=edges, node_inputs=nodes)\n\n    def reset(self):\n        \"\"\"Forces a deep reset on all nodes in the graph.\"\"\"\n\n        for node in self.nodes:\n            node.reset(True)\n\n    def copy_node(\n        self, new_node_settings: input_schema.NodePromise, existing_setting_input: Any, node_type: str\n    ) -&gt; None:\n        \"\"\"Creates a copy of an existing node.\n\n        Args:\n            new_node_settings: The promise containing new settings (like ID and position).\n            existing_setting_input: The settings object from the node being copied.\n            node_type: The type of the node being copied.\n        \"\"\"\n        self.add_node_promise(new_node_settings)\n\n        if isinstance(existing_setting_input, input_schema.NodePromise):\n            return\n\n        combined_settings = combine_existing_settings_and_new_settings(existing_setting_input, new_node_settings)\n        getattr(self, f\"add_{node_type}\")(combined_settings)\n\n    def generate_code(self):\n        \"\"\"Generates code for the flow graph.\n        This method exports the flow graph to a Polars-compatible format.\n        \"\"\"\n        from flowfile_core.flowfile.code_generator.code_generator import export_flow_to_polars\n\n        print(export_flow_to_polars(self))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.execution_location","title":"<code>execution_location</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the current execution location.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.execution_mode","title":"<code>execution_mode</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the current execution mode ('Development' or 'Performance').</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.flow_id","title":"<code>flow_id</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the unique identifier of the flow.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.graph_has_functions","title":"<code>graph_has_functions</code>  <code>property</code>","text":"<p>Checks if the graph has any nodes.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.graph_has_input_data","title":"<code>graph_has_input_data</code>  <code>property</code>","text":"<p>Checks if the graph has an initial input data source.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.node_connections","title":"<code>node_connections</code>  <code>property</code>","text":"<p>Computes and returns a list of all connections in the graph.</p> <p>Returns:</p> Type Description <code>list[tuple[int, int]]</code> <p>A list of tuples, where each tuple is a (source_id, target_id) pair.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.nodes","title":"<code>nodes</code>  <code>property</code>","text":"<p>Gets a list of all FlowNode objects in the graph.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.__init__","title":"<code>__init__(flow_settings, name=None, input_cols=None, output_cols=None, path_ref=None, input_flow=None, cache_results=False)</code>","text":"<p>Initializes a new FlowGraph instance.</p> <p>Parameters:</p> Name Type Description Default <code>flow_settings</code> <code>FlowSettings | FlowGraphConfig</code> <p>The configuration settings for the flow.</p> required <code>name</code> <code>str</code> <p>The name of the flow.</p> <code>None</code> <code>input_cols</code> <code>list[str]</code> <p>A list of input column names.</p> <code>None</code> <code>output_cols</code> <code>list[str]</code> <p>A list of output column names.</p> <code>None</code> <code>path_ref</code> <code>str</code> <p>An optional path to an initial data source.</p> <code>None</code> <code>input_flow</code> <code>Union[ParquetFile, FlowDataEngine, FlowGraph]</code> <p>An optional existing data object to start the flow with.</p> <code>None</code> <code>cache_results</code> <code>bool</code> <p>A global flag to enable or disable result caching.</p> <code>False</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def __init__(\n    self,\n    flow_settings: schemas.FlowSettings | schemas.FlowGraphConfig,\n    name: str = None,\n    input_cols: list[str] = None,\n    output_cols: list[str] = None,\n    path_ref: str = None,\n    input_flow: Union[ParquetFile, FlowDataEngine, \"FlowGraph\"] = None,\n    cache_results: bool = False,\n):\n    \"\"\"Initializes a new FlowGraph instance.\n\n    Args:\n        flow_settings: The configuration settings for the flow.\n        name: The name of the flow.\n        input_cols: A list of input column names.\n        output_cols: A list of output column names.\n        path_ref: An optional path to an initial data source.\n        input_flow: An optional existing data object to start the flow with.\n        cache_results: A global flag to enable or disable result caching.\n    \"\"\"\n    if isinstance(flow_settings, schemas.FlowGraphConfig):\n        flow_settings = schemas.FlowSettings.from_flow_settings_input(flow_settings)\n\n    self._flow_settings = flow_settings\n    self.uuid = str(uuid1())\n    self.start_datetime = None\n    self.end_datetime = None\n    self.latest_run_info = None\n    self._flow_id = flow_settings.flow_id\n    self.flow_logger = FlowLogger(flow_settings.flow_id)\n    self._flow_starts: list[FlowNode] = []\n    self._results = None\n    self.schema = None\n    self.has_over_row_function = False\n    self._input_cols = [] if input_cols is None else input_cols\n    self._output_cols = [] if output_cols is None else output_cols\n    self._node_ids = []\n    self._node_db = {}\n    self.cache_results = cache_results\n    self.__name__ = name if name else \"flow_\" + str(id(self))\n    self.depends_on = {}\n    self.artifact_context = ArtifactContext()\n\n    # Initialize history manager for undo/redo support\n    from flowfile_core.flowfile.history_manager import HistoryManager\n    from flowfile_core.schemas.history_schema import HistoryConfig\n\n    history_config = HistoryConfig(enabled=flow_settings.track_history)\n    self._history_manager = HistoryManager(config=history_config)\n\n    if path_ref is not None:\n        self.add_datasource(input_schema.NodeDatasource(file_path=path_ref))\n    elif input_flow is not None:\n        self.add_datasource(input_file=input_flow)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.__repr__","title":"<code>__repr__()</code>","text":"<p>Provides the official string representation of the FlowGraph instance.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def __repr__(self):\n    \"\"\"Provides the official string representation of the FlowGraph instance.\"\"\"\n    settings_str = \"  -\" + \"\\n  -\".join(f\"{k}: {v}\" for k, v in self.flow_settings)\n    return f\"FlowGraph(\\nNodes: {self._node_db}\\n\\nSettings:\\n{settings_str}\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_cloud_storage_reader","title":"<code>add_cloud_storage_reader(node_cloud_storage_reader)</code>","text":"<p>Adds a cloud storage read node to the flow graph.</p> <p>Parameters:</p> Name Type Description Default <code>node_cloud_storage_reader</code> <code>NodeCloudStorageReader</code> <p>The settings for the cloud storage read node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_cloud_storage_reader(self, node_cloud_storage_reader: input_schema.NodeCloudStorageReader) -&gt; None:\n    \"\"\"Adds a cloud storage read node to the flow graph.\n\n    Args:\n        node_cloud_storage_reader: The settings for the cloud storage read node.\n    \"\"\"\n    node_type = \"cloud_storage_reader\"\n    logger.info(\"Adding cloud storage reader\")\n    cloud_storage_read_settings = node_cloud_storage_reader.cloud_storage_settings\n\n    def _func():\n        logger.info(\"Starting to run the schema callback for cloud storage reader\")\n        self.flow_logger.info(\"Starting to run the schema callback for cloud storage reader\")\n        settings = CloudStorageReadSettingsInternal(\n            read_settings=cloud_storage_read_settings,\n            connection=get_cloud_connection_settings(\n                connection_name=cloud_storage_read_settings.connection_name,\n                user_id=node_cloud_storage_reader.user_id,\n                auth_mode=cloud_storage_read_settings.auth_mode,\n            ),\n        )\n        fl = FlowDataEngine.from_cloud_storage_obj(settings)\n        return fl\n\n    node = self.add_node_step(\n        node_id=node_cloud_storage_reader.node_id,\n        function=_func,\n        cache_results=node_cloud_storage_reader.cache_results,\n        setting_input=node_cloud_storage_reader,\n        node_type=node_type,\n    )\n    self.add_node_to_starting_list(node)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_cloud_storage_writer","title":"<code>add_cloud_storage_writer(node_cloud_storage_writer)</code>","text":"<p>Adds a node to write data to a cloud storage provider.</p> <p>Parameters:</p> Name Type Description Default <code>node_cloud_storage_writer</code> <code>NodeCloudStorageWriter</code> <p>The settings for the cloud storage writer node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_cloud_storage_writer(self, node_cloud_storage_writer: input_schema.NodeCloudStorageWriter) -&gt; None:\n    \"\"\"Adds a node to write data to a cloud storage provider.\n\n    Args:\n        node_cloud_storage_writer: The settings for the cloud storage writer node.\n    \"\"\"\n\n    node_type = \"cloud_storage_writer\"\n\n    def _func(df: FlowDataEngine):\n        df.lazy = True\n        execute_remote = self.execution_location != \"local\"\n        cloud_connection_settings = get_cloud_connection_settings(\n            connection_name=node_cloud_storage_writer.cloud_storage_settings.connection_name,\n            user_id=node_cloud_storage_writer.user_id,\n            auth_mode=node_cloud_storage_writer.cloud_storage_settings.auth_mode,\n        )\n        full_cloud_storage_connection = FullCloudStorageConnection(\n            storage_type=cloud_connection_settings.storage_type,\n            auth_method=cloud_connection_settings.auth_method,\n            aws_allow_unsafe_html=cloud_connection_settings.aws_allow_unsafe_html,\n            **CloudStorageReader.get_storage_options(cloud_connection_settings),\n        )\n        if execute_remote:\n            settings = get_cloud_storage_write_settings_worker_interface(\n                write_settings=node_cloud_storage_writer.cloud_storage_settings,\n                connection=full_cloud_storage_connection,\n                lf=df.data_frame,\n                user_id=node_cloud_storage_writer.user_id,\n                flowfile_node_id=node_cloud_storage_writer.node_id,\n                flowfile_flow_id=self.flow_id,\n            )\n            external_database_writer = ExternalCloudWriter(settings, wait_on_completion=False)\n            node._fetch_cached_df = external_database_writer\n            external_database_writer.get_result()\n        else:\n            cloud_storage_write_settings_internal = CloudStorageWriteSettingsInternal(\n                connection=full_cloud_storage_connection,\n                write_settings=node_cloud_storage_writer.cloud_storage_settings,\n            )\n            df.to_cloud_storage_obj(cloud_storage_write_settings_internal)\n        return df\n\n    def schema_callback():\n        logger.info(\"Starting to run the schema callback for cloud storage writer\")\n        if self.get_node(node_cloud_storage_writer.node_id).is_correct:\n            return self.get_node(node_cloud_storage_writer.node_id).node_inputs.main_inputs[0].schema\n        else:\n            return [FlowfileColumn.from_input(column_name=\"__error__\", data_type=\"String\")]\n\n    self.add_node_step(\n        node_id=node_cloud_storage_writer.node_id,\n        function=_func,\n        input_columns=[],\n        node_type=node_type,\n        setting_input=node_cloud_storage_writer,\n        schema_callback=schema_callback,\n        input_node_ids=[node_cloud_storage_writer.depending_on_id],\n    )\n\n    node = self.get_node(node_cloud_storage_writer.node_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_cross_join","title":"<code>add_cross_join(cross_join_settings)</code>","text":"<p>Adds a cross join node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>cross_join_settings</code> <code>NodeCrossJoin</code> <p>The settings for the cross join operation.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_cross_join(self, cross_join_settings: input_schema.NodeCrossJoin) -&gt; \"FlowGraph\":\n    \"\"\"Adds a cross join node to the graph.\n\n    Args:\n        cross_join_settings: The settings for the cross join operation.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n\n    def _func(main: FlowDataEngine, right: FlowDataEngine) -&gt; FlowDataEngine:\n        for left_select in cross_join_settings.cross_join_input.left_select.renames:\n            left_select.is_available = True if left_select.old_name in main.schema else False\n        for right_select in cross_join_settings.cross_join_input.right_select.renames:\n            right_select.is_available = True if right_select.old_name in right.schema else False\n        return main.do_cross_join(\n            cross_join_input=cross_join_settings.cross_join_input,\n            auto_generate_selection=cross_join_settings.auto_generate_selection,\n            verify_integrity=False,\n            other=right,\n        )\n\n    self.add_node_step(\n        node_id=cross_join_settings.node_id,\n        function=_func,\n        input_columns=[],\n        node_type=\"cross_join\",\n        setting_input=cross_join_settings,\n        input_node_ids=cross_join_settings.depending_on_ids,\n    )\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_database_reader","title":"<code>add_database_reader(node_database_reader)</code>","text":"<p>Adds a node to read data from a database.</p> <p>Parameters:</p> Name Type Description Default <code>node_database_reader</code> <code>NodeDatabaseReader</code> <p>The settings for the database reader node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_database_reader(self, node_database_reader: input_schema.NodeDatabaseReader):\n    \"\"\"Adds a node to read data from a database.\n\n    Args:\n        node_database_reader: The settings for the database reader node.\n    \"\"\"\n\n    logger.info(\"Adding database reader\")\n    node_type = \"database_reader\"\n    database_settings: input_schema.DatabaseSettings = node_database_reader.database_settings\n    database_connection: input_schema.DatabaseConnection | input_schema.FullDatabaseConnection | None\n    if database_settings.connection_mode == \"inline\":\n        database_connection: input_schema.DatabaseConnection = database_settings.database_connection\n        encrypted_password = get_encrypted_secret(\n            current_user_id=node_database_reader.user_id, secret_name=database_connection.password_ref\n        )\n        if encrypted_password is None:\n            raise HTTPException(status_code=400, detail=\"Password not found\")\n    else:\n        database_reference_settings = get_local_database_connection(\n            database_settings.database_connection_name, node_database_reader.user_id\n        )\n        database_connection = database_reference_settings\n        encrypted_password = database_reference_settings.password.get_secret_value()\n\n    def _func():\n        sql_source = BaseSqlSource(\n            query=None if database_settings.query_mode == \"table\" else database_settings.query,\n            table_name=database_settings.table_name,\n            schema_name=database_settings.schema_name,\n            fields=node_database_reader.fields,\n        )\n        database_external_read_settings = (\n            sql_models.DatabaseExternalReadSettings.create_from_from_node_database_reader(\n                node_database_reader=node_database_reader,\n                password=encrypted_password,\n                query=sql_source.query,\n                database_reference_settings=(\n                    database_reference_settings if database_settings.connection_mode == \"reference\" else None\n                ),\n            )\n        )\n\n        external_database_fetcher = ExternalDatabaseFetcher(\n            database_external_read_settings, wait_on_completion=False\n        )\n        node._fetch_cached_df = external_database_fetcher\n        fl = FlowDataEngine(external_database_fetcher.get_result())\n        node_database_reader.fields = [c.get_minimal_field_info() for c in fl.schema]\n        return fl\n\n    def schema_callback():\n        sql_source = SqlSource(\n            connection_string=sql_utils.construct_sql_uri(\n                database_type=database_connection.database_type,\n                host=database_connection.host,\n                port=database_connection.port,\n                database=database_connection.database,\n                username=database_connection.username,\n                password=decrypt_secret(encrypted_password),\n            ),\n            query=None if database_settings.query_mode == \"table\" else database_settings.query,\n            table_name=database_settings.table_name,\n            schema_name=database_settings.schema_name,\n            fields=node_database_reader.fields,\n        )\n        return sql_source.get_schema()\n\n    node = self.get_node(node_database_reader.node_id)\n    if node:\n        node.node_type = node_type\n        node.name = node_type\n        node.function = _func\n        node.setting_input = node_database_reader\n        node.node_settings.cache_results = node_database_reader.cache_results\n        self.add_node_to_starting_list(node)\n        node.schema_callback = schema_callback\n    else:\n        node = FlowNode(\n            node_database_reader.node_id,\n            function=_func,\n            setting_input=node_database_reader,\n            name=node_type,\n            node_type=node_type,\n            parent_uuid=self.uuid,\n            schema_callback=schema_callback,\n        )\n        self._node_db[node_database_reader.node_id] = node\n        self.add_node_to_starting_list(node)\n        self._node_ids.append(node_database_reader.node_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_database_writer","title":"<code>add_database_writer(node_database_writer)</code>","text":"<p>Adds a node to write data to a database.</p> <p>Parameters:</p> Name Type Description Default <code>node_database_writer</code> <code>NodeDatabaseWriter</code> <p>The settings for the database writer node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_database_writer(self, node_database_writer: input_schema.NodeDatabaseWriter):\n    \"\"\"Adds a node to write data to a database.\n\n    Args:\n        node_database_writer: The settings for the database writer node.\n    \"\"\"\n\n    node_type = \"database_writer\"\n    database_settings: input_schema.DatabaseWriteSettings = node_database_writer.database_write_settings\n    database_connection: input_schema.DatabaseConnection | input_schema.FullDatabaseConnection | None\n    if database_settings.connection_mode == \"inline\":\n        database_connection: input_schema.DatabaseConnection = database_settings.database_connection\n        encrypted_password = get_encrypted_secret(\n            current_user_id=node_database_writer.user_id, secret_name=database_connection.password_ref\n        )\n        if encrypted_password is None:\n            raise HTTPException(status_code=400, detail=\"Password not found\")\n    else:\n        database_reference_settings = get_local_database_connection(\n            database_settings.database_connection_name, node_database_writer.user_id\n        )\n        encrypted_password = database_reference_settings.password.get_secret_value()\n\n    def _func(df: FlowDataEngine):\n        df.lazy = True\n        database_external_write_settings = (\n            sql_models.DatabaseExternalWriteSettings.create_from_from_node_database_writer(\n                node_database_writer=node_database_writer,\n                password=encrypted_password,\n                table_name=(\n                    database_settings.schema_name + \".\" + database_settings.table_name\n                    if database_settings.schema_name\n                    else database_settings.table_name\n                ),\n                database_reference_settings=(\n                    database_reference_settings if database_settings.connection_mode == \"reference\" else None\n                ),\n                lf=df.data_frame,\n            )\n        )\n        external_database_writer = ExternalDatabaseWriter(\n            database_external_write_settings, wait_on_completion=False\n        )\n        node._fetch_cached_df = external_database_writer\n        external_database_writer.get_result()\n        return df\n\n    def schema_callback():\n        input_node: FlowNode = self.get_node(node_database_writer.node_id).node_inputs.main_inputs[0]\n        return input_node.schema\n\n    self.add_node_step(\n        node_id=node_database_writer.node_id,\n        function=_func,\n        input_columns=[],\n        node_type=node_type,\n        setting_input=node_database_writer,\n        schema_callback=schema_callback,\n    )\n    node = self.get_node(node_database_writer.node_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_datasource","title":"<code>add_datasource(input_file)</code>","text":"<p>Adds a data source node to the graph.</p> <p>This method serves as a factory for creating starting nodes, handling both file-based sources and direct manual data entry.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>NodeDatasource | NodeManualInput</code> <p>The configuration object for the data source.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_datasource(self, input_file: input_schema.NodeDatasource | input_schema.NodeManualInput) -&gt; \"FlowGraph\":\n    \"\"\"Adds a data source node to the graph.\n\n    This method serves as a factory for creating starting nodes, handling both\n    file-based sources and direct manual data entry.\n\n    Args:\n        input_file: The configuration object for the data source.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n    if isinstance(input_file, input_schema.NodeManualInput):\n        input_data = FlowDataEngine(input_file.raw_data_format)\n        ref = \"manual_input\"\n    else:\n        input_data = FlowDataEngine(path_ref=input_file.file_ref)\n        ref = \"datasource\"\n    node = self.get_node(input_file.node_id)\n    if node:\n        node.node_type = ref\n        node.name = ref\n        node.function = input_data\n        node.setting_input = input_file\n        self.add_node_to_starting_list(node)\n\n    else:\n        input_data.collect()\n        node = FlowNode(\n            input_file.node_id,\n            function=input_data,\n            setting_input=input_file,\n            name=ref,\n            node_type=ref,\n            parent_uuid=self.uuid,\n        )\n        self._node_db[input_file.node_id] = node\n        self.add_node_to_starting_list(node)\n        self._node_ids.append(input_file.node_id)\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_dependency_on_polars_lazy_frame","title":"<code>add_dependency_on_polars_lazy_frame(lazy_frame, node_id)</code>","text":"<p>Adds a special node that directly injects a Polars LazyFrame into the graph.</p> <p>Note: This is intended for backend use and will not work in the UI editor.</p> <p>Parameters:</p> Name Type Description Default <code>lazy_frame</code> <code>LazyFrame</code> <p>The Polars LazyFrame to inject.</p> required <code>node_id</code> <code>int</code> <p>The ID for the new node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_dependency_on_polars_lazy_frame(self, lazy_frame: pl.LazyFrame, node_id: int):\n    \"\"\"Adds a special node that directly injects a Polars LazyFrame into the graph.\n\n    Note: This is intended for backend use and will not work in the UI editor.\n\n    Args:\n        lazy_frame: The Polars LazyFrame to inject.\n        node_id: The ID for the new node.\n    \"\"\"\n\n    def _func():\n        return FlowDataEngine(lazy_frame)\n\n    node_promise = input_schema.NodePromise(\n        flow_id=self.flow_id, node_id=node_id, node_type=\"polars_lazy_frame\", is_setup=True\n    )\n    self.add_node_step(\n        node_id=node_promise.node_id, node_type=node_promise.node_type, function=_func, setting_input=node_promise\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_explore_data","title":"<code>add_explore_data(node_analysis)</code>","text":"<p>Adds a specialized node for data exploration and visualization.</p> <p>Parameters:</p> Name Type Description Default <code>node_analysis</code> <code>NodeExploreData</code> <p>The settings for the data exploration node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_explore_data(self, node_analysis: input_schema.NodeExploreData):\n    \"\"\"Adds a specialized node for data exploration and visualization.\n\n    Args:\n        node_analysis: The settings for the data exploration node.\n    \"\"\"\n    sample_size: int = 10000\n\n    def analysis_preparation(flowfile_table: FlowDataEngine):\n        if flowfile_table.number_of_records &lt;= 0:\n            number_of_records = flowfile_table.get_number_of_records(calculate_in_worker_process=True)\n        else:\n            number_of_records = flowfile_table.number_of_records\n        if number_of_records &gt; sample_size:\n            flowfile_table = flowfile_table.get_sample(sample_size, random=True)\n        external_sampler = ExternalDfFetcher(\n            lf=flowfile_table.data_frame,\n            file_ref=\"__gf_walker\" + node.hash,\n            wait_on_completion=True,\n            node_id=node.node_id,\n            flow_id=self.flow_id,\n        )\n        node.results.analysis_data_generator = get_read_top_n(\n            external_sampler.status.file_ref, n=min(sample_size, number_of_records)\n        )\n        return flowfile_table\n\n    def schema_callback():\n        node = self.get_node(node_analysis.node_id)\n        if len(node.all_inputs) == 1:\n            input_node = node.all_inputs[0]\n            return input_node.schema\n        else:\n            return [FlowfileColumn.from_input(\"col_1\", \"na\")]\n\n    self.add_node_step(\n        node_id=node_analysis.node_id,\n        node_type=\"explore_data\",\n        function=analysis_preparation,\n        setting_input=node_analysis,\n        schema_callback=schema_callback,\n    )\n    node = self.get_node(node_analysis.node_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_external_source","title":"<code>add_external_source(external_source_input)</code>","text":"<p>Adds a node for a custom external data source.</p> <p>Parameters:</p> Name Type Description Default <code>external_source_input</code> <code>NodeExternalSource</code> <p>The settings for the external source node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_external_source(self, external_source_input: input_schema.NodeExternalSource):\n    \"\"\"Adds a node for a custom external data source.\n\n    Args:\n        external_source_input: The settings for the external source node.\n    \"\"\"\n\n    node_type = \"external_source\"\n    external_source_script = getattr(external_sources.custom_external_sources, external_source_input.identifier)\n    source_settings = getattr(\n        input_schema, snake_case_to_camel_case(external_source_input.identifier)\n    ).model_validate(external_source_input.source_settings)\n    if hasattr(external_source_script, \"initial_getter\"):\n        initial_getter = external_source_script.initial_getter(source_settings)\n    else:\n        initial_getter = None\n    data_getter = external_source_script.getter(source_settings)\n    external_source = data_source_factory(\n        source_type=\"custom\",\n        data_getter=data_getter,\n        initial_data_getter=initial_getter,\n        orientation=external_source_input.source_settings.orientation,\n        schema=None,\n    )\n\n    def _func():\n        logger.info(\"Calling external source\")\n        fl = FlowDataEngine.create_from_external_source(external_source=external_source)\n        external_source_input.source_settings.fields = [c.get_minimal_field_info() for c in fl.schema]\n        return fl\n\n    node = self.get_node(external_source_input.node_id)\n    if node:\n        node.node_type = node_type\n        node.name = node_type\n        node.function = _func\n        node.setting_input = external_source_input\n        node.node_settings.cache_results = external_source_input.cache_results\n        self.add_node_to_starting_list(node)\n\n    else:\n        node = FlowNode(\n            external_source_input.node_id,\n            function=_func,\n            setting_input=external_source_input,\n            name=node_type,\n            node_type=node_type,\n            parent_uuid=self.uuid,\n        )\n        self._node_db[external_source_input.node_id] = node\n        self.add_node_to_starting_list(node)\n        self._node_ids.append(external_source_input.node_id)\n    if external_source_input.source_settings.fields and len(external_source_input.source_settings.fields) &gt; 0:\n        logger.info(\"Using provided schema in the node\")\n\n        def schema_callback():\n            return [\n                FlowfileColumn.from_input(f.name, f.data_type) for f in external_source_input.source_settings.fields\n            ]\n\n        node.schema_callback = schema_callback\n    else:\n        logger.warning(\"Removing schema\")\n        node._schema_callback = None\n    self.add_node_step(\n        node_id=external_source_input.node_id,\n        function=_func,\n        input_columns=[],\n        node_type=node_type,\n        setting_input=external_source_input,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_filter","title":"<code>add_filter(filter_settings)</code>","text":"<p>Adds a filter node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>filter_settings</code> <code>NodeFilter</code> <p>The settings for the filter operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_filter(self, filter_settings: input_schema.NodeFilter):\n    \"\"\"Adds a filter node to the graph.\n\n    Args:\n        filter_settings: The settings for the filter operation.\n    \"\"\"\n\n    def _func(fl: FlowDataEngine):\n        is_advanced = filter_settings.filter_input.is_advanced()\n\n        if is_advanced:\n            predicate = filter_settings.filter_input.advanced_filter\n            return fl.do_filter(predicate)\n        else:\n            basic_filter = filter_settings.filter_input.basic_filter\n            if basic_filter is None:\n                logger.warning(\"Basic filter is None, returning unfiltered data\")\n                return fl\n\n            try:\n                field_data_type = fl.get_schema_column(basic_filter.field).generic_datatype()\n            except Exception:\n                field_data_type = None\n\n            expression = build_filter_expression(basic_filter, field_data_type)\n            filter_settings.filter_input.advanced_filter = expression\n            return fl.do_filter(expression)\n\n    self.add_node_step(\n        filter_settings.node_id,\n        _func,\n        node_type=\"filter\",\n        renew_schema=False,\n        setting_input=filter_settings,\n        input_node_ids=[filter_settings.depending_on_id],\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_formula","title":"<code>add_formula(function_settings)</code>","text":"<p>Adds a node that applies a formula to create or modify a column.</p> <p>Parameters:</p> Name Type Description Default <code>function_settings</code> <code>NodeFormula</code> <p>The settings for the formula operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_formula(self, function_settings: input_schema.NodeFormula):\n    \"\"\"Adds a node that applies a formula to create or modify a column.\n\n    Args:\n        function_settings: The settings for the formula operation.\n    \"\"\"\n\n    error = \"\"\n    if function_settings.function.field.data_type not in (None, transform_schema.AUTO_DATA_TYPE):\n        output_type = cast_str_to_polars_type(function_settings.function.field.data_type)\n    else:\n        output_type = None\n    if output_type not in (None, transform_schema.AUTO_DATA_TYPE):\n        new_col = [\n            FlowfileColumn.from_input(column_name=function_settings.function.field.name, data_type=str(output_type))\n        ]\n    else:\n        new_col = [FlowfileColumn.from_input(function_settings.function.field.name, \"String\")]\n\n    def _func(fl: FlowDataEngine):\n        return fl.apply_sql_formula(\n            func=function_settings.function.function,\n            col_name=function_settings.function.field.name,\n            output_data_type=output_type,\n        )\n\n    self.add_node_step(\n        function_settings.node_id,\n        _func,\n        output_schema=new_col,\n        node_type=\"formula\",\n        renew_schema=False,\n        setting_input=function_settings,\n        input_node_ids=[function_settings.depending_on_id],\n    )\n    if error != \"\":\n        node = self.get_node(function_settings.node_id)\n        node.results.errors = error\n        return False, error\n    else:\n        return True, \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_fuzzy_match","title":"<code>add_fuzzy_match(fuzzy_settings)</code>","text":"<p>Adds a fuzzy matching node to join data on approximate string matches.</p> <p>Parameters:</p> Name Type Description Default <code>fuzzy_settings</code> <code>NodeFuzzyMatch</code> <p>The settings for the fuzzy match operation.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_fuzzy_match(self, fuzzy_settings: input_schema.NodeFuzzyMatch) -&gt; \"FlowGraph\":\n    \"\"\"Adds a fuzzy matching node to join data on approximate string matches.\n\n    Args:\n        fuzzy_settings: The settings for the fuzzy match operation.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n\n    def _func(main: FlowDataEngine, right: FlowDataEngine) -&gt; FlowDataEngine:\n        node = self.get_node(node_id=fuzzy_settings.node_id)\n        if self.execution_location == \"local\":\n            return main.fuzzy_join(\n                fuzzy_match_input=deepcopy(fuzzy_settings.join_input),\n                other=right,\n                node_logger=self.flow_logger.get_node_logger(fuzzy_settings.node_id),\n            )\n\n        f = main.start_fuzzy_join(\n            fuzzy_match_input=deepcopy(fuzzy_settings.join_input),\n            other=right,\n            file_ref=node.hash,\n            flow_id=self.flow_id,\n            node_id=fuzzy_settings.node_id,\n        )\n        logger.info(\"Started the fuzzy match action\")\n        node._fetch_cached_df = f  # Add to the node so it can be cancelled and fetch later if needed\n        return FlowDataEngine(f.get_result())\n\n    def schema_callback():\n        fm_input_copy = FuzzyMatchInputManager(\n            fuzzy_settings.join_input\n        )  # Deepcopy create an unique object per func\n        node = self.get_node(node_id=fuzzy_settings.node_id)\n        return calculate_fuzzy_match_schema(\n            fm_input_copy,\n            left_schema=node.node_inputs.main_inputs[0].schema,\n            right_schema=node.node_inputs.right_input.schema,\n        )\n\n    self.add_node_step(\n        node_id=fuzzy_settings.node_id,\n        function=_func,\n        input_columns=[],\n        node_type=\"fuzzy_match\",\n        setting_input=fuzzy_settings,\n        input_node_ids=fuzzy_settings.depending_on_ids,\n        schema_callback=schema_callback,\n    )\n\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_graph_solver","title":"<code>add_graph_solver(graph_solver_settings)</code>","text":"<p>Adds a node that solves graph-like problems within the data.</p> <p>This node can be used for operations like finding network paths, calculating connected components, or performing other graph algorithms on relational data that represents nodes and edges.</p> <p>Parameters:</p> Name Type Description Default <code>graph_solver_settings</code> <code>NodeGraphSolver</code> <p>The settings object defining the graph inputs and the specific algorithm to apply.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_graph_solver(self, graph_solver_settings: input_schema.NodeGraphSolver):\n    \"\"\"Adds a node that solves graph-like problems within the data.\n\n    This node can be used for operations like finding network paths,\n    calculating connected components, or performing other graph algorithms\n    on relational data that represents nodes and edges.\n\n    Args:\n        graph_solver_settings: The settings object defining the graph inputs\n            and the specific algorithm to apply.\n    \"\"\"\n\n    def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n        return fl.solve_graph(graph_solver_settings.graph_solver_input)\n\n    self.add_node_step(\n        node_id=graph_solver_settings.node_id,\n        function=_func,\n        node_type=\"graph_solver\",\n        setting_input=graph_solver_settings,\n        input_node_ids=[graph_solver_settings.depending_on_id],\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_group_by","title":"<code>add_group_by(group_by_settings)</code>","text":"<p>Adds a group-by aggregation node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>group_by_settings</code> <code>NodeGroupBy</code> <p>The settings for the group-by operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_group_by(self, group_by_settings: input_schema.NodeGroupBy):\n    \"\"\"Adds a group-by aggregation node to the graph.\n\n    Args:\n        group_by_settings: The settings for the group-by operation.\n    \"\"\"\n\n    def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n        return fl.do_group_by(group_by_settings.groupby_input, False)\n\n    self.add_node_step(\n        node_id=group_by_settings.node_id,\n        function=_func,\n        node_type=\"group_by\",\n        setting_input=group_by_settings,\n        input_node_ids=[group_by_settings.depending_on_id],\n    )\n\n    node = self.get_node(group_by_settings.node_id)\n\n    def schema_callback():\n        output_columns = [(c.old_name, c.new_name, c.output_type) for c in group_by_settings.groupby_input.agg_cols]\n        depends_on = node.node_inputs.main_inputs[0]\n        input_schema_dict: dict[str, str] = {s.name: s.data_type for s in depends_on.schema}\n        output_schema = []\n        for old_name, new_name, data_type in output_columns:\n            data_type = input_schema_dict[old_name] if data_type is None else data_type\n            output_schema.append(FlowfileColumn.from_input(data_type=data_type, column_name=new_name))\n        return output_schema\n\n    node.schema_callback = schema_callback\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_include_cols","title":"<code>add_include_cols(include_columns)</code>","text":"<p>Adds columns to both the input and output column lists.</p> <p>Parameters:</p> Name Type Description Default <code>include_columns</code> <code>list[str]</code> <p>A list of column names to include.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_include_cols(self, include_columns: list[str]):\n    \"\"\"Adds columns to both the input and output column lists.\n\n    Args:\n        include_columns: A list of column names to include.\n    \"\"\"\n    for column in include_columns:\n        if column not in self._input_cols:\n            self._input_cols.append(column)\n        if column not in self._output_cols:\n            self._output_cols.append(column)\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_initial_node_analysis","title":"<code>add_initial_node_analysis(node_promise, track_history=True)</code>","text":"<p>Adds a data exploration/analysis node based on a node promise.</p> <p>Automatically captures history for undo/redo support.</p> <p>Parameters:</p> Name Type Description Default <code>node_promise</code> <code>NodePromise</code> <p>The promise representing the node to be analyzed.</p> required <code>track_history</code> <code>bool</code> <p>Whether to track this change in history (default True).</p> <code>True</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_initial_node_analysis(self, node_promise: input_schema.NodePromise, track_history: bool = True):\n    \"\"\"Adds a data exploration/analysis node based on a node promise.\n\n    Automatically captures history for undo/redo support.\n\n    Args:\n        node_promise: The promise representing the node to be analyzed.\n        track_history: Whether to track this change in history (default True).\n    \"\"\"\n\n    def _do_add():\n        node_analysis = create_graphic_walker_node_from_node_promise(node_promise)\n        self.add_explore_data(node_analysis)\n\n    if track_history:\n        self._execute_with_history(\n            _do_add,\n            HistoryActionType.ADD_NODE,\n            f\"Add {node_promise.node_type} node\",\n            node_id=node_promise.node_id,\n        )\n    else:\n        _do_add()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_join","title":"<code>add_join(join_settings)</code>","text":"<p>Adds a join node to combine two data streams based on key columns.</p> <p>Parameters:</p> Name Type Description Default <code>join_settings</code> <code>NodeJoin</code> <p>The settings for the join operation.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_join(self, join_settings: input_schema.NodeJoin) -&gt; \"FlowGraph\":\n    \"\"\"Adds a join node to combine two data streams based on key columns.\n\n    Args:\n        join_settings: The settings for the join operation.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n\n    def _func(main: FlowDataEngine, right: FlowDataEngine) -&gt; FlowDataEngine:\n        for left_select in join_settings.join_input.left_select.renames:\n            left_select.is_available = True if left_select.old_name in main.schema else False\n        for right_select in join_settings.join_input.right_select.renames:\n            right_select.is_available = True if right_select.old_name in right.schema else False\n        return main.join(\n            join_input=join_settings.join_input,\n            auto_generate_selection=join_settings.auto_generate_selection,\n            verify_integrity=False,\n            other=right,\n        )\n\n    self.add_node_step(\n        node_id=join_settings.node_id,\n        function=_func,\n        input_columns=[],\n        node_type=\"join\",\n        setting_input=join_settings,\n        input_node_ids=join_settings.depending_on_ids,\n    )\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_manual_input","title":"<code>add_manual_input(input_file)</code>","text":"<p>Adds a node for manual data entry.</p> <p>This is a convenience alias for <code>add_datasource</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>NodeManualInput</code> <p>The settings and data for the manual input node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_manual_input(self, input_file: input_schema.NodeManualInput):\n    \"\"\"Adds a node for manual data entry.\n\n    This is a convenience alias for `add_datasource`.\n\n    Args:\n        input_file: The settings and data for the manual input node.\n    \"\"\"\n    self.add_datasource(input_file)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_node_promise","title":"<code>add_node_promise(node_promise, track_history=True)</code>","text":"<p>Adds a placeholder node to the graph that is not yet fully configured.</p> <p>Useful for building the graph structure before all settings are available. Automatically captures history for undo/redo support.</p> <p>Parameters:</p> Name Type Description Default <code>node_promise</code> <code>NodePromise</code> <p>A promise object containing basic node information.</p> required <code>track_history</code> <code>bool</code> <p>Whether to track this change in history (default True).</p> <code>True</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_node_promise(self, node_promise: input_schema.NodePromise, track_history: bool = True):\n    \"\"\"Adds a placeholder node to the graph that is not yet fully configured.\n\n    Useful for building the graph structure before all settings are available.\n    Automatically captures history for undo/redo support.\n\n    Args:\n        node_promise: A promise object containing basic node information.\n        track_history: Whether to track this change in history (default True).\n    \"\"\"\n\n    def _do_add():\n        def placeholder(n: FlowNode = None):\n            if n is None:\n                return FlowDataEngine()\n            return n\n\n        self.add_node_step(\n            node_id=node_promise.node_id,\n            node_type=node_promise.node_type,\n            function=placeholder,\n            setting_input=node_promise,\n        )\n        if node_promise.is_user_defined:\n            node_needs_settings: bool\n            custom_node = CUSTOM_NODE_STORE.get(node_promise.node_type)\n            if custom_node is None:\n                raise Exception(f\"Custom node type '{node_promise.node_type}' not found in registry.\")\n            settings_schema = custom_node.model_fields[\"settings_schema\"].default\n            node_needs_settings = settings_schema is not None and not settings_schema.is_empty()\n            if not node_needs_settings:\n                user_defined_node_settings = input_schema.UserDefinedNode(settings={}, **node_promise.model_dump())\n                initialized_model = custom_node()\n                self.add_user_defined_node(\n                    custom_node=initialized_model, user_defined_node_settings=user_defined_node_settings\n                )\n\n    if track_history:\n        self._execute_with_history(\n            _do_add,\n            HistoryActionType.ADD_NODE,\n            f\"Add {node_promise.node_type} node\",\n            node_id=node_promise.node_id,\n        )\n    else:\n        _do_add()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_node_step","title":"<code>add_node_step(node_id, function, input_columns=None, output_schema=None, node_type=None, drop_columns=None, renew_schema=True, setting_input=None, cache_results=None, schema_callback=None, input_node_ids=None)</code>","text":"<p>The core method for adding or updating a node in the graph.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>int | str</code> <p>The unique ID for the node.</p> required <code>function</code> <code>Callable</code> <p>The core processing function for the node.</p> required <code>input_columns</code> <code>list[str]</code> <p>A list of input column names required by the function.</p> <code>None</code> <code>output_schema</code> <code>list[FlowfileColumn]</code> <p>A predefined schema for the node's output.</p> <code>None</code> <code>node_type</code> <code>str</code> <p>A string identifying the type of node (e.g., 'filter', 'join').</p> <code>None</code> <code>drop_columns</code> <code>list[str]</code> <p>A list of columns to be dropped after the function executes.</p> <code>None</code> <code>renew_schema</code> <code>bool</code> <p>If True, the schema is recalculated after execution.</p> <code>True</code> <code>setting_input</code> <code>Any</code> <p>A configuration object containing settings for the node.</p> <code>None</code> <code>cache_results</code> <code>bool</code> <p>If True, the node's results are cached for future runs.</p> <code>None</code> <code>schema_callback</code> <code>Callable</code> <p>A function that dynamically calculates the output schema.</p> <code>None</code> <code>input_node_ids</code> <code>list[int]</code> <p>A list of IDs for the nodes that this node depends on.</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowNode</code> <p>The created or updated FlowNode object.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_node_step(\n    self,\n    node_id: int | str,\n    function: Callable,\n    input_columns: list[str] = None,\n    output_schema: list[FlowfileColumn] = None,\n    node_type: str = None,\n    drop_columns: list[str] = None,\n    renew_schema: bool = True,\n    setting_input: Any = None,\n    cache_results: bool = None,\n    schema_callback: Callable = None,\n    input_node_ids: list[int] = None,\n) -&gt; FlowNode:\n    \"\"\"The core method for adding or updating a node in the graph.\n\n    Args:\n        node_id: The unique ID for the node.\n        function: The core processing function for the node.\n        input_columns: A list of input column names required by the function.\n        output_schema: A predefined schema for the node's output.\n        node_type: A string identifying the type of node (e.g., 'filter', 'join').\n        drop_columns: A list of columns to be dropped after the function executes.\n        renew_schema: If True, the schema is recalculated after execution.\n        setting_input: A configuration object containing settings for the node.\n        cache_results: If True, the node's results are cached for future runs.\n        schema_callback: A function that dynamically calculates the output schema.\n        input_node_ids: A list of IDs for the nodes that this node depends on.\n\n    Returns:\n        The created or updated FlowNode object.\n    \"\"\"\n    # Wrap schema_callback with output_field_config support\n    # If the node has output_field_config enabled, use it for schema prediction\n    output_field_config = getattr(setting_input, \"output_field_config\", None) if setting_input else None\n\n    logger.info(\n        f\"add_node_step: node_id={node_id}, node_type={node_type}, \"\n        f\"has_setting_input={setting_input is not None}, \"\n        f\"has_output_field_config={output_field_config is not None}, \"\n        f\"config_enabled={output_field_config.enabled if output_field_config else False}, \"\n        f\"has_schema_callback={schema_callback is not None}\"\n    )\n\n    # IMPORTANT: Always create wrapped callback if output_field_config exists (even if enabled=False)\n    # This ensures nodes like PolarsCode get a schema callback when output_field_config is defined\n    if output_field_config:\n        if output_field_config.enabled:\n            logger.info(\n                f\"add_node_step: Creating/wrapping schema_callback for node {node_id} with output_field_config \"\n                f\"(validation_mode={output_field_config.validation_mode_behavior}, {len(output_field_config.fields)} fields, \"\n                f\"base_callback={'present' if schema_callback else 'None'})\"\n            )\n        else:\n            logger.debug(f\"add_node_step: output_field_config present for node {node_id} but disabled\")\n\n        # Even if schema_callback is None, create a wrapped one for output_field_config\n        schema_callback = create_schema_callback_with_output_config(schema_callback, output_field_config)\n        logger.info(\n            f\"add_node_step: schema_callback {'created' if schema_callback else 'failed'} for node {node_id}\"\n        )\n\n    existing_node = self.get_node(node_id)\n    if existing_node is not None:\n        if existing_node.node_type != node_type:\n            self.delete_node(existing_node.node_id)\n            existing_node = None\n    if existing_node:\n        input_nodes = existing_node.all_inputs\n    elif input_node_ids is not None:\n        input_nodes = [self.get_node(node_id) for node_id in input_node_ids]\n    else:\n        input_nodes = None\n    if isinstance(input_columns, str):\n        input_columns = [input_columns]\n    if (\n        input_nodes is not None\n        or function.__name__ in (\"placeholder\", \"analysis_preparation\")\n        or node_type in (\"cloud_storage_reader\", \"polars_lazy_frame\", \"input_data\")\n    ):\n        if not existing_node:\n            node = FlowNode(\n                node_id=node_id,\n                function=function,\n                output_schema=output_schema,\n                input_columns=input_columns,\n                drop_columns=drop_columns,\n                renew_schema=renew_schema,\n                setting_input=setting_input,\n                node_type=node_type,\n                name=function.__name__,\n                schema_callback=schema_callback,\n                parent_uuid=self.uuid,\n            )\n        else:\n            existing_node.update_node(\n                function=function,\n                output_schema=output_schema,\n                input_columns=input_columns,\n                drop_columns=drop_columns,\n                setting_input=setting_input,\n                schema_callback=schema_callback,\n            )\n            node = existing_node\n    else:\n        raise Exception(\"No data initialized\")\n    self._node_db[node_id] = node\n    self._node_ids.append(node_id)\n    return node\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_node_to_starting_list","title":"<code>add_node_to_starting_list(node)</code>","text":"<p>Adds a node to the list of starting nodes for the flow if not already present.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>FlowNode</code> <p>The FlowNode to add as a starting node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_node_to_starting_list(self, node: FlowNode) -&gt; None:\n    \"\"\"Adds a node to the list of starting nodes for the flow if not already present.\n\n    Args:\n        node: The FlowNode to add as a starting node.\n    \"\"\"\n    if node.node_id not in {self_node.node_id for self_node in self._flow_starts}:\n        self._flow_starts.append(node)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_output","title":"<code>add_output(output_file)</code>","text":"<p>Adds an output node to write the final data to a destination.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>NodeOutput</code> <p>The settings for the output file.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_output(self, output_file: input_schema.NodeOutput):\n    \"\"\"Adds an output node to write the final data to a destination.\n\n    Args:\n        output_file: The settings for the output file.\n    \"\"\"\n\n    def _func(df: FlowDataEngine):\n        execute_remote = self.execution_location != \"local\"\n        df.output(\n            output_fs=output_file.output_settings,\n            flow_id=self.flow_id,\n            node_id=output_file.node_id,\n            execute_remote=execute_remote,\n        )\n        return df\n\n    def schema_callback():\n        input_node: FlowNode = self.get_node(output_file.node_id).node_inputs.main_inputs[0]\n\n        return input_node.schema\n\n    input_node_id = output_file.depending_on_id if hasattr(output_file, \"depending_on_id\") else None\n    self.add_node_step(\n        node_id=output_file.node_id,\n        function=_func,\n        input_columns=[],\n        node_type=\"output\",\n        setting_input=output_file,\n        schema_callback=schema_callback,\n        input_node_ids=[input_node_id],\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_pivot","title":"<code>add_pivot(pivot_settings)</code>","text":"<p>Adds a pivot node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>pivot_settings</code> <code>NodePivot</code> <p>The settings for the pivot operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_pivot(self, pivot_settings: input_schema.NodePivot):\n    \"\"\"Adds a pivot node to the graph.\n\n    Args:\n        pivot_settings: The settings for the pivot operation.\n    \"\"\"\n\n    def _func(fl: FlowDataEngine):\n        return fl.do_pivot(pivot_settings.pivot_input, self.flow_logger.get_node_logger(pivot_settings.node_id))\n\n    self.add_node_step(\n        node_id=pivot_settings.node_id,\n        function=_func,\n        node_type=\"pivot\",\n        setting_input=pivot_settings,\n        input_node_ids=[pivot_settings.depending_on_id],\n    )\n\n    node = self.get_node(pivot_settings.node_id)\n\n    def schema_callback():\n        input_data = node.singular_main_input.get_resulting_data()  # get from the previous step the data\n        input_data.lazy = True  # ensure the dataset is lazy\n        input_lf = input_data.data_frame  # get the lazy frame\n        return pre_calculate_pivot_schema(input_data.schema, pivot_settings.pivot_input, input_lf=input_lf)\n\n    node.schema_callback = schema_callback\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_polars_code","title":"<code>add_polars_code(node_polars_code)</code>","text":"<p>Adds a node that executes custom Polars code.</p> <p>Parameters:</p> Name Type Description Default <code>node_polars_code</code> <code>NodePolarsCode</code> <p>The settings for the Polars code node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_polars_code(self, node_polars_code: input_schema.NodePolarsCode):\n    \"\"\"Adds a node that executes custom Polars code.\n\n    Args:\n        node_polars_code: The settings for the Polars code node.\n    \"\"\"\n\n    def _func(*flowfile_tables: FlowDataEngine) -&gt; FlowDataEngine:\n        return execute_polars_code(*flowfile_tables, code=node_polars_code.polars_code_input.polars_code)\n\n    self.add_node_step(\n        node_id=node_polars_code.node_id,\n        function=_func,\n        node_type=\"polars_code\",\n        setting_input=node_polars_code,\n        input_node_ids=node_polars_code.depending_on_ids,\n    )\n\n    try:\n        polars_code_parser.validate_code(node_polars_code.polars_code_input.polars_code)\n    except Exception as e:\n        node = self.get_node(node_id=node_polars_code.node_id)\n        node.results.errors = str(e)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_python_script","title":"<code>add_python_script(node_python_script)</code>","text":"<p>Adds a node that executes Python code on a kernel container.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_python_script(self, node_python_script: input_schema.NodePythonScript):\n    \"\"\"Adds a node that executes Python code on a kernel container.\"\"\"\n\n    def _func(*flowfile_tables: FlowDataEngine) -&gt; FlowDataEngine:\n        kernel_id = node_python_script.python_script_input.kernel_id\n        code = node_python_script.python_script_input.code\n\n        if not kernel_id:\n            raise ValueError(\"No kernel selected for python_script node\")\n\n        manager = get_kernel_manager()\n        node_id = node_python_script.node_id\n        flow_id = self.flow_id\n        node_logger = self.flow_logger.get_node_logger(node_id)\n\n        # 1. Make upstream artifacts visible to the kernel\n        self.artifact_context.compute_available(\n            node_id=node_id,\n            kernel_id=kernel_id,\n            upstream_node_ids=self._get_upstream_node_ids(node_id),\n        )\n\n        # 2. Write input tables to the shared volume\n        shared_base = manager.shared_volume_path\n        input_dir = os.path.join(shared_base, str(flow_id), str(node_id), \"inputs\")\n        output_dir = os.path.join(shared_base, str(flow_id), str(node_id), \"outputs\")\n        os.makedirs(input_dir, exist_ok=True)\n        os.makedirs(output_dir, exist_ok=True)\n        self.flow_logger.info(f\"Prepared shared directories for kernel execution: {input_dir}, {output_dir}\")\n\n        input_paths = write_inputs_to_parquet(flowfile_tables, manager, input_dir, flow_id, node_id)\n\n        # 3. Build request and execute on the kernel\n        request = build_execute_request(\n            node_id=node_id,\n            code=code,\n            input_paths=input_paths,\n            output_dir=output_dir,\n            flow_id=flow_id,\n            manager=manager,\n            source_registration_id=self._flow_settings.source_registration_id,\n        )\n\n        node = self.get_node(node_id)\n        cancel_event = threading.Event()\n        node._kernel_cancel_context = (kernel_id, manager)\n        node._kernel_cancel_event = cancel_event\n        try:\n            result = manager.execute_sync(\n                kernel_id, request, self.flow_logger, cancel_event=cancel_event\n            )\n        finally:\n            node._kernel_cancel_context = None\n            node._kernel_cancel_event = None\n\n        # 4. Forward kernel stdout/stderr and check for errors\n        forward_kernel_logs(result, node_logger)\n        if not result.success:\n            raise RuntimeError(f\"Kernel execution failed: {result.error}\")\n\n        # 5. Record artifact changes\n        if result.artifacts_published:\n            self.artifact_context.record_published(\n                node_id=node_id,\n                kernel_id=kernel_id,\n                artifacts=[{\"name\": n} for n in result.artifacts_published],\n            )\n        if result.artifacts_deleted:\n            self.artifact_context.record_deleted(\n                node_id=node_id,\n                kernel_id=kernel_id,\n                artifact_names=result.artifacts_deleted,\n            )\n\n        # 6. Read output parquet or pass through first input\n        output_path = os.path.join(output_dir, \"main.parquet\")\n        if os.path.exists(output_path):\n            return FlowDataEngine(pl.scan_parquet(output_path))\n        return flowfile_tables[0] if flowfile_tables else FlowDataEngine(pl.LazyFrame())\n\n    def schema_callback():\n        \"\"\"Best-effort schema prediction for python_script nodes.\n\n        Returns the input node(s) schema as a reasonable default\n        (most python_script nodes transform and pass through).\n        If nothing is available, returns [] \u2014 never raises.\n        \"\"\"\n        try:\n            node = self.get_node(node_python_script.node_id)\n            if node is None:\n                return []\n\n            main_inputs = node.node_inputs.main_inputs\n            if main_inputs:\n                first_input = main_inputs[0]\n                input_node_schema = first_input.schema\n                if input_node_schema:\n                    return input_node_schema\n            return []\n        except Exception:\n            return []\n\n    self.add_node_step(\n        node_id=node_python_script.node_id,\n        function=_func,\n        node_type=\"python_script\",\n        setting_input=node_python_script,\n        input_node_ids=node_python_script.depending_on_ids,\n        schema_callback=schema_callback,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_read","title":"<code>add_read(input_file)</code>","text":"<p>Adds a node to read data from a local file (e.g., CSV, Parquet, Excel).</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>NodeRead</code> <p>The settings for the read operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_read(self, input_file: input_schema.NodeRead):\n    \"\"\"Adds a node to read data from a local file (e.g., CSV, Parquet, Excel).\n\n    Args:\n        input_file: The settings for the read operation.\n    \"\"\"\n    if (\n        input_file.received_file.file_type in (\"xlsx\", \"excel\")\n        and input_file.received_file.table_settings.sheet_name == \"\"\n    ):\n        sheet_name = fastexcel.read_excel(input_file.received_file.path).sheet_names[0]\n        input_file.received_file.table_settings.sheet_name = sheet_name\n\n    received_file = input_file.received_file\n    input_file.received_file.set_absolute_filepath()\n\n    def _func():\n        input_file.received_file.set_absolute_filepath()\n        if input_file.received_file.file_type == \"parquet\":\n            input_data = FlowDataEngine.create_from_path(input_file.received_file)\n        elif (\n            input_file.received_file.file_type == \"csv\"\n            and \"utf\" in input_file.received_file.table_settings.encoding\n        ):\n            input_data = FlowDataEngine.create_from_path(input_file.received_file)\n        else:\n            input_data = FlowDataEngine.create_from_path_worker(\n                input_file.received_file, node_id=input_file.node_id, flow_id=self.flow_id\n            )\n        input_data.name = input_file.received_file.name\n        return input_data\n\n    node = self.get_node(input_file.node_id)\n    schema_callback = None\n    if node:\n        start_hash = node.hash\n        node.node_type = \"read\"\n        node.name = \"read\"\n        node.function = _func\n        node.setting_input = input_file\n        self.add_node_to_starting_list(node)\n\n        if start_hash != node.hash:\n            logger.info(\"Hash changed, updating schema\")\n            if len(received_file.fields) &gt; 0:\n                # If the file has fields defined, we can use them to create the schema\n                def schema_callback():\n                    return [FlowfileColumn.from_input(f.name, f.data_type) for f in received_file.fields]\n\n            elif input_file.received_file.file_type in (\"csv\", \"json\", \"parquet\"):\n                # everything that can be scanned by polars\n                def schema_callback():\n                    input_data = FlowDataEngine.create_from_path(input_file.received_file)\n                    return input_data.schema\n\n            elif input_file.received_file.file_type in (\"xlsx\", \"excel\"):\n                # If the file is an Excel file, we need to use the openpyxl engine to read the schema\n                schema_callback = get_xlsx_schema_callback(\n                    engine=\"openpyxl\",\n                    file_path=received_file.file_path,\n                    sheet_name=received_file.table_settings.sheet_name,\n                    start_row=received_file.table_settings.start_row,\n                    end_row=received_file.table_settings.end_row,\n                    start_column=received_file.table_settings.start_column,\n                    end_column=received_file.table_settings.end_column,\n                    has_headers=received_file.table_settings.has_headers,\n                )\n            else:\n                schema_callback = None\n    else:\n        node = FlowNode(\n            input_file.node_id,\n            function=_func,\n            setting_input=input_file,\n            name=\"read\",\n            node_type=\"read\",\n            parent_uuid=self.uuid,\n        )\n        self._node_db[input_file.node_id] = node\n        self.add_node_to_starting_list(node)\n        self._node_ids.append(input_file.node_id)\n\n    if schema_callback is not None:\n        node.schema_callback = schema_callback\n        node.user_provided_schema_callback = schema_callback\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_record_count","title":"<code>add_record_count(node_number_of_records)</code>","text":"<p>Adds a filter node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>node_number_of_records</code> <code>NodeRecordCount</code> <p>The settings for the record count operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_record_count(self, node_number_of_records: input_schema.NodeRecordCount):\n    \"\"\"Adds a filter node to the graph.\n\n    Args:\n        node_number_of_records: The settings for the record count operation.\n    \"\"\"\n\n    def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n        return fl.get_record_count()\n\n    self.add_node_step(\n        node_id=node_number_of_records.node_id,\n        function=_func,\n        node_type=\"record_count\",\n        setting_input=node_number_of_records,\n        input_node_ids=[node_number_of_records.depending_on_id],\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_record_id","title":"<code>add_record_id(record_id_settings)</code>","text":"<p>Adds a node to create a new column with a unique ID for each record.</p> <p>Parameters:</p> Name Type Description Default <code>record_id_settings</code> <code>NodeRecordId</code> <p>The settings object specifying the name of the new record ID column.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_record_id(self, record_id_settings: input_schema.NodeRecordId) -&gt; \"FlowGraph\":\n    \"\"\"Adds a node to create a new column with a unique ID for each record.\n\n    Args:\n        record_id_settings: The settings object specifying the name of the\n            new record ID column.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n\n    def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n        return table.add_record_id(record_id_settings.record_id_input)\n\n    self.add_node_step(\n        node_id=record_id_settings.node_id,\n        function=_func,\n        node_type=\"record_id\",\n        setting_input=record_id_settings,\n        input_node_ids=[record_id_settings.depending_on_id],\n    )\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_sample","title":"<code>add_sample(sample_settings)</code>","text":"<p>Adds a node to take a random or top-N sample of the data.</p> <p>Parameters:</p> Name Type Description Default <code>sample_settings</code> <code>NodeSample</code> <p>The settings object specifying the size of the sample.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_sample(self, sample_settings: input_schema.NodeSample) -&gt; \"FlowGraph\":\n    \"\"\"Adds a node to take a random or top-N sample of the data.\n\n    Args:\n        sample_settings: The settings object specifying the size of the sample.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n\n    def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n        return table.get_sample(sample_settings.sample_size)\n\n    self.add_node_step(\n        node_id=sample_settings.node_id,\n        function=_func,\n        node_type=\"sample\",\n        setting_input=sample_settings,\n        input_node_ids=[sample_settings.depending_on_id],\n    )\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_select","title":"<code>add_select(select_settings)</code>","text":"<p>Adds a node to select, rename, reorder, or drop columns.</p> <p>Parameters:</p> Name Type Description Default <code>select_settings</code> <code>NodeSelect</code> <p>The settings for the select operation.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_select(self, select_settings: input_schema.NodeSelect) -&gt; \"FlowGraph\":\n    \"\"\"Adds a node to select, rename, reorder, or drop columns.\n\n    Args:\n        select_settings: The settings for the select operation.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n\n    select_cols = select_settings.select_input\n    drop_cols = tuple(s.old_name for s in select_settings.select_input)\n\n    def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n        input_cols = set(f.name for f in table.schema)\n        ids_to_remove = []\n        for i, select_col in enumerate(select_cols):\n            if select_col.data_type is None:\n                select_col.data_type = table.get_schema_column(select_col.old_name).data_type\n            if select_col.old_name not in input_cols:\n                select_col.is_available = False\n                if not select_col.keep:\n                    ids_to_remove.append(i)\n            else:\n                select_col.is_available = True\n        ids_to_remove.reverse()\n        for i in ids_to_remove:\n            v = select_cols.pop(i)\n            del v\n        return table.do_select(\n            select_inputs=transform_schema.SelectInputs(select_cols), keep_missing=select_settings.keep_missing\n        )\n\n    self.add_node_step(\n        node_id=select_settings.node_id,\n        function=_func,\n        input_columns=[],\n        node_type=\"select\",\n        drop_columns=list(drop_cols),\n        setting_input=select_settings,\n        input_node_ids=[select_settings.depending_on_id],\n    )\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_sort","title":"<code>add_sort(sort_settings)</code>","text":"<p>Adds a node to sort the data based on one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>sort_settings</code> <code>NodeSort</code> <p>The settings for the sort operation.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_sort(self, sort_settings: input_schema.NodeSort) -&gt; \"FlowGraph\":\n    \"\"\"Adds a node to sort the data based on one or more columns.\n\n    Args:\n        sort_settings: The settings for the sort operation.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n\n    def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n        return table.do_sort(sort_settings.sort_input)\n\n    self.add_node_step(\n        node_id=sort_settings.node_id,\n        function=_func,\n        node_type=\"sort\",\n        setting_input=sort_settings,\n        input_node_ids=[sort_settings.depending_on_id],\n    )\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_sql_source","title":"<code>add_sql_source(external_source_input)</code>","text":"<p>Adds a node that reads data from a SQL source.</p> <p>This is a convenience alias for <code>add_external_source</code>.</p> <p>Parameters:</p> Name Type Description Default <code>external_source_input</code> <code>NodeExternalSource</code> <p>The settings for the external SQL source node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_sql_source(self, external_source_input: input_schema.NodeExternalSource):\n    \"\"\"Adds a node that reads data from a SQL source.\n\n    This is a convenience alias for `add_external_source`.\n\n    Args:\n        external_source_input: The settings for the external SQL source node.\n    \"\"\"\n    logger.info(\"Adding sql source\")\n    self.add_external_source(external_source_input)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_text_to_rows","title":"<code>add_text_to_rows(node_text_to_rows)</code>","text":"<p>Adds a node that splits cell values into multiple rows.</p> <p>This is useful for un-nesting data where a single field contains multiple values separated by a delimiter.</p> <p>Parameters:</p> Name Type Description Default <code>node_text_to_rows</code> <code>NodeTextToRows</code> <p>The settings object that specifies the column to split and the delimiter to use.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_text_to_rows(self, node_text_to_rows: input_schema.NodeTextToRows) -&gt; \"FlowGraph\":\n    \"\"\"Adds a node that splits cell values into multiple rows.\n\n    This is useful for un-nesting data where a single field contains multiple\n    values separated by a delimiter.\n\n    Args:\n        node_text_to_rows: The settings object that specifies the column to split\n            and the delimiter to use.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n\n    def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n        return table.split(node_text_to_rows.text_to_rows_input)\n\n    self.add_node_step(\n        node_id=node_text_to_rows.node_id,\n        function=_func,\n        node_type=\"text_to_rows\",\n        setting_input=node_text_to_rows,\n        input_node_ids=[node_text_to_rows.depending_on_id],\n    )\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_union","title":"<code>add_union(union_settings)</code>","text":"<p>Adds a union node to combine multiple data streams.</p> <p>Parameters:</p> Name Type Description Default <code>union_settings</code> <code>NodeUnion</code> <p>The settings for the union operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_union(self, union_settings: input_schema.NodeUnion):\n    \"\"\"Adds a union node to combine multiple data streams.\n\n    Args:\n        union_settings: The settings for the union operation.\n    \"\"\"\n\n    def _func(*flowfile_tables: FlowDataEngine):\n        dfs: list[pl.LazyFrame] | list[pl.DataFrame] = [flt.data_frame for flt in flowfile_tables]\n        return FlowDataEngine(pl.concat(dfs, how=\"diagonal_relaxed\"))\n\n    self.add_node_step(\n        node_id=union_settings.node_id,\n        function=_func,\n        node_type=\"union\",\n        setting_input=union_settings,\n        input_node_ids=union_settings.depending_on_ids,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_unique","title":"<code>add_unique(unique_settings)</code>","text":"<p>Adds a node to find and remove duplicate rows.</p> <p>Parameters:</p> Name Type Description Default <code>unique_settings</code> <code>NodeUnique</code> <p>The settings for the unique operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_unique(self, unique_settings: input_schema.NodeUnique):\n    \"\"\"Adds a node to find and remove duplicate rows.\n\n    Args:\n        unique_settings: The settings for the unique operation.\n    \"\"\"\n\n    def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n        return fl.make_unique(unique_settings.unique_input)\n\n    self.add_node_step(\n        node_id=unique_settings.node_id,\n        function=_func,\n        input_columns=[],\n        node_type=\"unique\",\n        setting_input=unique_settings,\n        input_node_ids=[unique_settings.depending_on_id],\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_unpivot","title":"<code>add_unpivot(unpivot_settings)</code>","text":"<p>Adds an unpivot node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>unpivot_settings</code> <code>NodeUnpivot</code> <p>The settings for the unpivot operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>@with_history_capture(HistoryActionType.UPDATE_SETTINGS)\ndef add_unpivot(self, unpivot_settings: input_schema.NodeUnpivot):\n    \"\"\"Adds an unpivot node to the graph.\n\n    Args:\n        unpivot_settings: The settings for the unpivot operation.\n    \"\"\"\n\n    def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n        return fl.unpivot(unpivot_settings.unpivot_input)\n\n    self.add_node_step(\n        node_id=unpivot_settings.node_id,\n        function=_func,\n        node_type=\"unpivot\",\n        setting_input=unpivot_settings,\n        input_node_ids=[unpivot_settings.depending_on_id],\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_user_defined_node","title":"<code>add_user_defined_node(*, custom_node, user_defined_node_settings)</code>","text":"<p>Adds a user-defined custom node to the graph.</p> <p>When the custom node has a <code>kernel_id</code> set, the process code is sent to the kernel for execution instead of running locally.  This enables custom nodes to use external packages installed on the kernel.</p> <p>Parameters:</p> Name Type Description Default <code>custom_node</code> <code>CustomNodeBase</code> <p>The custom node instance to add.</p> required <code>user_defined_node_settings</code> <code>UserDefinedNode</code> <p>The settings for the user-defined node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_user_defined_node(\n    self, *, custom_node: CustomNodeBase, user_defined_node_settings: input_schema.UserDefinedNode\n):\n    \"\"\"Adds a user-defined custom node to the graph.\n\n    When the custom node has a ``kernel_id`` set, the process code is sent\n    to the kernel for execution instead of running locally.  This enables\n    custom nodes to use external packages installed on the kernel.\n\n    Args:\n        custom_node: The custom node instance to add.\n        user_defined_node_settings: The settings for the user-defined node.\n    \"\"\"\n    # Enforce kernel selection when executing a kernel-required custom node\n    if custom_node.requires_kernel and not user_defined_node_settings.kernel_id:\n        raise ValueError(\"Kernel selection is required to execute this custom node.\")\n\n    # Propagate kernel_id from the schema settings if present\n    kernel_id = user_defined_node_settings.kernel_id or custom_node.kernel_id\n    output_names = user_defined_node_settings.output_names or custom_node.output_names\n\n    if kernel_id:\n        _func = self._make_kernel_user_defined_func(\n            custom_node=custom_node,\n            user_defined_node_settings=user_defined_node_settings,\n            kernel_id=kernel_id,\n            output_names=output_names,\n        )\n    else:\n        _func = self._make_local_user_defined_func(\n            custom_node=custom_node,\n            user_defined_node_settings=user_defined_node_settings,\n        )\n\n    self.add_node_step(\n        node_id=user_defined_node_settings.node_id,\n        function=_func,\n        setting_input=user_defined_node_settings,\n        input_node_ids=user_defined_node_settings.depending_on_ids,\n        node_type=custom_node.item,\n    )\n    if custom_node.number_of_inputs == 0:\n        node = self.get_node(user_defined_node_settings.node_id)\n        self.add_node_to_starting_list(node)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.apply_layout","title":"<code>apply_layout(y_spacing=150, x_spacing=200, initial_y=100)</code>","text":"<p>Calculates and applies a layered layout to all nodes in the graph.</p> <p>This updates their x and y positions for UI rendering.</p> <p>Parameters:</p> Name Type Description Default <code>y_spacing</code> <code>int</code> <p>The vertical spacing between layers.</p> <code>150</code> <code>x_spacing</code> <code>int</code> <p>The horizontal spacing between nodes in the same layer.</p> <code>200</code> <code>initial_y</code> <code>int</code> <p>The initial y-position for the first layer.</p> <code>100</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def apply_layout(self, y_spacing: int = 150, x_spacing: int = 200, initial_y: int = 100):\n    \"\"\"Calculates and applies a layered layout to all nodes in the graph.\n\n    This updates their x and y positions for UI rendering.\n\n    Args:\n        y_spacing: The vertical spacing between layers.\n        x_spacing: The horizontal spacing between nodes in the same layer.\n        initial_y: The initial y-position for the first layer.\n    \"\"\"\n    self.flow_logger.info(\"Applying layered layout...\")\n    start_time = time()\n    try:\n        # Calculate new positions for all nodes\n        new_positions = calculate_layered_layout(\n            self, y_spacing=y_spacing, x_spacing=x_spacing, initial_y=initial_y\n        )\n\n        if not new_positions:\n            self.flow_logger.warning(\"Layout calculation returned no positions.\")\n            return\n\n        # Apply the new positions to the setting_input of each node\n        updated_count = 0\n        for node_id, (pos_x, pos_y) in new_positions.items():\n            node = self.get_node(node_id)\n            if node and hasattr(node, \"setting_input\"):\n                setting = node.setting_input\n                if hasattr(setting, \"pos_x\") and hasattr(setting, \"pos_y\"):\n                    setting.pos_x = pos_x\n                    setting.pos_y = pos_y\n                    updated_count += 1\n                else:\n                    self.flow_logger.warning(\n                        f\"Node {node_id} setting_input ({type(setting)}) lacks pos_x/pos_y attributes.\"\n                    )\n            elif node:\n                self.flow_logger.warning(f\"Node {node_id} lacks setting_input attribute.\")\n            # else: Node not found, already warned by calculate_layered_layout\n\n        end_time = time()\n        self.flow_logger.info(\n            f\"Layout applied to {updated_count}/{len(self.nodes)} nodes in {end_time - start_time:.2f} seconds.\"\n        )\n\n    except Exception as e:\n        self.flow_logger.error(f\"Error applying layout: {e}\")\n        raise  # Optional: re-raise the exception\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.cancel","title":"<code>cancel()</code>","text":"<p>Cancels an ongoing graph execution.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def cancel(self):\n    \"\"\"Cancels an ongoing graph execution.\"\"\"\n\n    if not self.flow_settings.is_running:\n        return\n    self.flow_settings.is_canceled = True\n    for node in self.nodes:\n        node.cancel()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.capture_history_if_changed","title":"<code>capture_history_if_changed(pre_snapshot, action_type, description, node_id=None)</code>","text":"<p>Capture history only if the flow state actually changed.</p> <p>Use this for settings updates where the change might be a no-op. Call this AFTER the change is applied.</p> <p>Parameters:</p> Name Type Description Default <code>pre_snapshot</code> <code>FlowfileData</code> <p>The FlowfileData captured BEFORE the change.</p> required <code>action_type</code> <code>HistoryActionType</code> <p>The type of action that was performed.</p> required <code>description</code> <code>str</code> <p>Human-readable description of the action.</p> required <code>node_id</code> <code>int</code> <p>Optional ID of the affected node.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if a change was detected and snapshot was captured.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def capture_history_if_changed(\n    self,\n    pre_snapshot: schemas.FlowfileData,\n    action_type: HistoryActionType,\n    description: str,\n    node_id: int = None,\n) -&gt; bool:\n    \"\"\"Capture history only if the flow state actually changed.\n\n    Use this for settings updates where the change might be a no-op.\n    Call this AFTER the change is applied.\n\n    Args:\n        pre_snapshot: The FlowfileData captured BEFORE the change.\n        action_type: The type of action that was performed.\n        description: Human-readable description of the action.\n        node_id: Optional ID of the affected node.\n\n    Returns:\n        True if a change was detected and snapshot was captured.\n    \"\"\"\n    return self._history_manager.capture_if_changed(self, pre_snapshot, action_type, description, node_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.capture_history_snapshot","title":"<code>capture_history_snapshot(action_type, description, node_id=None)</code>","text":"<p>Capture the current state before a change for undo support.</p> <p>Parameters:</p> Name Type Description Default <code>action_type</code> <code>HistoryActionType</code> <p>The type of action being performed.</p> required <code>description</code> <code>str</code> <p>Human-readable description of the action.</p> required <code>node_id</code> <code>int</code> <p>Optional ID of the affected node.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if snapshot was captured, False if skipped.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def capture_history_snapshot(\n    self,\n    action_type: HistoryActionType,\n    description: str,\n    node_id: int = None,\n) -&gt; bool:\n    \"\"\"Capture the current state before a change for undo support.\n\n    Args:\n        action_type: The type of action being performed.\n        description: Human-readable description of the action.\n        node_id: Optional ID of the affected node.\n\n    Returns:\n        True if snapshot was captured, False if skipped.\n    \"\"\"\n    return self._history_manager.capture_snapshot(self, action_type, description, node_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.close_flow","title":"<code>close_flow()</code>","text":"<p>Performs cleanup operations, such as clearing node caches.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def close_flow(self):\n    \"\"\"Performs cleanup operations, such as clearing node caches.\"\"\"\n\n    for node in self.nodes:\n        node.remove_cache()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.copy_node","title":"<code>copy_node(new_node_settings, existing_setting_input, node_type)</code>","text":"<p>Creates a copy of an existing node.</p> <p>Parameters:</p> Name Type Description Default <code>new_node_settings</code> <code>NodePromise</code> <p>The promise containing new settings (like ID and position).</p> required <code>existing_setting_input</code> <code>Any</code> <p>The settings object from the node being copied.</p> required <code>node_type</code> <code>str</code> <p>The type of the node being copied.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def copy_node(\n    self, new_node_settings: input_schema.NodePromise, existing_setting_input: Any, node_type: str\n) -&gt; None:\n    \"\"\"Creates a copy of an existing node.\n\n    Args:\n        new_node_settings: The promise containing new settings (like ID and position).\n        existing_setting_input: The settings object from the node being copied.\n        node_type: The type of the node being copied.\n    \"\"\"\n    self.add_node_promise(new_node_settings)\n\n    if isinstance(existing_setting_input, input_schema.NodePromise):\n        return\n\n    combined_settings = combine_existing_settings_and_new_settings(existing_setting_input, new_node_settings)\n    getattr(self, f\"add_{node_type}\")(combined_settings)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.delete_node","title":"<code>delete_node(node_id)</code>","text":"<p>Deletes a node from the graph and updates all its connections.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>int | str</code> <p>The ID of the node to delete.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If the node with the given ID does not exist.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def delete_node(self, node_id: int | str):\n    \"\"\"Deletes a node from the graph and updates all its connections.\n\n    Args:\n        node_id: The ID of the node to delete.\n\n    Raises:\n        Exception: If the node with the given ID does not exist.\n    \"\"\"\n    logger.info(f\"Starting deletion of node with ID: {node_id}\")\n\n    node = self._node_db.get(node_id)\n    if node:\n        logger.info(f\"Found node: {node_id}, processing deletion\")\n\n        lead_to_steps: list[FlowNode] = node.leads_to_nodes\n        logger.debug(f\"Node {node_id} leads to {len(lead_to_steps)} other nodes\")\n\n        if len(lead_to_steps) &gt; 0:\n            for lead_to_step in lead_to_steps:\n                logger.debug(f\"Deleting input node {node_id} from dependent node {lead_to_step}\")\n                lead_to_step.delete_input_node(node_id, complete=True)\n\n        if not node.is_start:\n            depends_on: list[FlowNode] = node.node_inputs.get_all_inputs()\n            logger.debug(f\"Node {node_id} depends on {len(depends_on)} other nodes\")\n\n            for depend_on in depends_on:\n                logger.debug(f\"Removing lead_to reference {node_id} from node {depend_on}\")\n                depend_on.delete_lead_to_node(node_id)\n\n        self._node_db.pop(node_id)\n        logger.debug(f\"Successfully removed node {node_id} from node_db\")\n        del node\n        logger.info(\"Node object deleted\")\n    else:\n        logger.error(f\"Failed to find node with id {node_id}\")\n        raise Exception(f\"Node with id {node_id} does not exist\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.generate_code","title":"<code>generate_code()</code>","text":"<p>Generates code for the flow graph. This method exports the flow graph to a Polars-compatible format.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def generate_code(self):\n    \"\"\"Generates code for the flow graph.\n    This method exports the flow graph to a Polars-compatible format.\n    \"\"\"\n    from flowfile_core.flowfile.code_generator.code_generator import export_flow_to_polars\n\n    print(export_flow_to_polars(self))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_frontend_data","title":"<code>get_frontend_data()</code>","text":"<p>Formats the graph structure into a JSON-like dictionary for a specific legacy frontend.</p> <p>This method transforms the graph's state into a format compatible with the Drawflow.js library.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary representing the graph in Drawflow format.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_frontend_data(self) -&gt; dict:\n    \"\"\"Formats the graph structure into a JSON-like dictionary for a specific legacy frontend.\n\n    This method transforms the graph's state into a format compatible with the\n    Drawflow.js library.\n\n    Returns:\n        A dictionary representing the graph in Drawflow format.\n    \"\"\"\n    result = {\"Home\": {\"data\": {}}}\n    flow_info: schemas.FlowInformation = self.get_node_storage()\n\n    for node_id, node_info in flow_info.data.items():\n        if node_info.is_setup:\n            try:\n                pos_x = node_info.data.pos_x\n                pos_y = node_info.data.pos_y\n                # Basic node structure\n                result[\"Home\"][\"data\"][str(node_id)] = {\n                    \"id\": node_info.id,\n                    \"name\": node_info.type,\n                    \"data\": {},  # Additional data can go here\n                    \"class\": node_info.type,\n                    \"html\": node_info.type,\n                    \"typenode\": \"vue\",\n                    \"inputs\": {},\n                    \"outputs\": {},\n                    \"pos_x\": pos_x,\n                    \"pos_y\": pos_y,\n                }\n            except Exception as e:\n                logger.error(e)\n        # Add outputs to the node based on `outputs` in your backend data\n        if node_info.outputs:\n            outputs = {o: 0 for o in node_info.outputs}\n            for o in node_info.outputs:\n                outputs[o] += 1\n            connections = []\n            for output_node_id, n_connections in outputs.items():\n                leading_to_node = self.get_node(output_node_id)\n                input_types = leading_to_node.get_input_type(node_info.id)\n                for input_type in input_types:\n                    if input_type == \"main\":\n                        input_frontend_id = \"input_1\"\n                    elif input_type == \"right\":\n                        input_frontend_id = \"input_2\"\n                    elif input_type == \"left\":\n                        input_frontend_id = \"input_3\"\n                    else:\n                        input_frontend_id = \"input_1\"\n                    connection = {\"node\": str(output_node_id), \"input\": input_frontend_id}\n                    connections.append(connection)\n\n            result[\"Home\"][\"data\"][str(node_id)][\"outputs\"][\"output_1\"] = {\"connections\": connections}\n        else:\n            result[\"Home\"][\"data\"][str(node_id)][\"outputs\"] = {\"output_1\": {\"connections\": []}}\n\n        # Add input to the node based on `depending_on_id` in your backend data\n        if (\n            node_info.left_input_id is not None\n            or node_info.right_input_id is not None\n            or node_info.input_ids is not None\n        ):\n            main_inputs = node_info.main_input_ids\n            result[\"Home\"][\"data\"][str(node_id)][\"inputs\"][\"input_1\"] = {\n                \"connections\": [{\"node\": str(main_node_id), \"input\": \"output_1\"} for main_node_id in main_inputs]\n            }\n            if node_info.right_input_id is not None:\n                result[\"Home\"][\"data\"][str(node_id)][\"inputs\"][\"input_2\"] = {\n                    \"connections\": [{\"node\": str(node_info.right_input_id), \"input\": \"output_1\"}]\n                }\n            if node_info.left_input_id is not None:\n                result[\"Home\"][\"data\"][str(node_id)][\"inputs\"][\"input_3\"] = {\n                    \"connections\": [{\"node\": str(node_info.left_input_id), \"input\": \"output_1\"}]\n                }\n    return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_history_state","title":"<code>get_history_state()</code>","text":"<p>Get the current state of the history system.</p> <p>Returns:</p> Type Description <code>HistoryState</code> <p>HistoryState with information about available undo/redo operations.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_history_state(self) -&gt; HistoryState:\n    \"\"\"Get the current state of the history system.\n\n    Returns:\n        HistoryState with information about available undo/redo operations.\n    \"\"\"\n    return self._history_manager.get_state()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_implicit_starter_nodes","title":"<code>get_implicit_starter_nodes()</code>","text":"<p>Finds nodes that can act as starting points but are not explicitly defined as such.</p> <p>Some nodes, like the Polars Code node, can function without an input. This method identifies such nodes if they have no incoming connections.</p> <p>Returns:</p> Type Description <code>list[FlowNode]</code> <p>A list of <code>FlowNode</code> objects that are implicit starting nodes.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_implicit_starter_nodes(self) -&gt; list[FlowNode]:\n    \"\"\"Finds nodes that can act as starting points but are not explicitly defined as such.\n\n    Some nodes, like the Polars Code node, can function without an input. This\n    method identifies such nodes if they have no incoming connections.\n\n    Returns:\n        A list of `FlowNode` objects that are implicit starting nodes.\n    \"\"\"\n    starting_node_ids = [node.node_id for node in self._flow_starts]\n    implicit_starting_nodes = []\n    for node in self.nodes:\n        if node.node_template.can_be_start and not node.has_input and node.node_id not in starting_node_ids:\n            implicit_starting_nodes.append(node)\n    return implicit_starting_nodes\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_node","title":"<code>get_node(node_id=None)</code>","text":"<p>Retrieves a node from the graph by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>int | str</code> <p>The ID of the node to retrieve. If None, retrieves the last added node.</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowNode | None</code> <p>The FlowNode object, or None if not found.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_node(self, node_id: int | str = None) -&gt; FlowNode | None:\n    \"\"\"Retrieves a node from the graph by its ID.\n\n    Args:\n        node_id: The ID of the node to retrieve. If None, retrieves the last added node.\n\n    Returns:\n        The FlowNode object, or None if not found.\n    \"\"\"\n    if node_id is None:\n        node_id = self._node_ids[-1]\n    node = self._node_db.get(node_id)\n    if node is not None:\n        return node\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_node_data","title":"<code>get_node_data(node_id, include_example=True)</code>","text":"<p>Retrieves all data needed to render a node in the UI.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>int</code> <p>The ID of the node.</p> required <code>include_example</code> <code>bool</code> <p>Whether to include data samples in the result.</p> <code>True</code> <p>Returns:</p> Type Description <code>NodeData</code> <p>A NodeData object, or None if the node is not found.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_node_data(self, node_id: int, include_example: bool = True) -&gt; NodeData:\n    \"\"\"Retrieves all data needed to render a node in the UI.\n\n    Args:\n        node_id: The ID of the node.\n        include_example: Whether to include data samples in the result.\n\n    Returns:\n        A NodeData object, or None if the node is not found.\n    \"\"\"\n    node = self._node_db[node_id]\n    return node.get_node_data(flow_id=self.flow_id, include_example=include_example)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_node_storage","title":"<code>get_node_storage()</code>","text":"<p>Serializes the entire graph's state into a storable format.</p> <p>Returns:</p> Type Description <code>FlowInformation</code> <p>A FlowInformation object representing the complete graph.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_node_storage(self) -&gt; schemas.FlowInformation:\n    \"\"\"Serializes the entire graph's state into a storable format.\n\n    Returns:\n        A FlowInformation object representing the complete graph.\n    \"\"\"\n    node_information = {\n        node.node_id: node.get_node_information() for node in self.nodes if node.is_setup and node.is_correct\n    }\n\n    return schemas.FlowInformation(\n        flow_id=self.flow_id,\n        flow_name=self.__name__,\n        flow_settings=self.flow_settings,\n        data=node_information,\n        node_starts=[v.node_id for v in self._flow_starts],\n        node_connections=self.node_connections,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_nodes_overview","title":"<code>get_nodes_overview()</code>","text":"<p>Gets a list of dictionary representations for all nodes in the graph.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_nodes_overview(self):\n    \"\"\"Gets a list of dictionary representations for all nodes in the graph.\"\"\"\n    output = []\n    for v in self._node_db.values():\n        output.append(v.get_repr())\n    return output\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_run_info","title":"<code>get_run_info()</code>","text":"<p>Gets a summary of the most recent graph execution.</p> <p>Returns:</p> Type Description <code>RunInformation</code> <p>A RunInformation object with details about the last run.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_run_info(self) -&gt; RunInformation:\n    \"\"\"Gets a summary of the most recent graph execution.\n\n    Returns:\n        A RunInformation object with details about the last run.\n    \"\"\"\n    is_running = self.flow_settings.is_running\n    if self.latest_run_info is None:\n        return self.create_empty_run_information()\n\n    elif not is_running and self.latest_run_info.success is not None:\n        return self.latest_run_info\n\n    run_info = self.latest_run_info\n    if not is_running:\n        run_info.success = all(nr.success for nr in run_info.node_step_result)\n    return run_info\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_vue_flow_input","title":"<code>get_vue_flow_input()</code>","text":"<p>Formats the graph's nodes and edges into a schema suitable for the VueFlow frontend.</p> <p>Returns:</p> Type Description <code>VueFlowInput</code> <p>A VueFlowInput object.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_vue_flow_input(self) -&gt; schemas.VueFlowInput:\n    \"\"\"Formats the graph's nodes and edges into a schema suitable for the VueFlow frontend.\n\n    Returns:\n        A VueFlowInput object.\n    \"\"\"\n    edges: list[schemas.NodeEdge] = []\n    nodes: list[schemas.NodeInput] = []\n    for node in self.nodes:\n        nodes.append(node.get_node_input())\n        edges.extend(node.get_edge_input())\n    return schemas.VueFlowInput(node_edges=edges, node_inputs=nodes)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.print_tree","title":"<code>print_tree()</code>","text":"<p>Print flow_graph as a visual tree structure, showing the DAG relationships with ASCII art.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def print_tree(self):\n    \"\"\"Print flow_graph as a visual tree structure, showing the DAG relationships with ASCII art.\"\"\"\n    if not self._node_db:\n        self.flow_logger.info(\"Empty flow graph\")\n        return\n\n    # Build node information\n    node_info = build_node_info(self.nodes)\n\n    # Calculate depths for all nodes\n    for node_id in node_info:\n        calculate_depth(node_id, node_info)\n\n    # Group nodes by depth\n    depth_groups, max_depth = group_nodes_by_depth(node_info)\n\n    # Sort nodes within each depth group\n    for depth in depth_groups:\n        depth_groups[depth].sort()\n\n    # Create the main flow visualization\n    lines = [\"=\" * 80, \"Flow Graph Visualization\", \"=\" * 80, \"\"]\n\n    # Track which nodes connect to what\n    merge_points = define_node_connections(node_info)\n\n    # Build the flow paths\n\n    # Find the maximum label length for each depth level\n    max_label_length = {}\n    for depth in range(max_depth + 1):\n        if depth in depth_groups:\n            max_len = max(len(node_info[nid].label) for nid in depth_groups[depth])\n            max_label_length[depth] = max_len\n\n    # Draw the paths\n    drawn_nodes = set()\n    merge_drawn = set()\n\n    # Group paths by their merge points\n    paths_by_merge = {}\n    standalone_paths = []\n\n    # Build flow paths\n    paths = build_flow_paths(node_info, self._flow_starts, merge_points)\n\n    # Define paths to merge and standalone paths\n    for path in paths:\n        if len(path) &gt; 1 and path[-1] in merge_points and len(merge_points[path[-1]]) &gt; 1:\n            merge_id = path[-1]\n            if merge_id not in paths_by_merge:\n                paths_by_merge[merge_id] = []\n            paths_by_merge[merge_id].append(path)\n        else:\n            standalone_paths.append(path)\n\n    # Draw merged paths\n    draw_merged_paths(node_info, merge_points, paths_by_merge, merge_drawn, drawn_nodes, lines)\n\n    # Draw standlone paths\n    draw_standalone_paths(drawn_nodes, standalone_paths, lines, node_info)\n\n    # Add undrawn nodes\n    add_un_drawn_nodes(drawn_nodes, node_info, lines)\n\n    try:\n        execution_plan = compute_execution_plan(\n            nodes=self.nodes, flow_starts=self._flow_starts + self.get_implicit_starter_nodes()\n        )\n        ordered_nodes = execution_plan.all_nodes\n        if ordered_nodes:\n            for i, node in enumerate(ordered_nodes, 1):\n                lines.append(f\"  {i:3d}. {node_info[node.node_id].label}\")\n    except Exception as e:\n        lines.append(f\"  Could not determine execution order: {e}\")\n\n    # Print everything\n    output = \"\\n\".join(lines)\n\n    print(output)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.redo","title":"<code>redo()</code>","text":"<p>Redo the last undone action.</p> <p>Returns:</p> Type Description <code>UndoRedoResult</code> <p>UndoRedoResult indicating success or failure.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def redo(self) -&gt; UndoRedoResult:\n    \"\"\"Redo the last undone action.\n\n    Returns:\n        UndoRedoResult indicating success or failure.\n    \"\"\"\n    return self._history_manager.redo(self)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.remove_from_output_cols","title":"<code>remove_from_output_cols(columns)</code>","text":"<p>Removes specified columns from the list of expected output columns.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list[str]</code> <p>A list of column names to remove.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def remove_from_output_cols(self, columns: list[str]):\n    \"\"\"Removes specified columns from the list of expected output columns.\n\n    Args:\n        columns: A list of column names to remove.\n    \"\"\"\n    cols = set(columns)\n    self._output_cols = [c for c in self._output_cols if c not in cols]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.reset","title":"<code>reset()</code>","text":"<p>Forces a deep reset on all nodes in the graph.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def reset(self):\n    \"\"\"Forces a deep reset on all nodes in the graph.\"\"\"\n\n    for node in self.nodes:\n        node.reset(True)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.restore_from_snapshot","title":"<code>restore_from_snapshot(snapshot)</code>","text":"<p>Clear current state and rebuild from a snapshot.</p> <p>This method is used internally by undo/redo to restore a previous state.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot</code> <code>FlowfileData</code> <p>The FlowfileData snapshot to restore from.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def restore_from_snapshot(self, snapshot: schemas.FlowfileData) -&gt; None:\n    \"\"\"Clear current state and rebuild from a snapshot.\n\n    This method is used internally by undo/redo to restore a previous state.\n\n    Args:\n        snapshot: The FlowfileData snapshot to restore from.\n    \"\"\"\n    from flowfile_core.flowfile.manage.io_flowfile import (\n        _flowfile_data_to_flow_information,\n        determine_insertion_order,\n    )\n\n    # Preserve the current flow_id and source_registration_id\n    original_flow_id = self._flow_id\n    original_source_registration_id = self._flow_settings.source_registration_id\n\n    # Convert snapshot to FlowInformation\n    flow_info = _flowfile_data_to_flow_information(snapshot)\n\n    # Clear current state\n    self._node_db.clear()\n    self._node_ids.clear()\n    self._flow_starts.clear()\n    self._results = None\n\n    # Restore flow settings (preserve original flow_id and source_registration_id)\n    self._flow_settings = flow_info.flow_settings\n    self._flow_settings.flow_id = original_flow_id\n    self._flow_id = original_flow_id\n    if self._flow_settings.source_registration_id is None:\n        self._flow_settings.source_registration_id = original_source_registration_id\n    self.__name__ = flow_info.flow_name or self.__name__\n\n    # Determine node insertion order\n    ingestion_order = determine_insertion_order(flow_info)\n\n    # First pass: Create all nodes as promises\n    for node_id in ingestion_order:\n        node_info = flow_info.data[node_id]\n        node_promise = input_schema.NodePromise(\n            flow_id=original_flow_id,\n            node_id=node_info.id,\n            pos_x=node_info.x_position or 0,\n            pos_y=node_info.y_position or 0,\n            node_type=node_info.type,\n        )\n        if hasattr(node_info.setting_input, \"cache_results\"):\n            node_promise.cache_results = node_info.setting_input.cache_results\n        self.add_node_promise(node_promise)\n\n    # Second pass: Apply settings using add_&lt;node_type&gt; methods\n    for node_id in ingestion_order:\n        node_info = flow_info.data[node_id]\n        if node_info.is_setup and node_info.setting_input is not None:\n            # Update flow_id in setting_input\n            if hasattr(node_info.setting_input, \"flow_id\"):\n                node_info.setting_input.flow_id = original_flow_id\n\n            if hasattr(node_info.setting_input, \"is_user_defined\") and node_info.setting_input.is_user_defined:\n                if node_info.type in CUSTOM_NODE_STORE:\n                    user_defined_node_class = CUSTOM_NODE_STORE[node_info.type]\n                    self.add_user_defined_node(\n                        custom_node=user_defined_node_class.from_settings(node_info.setting_input.settings),\n                        user_defined_node_settings=node_info.setting_input,\n                    )\n            else:\n                add_method = getattr(self, \"add_\" + node_info.type, None)\n                if add_method:\n                    add_method(node_info.setting_input)\n\n    # Third pass: Restore connections\n    for node_id in ingestion_order:\n        node_info = flow_info.data[node_id]\n        from_node = self.get_node(node_id)\n        if from_node is None:\n            continue\n\n        for output_node_id in node_info.outputs or []:\n            to_node = self.get_node(output_node_id)\n            if to_node is None:\n                continue\n\n            output_node_info = flow_info.data.get(output_node_id)\n            if output_node_info is None:\n                continue\n\n            # Determine connection type\n            is_left_input = (output_node_info.left_input_id == node_id) and (\n                to_node.left_input is None or to_node.left_input.node_id != node_id\n            )\n            is_right_input = (output_node_info.right_input_id == node_id) and (\n                to_node.right_input is None or to_node.right_input.node_id != node_id\n            )\n            is_main_input = node_id in (output_node_info.input_ids or [])\n\n            if is_left_input:\n                insert_type = \"left\"\n            elif is_right_input:\n                insert_type = \"right\"\n            elif is_main_input:\n                insert_type = \"main\"\n            else:\n                continue\n\n            to_node.add_node_connection(from_node, insert_type)\n\n    logger.info(f\"Restored flow from snapshot with {len(self._node_db)} nodes\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.run_graph","title":"<code>run_graph()</code>","text":"<p>Executes the entire data flow graph from start to finish.</p> <p>Independent nodes within the same execution stage are run in parallel using threads. Stages are processed sequentially so that all dependencies are satisfied before a stage begins.</p> <p>Returns:</p> Type Description <code>RunInformation | None</code> <p>A RunInformation object summarizing the execution results.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the flow is already running.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def run_graph(self) -&gt; RunInformation | None:\n    \"\"\"Executes the entire data flow graph from start to finish.\n\n    Independent nodes within the same execution stage are run in parallel\n    using threads. Stages are processed sequentially so that all dependencies\n    are satisfied before a stage begins.\n\n    Returns:\n        A RunInformation object summarizing the execution results.\n\n    Raises:\n        Exception: If the flow is already running.\n    \"\"\"\n    if self.flow_settings.is_running:\n        raise Exception(\"Flow is already running\")\n    try:\n        self.flow_settings.is_running = True\n        self.flow_settings.is_canceled = False\n        self.flow_logger.clear_log_file()\n        self.flow_logger.info(\"Starting to run flowfile flow...\")\n\n        execution_plan = compute_execution_plan(\n            nodes=self.nodes, flow_starts=self._flow_starts + self.get_implicit_starter_nodes()\n        )\n\n        # Selectively clear artifacts only for nodes that will re-run.\n        # Nodes that are up-to-date keep their artifacts in both the\n        # metadata tracker AND the kernel's in-memory store so that\n        # downstream nodes can still read them.\n        plan_skip_ids: set[str | int] = {n.node_id for n in execution_plan.skip_nodes}\n        rerun_node_ids = self._compute_rerun_python_script_node_ids(plan_skip_ids)\n\n        # Expand re-run set: if a re-running node previously deleted\n        # artifacts, the original producer nodes must also re-run so\n        # those artifacts are available again in the kernel store.\n        while True:\n            deleted_producers = self.artifact_context.get_producer_nodes_for_deletions(\n                rerun_node_ids,\n            )\n            new_ids = deleted_producers - rerun_node_ids\n            if not new_ids:\n                break\n            rerun_node_ids |= new_ids\n\n        # Force producer nodes (added due to artifact deletions) to\n        # actually re-execute by marking their execution state stale.\n        for nid in rerun_node_ids:\n            node = self.get_node(nid)\n            if node is not None and node._execution_state.has_run_with_current_setup:\n                node._execution_state.has_run_with_current_setup = False\n\n        # Also purge stale metadata for nodes not in this graph\n        # (e.g. injected externally or left over from removed nodes).\n        graph_node_ids = set(self._node_db.keys())\n        stale_node_ids = {nid for nid in self.artifact_context._node_states if nid not in graph_node_ids}\n        nodes_to_clear = rerun_node_ids | stale_node_ids\n        if nodes_to_clear:\n            self.artifact_context.clear_nodes(nodes_to_clear)\n\n        if rerun_node_ids:\n            # Clear the actual kernel-side artifacts for re-running nodes\n            kernel_node_map = self._group_rerun_nodes_by_kernel(rerun_node_ids)\n            for kid, node_ids_for_kernel in kernel_node_map.items():\n                try:\n                    manager = get_kernel_manager()\n                    manager.clear_node_artifacts_sync(\n                        kid, list(node_ids_for_kernel), flow_id=self.flow_id, flow_logger=self.flow_logger\n                    )\n                except Exception:\n                    logger.debug(\n                        \"Could not clear node artifacts for kernel '%s', nodes %s\",\n                        kid,\n                        sorted(node_ids_for_kernel),\n                    )\n\n        self.latest_run_info = self.create_initial_run_information(execution_plan.node_count, \"full_run\")\n\n        skip_node_message(self.flow_logger, execution_plan.skip_nodes)\n        execution_order_message(self.flow_logger, execution_plan.stages)\n        performance_mode = self.flow_settings.execution_mode == \"Performance\"\n\n        run_info_lock = threading.Lock()\n        skip_node_ids: set[str | int] = plan_skip_ids\n\n        for stage in execution_plan.stages:\n            if self.flow_settings.is_canceled:\n                self.flow_logger.info(\"Flow canceled\")\n                break\n\n            nodes_to_run = [n for n in stage.nodes if n.node_id not in skip_node_ids]\n\n            for skipped in stage.nodes:\n                if skipped.node_id in skip_node_ids:\n                    node_logger = self.flow_logger.get_node_logger(skipped.node_id)\n                    node_logger.info(f\"Skipping node {skipped.node_id}\")\n\n            if not nodes_to_run:\n                continue\n\n            is_local = self.flow_settings.execution_location == \"local\"\n            max_workers = 1 if is_local else self.flow_settings.max_parallel_workers\n            if len(nodes_to_run) == 1 or max_workers == 1:\n                # Single node or parallelism disabled \u2014 run sequentially\n                stage_results = [\n                    self._execute_single_node(node, performance_mode, run_info_lock) for node in nodes_to_run\n                ]\n            else:\n                # Multiple independent nodes \u2014 run in parallel\n                stage_results: list[tuple[NodeResult, FlowNode]] = []\n                workers = min(max_workers, len(nodes_to_run))\n                with ThreadPoolExecutor(max_workers=workers) as executor:\n                    futures = {\n                        executor.submit(self._execute_single_node, node, performance_mode, run_info_lock): node\n                        for node in nodes_to_run\n                    }\n                    for future in as_completed(futures):\n                        stage_results.append(future.result())\n\n            # After the stage completes, propagate failures to downstream nodes\n            for node_result, node in stage_results:\n                if not node_result.success:\n                    for dep in node.get_all_dependent_nodes():\n                        skip_node_ids.add(dep.node_id)\n\n        self.latest_run_info.end_time = datetime.datetime.now()\n        self.flow_logger.info(\"Flow completed!\")\n        self.end_datetime = datetime.datetime.now()\n        self.flow_settings.is_running = False\n        if self.flow_settings.is_canceled:\n            self.flow_logger.info(\"Flow canceled\")\n        return self.get_run_info()\n    except Exception as e:\n        raise e\n    finally:\n        self.flow_settings.is_running = False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.save_flow","title":"<code>save_flow(flow_path)</code>","text":"<p>Saves the current state of the flow graph to a file.</p> <p>Supports multiple formats based on file extension: - .yaml / .yml: New YAML format - .json: JSON format</p> <p>Parameters:</p> Name Type Description Default <code>flow_path</code> <code>str</code> <p>The path where the flow file will be saved.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def save_flow(self, flow_path: str):\n    \"\"\"Saves the current state of the flow graph to a file.\n\n    Supports multiple formats based on file extension:\n    - .yaml / .yml: New YAML format\n    - .json: JSON format\n\n    Args:\n        flow_path: The path where the flow file will be saved.\n    \"\"\"\n    logger.info(\"Saving flow to %s\", flow_path)\n    path = Path(flow_path)\n    os.makedirs(path.parent, exist_ok=True)\n    suffix = path.suffix.lower()\n    new_flow_name = path.name.replace(suffix, \"\")\n    self._handle_flow_renaming(new_flow_name, path)\n    self.flow_settings.modified_on = datetime.datetime.now().timestamp()\n    try:\n        if suffix == \".flowfile\":\n            raise DeprecationWarning(\n                \"The .flowfile format is deprecated. Please use .yaml or .json formats.\\n\\n\"\n                \"Or stay on v0.4.1 if you still need .flowfile support.\\n\\n\"\n            )\n        elif suffix in (\".yaml\", \".yml\"):\n            flowfile_data = self.get_flowfile_data()\n            data = flowfile_data.model_dump(mode=\"json\")\n            with open(flow_path, \"w\", encoding=\"utf-8\") as f:\n                yaml.dump(data, f, default_flow_style=False, sort_keys=False, allow_unicode=True)\n        elif suffix == \".json\":\n            flowfile_data = self.get_flowfile_data()\n            data = flowfile_data.model_dump(mode=\"json\")\n            with open(flow_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(data, f, indent=2, ensure_ascii=False)\n\n        else:\n            flowfile_data = self.get_flowfile_data()\n            logger.warning(f\"Unknown file extension {suffix}. Defaulting to YAML format.\")\n            data = flowfile_data.model_dump(mode=\"json\")\n            with open(flow_path, \"w\", encoding=\"utf-8\") as f:\n                yaml.dump(data, f, default_flow_style=False, sort_keys=False, allow_unicode=True)\n\n    except Exception as e:\n        logger.error(f\"Error saving flow: {e}\")\n        raise\n\n    self.flow_settings.path = flow_path\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.trigger_fetch_node","title":"<code>trigger_fetch_node(node_id)</code>","text":"<p>Executes a specific node in the graph by its ID.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def trigger_fetch_node(self, node_id: int) -&gt; RunInformation | None:\n    \"\"\"Executes a specific node in the graph by its ID.\"\"\"\n    if self.flow_settings.is_running:\n        raise Exception(\"Flow is already running\")\n    flow_node = self.get_node(node_id)\n    self.flow_settings.is_running = True\n    self.flow_settings.is_canceled = False\n    self.flow_logger.clear_log_file()\n    self.latest_run_info = self.create_initial_run_information(1, \"fetch_one\")\n    node_logger = self.flow_logger.get_node_logger(flow_node.node_id)\n    node_result = NodeResult(node_id=flow_node.node_id, node_name=flow_node.name)\n    logger.info(f\"Starting to run: node {flow_node.node_id}, start time: {node_result.start_timestamp}\")\n    try:\n        self.latest_run_info.node_step_result.append(node_result)\n        flow_node.execute_node(\n            run_location=self.flow_settings.execution_location,\n            performance_mode=False,\n            node_logger=node_logger,\n            optimize_for_downstream=False,\n            reset_cache=True,\n        )\n        node_result.error = str(flow_node.results.errors)\n        if self.flow_settings.is_canceled:\n            node_result.success = None\n            node_result.success = None\n            node_result.is_running = False\n        node_result.success = flow_node.results.errors is None\n        node_result.end_timestamp = time()\n        node_result.run_time = int(node_result.end_timestamp - node_result.start_timestamp)\n        node_result.is_running = False\n        self.latest_run_info.nodes_completed += 1\n        self.latest_run_info.end_time = datetime.datetime.now()\n        self.flow_settings.is_running = False\n        return self.get_run_info()\n    except Exception as e:\n        node_result.error = \"Node did not run\"\n        node_result.success = False\n        node_result.end_timestamp = time()\n        node_result.run_time = int(node_result.end_timestamp - node_result.start_timestamp)\n        node_result.is_running = False\n        node_logger.error(f\"Error in node {flow_node.node_id}: {e}\")\n    finally:\n        self.flow_settings.is_running = False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.undo","title":"<code>undo()</code>","text":"<p>Undo the last action by restoring to the previous state.</p> <p>Returns:</p> Type Description <code>UndoRedoResult</code> <p>UndoRedoResult indicating success or failure.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def undo(self) -&gt; UndoRedoResult:\n    \"\"\"Undo the last action by restoring to the previous state.\n\n    Returns:\n        UndoRedoResult indicating success or failure.\n    \"\"\"\n    return self._history_manager.undo(self)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flownode","title":"FlowNode","text":"<p>The <code>FlowNode</code> represents a single operation in the <code>FlowGraph</code>. Each node corresponds to a specific transformation or action, such as filtering or grouping data.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode","title":"<code>flowfile_core.flowfile.flow_node.flow_node.FlowNode</code>","text":"<p>Represents a single node in a data flow graph.</p> <p>This class manages the node's state, its data processing function, and its connections to other nodes within the graph.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Makes the node instance callable, acting as an alias for execute_node.</p> <code>__init__</code> <p>Initializes a FlowNode instance.</p> <code>__repr__</code> <p>Provides a string representation of the FlowNode instance.</p> <code>add_lead_to_in_depend_source</code> <p>Ensures this node is registered in the <code>leads_to_nodes</code> list of its inputs.</p> <code>add_node_connection</code> <p>Adds a connection from a source node to this node.</p> <code>calculate_hash</code> <p>Calculates a hash based on settings and input node hashes.</p> <code>cancel</code> <p>Cancels an ongoing external process if one is running.</p> <code>clear_table_example</code> <p>Clear the table example in the results so that it clears the existing results</p> <code>create_schema_callback_from_function</code> <p>Wraps a node's function to create a schema callback that extracts the schema.</p> <code>delete_input_node</code> <p>Removes a connection from a specific input node.</p> <code>delete_lead_to_node</code> <p>Removes a connection to a specific downstream node.</p> <code>evaluate_nodes</code> <p>Triggers a state reset for all directly connected downstream nodes.</p> <code>execute_full_local</code> <p>Backward-compatible alias for _do_execute_full_local.</p> <code>execute_local</code> <p>Backward-compatible alias for _do_execute_local_with_sampling.</p> <code>execute_node</code> <p>Execute the node based on its current state and settings.</p> <code>execute_remote</code> <p>Backward-compatible alias for _do_execute_remote.</p> <code>get_all_dependent_node_ids</code> <p>Yields the IDs of all downstream nodes recursively.</p> <code>get_all_dependent_nodes</code> <p>Yields all downstream nodes recursively.</p> <code>get_edge_input</code> <p>Generates <code>NodeEdge</code> objects for all input connections to this node.</p> <code>get_flow_file_column_schema</code> <p>Retrieves the schema for a specific column from the output schema.</p> <code>get_input_type</code> <p>Gets the type of connection ('main', 'left', 'right') for a given input node ID.</p> <code>get_node_data</code> <p>Gathers all necessary data for representing the node in the UI.</p> <code>get_node_information</code> <p>Updates and returns the node's information object.</p> <code>get_node_input</code> <p>Creates a <code>NodeInput</code> schema object for representing this node in the UI.</p> <code>get_output</code> <p>Get the result for a specific output handle.</p> <code>get_output_data</code> <p>Gets the full output data sample for this node.</p> <code>get_predicted_resulting_data</code> <p>Creates a <code>FlowDataEngine</code> instance based on the predicted schema.</p> <code>get_predicted_schema</code> <p>Predicts the output schema of the node without full execution.</p> <code>get_repr</code> <p>Gets a detailed dictionary representation of the node's state.</p> <code>get_resulting_data</code> <p>Executes the node's function to produce the actual output data.</p> <code>get_table_example</code> <p>Generates a <code>TableExample</code> model summarizing the node's output.</p> <code>needs_reset</code> <p>Checks if the node's hash has changed, indicating an outdated state.</p> <code>needs_run</code> <p>Determines if the node needs to be executed.</p> <code>post_init</code> <p>Initializes or resets the node's attributes to their default states.</p> <code>prepare_before_run</code> <p>Resets results and errors before a new execution.</p> <code>print</code> <p>Helper method to log messages with node context.</p> <code>remove_cache</code> <p>Removes cached results for this node.</p> <code>reset</code> <p>Resets the node's execution state and schema information.</p> <code>set_node_information</code> <p>Populates the <code>node_information</code> attribute with the current state.</p> <code>store_example_data_generator</code> <p>Stores a generator function for fetching a sample of the result data.</p> <code>update_node</code> <p>Updates the properties of the node.</p> <p>Attributes:</p> Name Type Description <code>all_inputs</code> <code>list[FlowNode]</code> <p>Gets a list of all nodes connected to any input port.</p> <code>executor</code> <code>NodeExecutor</code> <p>Lazy-initialized executor instance.</p> <code>function</code> <code>Callable</code> <p>Gets the core processing function of the node.</p> <code>has_input</code> <code>bool</code> <p>Checks if this node has any input connections.</p> <code>has_next_step</code> <code>bool</code> <p>Checks if this node has any downstream connections.</p> <code>hash</code> <code>str</code> <p>Gets the cached hash for the node, calculating it if it doesn't exist.</p> <code>is_correct</code> <code>bool</code> <p>Checks if the node's input connections satisfy its template requirements.</p> <code>is_setup</code> <code>bool</code> <p>Checks if the node has been properly configured and is ready for execution.</p> <code>is_start</code> <code>bool</code> <p>Determines if the node is a starting node in the flow.</p> <code>left_input</code> <code>Optional[FlowNode]</code> <p>Gets the node connected to the left input port.</p> <code>main_input</code> <code>list[FlowNode]</code> <p>Gets the list of nodes connected to the main input port(s).</p> <code>name</code> <code>str</code> <p>Gets the name of the node.</p> <code>node_id</code> <code>str | int</code> <p>Gets the unique identifier of the node.</p> <code>number_of_leads_to_nodes</code> <code>int | None</code> <p>Counts the number of downstream node connections.</p> <code>right_input</code> <code>Optional[FlowNode]</code> <p>Gets the node connected to the right input port.</p> <code>schema</code> <code>list[FlowfileColumn]</code> <p>Gets the definitive output schema of the node.</p> <code>schema_callback</code> <code>SingleExecutionFuture</code> <p>Gets the schema callback function, creating one if it doesn't exist.</p> <code>setting_input</code> <code>Any</code> <p>Gets the node's specific configuration settings.</p> <code>singular_input</code> <code>bool</code> <p>Checks if the node template specifies exactly one input.</p> <code>singular_main_input</code> <code>FlowNode</code> <p>Gets the input node, assuming it is a single-input type.</p> <code>state_needs_reset</code> <code>bool</code> <p>Checks if the node's state needs to be reset.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>class FlowNode:\n    \"\"\"Represents a single node in a data flow graph.\n\n    This class manages the node's state, its data processing function,\n    and its connections to other nodes within the graph.\n    \"\"\"\n\n    parent_uuid: str\n    node_type: str\n    node_template: node_store.NodeTemplate\n    node_default: schemas.NodeDefault\n    node_schema: NodeSchemaInformation\n    node_inputs: NodeStepInputs\n    node_stats: NodeStepStats\n    node_settings: NodeStepSettings\n    results: NodeResults\n    node_information: schemas.NodeInformation | None = None\n    leads_to_nodes: list[\"FlowNode\"] = []  # list with target flows, after execution the step will trigger those step(s)\n    user_provided_schema_callback: Callable | None = None  # user provided callback function for schema calculation\n    _setting_input: Any = None\n    _hash: str | None = None  # host this for caching results\n    _function: Callable = None  # the function that needs to be executed when triggered\n    _name: str = None  # name of the node, used for display\n    _schema_callback: SingleExecutionFuture | None = None  # Function that calculates the schema without executing\n    _state_needs_reset: bool = False\n    _fetch_cached_df: (\n        ExternalDfFetcher | ExternalDatabaseFetcher | ExternalDatabaseWriter | ExternalCloudWriter | None\n    ) = None\n    _cache_progress: (\n        ExternalDfFetcher | ExternalDatabaseFetcher | ExternalDatabaseWriter | ExternalCloudWriter | None\n    ) = None\n    _execution_state: NodeExecutionState = None\n    _executor: NodeExecutor | None = None  # Lazy-initialized\n\n    def __init__(\n        self,\n        node_id: str | int,\n        function: Callable,\n        parent_uuid: str,\n        setting_input: Any,\n        name: str,\n        node_type: str,\n        input_columns: list[str] = None,\n        output_schema: list[FlowfileColumn] = None,\n        drop_columns: list[str] = None,\n        renew_schema: bool = True,\n        pos_x: float = 0,\n        pos_y: float = 0,\n        schema_callback: Callable = None,\n    ):\n        \"\"\"Initializes a FlowNode instance.\n\n        Args:\n            node_id: Unique identifier for the node.\n            function: The core data processing function for the node.\n            parent_uuid: The UUID of the parent flow.\n            setting_input: The configuration/settings object for the node.\n            name: The name of the node.\n            node_type: The type identifier of the node (e.g., 'join', 'filter').\n            input_columns: List of column names expected as input.\n            output_schema: The schema of the columns to be added.\n            drop_columns: List of column names to be dropped.\n            renew_schema: Flag to indicate if the schema should be renewed.\n            pos_x: The x-coordinate on the canvas.\n            pos_y: The y-coordinate on the canvas.\n            schema_callback: A custom function to calculate the output schema.\n        \"\"\"\n        self._name = None\n        self.parent_uuid = parent_uuid\n        self.post_init()\n        self.active = True\n        self.node_information.id = node_id\n        self.node_type = node_type\n        self.node_settings.renew_schema = renew_schema\n        self.update_node(\n            function=function,\n            input_columns=input_columns,\n            output_schema=output_schema,\n            drop_columns=drop_columns,\n            setting_input=setting_input,\n            name=name,\n            pos_x=pos_x,\n            pos_y=pos_y,\n            schema_callback=schema_callback,\n        )\n\n    def post_init(self):\n        \"\"\"Initializes or resets the node's attributes to their default states.\"\"\"\n        self.node_inputs = NodeStepInputs()\n        self.node_stats = NodeStepStats()\n        self.node_settings = NodeStepSettings()\n        self.node_schema = NodeSchemaInformation()\n        self.results = NodeResults()\n        self.node_information = schemas.NodeInformation()\n        self.leads_to_nodes = []\n        self._setting_input = None\n        self._cache_progress = None\n        self._schema_callback = None\n        self._state_needs_reset = False\n        self._execution_lock = threading.RLock()  # Protects concurrent access to get_resulting_data\n        self._kernel_cancel_context = None\n        self._kernel_cancel_event: threading.Event | None = None\n        # Initialize execution state\n        self._execution_state = NodeExecutionState()\n        self._executor = None  # Will be lazily created\n        # Multi-output support: maps output handle (e.g. \"output-0\") to FlowDataEngine\n        self._named_outputs: dict[str, FlowDataEngine] = {}\n        # Maps source node id -&gt; output handle used in the connection\n        self._input_output_handles: dict[int, str] = {}\n\n    @property\n    def state_needs_reset(self) -&gt; bool:\n        \"\"\"Checks if the node's state needs to be reset.\n\n        Returns:\n            True if a reset is required, False otherwise.\n        \"\"\"\n        return self._state_needs_reset\n\n    @state_needs_reset.setter\n    def state_needs_reset(self, v: bool):\n        \"\"\"Sets the flag indicating that the node's state needs to be reset.\n\n        Args:\n            v: The boolean value to set.\n        \"\"\"\n        self._state_needs_reset = v\n\n    def create_schema_callback_from_function(self, f: Callable) -&gt; Callable[[], list[FlowfileColumn]]:\n        \"\"\"Wraps a node's function to create a schema callback that extracts the schema.\n\n        Thread-safe: uses _execution_lock to prevent concurrent execution with get_resulting_data.\n\n        Args:\n            f: The node's core function that returns a FlowDataEngine instance.\n\n        Returns:\n            A callable that, when executed, returns the output schema.\n        \"\"\"\n\n        def schema_callback() -&gt; list[FlowfileColumn]:\n            try:\n                logger.info(\"Executing the schema callback function based on the node function\")\n                with self._execution_lock:\n                    return f().schema\n            except Exception as e:\n                logger.warning(f\"Error with the schema callback: {e}\")\n                return []\n\n        return schema_callback\n\n    @property\n    def schema_callback(self) -&gt; SingleExecutionFuture:\n        \"\"\"Gets the schema callback function, creating one if it doesn't exist.\n\n        The callback is used for predicting the output schema without full execution.\n\n        Returns:\n            A SingleExecutionFuture instance wrapping the schema function.\n        \"\"\"\n        if self._schema_callback is None:\n            if self.user_provided_schema_callback is not None:\n                self.schema_callback = self.user_provided_schema_callback\n            elif self.is_start:\n                self.schema_callback = self.create_schema_callback_from_function(self._function)\n        return self._schema_callback\n\n    @schema_callback.setter\n    def schema_callback(self, f: Callable):\n        \"\"\"Sets the schema callback function for the node.\n\n        If the node has an enabled output_field_config, the callback is automatically\n        wrapped to use the output_field_config schema for prediction.\n\n        Args:\n            f: The function to be used for schema calculation.\n        \"\"\"\n        if f is None:\n            return\n\n        # Wrap callback with output_field_config support if present and enabled\n        output_field_config = getattr(self._setting_input, \"output_field_config\", None)\n        if output_field_config and output_field_config.enabled:\n            f = create_schema_callback_with_output_config(f, output_field_config)\n\n        def error_callback(e: Exception) -&gt; list:\n            logger.warning(e)\n\n            self.node_settings.setup_errors = True\n            return []\n\n        self._schema_callback = SingleExecutionFuture(f, error_callback)\n\n    @property\n    def executor(self) -&gt; NodeExecutor:\n        \"\"\"Lazy-initialized executor instance.\n\n        Reusing the same executor avoids object creation overhead\n        when execute_node is called multiple times.\n        \"\"\"\n        if self._executor is None:\n            self._executor = NodeExecutor(self)\n        return self._executor\n\n    @property\n    def is_start(self) -&gt; bool:\n        \"\"\"Determines if the node is a starting node in the flow.\n\n        A starting node requires no inputs.\n\n        Returns:\n            True if the node is a start node, False otherwise.\n        \"\"\"\n        return not self.has_input and self.node_template.input == 0\n\n    def get_input_type(self, node_id: int) -&gt; list:\n        \"\"\"Gets the type of connection ('main', 'left', 'right') for a given input node ID.\n\n        Args:\n            node_id: The ID of the input node.\n\n        Returns:\n            A list of connection types for that node ID.\n        \"\"\"\n        relation_type = []\n        if node_id in [n.node_id for n in self.node_inputs.main_inputs]:\n            relation_type.append(\"main\")\n        if self.node_inputs.left_input is not None and node_id == self.node_inputs.left_input.node_id:\n            relation_type.append(\"left\")\n        if self.node_inputs.right_input is not None and node_id == self.node_inputs.right_input.node_id:\n            relation_type.append(\"right\")\n        return list(set(relation_type))\n\n    def update_node(\n        self,\n        function: Callable,\n        input_columns: list[str] = None,\n        output_schema: list[FlowfileColumn] = None,\n        drop_columns: list[str] = None,\n        name: str = None,\n        setting_input: Any = None,\n        pos_x: float = 0,\n        pos_y: float = 0,\n        schema_callback: Callable = None,\n    ):\n        \"\"\"Updates the properties of the node.\n\n        This is called during initialization and when settings are changed.\n\n        Args:\n            function: The new core data processing function.\n            input_columns: The new list of input columns.\n            output_schema: The new schema of added columns.\n            drop_columns: The new list of dropped columns.\n            name: The new name for the node.\n            setting_input: The new settings object.\n            pos_x: The new x-coordinate.\n            pos_y: The new y-coordinate.\n            schema_callback: The new custom schema callback function.\n        \"\"\"\n        self.user_provided_schema_callback = schema_callback\n        self.node_information.y_position = int(pos_y)\n        self.node_information.x_position = int(pos_x)\n        self.node_information.setting_input = setting_input\n        self.name = self.node_type if name is None else name\n        self._function = function\n\n        self.node_schema.input_columns = [] if input_columns is None else input_columns\n        self.node_schema.output_columns = [] if output_schema is None else output_schema\n        self.node_schema.drop_columns = [] if drop_columns is None else drop_columns\n        self.node_settings.renew_schema = True\n        if hasattr(setting_input, \"cache_results\"):\n            self.node_settings.cache_results = setting_input.cache_results\n\n        self.results.errors = None\n        self.add_lead_to_in_depend_source()\n        _ = self.hash\n        self.node_template = node_store.node_dict.get(self.node_type)\n        if self.node_template is None:\n            raise Exception(f\"Node template {self.node_type} not found\")\n        self.node_default = node_store.node_defaults.get(self.node_type)\n        self.setting_input = setting_input  # wait until the end so that the hash is calculated correctly\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Gets the name of the node.\n\n        Returns:\n            The node's name.\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, name: str):\n        \"\"\"Sets the name of the node.\n\n        Args:\n            name: The new name.\n        \"\"\"\n        self._name = name\n        self.__name__ = name\n\n    @property\n    def setting_input(self) -&gt; Any:\n        \"\"\"Gets the node's specific configuration settings.\n\n        Returns:\n            The settings object.\n        \"\"\"\n        return self._setting_input\n\n    @setting_input.setter\n    def setting_input(self, setting_input: Any):\n        \"\"\"Sets the node's configuration and triggers a reset if necessary.\n\n        Args:\n            setting_input: The new settings object.\n        \"\"\"\n        is_manual_input = (\n            self.node_type == \"manual_input\"\n            and isinstance(setting_input, input_schema.NodeManualInput)\n            and isinstance(self._setting_input, input_schema.NodeManualInput)\n        )\n        if is_manual_input:\n            _ = self.hash\n        self._setting_input = setting_input\n        # Copy cache_results from setting_input to node_settings\n        if hasattr(setting_input, \"cache_results\"):\n            self.node_settings.cache_results = setting_input.cache_results\n        self.set_node_information()\n        if is_manual_input:\n            if self.hash != self.calculate_hash(setting_input) or not self.node_stats.has_run_with_current_setup:\n                self.function = FlowDataEngine(setting_input.raw_data_format)\n                self.reset()\n                self.get_predicted_schema()\n        elif self._setting_input is not None:\n            self.reset()\n\n    @property\n    def node_id(self) -&gt; str | int:\n        \"\"\"Gets the unique identifier of the node.\n\n        Returns:\n            The node's ID.\n        \"\"\"\n        return self.node_information.id\n\n    @property\n    def left_input(self) -&gt; Optional[\"FlowNode\"]:\n        \"\"\"Gets the node connected to the left input port.\n\n        Returns:\n            The left input FlowNode, or None.\n        \"\"\"\n        return self.node_inputs.left_input\n\n    @property\n    def right_input(self) -&gt; Optional[\"FlowNode\"]:\n        \"\"\"Gets the node connected to the right input port.\n\n        Returns:\n            The right input FlowNode, or None.\n        \"\"\"\n        return self.node_inputs.right_input\n\n    @property\n    def main_input(self) -&gt; list[\"FlowNode\"]:\n        \"\"\"Gets the list of nodes connected to the main input port(s).\n\n        Returns:\n            A list of main input FlowNodes.\n        \"\"\"\n        return self.node_inputs.main_inputs\n\n    @property\n    def is_correct(self) -&gt; bool:\n        \"\"\"Checks if the node's input connections satisfy its template requirements.\n\n        Returns:\n            True if connections are valid, False otherwise.\n        \"\"\"\n        if isinstance(self.setting_input, input_schema.NodePromise):\n            return False\n        return (\n            self.node_template.input == len(self.node_inputs.get_all_inputs())\n            or (self.node_template.multi and len(self.node_inputs.get_all_inputs()) &gt; 0)\n            or (self.node_template.multi and self.node_template.can_be_start)\n        )\n\n    def set_node_information(self):\n        \"\"\"Populates the `node_information` attribute with the current state.\n\n        This includes the node's connections, settings, and position.\n        \"\"\"\n        node_information = self.node_information\n        node_information.left_input_id = self.node_inputs.left_input.node_id if self.left_input else None\n        node_information.right_input_id = self.node_inputs.right_input.node_id if self.right_input else None\n        node_information.input_ids = (\n            [mi.node_id for mi in self.node_inputs.main_inputs] if self.node_inputs.main_inputs is not None else None\n        )\n        node_information.setting_input = self.setting_input\n        node_information.outputs = [n.node_id for n in self.leads_to_nodes]\n        user_description = self.setting_input.description if hasattr(self.setting_input, \"description\") else \"\"\n        if user_description:\n            node_information.description = user_description\n        elif hasattr(self.setting_input, \"get_default_description\"):\n            node_information.description = self.setting_input.get_default_description()\n        else:\n            node_information.description = \"\"\n        node_information.node_reference = (\n            self.setting_input.node_reference if hasattr(self.setting_input, \"node_reference\") else None\n        )\n        node_information.is_setup = self.is_setup\n        node_information.x_position = self.setting_input.pos_x\n        node_information.y_position = self.setting_input.pos_y\n        node_information.type = self.node_type\n\n    def get_node_information(self) -&gt; schemas.NodeInformation:\n        \"\"\"Updates and returns the node's information object.\n\n        Returns:\n            The `NodeInformation` object for this node.\n        \"\"\"\n        self.set_node_information()\n        return self.node_information\n\n    @property\n    def function(self) -&gt; Callable:\n        \"\"\"Gets the core processing function of the node.\n\n        Returns:\n            The callable function.\n        \"\"\"\n        return self._function\n\n    @function.setter\n    def function(self, function: Callable):\n        \"\"\"Sets the core processing function of the node.\n\n        Args:\n            function: The new callable function.\n        \"\"\"\n        self._function = function\n\n    @property\n    def all_inputs(self) -&gt; list[\"FlowNode\"]:\n        \"\"\"Gets a list of all nodes connected to any input port.\n\n        Returns:\n            A list of all input FlowNodes.\n        \"\"\"\n        return self.node_inputs.get_all_inputs()\n\n    def calculate_hash(self, setting_input: Any) -&gt; str:\n        \"\"\"Calculates a hash based on settings and input node hashes.\n\n        Args:\n            setting_input: The node's settings object to be included in the hash.\n\n        Returns:\n            A string hash value.\n        \"\"\"\n        depends_on_hashes = [_node.hash for _node in self.all_inputs]\n        node_data_hash = get_hash(setting_input)\n        return get_hash(depends_on_hashes + [node_data_hash, self.parent_uuid])\n\n    @property\n    def hash(self) -&gt; str:\n        \"\"\"Gets the cached hash for the node, calculating it if it doesn't exist.\n\n        Returns:\n            The string hash value.\n        \"\"\"\n        if not self._hash:\n            self._hash = self.calculate_hash(self.setting_input)\n        return self._hash\n\n    def add_node_connection(\n        self,\n        from_node: \"FlowNode\",\n        insert_type: Literal[\"main\", \"left\", \"right\"] = \"main\",\n        output_handle: str = \"output-0\",\n    ) -&gt; None:\n        \"\"\"Adds a connection from a source node to this node.\n\n        Args:\n            from_node: The node to connect from.\n            insert_type: The type of input to connect to ('main', 'left', 'right').\n            output_handle: The output handle on the source node (e.g. 'output-0', 'output-1').\n\n        Raises:\n            Exception: If the insert_type is invalid.\n        \"\"\"\n        from_node.leads_to_nodes.append(self)\n        if insert_type == \"main\":\n            if self.node_template.input &lt;= 2 or self.node_inputs.main_inputs is None:\n                self.node_inputs.main_inputs = [from_node]\n            else:\n                self.node_inputs.main_inputs.append(from_node)\n        elif insert_type == \"right\":\n            self.node_inputs.right_input = from_node\n        elif insert_type == \"left\":\n            self.node_inputs.left_input = from_node\n        else:\n            raise Exception(\"Cannot find the connection\")\n        # Track which output handle of the source node this connection uses\n        self._input_output_handles[from_node.node_id] = output_handle\n        if self.setting_input.is_setup:\n            if hasattr(self.setting_input, \"depending_on_id\") and insert_type == \"main\":\n                self.setting_input.depending_on_id = from_node.node_id\n        self.reset()\n        from_node.reset()\n\n    def evaluate_nodes(self, deep: bool = False) -&gt; None:\n        \"\"\"Triggers a state reset for all directly connected downstream nodes.\n\n        Args:\n            deep: If True, the reset propagates recursively through the entire downstream graph.\n        \"\"\"\n        for node in self.leads_to_nodes:\n            self.print(f\"resetting node: {node.node_id}\")\n            node.reset(deep)\n\n    def get_flow_file_column_schema(self, col_name: str) -&gt; FlowfileColumn | None:\n        \"\"\"Retrieves the schema for a specific column from the output schema.\n\n        Args:\n            col_name: The name of the column.\n\n        Returns:\n            The FlowfileColumn object for that column, or None if not found.\n        \"\"\"\n        for s in self.schema:\n            if s.column_name == col_name:\n                return s\n\n    def get_predicted_schema(self, force: bool = False) -&gt; list[FlowfileColumn] | None:\n        \"\"\"Predicts the output schema of the node without full execution.\n\n        It uses the schema_callback or infers from predicted data.\n\n        Args:\n            force: If True, forces recalculation even if a predicted schema exists.\n\n        Returns:\n            A list of FlowfileColumn objects representing the predicted schema.\n        \"\"\"\n        logger.info(\n            f\"get_predicted_schema: node_id={self.node_id}, node_type={self.node_type}, force={force}, \"\n            f\"has_predicted_schema={self.node_schema.predicted_schema is not None}, \"\n            f\"has_schema_callback={self.schema_callback is not None}, \"\n            f\"has_output_field_config={hasattr(self._setting_input, 'output_field_config') and self._setting_input.output_field_config is not None if self._setting_input else False}\"\n        )\n\n        if self.node_schema.predicted_schema and not force:\n            logger.debug(f\"get_predicted_schema: node_id={self.node_id} - returning cached predicted_schema\")\n            return self.node_schema.predicted_schema\n\n        if self.schema_callback is not None and (self.node_schema.predicted_schema is None or force):\n            self.print(\"Getting the data from a schema callback\")\n            logger.info(f\"get_predicted_schema: node_id={self.node_id} - invoking schema_callback\")\n            if force:\n                # Force the schema callback to reset, so that it will be executed again\n                logger.debug(f\"get_predicted_schema: node_id={self.node_id} - forcing schema_callback reset\")\n                self.schema_callback.reset()\n\n            try:\n                schema = self.schema_callback()\n                logger.info(\n                    f\"get_predicted_schema: node_id={self.node_id} - schema_callback returned \"\n                    f\"{len(schema) if schema else 0} columns: {[c.name for c in schema] if schema else []}\"\n                )\n            except Exception as e:\n                logger.error(f\"get_predicted_schema: node_id={self.node_id} - schema_callback raised exception: {e}\")\n                schema = None\n\n            if schema is not None and len(schema) &gt; 0:\n                self.print(\"Calculating the schema based on the schema callback\")\n                self.node_schema.predicted_schema = schema\n                logger.info(f\"get_predicted_schema: node_id={self.node_id} - set predicted_schema from schema_callback\")\n                return self.node_schema.predicted_schema\n            else:\n                logger.warning(\n                    f\"get_predicted_schema: node_id={self.node_id} - schema_callback returned empty/None schema\"\n                )\n        else:\n            logger.debug(f\"get_predicted_schema: node_id={self.node_id} - no schema_callback available\")\n\n        logger.debug(f\"get_predicted_schema: node_id={self.node_id} - falling back to _predicted_data_getter\")\n        predicted_data = self._predicted_data_getter()\n        if predicted_data is not None and predicted_data.schema is not None:\n            self.print(\"Calculating the schema based on the predicted resulting data\")\n            logger.info(\n                f\"get_predicted_schema: node_id={self.node_id} - using schema from predicted_data \"\n                f\"({len(predicted_data.schema)} columns)\"\n            )\n            self.node_schema.predicted_schema = self._predicted_data_getter().schema\n        else:\n            logger.warning(\n                f\"get_predicted_schema: node_id={self.node_id} - no schema available from any source \"\n                f\"(predicted_data={'None' if predicted_data is None else 'has_data'}, \"\n                f\"schema={'None' if predicted_data is None or predicted_data.schema is None else 'has_schema'})\"\n            )\n\n        return self.node_schema.predicted_schema\n\n    @property\n    def is_setup(self) -&gt; bool:\n        \"\"\"Checks if the node has been properly configured and is ready for execution.\n\n        Returns:\n            True if the node is set up, False otherwise.\n        \"\"\"\n        if not self.node_information.is_setup:\n            if self.function.__name__ != \"placeholder\":\n                self.node_information.is_setup = True\n                self.setting_input.is_setup = True\n        return self.node_information.is_setup\n\n    def print(self, v: Any):\n        \"\"\"Helper method to log messages with node context.\n\n        Args:\n            v: The message or value to log.\n        \"\"\"\n        logger.info(f\"{self.node_type}, node_id: {self.node_id}: {v}\")\n\n    def get_output(self, handle: str = \"output-0\") -&gt; FlowDataEngine | None:\n        \"\"\"Get the result for a specific output handle.\n\n        For nodes with multiple outputs (e.g. kernel-based custom nodes),\n        returns the FlowDataEngine associated with the given handle.\n        Falls back to the default ``results.resulting_data`` for single-output nodes.\n\n        Args:\n            handle: The output handle identifier (e.g. ``\"output-0\"``, ``\"output-1\"``).\n\n        Returns:\n            The FlowDataEngine for the requested output, or None.\n        \"\"\"\n        # Ensure the node has been executed first\n        self.get_resulting_data()\n        if handle in self._named_outputs:\n            return self._named_outputs[handle]\n        # Fall back to the default (single) result\n        return self.results.resulting_data\n\n    def _resolve_input_result(self, input_node: \"FlowNode\") -&gt; FlowDataEngine | None:\n        \"\"\"Resolve the correct output from an input node based on connection handle.\n\n        Args:\n            input_node: The upstream node to get data from.\n\n        Returns:\n            The FlowDataEngine from the appropriate output handle.\n        \"\"\"\n        handle = self._input_output_handles.get(input_node.node_id, \"output-0\")\n        if handle != \"output-0\" and input_node._named_outputs:\n            return input_node.get_output(handle)\n        return input_node.get_resulting_data()\n\n    def get_resulting_data(self) -&gt; FlowDataEngine | None:\n        \"\"\"Executes the node's function to produce the actual output data.\n\n        Handles both regular functions and external data sources.\n        Thread-safe: uses _execution_lock to prevent concurrent execution\n        and concurrent access to the underlying LazyFrame by sibling nodes.\n\n        Returns:\n            A FlowDataEngine instance containing the result, or None on error.\n\n        Raises:\n            Exception: Propagates exceptions from the node's function execution.\n        \"\"\"\n        if self.is_setup:\n            with self._execution_lock:\n                if self.results.resulting_data is None and self.results.errors is None:\n                    self.print(\"getting resulting data\")\n                    try:\n                        if isinstance(self.function, FlowDataEngine):\n                            fl: FlowDataEngine = self.function\n                        elif self.node_type == \"external_source\":\n                            fl: FlowDataEngine = self.function()\n                            fl.collect_external()\n                            self.node_settings.streamable = False\n                        else:\n                            try:\n                                self.print(\"Collecting input data from all inputs\")\n                                input_data = []\n                                input_locks = []\n                                try:\n                                    for i, v in enumerate(self.all_inputs):\n                                        self.print(f\"Getting resulting data from input {i} (node {v.node_id})\")\n                                        # Lock the input node to prevent sibling nodes from\n                                        # concurrently accessing the same upstream LazyFrame.\n                                        v._execution_lock.acquire()\n                                        input_locks.append(v._execution_lock)\n                                        input_result = self._resolve_input_result(v)\n                                        self.print(\n                                            f\"Input {i} data type: {type(input_result)}, dataframe type: {type(input_result.data_frame) if input_result else 'None'}\"\n                                        )\n                                        input_data.append(input_result)\n                                    self.print(f\"All {len(input_data)} inputs collected, calling node function\")\n                                    fl = self._function(*input_data)\n                                finally:\n                                    for lock in input_locks:\n                                        lock.release()\n                            except Exception as e:\n                                raise e\n                        fl.set_streamable(self.node_settings.streamable)\n\n                        # Apply output field configuration if enabled\n                        if (\n                            hasattr(self._setting_input, \"output_field_config\")\n                            and self._setting_input.output_field_config\n                        ):\n                            try:\n                                fl = apply_output_field_config(fl, self._setting_input.output_field_config)\n                            except Exception as e:\n                                logger.error(f\"Error applying output field config for node {self.node_id}: {e}\")\n                                raise\n\n                        self.results.resulting_data = fl\n                        self.node_schema.result_schema = fl.schema\n                    except Exception as e:\n                        self.results.resulting_data = FlowDataEngine()\n                        self.results.errors = str(e)\n                        self.node_stats.has_run_with_current_setup = False\n                        self.node_stats.has_completed_last_run = False\n                        raise e\n                return self.results.resulting_data\n\n    def _predicted_data_getter(self) -&gt; FlowDataEngine | None:\n        \"\"\"Internal helper to get a predicted data result.\n\n        This calls the function with predicted data from input nodes.\n\n        Returns:\n            A FlowDataEngine instance with predicted data, or an empty one on error.\n        \"\"\"\n        try:\n            fl = self._function(*[v.get_predicted_resulting_data() for v in self.all_inputs])\n\n            # Apply output field configuration if enabled (mirrors get_resulting_data behavior)\n            # This ensures schema prediction accounts for output_field_config validation\n            if hasattr(self._setting_input, \"output_field_config\") and self._setting_input.output_field_config:\n                if self._setting_input.output_field_config.enabled:\n                    fl = apply_output_field_config(fl, self._setting_input.output_field_config)\n\n            return fl\n        except ValueError as e:\n            if str(e) == \"generator already executing\":\n                logger.info(\"Generator already executing, waiting for the result\")\n                sleep(1)\n                return self._predicted_data_getter()\n            fl = FlowDataEngine()\n            return fl\n\n        except Exception as e:\n            logger.warning(\"there was an issue with the function, returning an empty Flowfile\")\n            logger.warning(e)\n\n    def get_predicted_resulting_data(self) -&gt; FlowDataEngine:\n        \"\"\"Creates a `FlowDataEngine` instance based on the predicted schema.\n\n        This avoids executing the node's full logic.\n\n        Returns:\n            A FlowDataEngine instance with a schema but no data.\n        \"\"\"\n        if self.needs_run(False) and self.schema_callback is not None or self.node_schema.result_schema is not None:\n            self.print(\"Getting data based on the schema\")\n\n            _s = self.schema_callback() if self.node_schema.result_schema is None else self.node_schema.result_schema\n            return FlowDataEngine.create_from_schema(_s)\n        else:\n            if isinstance(self.function, FlowDataEngine):\n                fl = self.function\n            else:\n                fl = FlowDataEngine.create_from_schema(self.get_predicted_schema())\n            return fl\n\n    def add_lead_to_in_depend_source(self):\n        \"\"\"Ensures this node is registered in the `leads_to_nodes` list of its inputs.\"\"\"\n        for input_node in self.all_inputs:\n            if self.node_id not in [n.node_id for n in input_node.leads_to_nodes]:\n                input_node.leads_to_nodes.append(self)\n\n    def get_all_dependent_nodes(self) -&gt; Generator[\"FlowNode\", None, None]:\n        \"\"\"Yields all downstream nodes recursively.\n\n        Returns:\n            A generator of all dependent FlowNode objects.\n        \"\"\"\n        for node in self.leads_to_nodes:\n            yield node\n            for n in node.get_all_dependent_nodes():\n                yield n\n\n    def get_all_dependent_node_ids(self) -&gt; Generator[int, None, None]:\n        \"\"\"Yields the IDs of all downstream nodes recursively.\n\n        Returns:\n            A generator of all dependent node IDs.\n        \"\"\"\n        for node in self.leads_to_nodes:\n            yield node.node_id\n            for n in node.get_all_dependent_node_ids():\n                yield n\n\n    @property\n    def schema(self) -&gt; list[FlowfileColumn]:\n        \"\"\"Gets the definitive output schema of the node.\n\n        If not already run, it falls back to the predicted schema.\n\n        Returns:\n            A list of FlowfileColumn objects.\n        \"\"\"\n        try:\n            if self.is_setup and self.results.errors is None:\n                if self.node_schema.result_schema is not None and len(self.node_schema.result_schema) &gt; 0:\n                    return self.node_schema.result_schema\n                elif self.node_type == \"output\":\n                    if len(self.node_inputs.main_inputs) &gt; 0:\n                        self.node_schema.result_schema = self.node_inputs.main_inputs[0].schema\n                else:\n                    self.node_schema.result_schema = self.get_predicted_schema()\n                return self.node_schema.result_schema\n            else:\n                return []\n        except Exception as e:\n            logger.error(e)\n            return []\n\n    def remove_cache(self):\n        \"\"\"Removes cached results for this node.\n\n        Note: Currently not fully implemented.\n        \"\"\"\n\n        if results_exists(self.hash):\n            logger.warning(\"Not implemented\")\n            clear_task_from_worker(self.hash)\n\n    def needs_run(\n        self,\n        performance_mode: bool,\n        node_logger: NodeLogger = None,\n        execution_location: schemas.ExecutionLocationsLiteral = \"remote\",\n    ) -&gt; bool:\n        \"\"\"Determines if the node needs to be executed.\n\n        The decision is based on its run state, caching settings, and execution mode.\n\n        Args:\n            performance_mode: True if the flow is in performance mode.\n            node_logger: The logger instance for this node.\n            execution_location: The target execution location.\n\n        Returns:\n            True if the node should be run, False otherwise.\n        \"\"\"\n        if execution_location == \"local\":\n            return False\n\n        flow_logger = logger if node_logger is None else node_logger\n        cache_result_exists = results_exists(self.hash)\n        if not self.node_stats.has_run_with_current_setup:\n            flow_logger.info(\"Node has not run, needs to run\")\n            return True\n        if self.node_settings.cache_results and cache_result_exists:\n            return False\n        elif self.node_settings.cache_results and not cache_result_exists:\n            return True\n        elif not performance_mode and cache_result_exists:\n            return False\n        else:\n            return True\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"Makes the node instance callable, acting as an alias for execute_node.\"\"\"\n        self.execute_node(*args, **kwargs)\n\n    def _can_skip_execution_fast(\n        self,\n        run_location: schemas.ExecutionLocationsLiteral,\n        performance_mode: bool,\n        reset_cache: bool,\n    ) -&gt; bool:\n        \"\"\"Fast-path check to avoid executor overhead when we can skip.\n\n        This inlines the most common skip conditions to avoid\n        creating an executor instance when not needed.\n\n        Returns True if execution can definitely be skipped.\n        Returns False if full execution logic is needed.\n        \"\"\"\n        # Can't skip if forced refresh\n        if reset_cache:\n            return False\n\n        # Output nodes always run\n        if self.node_template.node_group == \"output\":\n            return False\n\n        # Must run if never ran before\n        if not self._execution_state.has_run_with_current_setup:\n            return False\n\n        # Check for source file changes (read nodes only)\n        if self.node_type == \"read\" and self._execution_state.source_file_info:\n            if self._execution_state.source_file_info.has_changed():\n                return False\n\n        # Cache-enabled nodes: only skip if the cache file is still present\n        if self.node_settings.cache_results:\n            return results_exists(self.hash)\n\n        # Already ran with current settings \u2192 skip\n        # Results are available in memory from previous execution\n        return True\n\n    def _do_execute_full_local(self, performance_mode: bool = False) -&gt; None:\n        \"\"\"Executes the node's logic locally, including example data generation.\n\n        Internal method called by NodeExecutor.\n\n        Args:\n            performance_mode: If True, skips generating example data.\n\n        Raises:\n            Exception: Propagates exceptions from the execution.\n        \"\"\"\n        self.clear_table_example()\n\n        def example_data_generator():\n            example_data = None\n\n            def get_example_data():\n                nonlocal example_data\n                if example_data is None:\n                    example_data = resulting_data.get_sample(100).to_arrow()\n                return example_data\n\n            return get_example_data\n\n        resulting_data = self.get_resulting_data()\n\n        if not performance_mode:\n            self.node_stats.has_run_with_current_setup = True\n            self.results.example_data_generator = example_data_generator()\n            self.node_schema.result_schema = self.results.resulting_data.schema\n            self.node_stats.has_completed_last_run = True\n\n    def _do_execute_local_with_sampling(self, performance_mode: bool = False, flow_id: int = None):\n        \"\"\"Executes the node's logic locally with external sampling.\n\n        Internal method called by NodeExecutor.\n\n        Args:\n            performance_mode: If True, skips generating example data.\n            flow_id: The ID of the parent flow.\n\n        Raises:\n            Exception: Propagates exceptions from the execution.\n        \"\"\"\n        try:\n            resulting_data = self.get_resulting_data()\n            if not performance_mode:\n                external_sampler = ExternalSampler(\n                    lf=resulting_data.data_frame,\n                    file_ref=self.hash,\n                    wait_on_completion=True,\n                    node_id=self.node_id,\n                    flow_id=flow_id,\n                )\n                self.store_example_data_generator(external_sampler)\n                if self.results.errors is None and not self.node_stats.is_canceled:\n                    self.node_stats.has_run_with_current_setup = True\n            self.node_schema.result_schema = resulting_data.schema\n\n        except Exception as e:\n            logger.warning(f\"Error with step {self.__name__}\")\n            logger.error(str(e))\n            self.results.errors = str(e)\n            self.node_stats.has_run_with_current_setup = False\n            self.node_stats.has_completed_last_run = False\n            raise e\n\n        if self.node_stats.has_run_with_current_setup:\n            for step in self.leads_to_nodes:\n                if not self.node_settings.streamable:\n                    step.node_settings.streamable = self.node_settings.streamable\n\n    def _do_execute_remote(self, performance_mode: bool = False, node_logger: NodeLogger = None):\n        \"\"\"Executes the node's logic remotely or handles cached results.\n\n        Internal method called by NodeExecutor.\n\n        Args:\n            performance_mode: If True, skips generating example data.\n            node_logger: The logger for this node execution.\n\n        Raises:\n            Exception: If the node_logger is not provided or if execution fails.\n        \"\"\"\n        if node_logger is None:\n            raise Exception(\"Node logger is not defined\")\n        if self.node_settings.cache_results and results_exists(self.hash):\n            try:\n                self.results.resulting_data = FlowDataEngine(get_external_df_result(self.hash))\n                self._cache_progress = None\n                return\n            except Exception:\n                node_logger.warning(\"Failed to read the cache, rerunning the code\")\n        if self.node_type == \"output\":\n            self.results.resulting_data = self.get_resulting_data()\n            self.node_stats.has_run_with_current_setup = True\n            return\n\n        try:\n            result_data = self.get_resulting_data()\n            # Use 'is not None' instead of truthiness check to avoid triggering __len__()\n            # which calls .collect() on the LazyFrame and can cause issues\n            if result_data is None:\n                self.results.errors = \"Error with creating the lazy frame, most likely due to invalid graph\"\n                raise Exception(\"get_resulting_data returned None\")\n        except Exception as e:\n            self.results.errors = \"Error with creating the lazy frame, most likely due to invalid graph\"\n            raise e\n\n        if not performance_mode:\n            external_df_fetcher = ExternalDfFetcher(\n                lf=self.get_resulting_data().data_frame,\n                file_ref=self.hash,\n                wait_on_completion=False,\n                flow_id=node_logger.flow_id,\n                node_id=self.node_id,\n            )\n            self._fetch_cached_df = external_df_fetcher\n\n            try:\n                lf = external_df_fetcher.get_result()\n                self.results.resulting_data = FlowDataEngine(\n                    lf,\n                    number_of_records=ExternalDfFetcher(\n                        lf=lf,\n                        operation_type=\"calculate_number_of_records\",\n                        flow_id=node_logger.flow_id,\n                        node_id=self.node_id,\n                    ).result,\n                )\n\n                if not performance_mode:\n                    self.store_example_data_generator(external_df_fetcher)\n                    self.node_stats.has_run_with_current_setup = True\n\n            except Exception as e:\n                node_logger.error(\"Error with external process\")\n                if external_df_fetcher.error_code == -1:\n                    try:\n                        self.results.resulting_data = self.get_resulting_data()\n                        self.results.warnings = (\n                            \"Error with external process (unknown error), \"\n                            \"likely the process was killed by the server because of memory constraints, \"\n                            \"continue with the process. \"\n                            \"We cannot display example data...\"\n                        )\n                    except Exception as e:\n                        self.results.errors = str(e)\n                        raise e\n                elif external_df_fetcher.error_description is None:\n                    self.results.errors = str(e)\n                    raise e\n                else:\n                    self.results.errors = external_df_fetcher.error_description\n                    raise Exception(external_df_fetcher.error_description)\n            finally:\n                self._fetch_cached_df = None\n\n    # Backward-compatible aliases for renamed methods\n    def execute_full_local(self, performance_mode: bool = False) -&gt; None:\n        \"\"\"Backward-compatible alias for _do_execute_full_local.\"\"\"\n        return self._do_execute_full_local(performance_mode)\n\n    def execute_local(self, flow_id: int, performance_mode: bool = False):\n        \"\"\"Backward-compatible alias for _do_execute_local_with_sampling.\"\"\"\n        return self._do_execute_local_with_sampling(performance_mode, flow_id)\n\n    def execute_remote(self, performance_mode: bool = False, node_logger: NodeLogger = None):\n        \"\"\"Backward-compatible alias for _do_execute_remote.\"\"\"\n        return self._do_execute_remote(performance_mode, node_logger)\n\n    def prepare_before_run(self):\n        \"\"\"Resets results and errors before a new execution.\"\"\"\n\n        self.results.errors = None\n        self.results.resulting_data = None\n        self.results.example_data = None\n\n    def cancel(self):\n        \"\"\"Cancels an ongoing external process if one is running.\"\"\"\n\n        if self._fetch_cached_df is not None:\n            self._fetch_cached_df.cancel()\n        elif self._kernel_cancel_context is not None:\n            kernel_id, manager = self._kernel_cancel_context\n            logger.info(\"Cancelling kernel execution for kernel '%s'\", kernel_id)\n            # Signal the cancel event so execute_sync returns promptly\n            if self._kernel_cancel_event is not None:\n                self._kernel_cancel_event.set()\n            try:\n                manager.interrupt_execution_sync(kernel_id)\n            except Exception:\n                logger.exception(\"Failed to interrupt kernel execution for kernel '%s'\", kernel_id)\n        else:\n            logger.warning(\"No external process to cancel\")\n        self.node_stats.is_canceled = True\n        self._execution_state.is_canceled = True\n\n    def execute_node(\n        self,\n        run_location: schemas.ExecutionLocationsLiteral,\n        reset_cache: bool = False,\n        performance_mode: bool = False,\n        retry: bool = True,\n        node_logger: NodeLogger = None,\n        optimize_for_downstream: bool = True,\n    ) -&gt; None:\n        \"\"\"Execute the node based on its current state and settings.\n\n        This method uses a fast-path to quickly skip execution when possible,\n        avoiding executor overhead. For cases requiring full execution logic,\n        it delegates to the NodeExecutor.\n\n        Args:\n            run_location: Where to execute ('local' or 'remote')\n            reset_cache: Force cache invalidation\n            performance_mode: Skip example data generation for speed\n            retry: Allow retry on recoverable errors\n            node_logger: Logger for this node's execution\n            optimize_for_downstream: Cache wide transforms for downstream nodes\n        \"\"\"\n        if node_logger is None:\n            raise ValueError(\"node_logger is required\")\n\n        if not self.is_setup:\n            node_logger.warning(f\"Node {self.__name__} is not setup, cannot run\")\n            return\n\n        # Fast-path: check if we can skip without creating executor\n        if self._can_skip_execution_fast(run_location, performance_mode, reset_cache):\n            node_logger.info(\"Node is up-to-date, skipping execution\")\n            return\n\n        # Full execution logic via executor\n        self.executor.execute(\n            run_location=run_location,\n            reset_cache=reset_cache,\n            performance_mode=performance_mode,\n            retry=retry,\n            node_logger=node_logger,\n            optimize_for_downstream=optimize_for_downstream,\n        )\n\n    def store_example_data_generator(self, external_df_fetcher: ExternalDfFetcher | ExternalSampler):\n        \"\"\"Stores a generator function for fetching a sample of the result data.\n\n        Args:\n            external_df_fetcher: The process that generated the sample data.\n        \"\"\"\n        if external_df_fetcher.status is not None:\n            file_ref = external_df_fetcher.status.file_ref\n            self.results.example_data_path = file_ref\n            self.results.example_data_generator = get_read_top_n(file_path=file_ref, n=100)\n        else:\n            logger.error(\"Could not get the sample data, the external process is not ready\")\n\n    def needs_reset(self) -&gt; bool:\n        \"\"\"Checks if the node's hash has changed, indicating an outdated state.\n\n        Returns:\n            True if the calculated hash differs from the stored hash.\n        \"\"\"\n        return self._hash != self.calculate_hash(self.setting_input)\n\n    def reset(self, deep: bool = False):\n        \"\"\"Resets the node's execution state and schema information.\n\n        This also triggers a reset on all downstream nodes.\n\n        Args:\n            deep: If True, forces a reset even if the hash hasn't changed.\n        \"\"\"\n        needs_reset = self.needs_reset() or deep\n        if needs_reset:\n            logger.info(f\"{self.node_id}: Node needs reset\")\n            self.node_stats.has_run_with_current_setup = False\n            self.results.reset()\n            self.node_schema.result_schema = None\n            self.node_schema.predicted_schema = None\n            self._hash = None\n            self.node_information.is_setup = None\n            self.results.errors = None\n\n            # Reset execution state but preserve source file info for change detection\n            self._execution_state.has_run_with_current_setup = False\n            self._execution_state.has_completed_last_run = False\n            self._execution_state.result_schema = None\n            self._execution_state.predicted_schema = None\n            self._execution_state.execution_hash = None\n            # Note: source_file_info NOT reset - needed for change detection\n\n            if self.is_correct:\n                self._schema_callback = None  # Ensure the schema callback is reset\n                if self.schema_callback:\n                    logger.info(f\"{self.node_id}: Resetting the schema callback\")\n                    self.schema_callback.start()\n            self.evaluate_nodes()\n            _ = self.hash  # Recalculate the hash after reset\n\n    def delete_lead_to_node(self, node_id: int) -&gt; bool:\n        \"\"\"Removes a connection to a specific downstream node.\n\n        Args:\n            node_id: The ID of the downstream node to disconnect.\n\n        Returns:\n            True if the connection was found and removed, False otherwise.\n        \"\"\"\n        logger.info(f\"Deleting lead to node: {node_id}\")\n        for i, lead_to_node in enumerate(self.leads_to_nodes):\n            logger.info(f\"Checking lead to node: {lead_to_node.node_id}\")\n            if lead_to_node.node_id == node_id:\n                logger.info(f\"Found the node to delete: {node_id}\")\n                self.leads_to_nodes.pop(i)\n                return True\n        return False\n\n    def delete_input_node(\n        self, node_id: int, connection_type: input_schema.InputConnectionClass = \"input-0\", complete: bool = False\n    ) -&gt; bool:\n        \"\"\"Removes a connection from a specific input node.\n\n        Args:\n            node_id: The ID of the input node to disconnect.\n            connection_type: The specific input handle (e.g., 'input-0', 'input-1').\n            complete: If True, tries to delete from all input types.\n\n        Returns:\n            True if a connection was found and removed, False otherwise.\n        \"\"\"\n        deleted: bool = False\n        if connection_type == \"input-0\":\n            for i, node in enumerate(self.node_inputs.main_inputs):\n                if node.node_id == node_id:\n                    self.node_inputs.main_inputs.pop(i)\n                    deleted = True\n                    if not complete:\n                        continue\n        elif connection_type == \"input-1\" or complete:\n            if self.node_inputs.right_input is not None and self.node_inputs.right_input.node_id == node_id:\n                self.node_inputs.right_input = None\n                deleted = True\n        elif connection_type == \"input-2\" or complete:\n            if self.node_inputs.left_input is not None and self.node_inputs.right_input.node_id == node_id:\n                self.node_inputs.left_input = None\n                deleted = True\n        else:\n            logger.warning(\"Could not find the connection to delete...\")\n        if deleted:\n            # Clean up the output handle mapping for the removed connection\n            self._input_output_handles.pop(node_id, None)\n            self.reset()\n        return deleted\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Provides a string representation of the FlowNode instance.\n\n        Returns:\n            A string showing the node's ID and type.\n        \"\"\"\n        return f\"Node id: {self.node_id} ({self.node_type})\"\n\n    def _get_readable_schema(self) -&gt; list[dict] | None:\n        \"\"\"Helper to get a simplified, dictionary representation of the output schema.\n\n        Returns:\n            A list of dictionaries, each with 'column_name' and 'data_type'.\n        \"\"\"\n        if self.is_setup:\n            output = []\n            for s in self.schema:\n                output.append(dict(column_name=s.column_name, data_type=s.data_type))\n            return output\n\n    def get_repr(self) -&gt; dict:\n        \"\"\"Gets a detailed dictionary representation of the node's state.\n\n        Returns:\n            A dictionary containing key information about the node.\n        \"\"\"\n        return dict(\n            FlowNode=dict(\n                node_id=self.node_id,\n                step_name=self.__name__,\n                output_columns=self.node_schema.output_columns,\n                output_schema=self._get_readable_schema(),\n            )\n        )\n\n    @property\n    def number_of_leads_to_nodes(self) -&gt; int | None:\n        \"\"\"Counts the number of downstream node connections.\n\n        Returns:\n            The number of nodes this node leads to.\n        \"\"\"\n        if self.is_setup:\n            return len(self.leads_to_nodes)\n\n    @property\n    def has_next_step(self) -&gt; bool:\n        \"\"\"Checks if this node has any downstream connections.\n\n        Returns:\n            True if it has at least one downstream node.\n        \"\"\"\n        return len(self.leads_to_nodes) &gt; 0\n\n    @property\n    def has_input(self) -&gt; bool:\n        \"\"\"Checks if this node has any input connections.\n\n        Returns:\n            True if it has at least one input node.\n        \"\"\"\n        return len(self.all_inputs) &gt; 0\n\n    @property\n    def singular_input(self) -&gt; bool:\n        \"\"\"Checks if the node template specifies exactly one input.\n\n        Returns:\n            True if the node is a single-input type.\n        \"\"\"\n        return self.node_template.input == 1\n\n    @property\n    def singular_main_input(self) -&gt; \"FlowNode\":\n        \"\"\"Gets the input node, assuming it is a single-input type.\n\n        Returns:\n            The single input FlowNode, or None.\n        \"\"\"\n        if self.singular_input:\n            return self.all_inputs[0]\n\n    def clear_table_example(self) -&gt; None:\n        \"\"\"\n        Clear the table example in the results so that it clears the existing results\n        Returns:\n            None\n        \"\"\"\n\n        self.results.example_data = None\n        self.results.example_data_generator = None\n        self.results.example_data_path = None\n\n    def get_table_example(self, include_data: bool = False) -&gt; TableExample | None:\n        \"\"\"Generates a `TableExample` model summarizing the node's output.\n\n        This can optionally include a sample of the data.\n\n        Args:\n            include_data: If True, includes a data sample in the result.\n\n        Returns:\n            A `TableExample` object, or None if the node is not set up.\n        \"\"\"\n        self.print(\"Getting a table example\")\n        if self.is_setup and include_data and self.node_stats.has_completed_last_run:\n            if self.node_template.node_group == \"output\":\n                self.print(\"getting the table example\")\n                return self.main_input[0].get_table_example(include_data)\n\n            logger.info(\"getting the table example since the node has run\")\n            example_data_getter = self.results.example_data_generator\n            if example_data_getter is not None:\n                data = example_data_getter().to_pylist()\n                if data is None:\n                    data = []\n            else:\n                data = []\n            schema = [FileColumn.model_validate(c.get_column_repr()) for c in self.schema]\n            has_example_data = self.results.example_data_generator is not None\n\n            return TableExample(\n                node_id=self.node_id,\n                name=str(self.node_id),\n                number_of_records=999,\n                number_of_columns=len(schema),\n                table_schema=schema,\n                columns=[c.name for c in schema],\n                data=data,\n                has_example_data=has_example_data,\n                has_run_with_current_setup=self.node_stats.has_run_with_current_setup,\n            )\n        else:\n            logger.warning(\"getting the table example but the node has not run\")\n            try:\n                schema = [FileColumn.model_validate(c.get_column_repr()) for c in self.schema]\n            except Exception as e:\n                logger.warning(e)\n                schema = []\n            columns = [s.name for s in schema]\n            return TableExample(\n                node_id=self.node_id,\n                name=str(self.node_id),\n                number_of_records=0,\n                number_of_columns=len(columns),\n                table_schema=schema,\n                columns=columns,\n                data=[],\n            )\n\n    def get_node_data(self, flow_id: int, include_example: bool = False) -&gt; NodeData:\n        \"\"\"Gathers all necessary data for representing the node in the UI.\n\n        Args:\n            flow_id: The ID of the parent flow.\n            include_example: If True, includes data samples.\n\n        Returns:\n            A `NodeData` object.\n        \"\"\"\n        node = NodeData(\n            flow_id=flow_id,\n            node_id=self.node_id,\n            has_run=self.node_stats.has_run_with_current_setup,\n            setting_input=self.setting_input,\n            flow_type=self.node_type,\n        )\n        if self.main_input:\n            node.main_input = self.main_input[0].get_table_example()\n        if self.left_input:\n            node.left_input = self.left_input.get_table_example()\n        if self.right_input:\n            node.right_input = self.right_input.get_table_example()\n        if self.is_setup:\n            node.main_output = self.get_table_example(include_example)\n        node = setting_generator.get_setting_generator(self.node_type)(node)\n\n        node = setting_updator.get_setting_updator(self.node_type)(node)\n        # Save the updated settings back to the node so they persist across calls\n        if node.setting_input is not None and not isinstance(node.setting_input, input_schema.NodePromise):\n            self.setting_input = node.setting_input\n        return node\n\n    def get_output_data(self) -&gt; TableExample:\n        \"\"\"Gets the full output data sample for this node.\n\n        Returns:\n            A `TableExample` object with data.\n        \"\"\"\n        return self.get_table_example(True)\n\n    def get_node_input(self) -&gt; schemas.NodeInput:\n        \"\"\"Creates a `NodeInput` schema object for representing this node in the UI.\n\n        Returns:\n            A `NodeInput` object.\n        \"\"\"\n        return schemas.NodeInput(\n            pos_y=self.setting_input.pos_y,\n            pos_x=self.setting_input.pos_x,\n            id=self.node_id,\n            **self.node_template.__dict__,\n        )\n\n    def get_edge_input(self) -&gt; list[schemas.NodeEdge]:\n        \"\"\"Generates `NodeEdge` objects for all input connections to this node.\n\n        Returns:\n            A list of `NodeEdge` objects.\n        \"\"\"\n        edges = []\n        if self.node_inputs.main_inputs is not None:\n            for i, main_input in enumerate(self.node_inputs.main_inputs):\n                source_handle = self._input_output_handles.get(main_input.node_id, \"output-0\")\n                edges.append(\n                    schemas.NodeEdge(\n                        id=f\"{main_input.node_id}-{self.node_id}-{i}\",\n                        source=main_input.node_id,\n                        target=self.node_id,\n                        sourceHandle=source_handle,\n                        targetHandle=\"input-0\",\n                    )\n                )\n        if self.node_inputs.left_input is not None:\n            left_handle = self._input_output_handles.get(self.node_inputs.left_input.node_id, \"output-0\")\n            edges.append(\n                schemas.NodeEdge(\n                    id=f\"{self.node_inputs.left_input.node_id}-{self.node_id}-right\",\n                    source=self.node_inputs.left_input.node_id,\n                    target=self.node_id,\n                    sourceHandle=left_handle,\n                    targetHandle=\"input-2\",\n                )\n            )\n        if self.node_inputs.right_input is not None:\n            right_handle = self._input_output_handles.get(self.node_inputs.right_input.node_id, \"output-0\")\n            edges.append(\n                schemas.NodeEdge(\n                    id=f\"{self.node_inputs.right_input.node_id}-{self.node_id}-left\",\n                    source=self.node_inputs.right_input.node_id,\n                    target=self.node_id,\n                    sourceHandle=right_handle,\n                    targetHandle=\"input-1\",\n                )\n            )\n        return edges\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.all_inputs","title":"<code>all_inputs</code>  <code>property</code>","text":"<p>Gets a list of all nodes connected to any input port.</p> <p>Returns:</p> Type Description <code>list[FlowNode]</code> <p>A list of all input FlowNodes.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.executor","title":"<code>executor</code>  <code>property</code>","text":"<p>Lazy-initialized executor instance.</p> <p>Reusing the same executor avoids object creation overhead when execute_node is called multiple times.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.function","title":"<code>function</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the core processing function of the node.</p> <p>Returns:</p> Type Description <code>Callable</code> <p>The callable function.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.has_input","title":"<code>has_input</code>  <code>property</code>","text":"<p>Checks if this node has any input connections.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if it has at least one input node.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.has_next_step","title":"<code>has_next_step</code>  <code>property</code>","text":"<p>Checks if this node has any downstream connections.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if it has at least one downstream node.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.hash","title":"<code>hash</code>  <code>property</code>","text":"<p>Gets the cached hash for the node, calculating it if it doesn't exist.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string hash value.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.is_correct","title":"<code>is_correct</code>  <code>property</code>","text":"<p>Checks if the node's input connections satisfy its template requirements.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if connections are valid, False otherwise.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.is_setup","title":"<code>is_setup</code>  <code>property</code>","text":"<p>Checks if the node has been properly configured and is ready for execution.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the node is set up, False otherwise.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.is_start","title":"<code>is_start</code>  <code>property</code>","text":"<p>Determines if the node is a starting node in the flow.</p> <p>A starting node requires no inputs.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the node is a start node, False otherwise.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.left_input","title":"<code>left_input</code>  <code>property</code>","text":"<p>Gets the node connected to the left input port.</p> <p>Returns:</p> Type Description <code>Optional[FlowNode]</code> <p>The left input FlowNode, or None.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.main_input","title":"<code>main_input</code>  <code>property</code>","text":"<p>Gets the list of nodes connected to the main input port(s).</p> <p>Returns:</p> Type Description <code>list[FlowNode]</code> <p>A list of main input FlowNodes.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.name","title":"<code>name</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the name of the node.</p> <p>Returns:</p> Type Description <code>str</code> <p>The node's name.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.node_id","title":"<code>node_id</code>  <code>property</code>","text":"<p>Gets the unique identifier of the node.</p> <p>Returns:</p> Type Description <code>str | int</code> <p>The node's ID.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.number_of_leads_to_nodes","title":"<code>number_of_leads_to_nodes</code>  <code>property</code>","text":"<p>Counts the number of downstream node connections.</p> <p>Returns:</p> Type Description <code>int | None</code> <p>The number of nodes this node leads to.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.right_input","title":"<code>right_input</code>  <code>property</code>","text":"<p>Gets the node connected to the right input port.</p> <p>Returns:</p> Type Description <code>Optional[FlowNode]</code> <p>The right input FlowNode, or None.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.schema","title":"<code>schema</code>  <code>property</code>","text":"<p>Gets the definitive output schema of the node.</p> <p>If not already run, it falls back to the predicted schema.</p> <p>Returns:</p> Type Description <code>list[FlowfileColumn]</code> <p>A list of FlowfileColumn objects.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.schema_callback","title":"<code>schema_callback</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the schema callback function, creating one if it doesn't exist.</p> <p>The callback is used for predicting the output schema without full execution.</p> <p>Returns:</p> Type Description <code>SingleExecutionFuture</code> <p>A SingleExecutionFuture instance wrapping the schema function.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.setting_input","title":"<code>setting_input</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the node's specific configuration settings.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The settings object.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.singular_input","title":"<code>singular_input</code>  <code>property</code>","text":"<p>Checks if the node template specifies exactly one input.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the node is a single-input type.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.singular_main_input","title":"<code>singular_main_input</code>  <code>property</code>","text":"<p>Gets the input node, assuming it is a single-input type.</p> <p>Returns:</p> Type Description <code>FlowNode</code> <p>The single input FlowNode, or None.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.state_needs_reset","title":"<code>state_needs_reset</code>  <code>property</code> <code>writable</code>","text":"<p>Checks if the node's state needs to be reset.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if a reset is required, False otherwise.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Makes the node instance callable, acting as an alias for execute_node.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def __call__(self, *args, **kwargs):\n    \"\"\"Makes the node instance callable, acting as an alias for execute_node.\"\"\"\n    self.execute_node(*args, **kwargs)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.__init__","title":"<code>__init__(node_id, function, parent_uuid, setting_input, name, node_type, input_columns=None, output_schema=None, drop_columns=None, renew_schema=True, pos_x=0, pos_y=0, schema_callback=None)</code>","text":"<p>Initializes a FlowNode instance.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str | int</code> <p>Unique identifier for the node.</p> required <code>function</code> <code>Callable</code> <p>The core data processing function for the node.</p> required <code>parent_uuid</code> <code>str</code> <p>The UUID of the parent flow.</p> required <code>setting_input</code> <code>Any</code> <p>The configuration/settings object for the node.</p> required <code>name</code> <code>str</code> <p>The name of the node.</p> required <code>node_type</code> <code>str</code> <p>The type identifier of the node (e.g., 'join', 'filter').</p> required <code>input_columns</code> <code>list[str]</code> <p>List of column names expected as input.</p> <code>None</code> <code>output_schema</code> <code>list[FlowfileColumn]</code> <p>The schema of the columns to be added.</p> <code>None</code> <code>drop_columns</code> <code>list[str]</code> <p>List of column names to be dropped.</p> <code>None</code> <code>renew_schema</code> <code>bool</code> <p>Flag to indicate if the schema should be renewed.</p> <code>True</code> <code>pos_x</code> <code>float</code> <p>The x-coordinate on the canvas.</p> <code>0</code> <code>pos_y</code> <code>float</code> <p>The y-coordinate on the canvas.</p> <code>0</code> <code>schema_callback</code> <code>Callable</code> <p>A custom function to calculate the output schema.</p> <code>None</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def __init__(\n    self,\n    node_id: str | int,\n    function: Callable,\n    parent_uuid: str,\n    setting_input: Any,\n    name: str,\n    node_type: str,\n    input_columns: list[str] = None,\n    output_schema: list[FlowfileColumn] = None,\n    drop_columns: list[str] = None,\n    renew_schema: bool = True,\n    pos_x: float = 0,\n    pos_y: float = 0,\n    schema_callback: Callable = None,\n):\n    \"\"\"Initializes a FlowNode instance.\n\n    Args:\n        node_id: Unique identifier for the node.\n        function: The core data processing function for the node.\n        parent_uuid: The UUID of the parent flow.\n        setting_input: The configuration/settings object for the node.\n        name: The name of the node.\n        node_type: The type identifier of the node (e.g., 'join', 'filter').\n        input_columns: List of column names expected as input.\n        output_schema: The schema of the columns to be added.\n        drop_columns: List of column names to be dropped.\n        renew_schema: Flag to indicate if the schema should be renewed.\n        pos_x: The x-coordinate on the canvas.\n        pos_y: The y-coordinate on the canvas.\n        schema_callback: A custom function to calculate the output schema.\n    \"\"\"\n    self._name = None\n    self.parent_uuid = parent_uuid\n    self.post_init()\n    self.active = True\n    self.node_information.id = node_id\n    self.node_type = node_type\n    self.node_settings.renew_schema = renew_schema\n    self.update_node(\n        function=function,\n        input_columns=input_columns,\n        output_schema=output_schema,\n        drop_columns=drop_columns,\n        setting_input=setting_input,\n        name=name,\n        pos_x=pos_x,\n        pos_y=pos_y,\n        schema_callback=schema_callback,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.__repr__","title":"<code>__repr__()</code>","text":"<p>Provides a string representation of the FlowNode instance.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string showing the node's ID and type.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Provides a string representation of the FlowNode instance.\n\n    Returns:\n        A string showing the node's ID and type.\n    \"\"\"\n    return f\"Node id: {self.node_id} ({self.node_type})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.add_lead_to_in_depend_source","title":"<code>add_lead_to_in_depend_source()</code>","text":"<p>Ensures this node is registered in the <code>leads_to_nodes</code> list of its inputs.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def add_lead_to_in_depend_source(self):\n    \"\"\"Ensures this node is registered in the `leads_to_nodes` list of its inputs.\"\"\"\n    for input_node in self.all_inputs:\n        if self.node_id not in [n.node_id for n in input_node.leads_to_nodes]:\n            input_node.leads_to_nodes.append(self)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.add_node_connection","title":"<code>add_node_connection(from_node, insert_type='main', output_handle='output-0')</code>","text":"<p>Adds a connection from a source node to this node.</p> <p>Parameters:</p> Name Type Description Default <code>from_node</code> <code>FlowNode</code> <p>The node to connect from.</p> required <code>insert_type</code> <code>Literal['main', 'left', 'right']</code> <p>The type of input to connect to ('main', 'left', 'right').</p> <code>'main'</code> <code>output_handle</code> <code>str</code> <p>The output handle on the source node (e.g. 'output-0', 'output-1').</p> <code>'output-0'</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the insert_type is invalid.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def add_node_connection(\n    self,\n    from_node: \"FlowNode\",\n    insert_type: Literal[\"main\", \"left\", \"right\"] = \"main\",\n    output_handle: str = \"output-0\",\n) -&gt; None:\n    \"\"\"Adds a connection from a source node to this node.\n\n    Args:\n        from_node: The node to connect from.\n        insert_type: The type of input to connect to ('main', 'left', 'right').\n        output_handle: The output handle on the source node (e.g. 'output-0', 'output-1').\n\n    Raises:\n        Exception: If the insert_type is invalid.\n    \"\"\"\n    from_node.leads_to_nodes.append(self)\n    if insert_type == \"main\":\n        if self.node_template.input &lt;= 2 or self.node_inputs.main_inputs is None:\n            self.node_inputs.main_inputs = [from_node]\n        else:\n            self.node_inputs.main_inputs.append(from_node)\n    elif insert_type == \"right\":\n        self.node_inputs.right_input = from_node\n    elif insert_type == \"left\":\n        self.node_inputs.left_input = from_node\n    else:\n        raise Exception(\"Cannot find the connection\")\n    # Track which output handle of the source node this connection uses\n    self._input_output_handles[from_node.node_id] = output_handle\n    if self.setting_input.is_setup:\n        if hasattr(self.setting_input, \"depending_on_id\") and insert_type == \"main\":\n            self.setting_input.depending_on_id = from_node.node_id\n    self.reset()\n    from_node.reset()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.calculate_hash","title":"<code>calculate_hash(setting_input)</code>","text":"<p>Calculates a hash based on settings and input node hashes.</p> <p>Parameters:</p> Name Type Description Default <code>setting_input</code> <code>Any</code> <p>The node's settings object to be included in the hash.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string hash value.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def calculate_hash(self, setting_input: Any) -&gt; str:\n    \"\"\"Calculates a hash based on settings and input node hashes.\n\n    Args:\n        setting_input: The node's settings object to be included in the hash.\n\n    Returns:\n        A string hash value.\n    \"\"\"\n    depends_on_hashes = [_node.hash for _node in self.all_inputs]\n    node_data_hash = get_hash(setting_input)\n    return get_hash(depends_on_hashes + [node_data_hash, self.parent_uuid])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.cancel","title":"<code>cancel()</code>","text":"<p>Cancels an ongoing external process if one is running.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def cancel(self):\n    \"\"\"Cancels an ongoing external process if one is running.\"\"\"\n\n    if self._fetch_cached_df is not None:\n        self._fetch_cached_df.cancel()\n    elif self._kernel_cancel_context is not None:\n        kernel_id, manager = self._kernel_cancel_context\n        logger.info(\"Cancelling kernel execution for kernel '%s'\", kernel_id)\n        # Signal the cancel event so execute_sync returns promptly\n        if self._kernel_cancel_event is not None:\n            self._kernel_cancel_event.set()\n        try:\n            manager.interrupt_execution_sync(kernel_id)\n        except Exception:\n            logger.exception(\"Failed to interrupt kernel execution for kernel '%s'\", kernel_id)\n    else:\n        logger.warning(\"No external process to cancel\")\n    self.node_stats.is_canceled = True\n    self._execution_state.is_canceled = True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.clear_table_example","title":"<code>clear_table_example()</code>","text":"<p>Clear the table example in the results so that it clears the existing results Returns:     None</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def clear_table_example(self) -&gt; None:\n    \"\"\"\n    Clear the table example in the results so that it clears the existing results\n    Returns:\n        None\n    \"\"\"\n\n    self.results.example_data = None\n    self.results.example_data_generator = None\n    self.results.example_data_path = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.create_schema_callback_from_function","title":"<code>create_schema_callback_from_function(f)</code>","text":"<p>Wraps a node's function to create a schema callback that extracts the schema.</p> <p>Thread-safe: uses _execution_lock to prevent concurrent execution with get_resulting_data.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>The node's core function that returns a FlowDataEngine instance.</p> required <p>Returns:</p> Type Description <code>Callable[[], list[FlowfileColumn]]</code> <p>A callable that, when executed, returns the output schema.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def create_schema_callback_from_function(self, f: Callable) -&gt; Callable[[], list[FlowfileColumn]]:\n    \"\"\"Wraps a node's function to create a schema callback that extracts the schema.\n\n    Thread-safe: uses _execution_lock to prevent concurrent execution with get_resulting_data.\n\n    Args:\n        f: The node's core function that returns a FlowDataEngine instance.\n\n    Returns:\n        A callable that, when executed, returns the output schema.\n    \"\"\"\n\n    def schema_callback() -&gt; list[FlowfileColumn]:\n        try:\n            logger.info(\"Executing the schema callback function based on the node function\")\n            with self._execution_lock:\n                return f().schema\n        except Exception as e:\n            logger.warning(f\"Error with the schema callback: {e}\")\n            return []\n\n    return schema_callback\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.delete_input_node","title":"<code>delete_input_node(node_id, connection_type='input-0', complete=False)</code>","text":"<p>Removes a connection from a specific input node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>int</code> <p>The ID of the input node to disconnect.</p> required <code>connection_type</code> <code>InputConnectionClass</code> <p>The specific input handle (e.g., 'input-0', 'input-1').</p> <code>'input-0'</code> <code>complete</code> <code>bool</code> <p>If True, tries to delete from all input types.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if a connection was found and removed, False otherwise.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def delete_input_node(\n    self, node_id: int, connection_type: input_schema.InputConnectionClass = \"input-0\", complete: bool = False\n) -&gt; bool:\n    \"\"\"Removes a connection from a specific input node.\n\n    Args:\n        node_id: The ID of the input node to disconnect.\n        connection_type: The specific input handle (e.g., 'input-0', 'input-1').\n        complete: If True, tries to delete from all input types.\n\n    Returns:\n        True if a connection was found and removed, False otherwise.\n    \"\"\"\n    deleted: bool = False\n    if connection_type == \"input-0\":\n        for i, node in enumerate(self.node_inputs.main_inputs):\n            if node.node_id == node_id:\n                self.node_inputs.main_inputs.pop(i)\n                deleted = True\n                if not complete:\n                    continue\n    elif connection_type == \"input-1\" or complete:\n        if self.node_inputs.right_input is not None and self.node_inputs.right_input.node_id == node_id:\n            self.node_inputs.right_input = None\n            deleted = True\n    elif connection_type == \"input-2\" or complete:\n        if self.node_inputs.left_input is not None and self.node_inputs.right_input.node_id == node_id:\n            self.node_inputs.left_input = None\n            deleted = True\n    else:\n        logger.warning(\"Could not find the connection to delete...\")\n    if deleted:\n        # Clean up the output handle mapping for the removed connection\n        self._input_output_handles.pop(node_id, None)\n        self.reset()\n    return deleted\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.delete_lead_to_node","title":"<code>delete_lead_to_node(node_id)</code>","text":"<p>Removes a connection to a specific downstream node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>int</code> <p>The ID of the downstream node to disconnect.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the connection was found and removed, False otherwise.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def delete_lead_to_node(self, node_id: int) -&gt; bool:\n    \"\"\"Removes a connection to a specific downstream node.\n\n    Args:\n        node_id: The ID of the downstream node to disconnect.\n\n    Returns:\n        True if the connection was found and removed, False otherwise.\n    \"\"\"\n    logger.info(f\"Deleting lead to node: {node_id}\")\n    for i, lead_to_node in enumerate(self.leads_to_nodes):\n        logger.info(f\"Checking lead to node: {lead_to_node.node_id}\")\n        if lead_to_node.node_id == node_id:\n            logger.info(f\"Found the node to delete: {node_id}\")\n            self.leads_to_nodes.pop(i)\n            return True\n    return False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.evaluate_nodes","title":"<code>evaluate_nodes(deep=False)</code>","text":"<p>Triggers a state reset for all directly connected downstream nodes.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, the reset propagates recursively through the entire downstream graph.</p> <code>False</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def evaluate_nodes(self, deep: bool = False) -&gt; None:\n    \"\"\"Triggers a state reset for all directly connected downstream nodes.\n\n    Args:\n        deep: If True, the reset propagates recursively through the entire downstream graph.\n    \"\"\"\n    for node in self.leads_to_nodes:\n        self.print(f\"resetting node: {node.node_id}\")\n        node.reset(deep)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.execute_full_local","title":"<code>execute_full_local(performance_mode=False)</code>","text":"<p>Backward-compatible alias for _do_execute_full_local.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def execute_full_local(self, performance_mode: bool = False) -&gt; None:\n    \"\"\"Backward-compatible alias for _do_execute_full_local.\"\"\"\n    return self._do_execute_full_local(performance_mode)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.execute_local","title":"<code>execute_local(flow_id, performance_mode=False)</code>","text":"<p>Backward-compatible alias for _do_execute_local_with_sampling.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def execute_local(self, flow_id: int, performance_mode: bool = False):\n    \"\"\"Backward-compatible alias for _do_execute_local_with_sampling.\"\"\"\n    return self._do_execute_local_with_sampling(performance_mode, flow_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.execute_node","title":"<code>execute_node(run_location, reset_cache=False, performance_mode=False, retry=True, node_logger=None, optimize_for_downstream=True)</code>","text":"<p>Execute the node based on its current state and settings.</p> <p>This method uses a fast-path to quickly skip execution when possible, avoiding executor overhead. For cases requiring full execution logic, it delegates to the NodeExecutor.</p> <p>Parameters:</p> Name Type Description Default <code>run_location</code> <code>ExecutionLocationsLiteral</code> <p>Where to execute ('local' or 'remote')</p> required <code>reset_cache</code> <code>bool</code> <p>Force cache invalidation</p> <code>False</code> <code>performance_mode</code> <code>bool</code> <p>Skip example data generation for speed</p> <code>False</code> <code>retry</code> <code>bool</code> <p>Allow retry on recoverable errors</p> <code>True</code> <code>node_logger</code> <code>NodeLogger</code> <p>Logger for this node's execution</p> <code>None</code> <code>optimize_for_downstream</code> <code>bool</code> <p>Cache wide transforms for downstream nodes</p> <code>True</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def execute_node(\n    self,\n    run_location: schemas.ExecutionLocationsLiteral,\n    reset_cache: bool = False,\n    performance_mode: bool = False,\n    retry: bool = True,\n    node_logger: NodeLogger = None,\n    optimize_for_downstream: bool = True,\n) -&gt; None:\n    \"\"\"Execute the node based on its current state and settings.\n\n    This method uses a fast-path to quickly skip execution when possible,\n    avoiding executor overhead. For cases requiring full execution logic,\n    it delegates to the NodeExecutor.\n\n    Args:\n        run_location: Where to execute ('local' or 'remote')\n        reset_cache: Force cache invalidation\n        performance_mode: Skip example data generation for speed\n        retry: Allow retry on recoverable errors\n        node_logger: Logger for this node's execution\n        optimize_for_downstream: Cache wide transforms for downstream nodes\n    \"\"\"\n    if node_logger is None:\n        raise ValueError(\"node_logger is required\")\n\n    if not self.is_setup:\n        node_logger.warning(f\"Node {self.__name__} is not setup, cannot run\")\n        return\n\n    # Fast-path: check if we can skip without creating executor\n    if self._can_skip_execution_fast(run_location, performance_mode, reset_cache):\n        node_logger.info(\"Node is up-to-date, skipping execution\")\n        return\n\n    # Full execution logic via executor\n    self.executor.execute(\n        run_location=run_location,\n        reset_cache=reset_cache,\n        performance_mode=performance_mode,\n        retry=retry,\n        node_logger=node_logger,\n        optimize_for_downstream=optimize_for_downstream,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.execute_remote","title":"<code>execute_remote(performance_mode=False, node_logger=None)</code>","text":"<p>Backward-compatible alias for _do_execute_remote.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def execute_remote(self, performance_mode: bool = False, node_logger: NodeLogger = None):\n    \"\"\"Backward-compatible alias for _do_execute_remote.\"\"\"\n    return self._do_execute_remote(performance_mode, node_logger)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_all_dependent_node_ids","title":"<code>get_all_dependent_node_ids()</code>","text":"<p>Yields the IDs of all downstream nodes recursively.</p> <p>Returns:</p> Type Description <code>None</code> <p>A generator of all dependent node IDs.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_all_dependent_node_ids(self) -&gt; Generator[int, None, None]:\n    \"\"\"Yields the IDs of all downstream nodes recursively.\n\n    Returns:\n        A generator of all dependent node IDs.\n    \"\"\"\n    for node in self.leads_to_nodes:\n        yield node.node_id\n        for n in node.get_all_dependent_node_ids():\n            yield n\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_all_dependent_nodes","title":"<code>get_all_dependent_nodes()</code>","text":"<p>Yields all downstream nodes recursively.</p> <p>Returns:</p> Type Description <code>None</code> <p>A generator of all dependent FlowNode objects.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_all_dependent_nodes(self) -&gt; Generator[\"FlowNode\", None, None]:\n    \"\"\"Yields all downstream nodes recursively.\n\n    Returns:\n        A generator of all dependent FlowNode objects.\n    \"\"\"\n    for node in self.leads_to_nodes:\n        yield node\n        for n in node.get_all_dependent_nodes():\n            yield n\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_edge_input","title":"<code>get_edge_input()</code>","text":"<p>Generates <code>NodeEdge</code> objects for all input connections to this node.</p> <p>Returns:</p> Type Description <code>list[NodeEdge]</code> <p>A list of <code>NodeEdge</code> objects.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_edge_input(self) -&gt; list[schemas.NodeEdge]:\n    \"\"\"Generates `NodeEdge` objects for all input connections to this node.\n\n    Returns:\n        A list of `NodeEdge` objects.\n    \"\"\"\n    edges = []\n    if self.node_inputs.main_inputs is not None:\n        for i, main_input in enumerate(self.node_inputs.main_inputs):\n            source_handle = self._input_output_handles.get(main_input.node_id, \"output-0\")\n            edges.append(\n                schemas.NodeEdge(\n                    id=f\"{main_input.node_id}-{self.node_id}-{i}\",\n                    source=main_input.node_id,\n                    target=self.node_id,\n                    sourceHandle=source_handle,\n                    targetHandle=\"input-0\",\n                )\n            )\n    if self.node_inputs.left_input is not None:\n        left_handle = self._input_output_handles.get(self.node_inputs.left_input.node_id, \"output-0\")\n        edges.append(\n            schemas.NodeEdge(\n                id=f\"{self.node_inputs.left_input.node_id}-{self.node_id}-right\",\n                source=self.node_inputs.left_input.node_id,\n                target=self.node_id,\n                sourceHandle=left_handle,\n                targetHandle=\"input-2\",\n            )\n        )\n    if self.node_inputs.right_input is not None:\n        right_handle = self._input_output_handles.get(self.node_inputs.right_input.node_id, \"output-0\")\n        edges.append(\n            schemas.NodeEdge(\n                id=f\"{self.node_inputs.right_input.node_id}-{self.node_id}-left\",\n                source=self.node_inputs.right_input.node_id,\n                target=self.node_id,\n                sourceHandle=right_handle,\n                targetHandle=\"input-1\",\n            )\n        )\n    return edges\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_flow_file_column_schema","title":"<code>get_flow_file_column_schema(col_name)</code>","text":"<p>Retrieves the schema for a specific column from the output schema.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column.</p> required <p>Returns:</p> Type Description <code>FlowfileColumn | None</code> <p>The FlowfileColumn object for that column, or None if not found.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_flow_file_column_schema(self, col_name: str) -&gt; FlowfileColumn | None:\n    \"\"\"Retrieves the schema for a specific column from the output schema.\n\n    Args:\n        col_name: The name of the column.\n\n    Returns:\n        The FlowfileColumn object for that column, or None if not found.\n    \"\"\"\n    for s in self.schema:\n        if s.column_name == col_name:\n            return s\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_input_type","title":"<code>get_input_type(node_id)</code>","text":"<p>Gets the type of connection ('main', 'left', 'right') for a given input node ID.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>int</code> <p>The ID of the input node.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of connection types for that node ID.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_input_type(self, node_id: int) -&gt; list:\n    \"\"\"Gets the type of connection ('main', 'left', 'right') for a given input node ID.\n\n    Args:\n        node_id: The ID of the input node.\n\n    Returns:\n        A list of connection types for that node ID.\n    \"\"\"\n    relation_type = []\n    if node_id in [n.node_id for n in self.node_inputs.main_inputs]:\n        relation_type.append(\"main\")\n    if self.node_inputs.left_input is not None and node_id == self.node_inputs.left_input.node_id:\n        relation_type.append(\"left\")\n    if self.node_inputs.right_input is not None and node_id == self.node_inputs.right_input.node_id:\n        relation_type.append(\"right\")\n    return list(set(relation_type))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_node_data","title":"<code>get_node_data(flow_id, include_example=False)</code>","text":"<p>Gathers all necessary data for representing the node in the UI.</p> <p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>int</code> <p>The ID of the parent flow.</p> required <code>include_example</code> <code>bool</code> <p>If True, includes data samples.</p> <code>False</code> <p>Returns:</p> Type Description <code>NodeData</code> <p>A <code>NodeData</code> object.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_node_data(self, flow_id: int, include_example: bool = False) -&gt; NodeData:\n    \"\"\"Gathers all necessary data for representing the node in the UI.\n\n    Args:\n        flow_id: The ID of the parent flow.\n        include_example: If True, includes data samples.\n\n    Returns:\n        A `NodeData` object.\n    \"\"\"\n    node = NodeData(\n        flow_id=flow_id,\n        node_id=self.node_id,\n        has_run=self.node_stats.has_run_with_current_setup,\n        setting_input=self.setting_input,\n        flow_type=self.node_type,\n    )\n    if self.main_input:\n        node.main_input = self.main_input[0].get_table_example()\n    if self.left_input:\n        node.left_input = self.left_input.get_table_example()\n    if self.right_input:\n        node.right_input = self.right_input.get_table_example()\n    if self.is_setup:\n        node.main_output = self.get_table_example(include_example)\n    node = setting_generator.get_setting_generator(self.node_type)(node)\n\n    node = setting_updator.get_setting_updator(self.node_type)(node)\n    # Save the updated settings back to the node so they persist across calls\n    if node.setting_input is not None and not isinstance(node.setting_input, input_schema.NodePromise):\n        self.setting_input = node.setting_input\n    return node\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_node_information","title":"<code>get_node_information()</code>","text":"<p>Updates and returns the node's information object.</p> <p>Returns:</p> Type Description <code>NodeInformation</code> <p>The <code>NodeInformation</code> object for this node.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_node_information(self) -&gt; schemas.NodeInformation:\n    \"\"\"Updates and returns the node's information object.\n\n    Returns:\n        The `NodeInformation` object for this node.\n    \"\"\"\n    self.set_node_information()\n    return self.node_information\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_node_input","title":"<code>get_node_input()</code>","text":"<p>Creates a <code>NodeInput</code> schema object for representing this node in the UI.</p> <p>Returns:</p> Type Description <code>NodeInput</code> <p>A <code>NodeInput</code> object.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_node_input(self) -&gt; schemas.NodeInput:\n    \"\"\"Creates a `NodeInput` schema object for representing this node in the UI.\n\n    Returns:\n        A `NodeInput` object.\n    \"\"\"\n    return schemas.NodeInput(\n        pos_y=self.setting_input.pos_y,\n        pos_x=self.setting_input.pos_x,\n        id=self.node_id,\n        **self.node_template.__dict__,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_output","title":"<code>get_output(handle='output-0')</code>","text":"<p>Get the result for a specific output handle.</p> <p>For nodes with multiple outputs (e.g. kernel-based custom nodes), returns the FlowDataEngine associated with the given handle. Falls back to the default <code>results.resulting_data</code> for single-output nodes.</p> <p>Parameters:</p> Name Type Description Default <code>handle</code> <code>str</code> <p>The output handle identifier (e.g. <code>\"output-0\"</code>, <code>\"output-1\"</code>).</p> <code>'output-0'</code> <p>Returns:</p> Type Description <code>FlowDataEngine | None</code> <p>The FlowDataEngine for the requested output, or None.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_output(self, handle: str = \"output-0\") -&gt; FlowDataEngine | None:\n    \"\"\"Get the result for a specific output handle.\n\n    For nodes with multiple outputs (e.g. kernel-based custom nodes),\n    returns the FlowDataEngine associated with the given handle.\n    Falls back to the default ``results.resulting_data`` for single-output nodes.\n\n    Args:\n        handle: The output handle identifier (e.g. ``\"output-0\"``, ``\"output-1\"``).\n\n    Returns:\n        The FlowDataEngine for the requested output, or None.\n    \"\"\"\n    # Ensure the node has been executed first\n    self.get_resulting_data()\n    if handle in self._named_outputs:\n        return self._named_outputs[handle]\n    # Fall back to the default (single) result\n    return self.results.resulting_data\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_output_data","title":"<code>get_output_data()</code>","text":"<p>Gets the full output data sample for this node.</p> <p>Returns:</p> Type Description <code>TableExample</code> <p>A <code>TableExample</code> object with data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_output_data(self) -&gt; TableExample:\n    \"\"\"Gets the full output data sample for this node.\n\n    Returns:\n        A `TableExample` object with data.\n    \"\"\"\n    return self.get_table_example(True)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_predicted_resulting_data","title":"<code>get_predicted_resulting_data()</code>","text":"<p>Creates a <code>FlowDataEngine</code> instance based on the predicted schema.</p> <p>This avoids executing the node's full logic.</p> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A FlowDataEngine instance with a schema but no data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_predicted_resulting_data(self) -&gt; FlowDataEngine:\n    \"\"\"Creates a `FlowDataEngine` instance based on the predicted schema.\n\n    This avoids executing the node's full logic.\n\n    Returns:\n        A FlowDataEngine instance with a schema but no data.\n    \"\"\"\n    if self.needs_run(False) and self.schema_callback is not None or self.node_schema.result_schema is not None:\n        self.print(\"Getting data based on the schema\")\n\n        _s = self.schema_callback() if self.node_schema.result_schema is None else self.node_schema.result_schema\n        return FlowDataEngine.create_from_schema(_s)\n    else:\n        if isinstance(self.function, FlowDataEngine):\n            fl = self.function\n        else:\n            fl = FlowDataEngine.create_from_schema(self.get_predicted_schema())\n        return fl\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_predicted_schema","title":"<code>get_predicted_schema(force=False)</code>","text":"<p>Predicts the output schema of the node without full execution.</p> <p>It uses the schema_callback or infers from predicted data.</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>If True, forces recalculation even if a predicted schema exists.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[FlowfileColumn] | None</code> <p>A list of FlowfileColumn objects representing the predicted schema.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_predicted_schema(self, force: bool = False) -&gt; list[FlowfileColumn] | None:\n    \"\"\"Predicts the output schema of the node without full execution.\n\n    It uses the schema_callback or infers from predicted data.\n\n    Args:\n        force: If True, forces recalculation even if a predicted schema exists.\n\n    Returns:\n        A list of FlowfileColumn objects representing the predicted schema.\n    \"\"\"\n    logger.info(\n        f\"get_predicted_schema: node_id={self.node_id}, node_type={self.node_type}, force={force}, \"\n        f\"has_predicted_schema={self.node_schema.predicted_schema is not None}, \"\n        f\"has_schema_callback={self.schema_callback is not None}, \"\n        f\"has_output_field_config={hasattr(self._setting_input, 'output_field_config') and self._setting_input.output_field_config is not None if self._setting_input else False}\"\n    )\n\n    if self.node_schema.predicted_schema and not force:\n        logger.debug(f\"get_predicted_schema: node_id={self.node_id} - returning cached predicted_schema\")\n        return self.node_schema.predicted_schema\n\n    if self.schema_callback is not None and (self.node_schema.predicted_schema is None or force):\n        self.print(\"Getting the data from a schema callback\")\n        logger.info(f\"get_predicted_schema: node_id={self.node_id} - invoking schema_callback\")\n        if force:\n            # Force the schema callback to reset, so that it will be executed again\n            logger.debug(f\"get_predicted_schema: node_id={self.node_id} - forcing schema_callback reset\")\n            self.schema_callback.reset()\n\n        try:\n            schema = self.schema_callback()\n            logger.info(\n                f\"get_predicted_schema: node_id={self.node_id} - schema_callback returned \"\n                f\"{len(schema) if schema else 0} columns: {[c.name for c in schema] if schema else []}\"\n            )\n        except Exception as e:\n            logger.error(f\"get_predicted_schema: node_id={self.node_id} - schema_callback raised exception: {e}\")\n            schema = None\n\n        if schema is not None and len(schema) &gt; 0:\n            self.print(\"Calculating the schema based on the schema callback\")\n            self.node_schema.predicted_schema = schema\n            logger.info(f\"get_predicted_schema: node_id={self.node_id} - set predicted_schema from schema_callback\")\n            return self.node_schema.predicted_schema\n        else:\n            logger.warning(\n                f\"get_predicted_schema: node_id={self.node_id} - schema_callback returned empty/None schema\"\n            )\n    else:\n        logger.debug(f\"get_predicted_schema: node_id={self.node_id} - no schema_callback available\")\n\n    logger.debug(f\"get_predicted_schema: node_id={self.node_id} - falling back to _predicted_data_getter\")\n    predicted_data = self._predicted_data_getter()\n    if predicted_data is not None and predicted_data.schema is not None:\n        self.print(\"Calculating the schema based on the predicted resulting data\")\n        logger.info(\n            f\"get_predicted_schema: node_id={self.node_id} - using schema from predicted_data \"\n            f\"({len(predicted_data.schema)} columns)\"\n        )\n        self.node_schema.predicted_schema = self._predicted_data_getter().schema\n    else:\n        logger.warning(\n            f\"get_predicted_schema: node_id={self.node_id} - no schema available from any source \"\n            f\"(predicted_data={'None' if predicted_data is None else 'has_data'}, \"\n            f\"schema={'None' if predicted_data is None or predicted_data.schema is None else 'has_schema'})\"\n        )\n\n    return self.node_schema.predicted_schema\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_repr","title":"<code>get_repr()</code>","text":"<p>Gets a detailed dictionary representation of the node's state.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing key information about the node.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_repr(self) -&gt; dict:\n    \"\"\"Gets a detailed dictionary representation of the node's state.\n\n    Returns:\n        A dictionary containing key information about the node.\n    \"\"\"\n    return dict(\n        FlowNode=dict(\n            node_id=self.node_id,\n            step_name=self.__name__,\n            output_columns=self.node_schema.output_columns,\n            output_schema=self._get_readable_schema(),\n        )\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_resulting_data","title":"<code>get_resulting_data()</code>","text":"<p>Executes the node's function to produce the actual output data.</p> <p>Handles both regular functions and external data sources. Thread-safe: uses _execution_lock to prevent concurrent execution and concurrent access to the underlying LazyFrame by sibling nodes.</p> <p>Returns:</p> Type Description <code>FlowDataEngine | None</code> <p>A FlowDataEngine instance containing the result, or None on error.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Propagates exceptions from the node's function execution.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_resulting_data(self) -&gt; FlowDataEngine | None:\n    \"\"\"Executes the node's function to produce the actual output data.\n\n    Handles both regular functions and external data sources.\n    Thread-safe: uses _execution_lock to prevent concurrent execution\n    and concurrent access to the underlying LazyFrame by sibling nodes.\n\n    Returns:\n        A FlowDataEngine instance containing the result, or None on error.\n\n    Raises:\n        Exception: Propagates exceptions from the node's function execution.\n    \"\"\"\n    if self.is_setup:\n        with self._execution_lock:\n            if self.results.resulting_data is None and self.results.errors is None:\n                self.print(\"getting resulting data\")\n                try:\n                    if isinstance(self.function, FlowDataEngine):\n                        fl: FlowDataEngine = self.function\n                    elif self.node_type == \"external_source\":\n                        fl: FlowDataEngine = self.function()\n                        fl.collect_external()\n                        self.node_settings.streamable = False\n                    else:\n                        try:\n                            self.print(\"Collecting input data from all inputs\")\n                            input_data = []\n                            input_locks = []\n                            try:\n                                for i, v in enumerate(self.all_inputs):\n                                    self.print(f\"Getting resulting data from input {i} (node {v.node_id})\")\n                                    # Lock the input node to prevent sibling nodes from\n                                    # concurrently accessing the same upstream LazyFrame.\n                                    v._execution_lock.acquire()\n                                    input_locks.append(v._execution_lock)\n                                    input_result = self._resolve_input_result(v)\n                                    self.print(\n                                        f\"Input {i} data type: {type(input_result)}, dataframe type: {type(input_result.data_frame) if input_result else 'None'}\"\n                                    )\n                                    input_data.append(input_result)\n                                self.print(f\"All {len(input_data)} inputs collected, calling node function\")\n                                fl = self._function(*input_data)\n                            finally:\n                                for lock in input_locks:\n                                    lock.release()\n                        except Exception as e:\n                            raise e\n                    fl.set_streamable(self.node_settings.streamable)\n\n                    # Apply output field configuration if enabled\n                    if (\n                        hasattr(self._setting_input, \"output_field_config\")\n                        and self._setting_input.output_field_config\n                    ):\n                        try:\n                            fl = apply_output_field_config(fl, self._setting_input.output_field_config)\n                        except Exception as e:\n                            logger.error(f\"Error applying output field config for node {self.node_id}: {e}\")\n                            raise\n\n                    self.results.resulting_data = fl\n                    self.node_schema.result_schema = fl.schema\n                except Exception as e:\n                    self.results.resulting_data = FlowDataEngine()\n                    self.results.errors = str(e)\n                    self.node_stats.has_run_with_current_setup = False\n                    self.node_stats.has_completed_last_run = False\n                    raise e\n            return self.results.resulting_data\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_table_example","title":"<code>get_table_example(include_data=False)</code>","text":"<p>Generates a <code>TableExample</code> model summarizing the node's output.</p> <p>This can optionally include a sample of the data.</p> <p>Parameters:</p> Name Type Description Default <code>include_data</code> <code>bool</code> <p>If True, includes a data sample in the result.</p> <code>False</code> <p>Returns:</p> Type Description <code>TableExample | None</code> <p>A <code>TableExample</code> object, or None if the node is not set up.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_table_example(self, include_data: bool = False) -&gt; TableExample | None:\n    \"\"\"Generates a `TableExample` model summarizing the node's output.\n\n    This can optionally include a sample of the data.\n\n    Args:\n        include_data: If True, includes a data sample in the result.\n\n    Returns:\n        A `TableExample` object, or None if the node is not set up.\n    \"\"\"\n    self.print(\"Getting a table example\")\n    if self.is_setup and include_data and self.node_stats.has_completed_last_run:\n        if self.node_template.node_group == \"output\":\n            self.print(\"getting the table example\")\n            return self.main_input[0].get_table_example(include_data)\n\n        logger.info(\"getting the table example since the node has run\")\n        example_data_getter = self.results.example_data_generator\n        if example_data_getter is not None:\n            data = example_data_getter().to_pylist()\n            if data is None:\n                data = []\n        else:\n            data = []\n        schema = [FileColumn.model_validate(c.get_column_repr()) for c in self.schema]\n        has_example_data = self.results.example_data_generator is not None\n\n        return TableExample(\n            node_id=self.node_id,\n            name=str(self.node_id),\n            number_of_records=999,\n            number_of_columns=len(schema),\n            table_schema=schema,\n            columns=[c.name for c in schema],\n            data=data,\n            has_example_data=has_example_data,\n            has_run_with_current_setup=self.node_stats.has_run_with_current_setup,\n        )\n    else:\n        logger.warning(\"getting the table example but the node has not run\")\n        try:\n            schema = [FileColumn.model_validate(c.get_column_repr()) for c in self.schema]\n        except Exception as e:\n            logger.warning(e)\n            schema = []\n        columns = [s.name for s in schema]\n        return TableExample(\n            node_id=self.node_id,\n            name=str(self.node_id),\n            number_of_records=0,\n            number_of_columns=len(columns),\n            table_schema=schema,\n            columns=columns,\n            data=[],\n        )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.needs_reset","title":"<code>needs_reset()</code>","text":"<p>Checks if the node's hash has changed, indicating an outdated state.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the calculated hash differs from the stored hash.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def needs_reset(self) -&gt; bool:\n    \"\"\"Checks if the node's hash has changed, indicating an outdated state.\n\n    Returns:\n        True if the calculated hash differs from the stored hash.\n    \"\"\"\n    return self._hash != self.calculate_hash(self.setting_input)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.needs_run","title":"<code>needs_run(performance_mode, node_logger=None, execution_location='remote')</code>","text":"<p>Determines if the node needs to be executed.</p> <p>The decision is based on its run state, caching settings, and execution mode.</p> <p>Parameters:</p> Name Type Description Default <code>performance_mode</code> <code>bool</code> <p>True if the flow is in performance mode.</p> required <code>node_logger</code> <code>NodeLogger</code> <p>The logger instance for this node.</p> <code>None</code> <code>execution_location</code> <code>ExecutionLocationsLiteral</code> <p>The target execution location.</p> <code>'remote'</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the node should be run, False otherwise.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def needs_run(\n    self,\n    performance_mode: bool,\n    node_logger: NodeLogger = None,\n    execution_location: schemas.ExecutionLocationsLiteral = \"remote\",\n) -&gt; bool:\n    \"\"\"Determines if the node needs to be executed.\n\n    The decision is based on its run state, caching settings, and execution mode.\n\n    Args:\n        performance_mode: True if the flow is in performance mode.\n        node_logger: The logger instance for this node.\n        execution_location: The target execution location.\n\n    Returns:\n        True if the node should be run, False otherwise.\n    \"\"\"\n    if execution_location == \"local\":\n        return False\n\n    flow_logger = logger if node_logger is None else node_logger\n    cache_result_exists = results_exists(self.hash)\n    if not self.node_stats.has_run_with_current_setup:\n        flow_logger.info(\"Node has not run, needs to run\")\n        return True\n    if self.node_settings.cache_results and cache_result_exists:\n        return False\n    elif self.node_settings.cache_results and not cache_result_exists:\n        return True\n    elif not performance_mode and cache_result_exists:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.post_init","title":"<code>post_init()</code>","text":"<p>Initializes or resets the node's attributes to their default states.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def post_init(self):\n    \"\"\"Initializes or resets the node's attributes to their default states.\"\"\"\n    self.node_inputs = NodeStepInputs()\n    self.node_stats = NodeStepStats()\n    self.node_settings = NodeStepSettings()\n    self.node_schema = NodeSchemaInformation()\n    self.results = NodeResults()\n    self.node_information = schemas.NodeInformation()\n    self.leads_to_nodes = []\n    self._setting_input = None\n    self._cache_progress = None\n    self._schema_callback = None\n    self._state_needs_reset = False\n    self._execution_lock = threading.RLock()  # Protects concurrent access to get_resulting_data\n    self._kernel_cancel_context = None\n    self._kernel_cancel_event: threading.Event | None = None\n    # Initialize execution state\n    self._execution_state = NodeExecutionState()\n    self._executor = None  # Will be lazily created\n    # Multi-output support: maps output handle (e.g. \"output-0\") to FlowDataEngine\n    self._named_outputs: dict[str, FlowDataEngine] = {}\n    # Maps source node id -&gt; output handle used in the connection\n    self._input_output_handles: dict[int, str] = {}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.prepare_before_run","title":"<code>prepare_before_run()</code>","text":"<p>Resets results and errors before a new execution.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def prepare_before_run(self):\n    \"\"\"Resets results and errors before a new execution.\"\"\"\n\n    self.results.errors = None\n    self.results.resulting_data = None\n    self.results.example_data = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.print","title":"<code>print(v)</code>","text":"<p>Helper method to log messages with node context.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Any</code> <p>The message or value to log.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def print(self, v: Any):\n    \"\"\"Helper method to log messages with node context.\n\n    Args:\n        v: The message or value to log.\n    \"\"\"\n    logger.info(f\"{self.node_type}, node_id: {self.node_id}: {v}\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.remove_cache","title":"<code>remove_cache()</code>","text":"<p>Removes cached results for this node.</p> <p>Note: Currently not fully implemented.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def remove_cache(self):\n    \"\"\"Removes cached results for this node.\n\n    Note: Currently not fully implemented.\n    \"\"\"\n\n    if results_exists(self.hash):\n        logger.warning(\"Not implemented\")\n        clear_task_from_worker(self.hash)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.reset","title":"<code>reset(deep=False)</code>","text":"<p>Resets the node's execution state and schema information.</p> <p>This also triggers a reset on all downstream nodes.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, forces a reset even if the hash hasn't changed.</p> <code>False</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def reset(self, deep: bool = False):\n    \"\"\"Resets the node's execution state and schema information.\n\n    This also triggers a reset on all downstream nodes.\n\n    Args:\n        deep: If True, forces a reset even if the hash hasn't changed.\n    \"\"\"\n    needs_reset = self.needs_reset() or deep\n    if needs_reset:\n        logger.info(f\"{self.node_id}: Node needs reset\")\n        self.node_stats.has_run_with_current_setup = False\n        self.results.reset()\n        self.node_schema.result_schema = None\n        self.node_schema.predicted_schema = None\n        self._hash = None\n        self.node_information.is_setup = None\n        self.results.errors = None\n\n        # Reset execution state but preserve source file info for change detection\n        self._execution_state.has_run_with_current_setup = False\n        self._execution_state.has_completed_last_run = False\n        self._execution_state.result_schema = None\n        self._execution_state.predicted_schema = None\n        self._execution_state.execution_hash = None\n        # Note: source_file_info NOT reset - needed for change detection\n\n        if self.is_correct:\n            self._schema_callback = None  # Ensure the schema callback is reset\n            if self.schema_callback:\n                logger.info(f\"{self.node_id}: Resetting the schema callback\")\n                self.schema_callback.start()\n        self.evaluate_nodes()\n        _ = self.hash  # Recalculate the hash after reset\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.set_node_information","title":"<code>set_node_information()</code>","text":"<p>Populates the <code>node_information</code> attribute with the current state.</p> <p>This includes the node's connections, settings, and position.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def set_node_information(self):\n    \"\"\"Populates the `node_information` attribute with the current state.\n\n    This includes the node's connections, settings, and position.\n    \"\"\"\n    node_information = self.node_information\n    node_information.left_input_id = self.node_inputs.left_input.node_id if self.left_input else None\n    node_information.right_input_id = self.node_inputs.right_input.node_id if self.right_input else None\n    node_information.input_ids = (\n        [mi.node_id for mi in self.node_inputs.main_inputs] if self.node_inputs.main_inputs is not None else None\n    )\n    node_information.setting_input = self.setting_input\n    node_information.outputs = [n.node_id for n in self.leads_to_nodes]\n    user_description = self.setting_input.description if hasattr(self.setting_input, \"description\") else \"\"\n    if user_description:\n        node_information.description = user_description\n    elif hasattr(self.setting_input, \"get_default_description\"):\n        node_information.description = self.setting_input.get_default_description()\n    else:\n        node_information.description = \"\"\n    node_information.node_reference = (\n        self.setting_input.node_reference if hasattr(self.setting_input, \"node_reference\") else None\n    )\n    node_information.is_setup = self.is_setup\n    node_information.x_position = self.setting_input.pos_x\n    node_information.y_position = self.setting_input.pos_y\n    node_information.type = self.node_type\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.store_example_data_generator","title":"<code>store_example_data_generator(external_df_fetcher)</code>","text":"<p>Stores a generator function for fetching a sample of the result data.</p> <p>Parameters:</p> Name Type Description Default <code>external_df_fetcher</code> <code>ExternalDfFetcher | ExternalSampler</code> <p>The process that generated the sample data.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def store_example_data_generator(self, external_df_fetcher: ExternalDfFetcher | ExternalSampler):\n    \"\"\"Stores a generator function for fetching a sample of the result data.\n\n    Args:\n        external_df_fetcher: The process that generated the sample data.\n    \"\"\"\n    if external_df_fetcher.status is not None:\n        file_ref = external_df_fetcher.status.file_ref\n        self.results.example_data_path = file_ref\n        self.results.example_data_generator = get_read_top_n(file_path=file_ref, n=100)\n    else:\n        logger.error(\"Could not get the sample data, the external process is not ready\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.update_node","title":"<code>update_node(function, input_columns=None, output_schema=None, drop_columns=None, name=None, setting_input=None, pos_x=0, pos_y=0, schema_callback=None)</code>","text":"<p>Updates the properties of the node.</p> <p>This is called during initialization and when settings are changed.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>The new core data processing function.</p> required <code>input_columns</code> <code>list[str]</code> <p>The new list of input columns.</p> <code>None</code> <code>output_schema</code> <code>list[FlowfileColumn]</code> <p>The new schema of added columns.</p> <code>None</code> <code>drop_columns</code> <code>list[str]</code> <p>The new list of dropped columns.</p> <code>None</code> <code>name</code> <code>str</code> <p>The new name for the node.</p> <code>None</code> <code>setting_input</code> <code>Any</code> <p>The new settings object.</p> <code>None</code> <code>pos_x</code> <code>float</code> <p>The new x-coordinate.</p> <code>0</code> <code>pos_y</code> <code>float</code> <p>The new y-coordinate.</p> <code>0</code> <code>schema_callback</code> <code>Callable</code> <p>The new custom schema callback function.</p> <code>None</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def update_node(\n    self,\n    function: Callable,\n    input_columns: list[str] = None,\n    output_schema: list[FlowfileColumn] = None,\n    drop_columns: list[str] = None,\n    name: str = None,\n    setting_input: Any = None,\n    pos_x: float = 0,\n    pos_y: float = 0,\n    schema_callback: Callable = None,\n):\n    \"\"\"Updates the properties of the node.\n\n    This is called during initialization and when settings are changed.\n\n    Args:\n        function: The new core data processing function.\n        input_columns: The new list of input columns.\n        output_schema: The new schema of added columns.\n        drop_columns: The new list of dropped columns.\n        name: The new name for the node.\n        setting_input: The new settings object.\n        pos_x: The new x-coordinate.\n        pos_y: The new y-coordinate.\n        schema_callback: The new custom schema callback function.\n    \"\"\"\n    self.user_provided_schema_callback = schema_callback\n    self.node_information.y_position = int(pos_y)\n    self.node_information.x_position = int(pos_x)\n    self.node_information.setting_input = setting_input\n    self.name = self.node_type if name is None else name\n    self._function = function\n\n    self.node_schema.input_columns = [] if input_columns is None else input_columns\n    self.node_schema.output_columns = [] if output_schema is None else output_schema\n    self.node_schema.drop_columns = [] if drop_columns is None else drop_columns\n    self.node_settings.renew_schema = True\n    if hasattr(setting_input, \"cache_results\"):\n        self.node_settings.cache_results = setting_input.cache_results\n\n    self.results.errors = None\n    self.add_lead_to_in_depend_source()\n    _ = self.hash\n    self.node_template = node_store.node_dict.get(self.node_type)\n    if self.node_template is None:\n        raise Exception(f\"Node template {self.node_type} not found\")\n    self.node_default = node_store.node_defaults.get(self.node_type)\n    self.setting_input = setting_input  # wait until the end so that the hash is calculated correctly\n</code></pre>"},{"location":"for-developers/python-api-reference.html#the-flowdataengine","title":"The FlowDataEngine","text":"<p>The <code>FlowDataEngine</code> is the primary engine of the library, providing a rich API for data manipulation, I/O, and transformation. Its methods are grouped below by functionality.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine","title":"<code>flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine</code>  <code>dataclass</code>","text":"<p>The core data handling engine for Flowfile.</p> <p>This class acts as a high-level wrapper around a Polars DataFrame or LazyFrame, providing a unified API for data ingestion, transformation, and output. It manages data state (lazy vs. eager), schema information, and execution logic.</p> <p>Attributes:</p> Name Type Description <code>_data_frame</code> <code>DataFrame | LazyFrame</code> <p>The underlying Polars DataFrame or LazyFrame.</p> <code>columns</code> <code>list[Any]</code> <p>A list of column names in the current data frame.</p> <code>name</code> <code>str</code> <p>An optional name for the data engine instance.</p> <code>number_of_records</code> <code>int</code> <p>The number of records. Can be -1 for lazy frames.</p> <code>errors</code> <code>list</code> <p>A list of errors encountered during operations.</p> <code>_schema</code> <code>list[FlowfileColumn] | None</code> <p>A cached list of <code>FlowfileColumn</code> objects representing the schema.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Makes the class instance callable, returning itself.</p> <code>__get_sample__</code> <p>Internal method to get a sample of the data.</p> <code>__getitem__</code> <p>Accesses a specific column or item from the DataFrame.</p> <code>__init__</code> <p>Initializes the FlowDataEngine from various data sources.</p> <code>__len__</code> <p>Returns the number of records in the table.</p> <code>__repr__</code> <p>Returns a string representation of the FlowDataEngine.</p> <code>add_new_values</code> <p>Adds a new column with the provided values.</p> <code>add_record_id</code> <p>Adds a record ID (row number) column to the DataFrame.</p> <code>apply_flowfile_formula</code> <p>Applies a formula to create a new column or transform an existing one.</p> <code>apply_sql_formula</code> <p>Applies an SQL-style formula using <code>pl.sql_expr</code>.</p> <code>assert_equal</code> <p>Asserts that this DataFrame is equal to another.</p> <code>cache</code> <p>Caches the current DataFrame to disk and updates the internal reference.</p> <code>calculate_schema</code> <p>Calculates and returns the schema.</p> <code>change_column_types</code> <p>Changes the data type of one or more columns.</p> <code>collect</code> <p>Collects the data and returns it as a Polars DataFrame.</p> <code>collect_external</code> <p>Materializes data from a tracked external source.</p> <code>concat</code> <p>Concatenates this DataFrame with one or more other DataFrames.</p> <code>count</code> <p>Gets the total number of records.</p> <code>create_from_external_source</code> <p>Creates a FlowDataEngine from an external data source.</p> <code>create_from_path</code> <p>Creates a FlowDataEngine from a local file path.</p> <code>create_from_path_worker</code> <p>Creates a FlowDataEngine from a path in a worker process.</p> <code>create_from_schema</code> <p>Creates an empty FlowDataEngine from a schema definition.</p> <code>create_from_sql</code> <p>Creates a FlowDataEngine by executing a SQL query.</p> <code>create_random</code> <p>Creates a FlowDataEngine with randomly generated data.</p> <code>do_cross_join</code> <p>Performs a cross join with another DataFrame.</p> <code>do_filter</code> <p>Filters rows based on a predicate expression.</p> <code>do_group_by</code> <p>Performs a group-by operation on the DataFrame.</p> <code>do_pivot</code> <p>Converts the DataFrame from a long to a wide format, aggregating values.</p> <code>do_select</code> <p>Performs a complex column selection, renaming, and reordering operation.</p> <code>do_sort</code> <p>Sorts the DataFrame by one or more columns.</p> <code>drop_columns</code> <p>Drops specified columns from the DataFrame.</p> <code>from_cloud_storage_obj</code> <p>Creates a FlowDataEngine from an object in cloud storage.</p> <code>generate_enumerator</code> <p>Generates a FlowDataEngine with a single column containing a sequence of integers.</p> <code>get_estimated_file_size</code> <p>Estimates the file size in bytes if the data originated from a local file.</p> <code>get_number_of_records</code> <p>Gets the total number of records in the DataFrame.</p> <code>get_number_of_records_in_process</code> <p>Get the number of records in the DataFrame in the local process.</p> <code>get_output_sample</code> <p>Gets a sample of the data as a list of dictionaries.</p> <code>get_record_count</code> <p>Returns a new FlowDataEngine with a single column 'number_of_records'</p> <code>get_sample</code> <p>Gets a sample of rows from the DataFrame.</p> <code>get_schema_column</code> <p>Retrieves the schema information for a single column by its name.</p> <code>get_select_inputs</code> <p>Gets <code>SelectInput</code> specifications for all columns in the current schema.</p> <code>get_subset</code> <p>Gets the first <code>n_rows</code> from the DataFrame.</p> <code>initialize_empty_fl</code> <p>Initializes an empty LazyFrame.</p> <code>iter_batches</code> <p>Iterates over the DataFrame in batches.</p> <code>join</code> <p>Performs a standard SQL-style join with another DataFrame.</p> <code>make_unique</code> <p>Gets the unique rows from the DataFrame.</p> <code>output</code> <p>Writes the DataFrame to an output file.</p> <code>reorganize_order</code> <p>Reorganizes columns into a specified order.</p> <code>save</code> <p>Saves the DataFrame to a file in a separate thread.</p> <code>select_columns</code> <p>Selects a subset of columns from the DataFrame.</p> <code>set_streamable</code> <p>Sets whether DataFrame operations should be streamable.</p> <code>solve_graph</code> <p>Solves a graph problem represented by 'from' and 'to' columns.</p> <code>split</code> <p>Splits a column's text values into multiple rows based on a delimiter.</p> <code>start_fuzzy_join</code> <p>Starts a fuzzy join operation in a background process.</p> <code>to_arrow</code> <p>Converts the DataFrame to a PyArrow Table.</p> <code>to_cloud_storage_obj</code> <p>Writes the DataFrame to an object in cloud storage.</p> <code>to_dict</code> <p>Converts the DataFrame to a Python dictionary of columns.</p> <code>to_pylist</code> <p>Converts the DataFrame to a list of Python dictionaries.</p> <code>to_raw_data</code> <p>Converts the DataFrame to a <code>RawData</code> schema object.</p> <code>unpivot</code> <p>Converts the DataFrame from a wide to a long format.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@dataclass\nclass FlowDataEngine:\n    \"\"\"The core data handling engine for Flowfile.\n\n    This class acts as a high-level wrapper around a Polars DataFrame or\n    LazyFrame, providing a unified API for data ingestion, transformation,\n    and output. It manages data state (lazy vs. eager), schema information,\n    and execution logic.\n\n    Attributes:\n        _data_frame: The underlying Polars DataFrame or LazyFrame.\n        columns: A list of column names in the current data frame.\n        name: An optional name for the data engine instance.\n        number_of_records: The number of records. Can be -1 for lazy frames.\n        errors: A list of errors encountered during operations.\n        _schema: A cached list of `FlowfileColumn` objects representing the schema.\n    \"\"\"\n\n    # Core attributes\n    _data_frame: pl.DataFrame | pl.LazyFrame\n    columns: list[Any]\n\n    # Metadata attributes\n    name: str = None\n    number_of_records: int = None\n    errors: list = None\n    _schema: list[FlowfileColumn] | None = None\n\n    # Configuration attributes\n    _optimize_memory: bool = False\n    _lazy: bool = None\n    _streamable: bool = True\n    _calculate_schema_stats: bool = False\n\n    # Cache and optimization attributes\n    __col_name_idx_map: dict = None\n    __data_map: dict = None\n    __optimized_columns: list = None\n    __sample__: str = None\n    __number_of_fields: int = None\n    _col_idx: dict[str, int] = None\n\n    # Source tracking\n    _org_path: str | None = None\n    _external_source: ExternalDataSource | None = None\n\n    # State tracking\n    sorted_by: int = None\n    is_future: bool = False\n    is_collected: bool = True\n    ind_schema_calculated: bool = False\n\n    # Callbacks\n    _future: Future = None\n    _number_of_records_callback: Callable = None\n    _data_callback: Callable = None\n\n    def __init__(\n        self,\n        raw_data: list[dict] | list[Any] | dict[str, Any] | ParquetFile | pl.DataFrame | pl.LazyFrame | input_schema.RawData = None,\n        path_ref: str = None,\n        name: str = None,\n        optimize_memory: bool = True,\n        schema: list[FlowfileColumn] | list[str] | pl.Schema = None,\n        number_of_records: int = None,\n        calculate_schema_stats: bool = False,\n        streamable: bool = True,\n        number_of_records_callback: Callable = None,\n        data_callback: Callable = None,\n    ):\n        \"\"\"Initializes the FlowDataEngine from various data sources.\n\n        Args:\n            raw_data: The input data. Can be a list of dicts, a Polars DataFrame/LazyFrame,\n                or a `RawData` schema object.\n            path_ref: A string path to a Parquet file.\n            name: An optional name for the data engine instance.\n            optimize_memory: If True, prefers lazy operations to conserve memory.\n            schema: An optional schema definition. Can be a list of `FlowfileColumn` objects,\n                a list of column names, or a Polars `Schema`.\n            number_of_records: The number of records, if known.\n            calculate_schema_stats: If True, computes detailed statistics for each column.\n            streamable: If True, allows for streaming operations when possible.\n            number_of_records_callback: A callback function to retrieve the number of records.\n            data_callback: A callback function to retrieve the data.\n        \"\"\"\n        self._initialize_attributes(number_of_records_callback, data_callback, streamable)\n\n        if raw_data is not None:\n            self._handle_raw_data(raw_data, number_of_records, optimize_memory)\n        elif path_ref:\n            self._handle_path_ref(path_ref, optimize_memory)\n        else:\n            self.initialize_empty_fl()\n        self._finalize_initialization(name, optimize_memory, schema, calculate_schema_stats)\n\n    def _initialize_attributes(self, number_of_records_callback, data_callback, streamable):\n        \"\"\"(Internal) Sets the initial default attributes for a new instance.\n\n        This helper is called first during initialization to ensure all state-tracking\n        and configuration attributes have a clean default value before data is processed.\n        \"\"\"\n        self._external_source = None\n        self._number_of_records_callback = number_of_records_callback\n        self._data_callback = data_callback\n        self.ind_schema_calculated = False\n        self._streamable = streamable\n        self._org_path = None\n        self._lazy = False\n        self.errors = []\n        self._calculate_schema_stats = False\n        self.is_collected = True\n        self.is_future = False\n\n    def _handle_raw_data(self, raw_data, number_of_records, optimize_memory):\n        \"\"\"(Internal) Dispatches raw data to the appropriate handler based on its type.\n\n        This acts as a router during initialization, inspecting the type of `raw_data`\n        and calling the corresponding specialized `_handle_*` method to process it.\n        \"\"\"\n        if isinstance(raw_data, input_schema.RawData):\n            self._handle_raw_data_format(raw_data)\n        elif isinstance(raw_data, pl.DataFrame):\n            self._handle_polars_dataframe(raw_data, number_of_records)\n        elif isinstance(raw_data, pl.LazyFrame):\n            self._handle_polars_lazy_frame(raw_data, number_of_records, optimize_memory)\n        elif isinstance(raw_data, (list, dict)):\n            self._handle_python_data(raw_data)\n\n    def _handle_polars_dataframe(self, df: pl.DataFrame, number_of_records: int | None):\n        \"\"\"(Internal) Initializes the engine from an eager Polars DataFrame.\"\"\"\n        self.data_frame = df\n        self.number_of_records = number_of_records or df.select(pl.len())[0, 0]\n\n    def _handle_polars_lazy_frame(self, lf: pl.LazyFrame, number_of_records: int | None, optimize_memory: bool):\n        \"\"\"(Internal) Initializes the engine from a Polars LazyFrame.\"\"\"\n        self.data_frame = lf\n        self._lazy = True\n        if number_of_records is not None:\n            self.number_of_records = number_of_records\n        elif optimize_memory:\n            self.number_of_records = -1\n        else:\n            self.number_of_records = lf.select(pl.len()).collect()[0, 0]\n\n    def _handle_python_data(self, data: list | dict):\n        \"\"\"(Internal) Dispatches Python collections to the correct handler.\"\"\"\n        if isinstance(data, dict):\n            self._handle_dict_input(data)\n        else:\n            self._handle_list_input(data)\n\n    def _handle_dict_input(self, data: dict):\n        \"\"\"(Internal) Initializes the engine from a Python dictionary.\"\"\"\n        if len(data) == 0:\n            self.initialize_empty_fl()\n        lengths = [len(v) if isinstance(v, (list, tuple)) else 1 for v in data.values()]\n\n        if len(set(lengths)) == 1 and lengths[0] &gt; 1:\n            self.number_of_records = lengths[0]\n            self.data_frame = pl.DataFrame(data)\n        else:\n            self.number_of_records = 1\n            self.data_frame = pl.DataFrame([data])\n        self.lazy = True\n\n    def _handle_raw_data_format(self, raw_data: input_schema.RawData):\n        \"\"\"(Internal) Initializes the engine from a `RawData` schema object.\n\n        This method uses the schema provided in the `RawData` object to correctly\n        infer data types when creating the Polars DataFrame.\n\n        Args:\n            raw_data: An instance of `RawData` containing the data and schema.\n        \"\"\"\n        flowfile_schema = list(FlowfileColumn.create_from_minimal_field_info(c) for c in raw_data.columns)\n        polars_schema = pl.Schema(\n            [\n                (flowfile_column.column_name, flowfile_column.get_polars_type().pl_datatype)\n                for flowfile_column in flowfile_schema\n            ]\n        )\n        try:\n            df = pl.DataFrame(raw_data.data, polars_schema, strict=False)\n        except TypeError as e:\n            logger.warning(f\"Could not parse the data with the schema:\\n{e}\")\n            df = pl.DataFrame(raw_data.data)\n        self.number_of_records = len(df)\n        self.data_frame = df.lazy()\n        self.lazy = True\n\n    def _handle_list_input(self, data: list):\n        \"\"\"(Internal) Initializes the engine from a list of records.\"\"\"\n        number_of_records = len(data)\n        if number_of_records &gt; 0:\n            processed_data = self._process_list_data(data)\n            self.number_of_records = number_of_records\n            self.data_frame = pl.DataFrame(processed_data)\n            self.lazy = True\n        else:\n            self.initialize_empty_fl()\n            self.number_of_records = 0\n\n    @staticmethod\n    def _process_list_data(data: list) -&gt; list[dict]:\n        \"\"\"(Internal) Normalizes list data into a list of dictionaries.\n\n        Ensures that a list of objects or non-dict items is converted into a\n        uniform list of dictionaries suitable for Polars DataFrame creation.\n        \"\"\"\n        if not (isinstance(data[0], dict) or hasattr(data[0], \"__dict__\")):\n            try:\n                return pl.DataFrame(data).to_dicts()\n            except TypeError:\n                raise Exception(\"Value must be able to be converted to dictionary\")\n            except Exception as e:\n                raise Exception(f\"Value must be able to be converted to dictionary: {e}\")\n\n        if not isinstance(data[0], dict):\n            data = [row.__dict__ for row in data]\n\n        return ensure_similarity_dicts(data)\n\n    def to_cloud_storage_obj(self, settings: cloud_storage_schemas.CloudStorageWriteSettingsInternal):\n        \"\"\"Writes the DataFrame to an object in cloud storage.\n\n        This method supports writing to various cloud storage providers like AWS S3,\n        Azure Data Lake Storage, and Google Cloud Storage.\n\n        Args:\n            settings: A `CloudStorageWriteSettingsInternal` object containing connection\n                details, file format, and write options.\n\n        Raises:\n            ValueError: If the specified file format is not supported for writing.\n            NotImplementedError: If the 'append' write mode is used with an unsupported format.\n            Exception: If the write operation to cloud storage fails for any reason.\n        \"\"\"\n        connection = settings.connection\n        write_settings = settings.write_settings\n\n        logger.info(f\"Writing to {connection.storage_type} storage: {write_settings.resource_path}\")\n\n        if write_settings.write_mode == \"append\" and write_settings.file_format != \"delta\":\n            raise NotImplementedError(\"The 'append' write mode is not yet supported for this destination.\")\n        storage_options = CloudStorageReader.get_storage_options(connection)\n        credential_provider = CloudStorageReader.get_credential_provider(connection)\n        # Dispatch to the correct writer based on file format\n        if write_settings.file_format == \"parquet\":\n            self._write_parquet_to_cloud(\n                write_settings.resource_path, storage_options, credential_provider, write_settings\n            )\n        elif write_settings.file_format == \"delta\":\n            self._write_delta_to_cloud(\n                write_settings.resource_path, storage_options, credential_provider, write_settings\n            )\n        elif write_settings.file_format == \"csv\":\n            self._write_csv_to_cloud(write_settings.resource_path, storage_options, credential_provider, write_settings)\n        elif write_settings.file_format == \"json\":\n            self._write_json_to_cloud(\n                write_settings.resource_path, storage_options, credential_provider, write_settings\n            )\n        else:\n            raise ValueError(f\"Unsupported file format for writing: {write_settings.file_format}\")\n\n        logger.info(f\"Successfully wrote data to {write_settings.resource_path}\")\n\n    def _write_parquet_to_cloud(\n        self,\n        resource_path: str,\n        storage_options: dict[str, Any],\n        credential_provider: Callable | None,\n        write_settings: cloud_storage_schemas.CloudStorageWriteSettings,\n    ):\n        \"\"\"(Internal) Writes the DataFrame to a Parquet file in cloud storage.\n\n        Uses `sink_parquet` for efficient streaming writes. Falls back to a\n        collect-then-write pattern if sinking fails.\n        \"\"\"\n        try:\n            sink_kwargs = {\n                \"path\": resource_path,\n                \"compression\": write_settings.parquet_compression,\n            }\n            if storage_options:\n                sink_kwargs[\"storage_options\"] = storage_options\n            if credential_provider:\n                sink_kwargs[\"credential_provider\"] = credential_provider\n            try:\n                self.data_frame.sink_parquet(**sink_kwargs)\n            except Exception as e:\n                logger.warning(f\"Failed to sink the data, falling back to collecing and writing. \\n {e}\")\n                pl_df = self.collect()\n                sink_kwargs[\"file\"] = sink_kwargs.pop(\"path\")\n                pl_df.write_parquet(**sink_kwargs)\n\n        except Exception as e:\n            logger.error(f\"Failed to write Parquet to {resource_path}: {str(e)}\")\n            raise Exception(f\"Failed to write Parquet to cloud storage: {str(e)}\")\n\n    def _write_delta_to_cloud(\n        self,\n        resource_path: str,\n        storage_options: dict[str, Any],\n        credential_provider: Callable | None,\n        write_settings: cloud_storage_schemas.CloudStorageWriteSettings,\n    ):\n        \"\"\"(Internal) Writes the DataFrame to a Delta Lake table in cloud storage.\n\n        This operation requires collecting the data first, as `write_delta` operates\n        on an eager DataFrame.\n        \"\"\"\n        sink_kwargs = {\n            \"target\": resource_path,\n            \"mode\": write_settings.write_mode,\n        }\n        if storage_options:\n            sink_kwargs[\"storage_options\"] = storage_options\n        if credential_provider:\n            sink_kwargs[\"credential_provider\"] = credential_provider\n        self.collect().write_delta(**sink_kwargs)\n\n    def _write_csv_to_cloud(\n        self,\n        resource_path: str,\n        storage_options: dict[str, Any],\n        credential_provider: Callable | None,\n        write_settings: cloud_storage_schemas.CloudStorageWriteSettings,\n    ):\n        \"\"\"(Internal) Writes the DataFrame to a CSV file in cloud storage.\n\n        Uses `sink_csv` for efficient, streaming writes of the data.\n        \"\"\"\n        try:\n            sink_kwargs = {\n                \"path\": resource_path,\n                \"separator\": write_settings.csv_delimiter,\n            }\n            if storage_options:\n                sink_kwargs[\"storage_options\"] = storage_options\n            if credential_provider:\n                sink_kwargs[\"credential_provider\"] = credential_provider\n\n            # sink_csv executes the lazy query and writes the result\n            self.data_frame.sink_csv(**sink_kwargs)\n\n        except Exception as e:\n            logger.error(f\"Failed to write CSV to {resource_path}: {str(e)}\")\n            raise Exception(f\"Failed to write CSV to cloud storage: {str(e)}\")\n\n    def _write_json_to_cloud(\n        self,\n        resource_path: str,\n        storage_options: dict[str, Any],\n        credential_provider: Callable | None,\n        write_settings: cloud_storage_schemas.CloudStorageWriteSettings,\n    ):\n        \"\"\"(Internal) Writes the DataFrame to a line-delimited JSON (NDJSON) file.\n\n        Uses `sink_ndjson` for efficient, streaming writes.\n        \"\"\"\n        try:\n            sink_kwargs = {\"path\": resource_path}\n            if storage_options:\n                sink_kwargs[\"storage_options\"] = storage_options\n            if credential_provider:\n                sink_kwargs[\"credential_provider\"] = credential_provider\n            self.data_frame.sink_ndjson(**sink_kwargs)\n\n        except Exception as e:\n            logger.error(f\"Failed to write JSON to {resource_path}: {str(e)}\")\n            raise Exception(f\"Failed to write JSON to cloud storage: {str(e)}\")\n\n    @classmethod\n    def from_cloud_storage_obj(\n        cls, settings: cloud_storage_schemas.CloudStorageReadSettingsInternal\n    ) -&gt; FlowDataEngine:\n        \"\"\"Creates a FlowDataEngine from an object in cloud storage.\n\n        This method supports reading from various cloud storage providers like AWS S3,\n        Azure Data Lake Storage, and Google Cloud Storage, with support for\n        various authentication methods.\n\n        Args:\n            settings: A `CloudStorageReadSettingsInternal` object containing connection\n                details, file format, and read options.\n\n        Returns:\n            A new `FlowDataEngine` instance containing the data from cloud storage.\n\n        Raises:\n            ValueError: If the storage type or file format is not supported.\n            NotImplementedError: If a requested file format like \"delta\" or \"iceberg\"\n                is not yet implemented.\n            Exception: If reading from cloud storage fails.\n        \"\"\"\n        connection = settings.connection\n        read_settings = settings.read_settings\n\n        logger.info(f\"Reading from {connection.storage_type} storage: {read_settings.resource_path}\")\n        # Get storage options based on connection type\n        storage_options = CloudStorageReader.get_storage_options(connection)\n        # Get credential provider if needed\n        credential_provider = CloudStorageReader.get_credential_provider(connection)\n        if read_settings.file_format == \"parquet\":\n            return cls._read_parquet_from_cloud(\n                read_settings.resource_path,\n                storage_options,\n                credential_provider,\n                read_settings.scan_mode == \"directory\",\n            )\n        elif read_settings.file_format == \"delta\":\n            return cls._read_delta_from_cloud(\n                read_settings.resource_path, storage_options, credential_provider, read_settings\n            )\n        elif read_settings.file_format == \"csv\":\n            return cls._read_csv_from_cloud(\n                read_settings.resource_path, storage_options, credential_provider, read_settings\n            )\n        elif read_settings.file_format == \"json\":\n            return cls._read_json_from_cloud(\n                read_settings.resource_path,\n                storage_options,\n                credential_provider,\n                read_settings.scan_mode == \"directory\",\n            )\n        elif read_settings.file_format == \"iceberg\":\n            return cls._read_iceberg_from_cloud(\n                read_settings.resource_path, storage_options, credential_provider, read_settings\n            )\n\n        elif read_settings.file_format in [\"delta\", \"iceberg\"]:\n            # These would require additional libraries\n            raise NotImplementedError(f\"File format {read_settings.file_format} not yet implemented\")\n        else:\n            raise ValueError(f\"Unsupported file format: {read_settings.file_format}\")\n\n    @staticmethod\n    def _get_schema_from_first_file_in_dir(\n        source: str, storage_options: dict[str, Any], file_format: Literal[\"csv\", \"parquet\", \"json\", \"delta\"]\n    ) -&gt; list[FlowfileColumn] | None:\n        \"\"\"Infers the schema by scanning the first file in a cloud directory.\"\"\"\n        try:\n            scan_func = getattr(pl, \"scan_\" + file_format)\n            first_file_ref = get_first_file_from_s3_dir(source, storage_options=storage_options)\n            return convert_stats_to_column_info(\n                FlowDataEngine._create_schema_stats_from_pl_schema(\n                    scan_func(first_file_ref, storage_options=storage_options).collect_schema()\n                )\n            )\n        except Exception as e:\n            logger.warning(f\"Could not read schema from first file in directory, using default schema: {e}\")\n\n    @classmethod\n    def _read_iceberg_from_cloud(\n        cls,\n        resource_path: str,\n        storage_options: dict[str, Any],\n        credential_provider: Callable | None,\n        read_settings: cloud_storage_schemas.CloudStorageReadSettings,\n    ) -&gt; FlowDataEngine:\n        \"\"\"Reads Iceberg table(s) from cloud storage.\"\"\"\n        raise NotImplementedError(\"Failed to read Iceberg table from cloud storage: Not yet implemented\")\n\n    @classmethod\n    def _read_parquet_from_cloud(\n        cls,\n        resource_path: str,\n        storage_options: dict[str, Any],\n        credential_provider: Callable | None,\n        is_directory: bool,\n    ) -&gt; FlowDataEngine:\n        \"\"\"Reads Parquet file(s) from cloud storage.\"\"\"\n        try:\n            # Use scan_parquet for lazy evaluation\n            if is_directory:\n                resource_path = ensure_path_has_wildcard_pattern(resource_path=resource_path, file_format=\"parquet\")\n            scan_kwargs = {\"source\": resource_path}\n\n            if storage_options:\n                scan_kwargs[\"storage_options\"] = storage_options\n\n            if credential_provider:\n                scan_kwargs[\"credential_provider\"] = credential_provider\n            if storage_options and is_directory:\n                schema = cls._get_schema_from_first_file_in_dir(resource_path, storage_options, \"parquet\")\n            else:\n                schema = None\n            lf = pl.scan_parquet(**scan_kwargs)\n\n            return cls(\n                lf,\n                number_of_records=6_666_666,  # Set so the provider is not accessed for this stat\n                optimize_memory=True,\n                streamable=True,\n                schema=schema,\n            )\n\n        except Exception as e:\n            logger.error(f\"Failed to read Parquet from {resource_path}: {str(e)}\")\n            raise Exception(f\"Failed to read Parquet from cloud storage: {str(e)}\")\n\n    @classmethod\n    def _read_delta_from_cloud(\n        cls,\n        resource_path: str,\n        storage_options: dict[str, Any],\n        credential_provider: Callable | None,\n        read_settings: cloud_storage_schemas.CloudStorageReadSettings,\n    ) -&gt; FlowDataEngine:\n        \"\"\"Reads a Delta Lake table from cloud storage.\"\"\"\n        try:\n            logger.info(\"Reading Delta file from cloud storage...\")\n            logger.info(f\"read_settings: {read_settings}\")\n            scan_kwargs = {\"source\": resource_path}\n            if read_settings.delta_version:\n                scan_kwargs[\"version\"] = read_settings.delta_version\n            if storage_options:\n                scan_kwargs[\"storage_options\"] = storage_options\n            if credential_provider:\n                scan_kwargs[\"credential_provider\"] = credential_provider\n            lf = pl.scan_delta(**scan_kwargs)\n\n            return cls(\n                lf,\n                number_of_records=6_666_666,  # Set so the provider is not accessed for this stat\n                optimize_memory=True,\n                streamable=True,\n            )\n        except Exception as e:\n            logger.error(f\"Failed to read Delta file from {resource_path}: {str(e)}\")\n            raise Exception(f\"Failed to read Delta file from cloud storage: {str(e)}\")\n\n    @classmethod\n    def _read_csv_from_cloud(\n        cls,\n        resource_path: str,\n        storage_options: dict[str, Any],\n        credential_provider: Callable | None,\n        read_settings: cloud_storage_schemas.CloudStorageReadSettings,\n    ) -&gt; FlowDataEngine:\n        \"\"\"Reads CSV file(s) from cloud storage.\"\"\"\n        try:\n            scan_kwargs = {\n                \"source\": resource_path,\n                \"has_header\": read_settings.csv_has_header,\n                \"separator\": read_settings.csv_delimiter,\n                \"encoding\": read_settings.csv_encoding,\n            }\n            if storage_options:\n                scan_kwargs[\"storage_options\"] = storage_options\n            if credential_provider:\n                scan_kwargs[\"credential_provider\"] = credential_provider\n\n            if read_settings.scan_mode == \"directory\":\n                resource_path = ensure_path_has_wildcard_pattern(resource_path=resource_path, file_format=\"csv\")\n                scan_kwargs[\"source\"] = resource_path\n            if storage_options and read_settings.scan_mode == \"directory\":\n                schema = cls._get_schema_from_first_file_in_dir(resource_path, storage_options, \"csv\")\n            else:\n                schema = None\n\n            lf = pl.scan_csv(**scan_kwargs)\n\n            return cls(\n                lf,\n                number_of_records=6_666_666,  # Will be calculated lazily\n                optimize_memory=True,\n                streamable=True,\n                schema=schema,\n            )\n\n        except Exception as e:\n            logger.error(f\"Failed to read CSV from {resource_path}: {str(e)}\")\n            raise Exception(f\"Failed to read CSV from cloud storage: {str(e)}\")\n\n    @classmethod\n    def _read_json_from_cloud(\n        cls,\n        resource_path: str,\n        storage_options: dict[str, Any],\n        credential_provider: Callable | None,\n        is_directory: bool,\n    ) -&gt; FlowDataEngine:\n        \"\"\"Reads JSON file(s) from cloud storage.\"\"\"\n        try:\n            if is_directory:\n                resource_path = ensure_path_has_wildcard_pattern(resource_path, \"json\")\n            scan_kwargs = {\"source\": resource_path}\n\n            if storage_options:\n                scan_kwargs[\"storage_options\"] = storage_options\n            if credential_provider:\n                scan_kwargs[\"credential_provider\"] = credential_provider\n\n            lf = pl.scan_ndjson(**scan_kwargs)  # Using NDJSON for line-delimited JSON\n\n            return cls(\n                lf,\n                number_of_records=-1,\n                optimize_memory=True,\n                streamable=True,\n            )\n\n        except Exception as e:\n            logger.error(f\"Failed to read JSON from {resource_path}: {str(e)}\")\n            raise Exception(f\"Failed to read JSON from cloud storage: {str(e)}\")\n\n    def _handle_path_ref(self, path_ref: str, optimize_memory: bool):\n        \"\"\"Handles file path reference input.\"\"\"\n        try:\n            pf = ParquetFile(path_ref)\n        except Exception as e:\n            logger.error(e)\n            raise Exception(\"Provided ref is not a parquet file\")\n\n        self.number_of_records = pf.metadata.num_rows\n        if optimize_memory:\n            self._lazy = True\n            self.data_frame = pl.scan_parquet(path_ref)\n        else:\n            self.data_frame = pl.read_parquet(path_ref)\n\n    def _finalize_initialization(\n        self, name: str, optimize_memory: bool, schema: Any | None, calculate_schema_stats: bool\n    ):\n        \"\"\"Finalizes initialization by setting remaining attributes.\"\"\"\n        _ = calculate_schema_stats\n        self.name = name\n        self._optimize_memory = optimize_memory\n        if assert_if_flowfile_schema(schema):\n            self._schema = schema\n            self.columns = [c.column_name for c in self._schema]\n        else:\n            pl_schema = self.data_frame.collect_schema()\n            self._schema = self._handle_schema(schema, pl_schema)\n            self.columns = [c.column_name for c in self._schema] if self._schema else pl_schema.names()\n\n    def __getitem__(self, item):\n        \"\"\"Accesses a specific column or item from the DataFrame.\"\"\"\n        return self.data_frame.select([item])\n\n    @property\n    def data_frame(self) -&gt; pl.LazyFrame | pl.DataFrame | None:\n        \"\"\"The underlying Polars DataFrame or LazyFrame.\n\n        This property provides access to the Polars object that backs the\n        FlowDataEngine. It handles lazy-loading from external sources if necessary.\n\n        Returns:\n            The active Polars `DataFrame` or `LazyFrame`.\n        \"\"\"\n        if self._data_frame is not None and not self.is_future:\n            return self._data_frame\n        elif self.is_future:\n            return self._data_frame\n        elif self._external_source is not None and self.lazy:\n            return self._data_frame\n        elif self._external_source is not None and not self.lazy:\n            if self._external_source.get_pl_df() is None:\n                data_frame = list(self._external_source.get_iter())\n                if len(data_frame) &gt; 0:\n                    self.data_frame = pl.DataFrame(data_frame)\n            else:\n                self.data_frame = self._external_source.get_pl_df()\n            self.calculate_schema()\n            return self._data_frame\n\n    @data_frame.setter\n    def data_frame(self, df: pl.LazyFrame | pl.DataFrame):\n        \"\"\"Sets the underlying Polars DataFrame or LazyFrame.\"\"\"\n        if self.lazy and isinstance(df, pl.DataFrame):\n            raise Exception(\"Cannot set a non-lazy dataframe to a lazy flowfile\")\n        self._data_frame = df\n        self._schema = None\n\n    @staticmethod\n    def _create_schema_stats_from_pl_schema(pl_schema: pl.Schema) -&gt; list[dict]:\n        \"\"\"Converts a Polars Schema into a list of schema statistics dictionaries.\"\"\"\n        return [dict(column_name=k, pl_datatype=v, col_index=i) for i, (k, v) in enumerate(pl_schema.items())]\n\n    def _add_schema_from_schema_stats(self, schema_stats: list[dict]):\n        \"\"\"Populates the schema from a list of schema statistics dictionaries.\"\"\"\n        self._schema = convert_stats_to_column_info(schema_stats)\n\n    @property\n    def schema(self) -&gt; list[FlowfileColumn]:\n        \"\"\"The schema of the DataFrame as a list of `FlowfileColumn` objects.\n\n        This property lazily calculates the schema if it hasn't been determined yet.\n\n        Returns:\n            A list of `FlowfileColumn` objects describing the schema.\n        \"\"\"\n        if self.number_of_fields == 0:\n            return []\n        if self._schema is None or (self._calculate_schema_stats and not self.ind_schema_calculated):\n            if self._calculate_schema_stats and not self.ind_schema_calculated:\n                schema_stats = self._calculate_schema()\n                self.ind_schema_calculated = True\n            else:\n                schema_stats = self._create_schema_stats_from_pl_schema(self.data_frame.collect_schema())\n            self._add_schema_from_schema_stats(schema_stats)\n        return self._schema\n\n    @property\n    def number_of_fields(self) -&gt; int:\n        \"\"\"The number of columns (fields) in the DataFrame.\n\n        Returns:\n            The integer count of columns.\n        \"\"\"\n        if self.__number_of_fields is None:\n            self.__number_of_fields = len(self.columns)\n        return self.__number_of_fields\n\n    def collect(self, n_records: int = None) -&gt; pl.DataFrame:\n        \"\"\"Collects the data and returns it as a Polars DataFrame.\n\n        This method triggers the execution of the lazy query plan (if applicable)\n        and returns the result. It supports streaming to optimize memory usage\n        for large datasets.\n\n        Args:\n            n_records: The maximum number of records to collect. If None, all\n                records are collected.\n\n        Returns:\n            A Polars `DataFrame` containing the collected data.\n        \"\"\"\n        if n_records is None:\n            logger.info(f'Fetching all data for Table object \"{id(self)}\". Settings: streaming={self._streamable}')\n        else:\n            logger.info(\n                f'Fetching {n_records} record(s) for Table object \"{id(self)}\". '\n                f\"Settings: streaming={self._streamable}\"\n            )\n\n        if not self.lazy:\n            return self.data_frame\n\n        try:\n            return self._collect_data(n_records)\n        except Exception as e:\n            self.errors = [e]\n            return self._handle_collection_error(n_records)\n\n    def _collect_data(self, n_records: int = None) -&gt; pl.DataFrame:\n        \"\"\"Internal method to handle data collection logic.\"\"\"\n        if n_records is None:\n            self.collect_external()\n            if self._streamable:\n                try:\n                    logger.info(\"Collecting data in streaming mode\")\n                    return self.data_frame.collect(engine=\"streaming\")\n                except PanicException:\n                    self._streamable = False\n\n            logger.info(\"Collecting data in non-streaming mode\")\n            return self.data_frame.collect()\n\n        if self.external_source is not None:\n            return self._collect_from_external_source(n_records)\n\n        if self._streamable:\n            return self.data_frame.head(n_records).collect(engine=\"streaming\")\n        return self.data_frame.head(n_records).collect()\n\n    def _collect_from_external_source(self, n_records: int) -&gt; pl.DataFrame:\n        \"\"\"Handles collection from an external source.\"\"\"\n        if self.external_source.get_pl_df() is not None:\n            all_data = self.external_source.get_pl_df().head(n_records)\n            self.data_frame = all_data\n        else:\n            all_data = self.external_source.get_sample(n_records)\n            self.data_frame = pl.LazyFrame(all_data)\n        return self.data_frame\n\n    def _handle_collection_error(self, n_records: int) -&gt; pl.DataFrame:\n        \"\"\"Handles errors during collection by attempting partial collection.\"\"\"\n        n_records = 100000000 if n_records is None else n_records\n        ok_cols, error_cols = self._identify_valid_columns(n_records)\n\n        if len(ok_cols) &gt; 0:\n            return self._create_partial_dataframe(ok_cols, error_cols, n_records)\n        return self._create_empty_dataframe(n_records)\n\n    def _identify_valid_columns(self, n_records: int) -&gt; tuple[list[str], list[tuple[str, Any]]]:\n        \"\"\"Identifies which columns can be collected successfully.\"\"\"\n        ok_cols = []\n        error_cols = []\n        for c in self.columns:\n            try:\n                _ = self.data_frame.select(c).head(n_records).collect()\n                ok_cols.append(c)\n            except:\n                error_cols.append((c, self.data_frame.schema[c]))\n        return ok_cols, error_cols\n\n    def _create_partial_dataframe(\n        self, ok_cols: list[str], error_cols: list[tuple[str, Any]], n_records: int\n    ) -&gt; pl.DataFrame:\n        \"\"\"Creates a DataFrame with partial data for columns that could be collected.\"\"\"\n        df = self.data_frame.select(ok_cols)\n        df = df.with_columns([pl.lit(None).alias(column_name).cast(data_type) for column_name, data_type in error_cols])\n        return df.select(self.columns).head(n_records).collect()\n\n    def _create_empty_dataframe(self, n_records: int) -&gt; pl.DataFrame:\n        \"\"\"Creates an empty DataFrame with the correct schema.\"\"\"\n        if self.number_of_records &gt; 0:\n            return pl.DataFrame(\n                {\n                    column_name: pl.Series(\n                        name=column_name, values=[None] * min(self.number_of_records, n_records)\n                    ).cast(data_type)\n                    for column_name, data_type in self.data_frame.schema.items()\n                }\n            )\n        return pl.DataFrame(schema=self.data_frame.schema)\n\n    def do_group_by(\n        self, group_by_input: transform_schemas.GroupByInput, calculate_schema_stats: bool = True\n    ) -&gt; FlowDataEngine:\n        \"\"\"Performs a group-by operation on the DataFrame.\n\n        Args:\n            group_by_input: A `GroupByInput` object defining the grouping columns\n                and aggregations.\n            calculate_schema_stats: If True, calculates schema statistics for the\n                resulting DataFrame.\n\n        Returns:\n            A new `FlowDataEngine` instance with the grouped and aggregated data.\n        \"\"\"\n        aggregations = [c for c in group_by_input.agg_cols if c.agg != \"groupby\"]\n        group_columns = [c for c in group_by_input.agg_cols if c.agg == \"groupby\"]\n\n        if len(group_columns) == 0:\n            return FlowDataEngine(\n                self.data_frame.select(ac.agg_func(ac.old_name).alias(ac.new_name) for ac in aggregations),\n                calculate_schema_stats=calculate_schema_stats,\n            )\n\n        df = self.data_frame.rename({c.old_name: c.new_name for c in group_columns})\n        group_by_columns = [n_c.new_name for n_c in group_columns]\n\n        # Handle case where there are no aggregations - just get unique combinations of group columns\n        if len(aggregations) == 0:\n            return FlowDataEngine(\n                df.select(group_by_columns).unique(),\n                calculate_schema_stats=calculate_schema_stats,\n            )\n\n        grouped_df = df.group_by(*group_by_columns)\n        agg_exprs = [ac.agg_func(ac.old_name).alias(ac.new_name) for ac in aggregations]\n        result_df = grouped_df.agg(agg_exprs)\n\n        return FlowDataEngine(\n            result_df,\n            calculate_schema_stats=calculate_schema_stats,\n        )\n\n    def do_sort(self, sorts: list[transform_schemas.SortByInput]) -&gt; FlowDataEngine:\n        \"\"\"Sorts the DataFrame by one or more columns.\n\n        Args:\n            sorts: A list of `SortByInput` objects, each specifying a column\n                and sort direction ('asc' or 'desc').\n\n        Returns:\n            A new `FlowDataEngine` instance with the sorted data.\n        \"\"\"\n        if not sorts:\n            return self\n\n        descending = [s.how == \"desc\" or s.how.lower() == \"descending\" for s in sorts]\n        df = self.data_frame.sort([sort_by.column for sort_by in sorts], descending=descending)\n        return FlowDataEngine(df, number_of_records=self.number_of_records, schema=self.schema)\n\n    def change_column_types(\n        self, transforms: list[transform_schemas.SelectInput], calculate_schema: bool = False\n    ) -&gt; FlowDataEngine:\n        \"\"\"Changes the data type of one or more columns.\n\n        Args:\n            transforms: A list of `SelectInput` objects, where each object specifies\n                the column and its new `polars_type`.\n            calculate_schema: If True, recalculates the schema after the type change.\n\n        Returns:\n            A new `FlowDataEngine` instance with the updated column types.\n        \"\"\"\n        dtypes = [dtype.base_type() for dtype in self.data_frame.collect_schema().dtypes()]\n        idx_mapping = list(\n            (transform.old_name, self.cols_idx.get(transform.old_name), get_polars_type(transform.polars_type))\n            for transform in transforms\n            if transform.data_type is not None\n        )\n\n        actual_transforms = [c for c in idx_mapping if c[2] != dtypes[c[1]]]\n        transformations = [\n            utils.define_pl_col_transformation(col_name=transform[0], col_type=transform[2])\n            for transform in actual_transforms\n        ]\n\n        df = self.data_frame.with_columns(transformations)\n        return FlowDataEngine(\n            df,\n            number_of_records=self.number_of_records,\n            calculate_schema_stats=calculate_schema,\n            streamable=self._streamable,\n        )\n\n    def save(self, path: str, data_type: str = \"parquet\") -&gt; Future:\n        \"\"\"Saves the DataFrame to a file in a separate thread.\n\n        Args:\n            path: The file path to save to.\n            data_type: The format to save in (e.g., 'parquet', 'csv').\n\n        Returns:\n            A `loky.Future` object representing the asynchronous save operation.\n        \"\"\"\n        estimated_size = deepcopy(self.get_estimated_file_size() * 4)\n        df = deepcopy(self.data_frame)\n        return write_threaded(_df=df, path=path, data_type=data_type, estimated_size=estimated_size)\n\n    def to_pylist(self) -&gt; list[dict]:\n        \"\"\"Converts the DataFrame to a list of Python dictionaries.\n\n        Returns:\n            A list where each item is a dictionary representing a row.\n        \"\"\"\n        if self.lazy:\n            return self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\").to_dicts()\n        return self.data_frame.to_dicts()\n\n    def to_arrow(self) -&gt; PaTable:\n        \"\"\"Converts the DataFrame to a PyArrow Table.\n\n        This method triggers a `.collect()` call if the data is lazy,\n        then converts the resulting eager DataFrame into a `pyarrow.Table`.\n\n        Returns:\n            A `pyarrow.Table` instance representing the data.\n        \"\"\"\n        if self.lazy:\n            return self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\").to_arrow()\n        else:\n            return self.data_frame.to_arrow()\n\n    def to_raw_data(self) -&gt; input_schema.RawData:\n        \"\"\"Converts the DataFrame to a `RawData` schema object.\n\n        Returns:\n            An `input_schema.RawData` object containing the schema and data.\n        \"\"\"\n        columns = [c.get_minimal_field_info() for c in self.schema]\n        data = list(self.to_dict().values())\n        return input_schema.RawData(columns=columns, data=data)\n\n    def to_dict(self) -&gt; dict[str, list]:\n        \"\"\"Converts the DataFrame to a Python dictionary of columns.\n\n        Each key in the dictionary is a column name, and the corresponding value\n        is a list of the data in that column.\n\n        Returns:\n            A dictionary mapping column names to lists of their values.\n        \"\"\"\n        if self.lazy:\n            return self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\").to_dict(as_series=False)\n        else:\n            return self.data_frame.to_dict(as_series=False)\n\n    @classmethod\n    def create_from_external_source(cls, external_source: ExternalDataSource) -&gt; FlowDataEngine:\n        \"\"\"Creates a FlowDataEngine from an external data source.\n\n        Args:\n            external_source: An object that conforms to the `ExternalDataSource`\n                interface.\n\n        Returns:\n            A new `FlowDataEngine` instance.\n        \"\"\"\n        if external_source.schema is not None:\n            ff = cls.create_from_schema(external_source.schema)\n        elif external_source.initial_data_getter is not None:\n            ff = cls(raw_data=external_source.initial_data_getter())\n        else:\n            ff = cls()\n        ff._external_source = external_source\n        return ff\n\n    @classmethod\n    def create_from_sql(cls, sql: str, conn: Any) -&gt; FlowDataEngine:\n        \"\"\"Creates a FlowDataEngine by executing a SQL query.\n\n        Args:\n            sql: The SQL query string to execute.\n            conn: A database connection object or connection URI string.\n\n        Returns:\n            A new `FlowDataEngine` instance with the query result.\n        \"\"\"\n        return cls(pl.read_sql(sql, conn))\n\n    @classmethod\n    def create_from_schema(cls, schema: list[FlowfileColumn]) -&gt; FlowDataEngine:\n        \"\"\"Creates an empty FlowDataEngine from a schema definition.\n\n        Args:\n            schema: A list of `FlowfileColumn` objects defining the schema.\n\n        Returns:\n            A new, empty `FlowDataEngine` instance with the specified schema.\n        \"\"\"\n        pl_schema = []\n        for i, flow_file_column in enumerate(schema):\n            pl_schema.append((flow_file_column.name, cast_str_to_polars_type(flow_file_column.data_type)))\n            schema[i].col_index = i\n        df = pl.LazyFrame(schema=pl_schema)\n        return cls(df, schema=schema, calculate_schema_stats=False, number_of_records=0)\n\n    @classmethod\n    def create_from_path(cls, received_table: input_schema.ReceivedTable) -&gt; FlowDataEngine:\n        \"\"\"Creates a FlowDataEngine from a local file path.\n\n        Supports various file types like CSV, Parquet, and Excel.\n\n        Args:\n            received_table: A `ReceivedTableBase` object containing the file path\n                and format details.\n\n        Returns:\n            A new `FlowDataEngine` instance with data from the file.\n        \"\"\"\n        received_table.set_absolute_filepath()\n        file_type_handlers = {\n            \"csv\": create_funcs.create_from_path_csv,\n            \"parquet\": create_funcs.create_from_path_parquet,\n            \"excel\": create_funcs.create_from_path_excel,\n        }\n\n        handler = file_type_handlers.get(received_table.file_type)\n        if not handler:\n            raise Exception(f\"Cannot create from {received_table.file_type}\")\n\n        flow_file = cls(handler(received_table))\n        flow_file._org_path = received_table.abs_file_path\n        return flow_file\n\n    @classmethod\n    def create_random(cls, number_of_records: int = 1000) -&gt; FlowDataEngine:\n        \"\"\"Creates a FlowDataEngine with randomly generated data.\n\n        Useful for testing and examples.\n\n        Args:\n            number_of_records: The number of random records to generate.\n\n        Returns:\n            A new `FlowDataEngine` instance with fake data.\n        \"\"\"\n        return cls(create_fake_data(number_of_records))\n\n    @classmethod\n    def generate_enumerator(cls, length: int = 1000, output_name: str = \"output_column\") -&gt; FlowDataEngine:\n        \"\"\"Generates a FlowDataEngine with a single column containing a sequence of integers.\n\n        Args:\n            length: The number of integers to generate in the sequence.\n            output_name: The name of the output column.\n\n        Returns:\n            A new `FlowDataEngine` instance.\n        \"\"\"\n        if length &gt; 10_000_000:\n            length = 10_000_000\n        return cls(pl.LazyFrame().select((pl.int_range(0, length, dtype=pl.UInt32)).alias(output_name)))\n\n    def _handle_schema(\n        self, schema: list[FlowfileColumn] | list[str] | pl.Schema | None, pl_schema: pl.Schema\n    ) -&gt; list[FlowfileColumn] | None:\n        \"\"\"Handles schema processing and validation during initialization.\"\"\"\n        if schema is None and pl_schema is not None:\n            return convert_stats_to_column_info(self._create_schema_stats_from_pl_schema(pl_schema))\n        elif schema is None and pl_schema is None:\n            return None\n        elif assert_if_flowfile_schema(schema) and pl_schema is None:\n            return schema\n        elif pl_schema is not None and schema is not None:\n            if schema.__len__() != pl_schema.__len__():\n                raise Exception(\n                    f\"Schema does not match the data got {schema.__len__()} columns expected {pl_schema.__len__()}\"\n                )\n            if isinstance(schema, pl.Schema):\n                return self._handle_polars_schema(schema, pl_schema)\n            elif isinstance(schema, list) and len(schema) == 0:\n                return []\n            elif isinstance(schema[0], str):\n                return self._handle_string_schema(schema, pl_schema)\n            return schema\n\n    def _handle_polars_schema(self, schema: pl.Schema, pl_schema: pl.Schema) -&gt; list[FlowfileColumn]:\n        \"\"\"Handles Polars schema conversion.\"\"\"\n        flow_file_columns = [\n            FlowfileColumn.create_from_polars_dtype(column_name=col_name, data_type=dtype)\n            for col_name, dtype in zip(schema.names(), schema.dtypes(), strict=False)\n        ]\n\n        select_arg = [\n            pl.col(o).alias(n).cast(schema_dtype)\n            for o, n, schema_dtype in zip(pl_schema.names(), schema.names(), schema.dtypes(), strict=False)\n        ]\n\n        self.data_frame = self.data_frame.select(select_arg)\n        return flow_file_columns\n\n    def _handle_string_schema(self, schema: list[str], pl_schema: pl.Schema) -&gt; list[FlowfileColumn]:\n        \"\"\"Handles string-based schema conversion.\"\"\"\n        flow_file_columns = [\n            FlowfileColumn.create_from_polars_dtype(column_name=col_name, data_type=dtype)\n            for col_name, dtype in zip(schema, pl_schema.dtypes(), strict=False)\n        ]\n\n        self.data_frame = self.data_frame.rename({o: n for o, n in zip(pl_schema.names(), schema, strict=False)})\n\n        return flow_file_columns\n\n    def split(self, split_input: transform_schemas.TextToRowsInput) -&gt; FlowDataEngine:\n        \"\"\"Splits a column's text values into multiple rows based on a delimiter.\n\n        This operation is often referred to as \"exploding\" the DataFrame, as it\n        increases the number of rows.\n\n        Args:\n            split_input: A `TextToRowsInput` object specifying the column to split,\n                the delimiter, and the output column name.\n\n        Returns:\n            A new `FlowDataEngine` instance with the exploded rows.\n        \"\"\"\n        output_column_name = (\n            split_input.output_column_name if split_input.output_column_name else split_input.column_to_split\n        )\n\n        split_value = (\n            split_input.split_fixed_value if split_input.split_by_fixed_value else pl.col(split_input.split_by_column)\n        )\n\n        df = self.data_frame.with_columns(\n            pl.col(split_input.column_to_split).str.split(by=split_value).alias(output_column_name)\n        ).explode(output_column_name)\n\n        return FlowDataEngine(df)\n\n    def unpivot(self, unpivot_input: transform_schemas.UnpivotInput) -&gt; FlowDataEngine:\n        \"\"\"Converts the DataFrame from a wide to a long format.\n\n        This is the inverse of a pivot operation, taking columns and transforming\n        them into `variable` and `value` rows.\n\n        Args:\n            unpivot_input: An `UnpivotInput` object specifying which columns to\n                unpivot and which to keep as index columns.\n\n        Returns:\n            A new, unpivoted `FlowDataEngine` instance.\n        \"\"\"\n        lf = self.data_frame\n\n        if unpivot_input.data_type_selector_expr is not None:\n            result = lf.unpivot(on=unpivot_input.data_type_selector_expr(), index=unpivot_input.index_columns)\n        elif unpivot_input.value_columns is not None:\n            result = lf.unpivot(on=unpivot_input.value_columns, index=unpivot_input.index_columns)\n        else:\n            result = lf.unpivot()\n\n        return FlowDataEngine(result)\n\n    def do_pivot(self, pivot_input: transform_schemas.PivotInput, node_logger: NodeLogger = None) -&gt; FlowDataEngine:\n        \"\"\"Converts the DataFrame from a long to a wide format, aggregating values.\n\n        Args:\n            pivot_input: A `PivotInput` object defining the index, pivot, and value\n                columns, along with the aggregation logic.\n            node_logger: An optional logger for reporting warnings, e.g., if the\n                pivot column has too many unique values.\n\n        Returns:\n            A new, pivoted `FlowDataEngine` instance.\n        \"\"\"\n        # Get unique values for pivot columns\n        max_unique_vals = 200\n        new_cols_unique = fetch_unique_values(\n            self.data_frame.select(pivot_input.pivot_column)\n            .unique()\n            .sort(pivot_input.pivot_column)\n            .limit(max_unique_vals)\n            .cast(pl.String)\n        )\n        if len(new_cols_unique) &gt;= max_unique_vals:\n            if node_logger:\n                node_logger.warning(\n                    \"Pivot column has too many unique values. Please consider using a different column.\"\n                    f\" Max unique values: {max_unique_vals}\"\n                )\n\n        if len(pivot_input.index_columns) == 0:\n            no_index_cols = True\n            pivot_input.index_columns = [\"__temp__\"]\n            ff = self.apply_flowfile_formula(\"1\", col_name=\"__temp__\")\n        else:\n            no_index_cols = False\n            ff = self\n\n        # Perform pivot operations\n        index_columns = pivot_input.get_index_columns()\n        grouped_ff = ff.do_group_by(pivot_input.get_group_by_input(), False)\n        pivot_column = pivot_input.get_pivot_column()\n\n        input_df = grouped_ff.data_frame.with_columns(pivot_column.cast(pl.String).alias(pivot_input.pivot_column))\n        number_of_aggregations = len(pivot_input.aggregations)\n        df = (\n            input_df.select(*index_columns, pivot_column, pivot_input.get_values_expr())\n            .group_by(*index_columns)\n            .agg(\n                [\n                    (pl.col(\"vals\").filter(pivot_column == new_col_value)).first().alias(new_col_value)\n                    for new_col_value in new_cols_unique\n                ]\n            )\n            .select(\n                *index_columns,\n                *[\n                    pl.col(new_col)\n                    .struct.field(agg)\n                    .alias(f'{new_col + \"_\" + agg if number_of_aggregations &gt; 1 else new_col }')\n                    for new_col in new_cols_unique\n                    for agg in pivot_input.aggregations\n                ],\n            )\n        )\n\n        # Clean up temporary columns if needed\n        if no_index_cols:\n            df = df.drop(\"__temp__\")\n            pivot_input.index_columns = []\n\n        return FlowDataEngine(df, calculate_schema_stats=False)\n\n    def do_filter(self, predicate: str) -&gt; FlowDataEngine:\n        \"\"\"Filters rows based on a predicate expression.\n\n        Args:\n            predicate: A string containing a Polars expression that evaluates to\n                a boolean value.\n\n        Returns:\n            A new `FlowDataEngine` instance containing only the rows that match\n            the predicate.\n        \"\"\"\n        try:\n            f = to_expr(predicate)\n        except Exception as e:\n            logger.warning(f\"Error in filter expression: {e}\")\n            f = to_expr(\"False\")\n        df = self.data_frame.filter(f)\n        return FlowDataEngine(df, schema=self.schema, streamable=self._streamable)\n\n    def add_record_id(self, record_id_settings: transform_schemas.RecordIdInput) -&gt; FlowDataEngine:\n        \"\"\"Adds a record ID (row number) column to the DataFrame.\n\n        Can generate a simple sequential ID or a grouped ID that resets for\n        each group.\n\n        Args:\n            record_id_settings: A `RecordIdInput` object specifying the output\n                column name, offset, and optional grouping columns.\n\n        Returns:\n            A new `FlowDataEngine` instance with the added record ID column.\n        \"\"\"\n        if record_id_settings.group_by and len(record_id_settings.group_by_columns) &gt; 0:\n            return self._add_grouped_record_id(record_id_settings)\n        return self._add_simple_record_id(record_id_settings)\n\n    def _add_grouped_record_id(self, record_id_settings: transform_schemas.RecordIdInput) -&gt; FlowDataEngine:\n        \"\"\"Adds a record ID column with grouping.\"\"\"\n        select_cols = [pl.col(record_id_settings.output_column_name)] + [pl.col(c) for c in self.columns]\n\n        df = (\n            self.data_frame.with_columns(pl.lit(1).alias(record_id_settings.output_column_name))\n            .with_columns(\n                (\n                    pl.cum_count(record_id_settings.output_column_name).over(record_id_settings.group_by_columns)\n                    + record_id_settings.offset\n                    - 1\n                ).alias(record_id_settings.output_column_name)\n            )\n            .select(select_cols)\n        )\n\n        output_schema = [FlowfileColumn.from_input(record_id_settings.output_column_name, \"UInt64\")]\n        output_schema.extend(self.schema)\n\n        return FlowDataEngine(df, schema=output_schema)\n\n    def _add_simple_record_id(self, record_id_settings: transform_schemas.RecordIdInput) -&gt; FlowDataEngine:\n        \"\"\"Adds a simple sequential record ID column.\"\"\"\n        df = self.data_frame.with_row_index(record_id_settings.output_column_name, record_id_settings.offset)\n\n        output_schema = [FlowfileColumn.from_input(record_id_settings.output_column_name, \"UInt64\")]\n        output_schema.extend(self.schema)\n\n        return FlowDataEngine(df, schema=output_schema)\n\n    def get_schema_column(self, col_name: str) -&gt; FlowfileColumn:\n        \"\"\"Retrieves the schema information for a single column by its name.\n\n        Args:\n            col_name: The name of the column to retrieve.\n\n        Returns:\n            A `FlowfileColumn` object for the specified column, or `None` if not found.\n        \"\"\"\n        for s in self.schema:\n            if s.name == col_name:\n                return s\n\n    def get_estimated_file_size(self) -&gt; int:\n        \"\"\"Estimates the file size in bytes if the data originated from a local file.\n\n        This relies on the original path being tracked during file ingestion.\n\n        Returns:\n            The file size in bytes, or 0 if the original path is unknown.\n        \"\"\"\n        if self._org_path is not None:\n            return os.path.getsize(self._org_path)\n        return 0\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Returns a string representation of the FlowDataEngine.\"\"\"\n        return f\"flow data engine\\n{self.data_frame.__repr__()}\"\n\n    def __call__(self) -&gt; FlowDataEngine:\n        \"\"\"Makes the class instance callable, returning itself.\"\"\"\n        return self\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of records in the table.\"\"\"\n        return self.number_of_records if self.number_of_records &gt;= 0 else self.get_number_of_records()\n\n    def cache(self) -&gt; FlowDataEngine:\n        \"\"\"Caches the current DataFrame to disk and updates the internal reference.\n\n        This triggers a background process to write the current LazyFrame's result\n        to a temporary file. Subsequent operations on this `FlowDataEngine` instance\n        will read from the cached file, which can speed up downstream computations.\n\n        Returns:\n            The same `FlowDataEngine` instance, now backed by the cached data.\n        \"\"\"\n        edf = ExternalDfFetcher(\n            lf=self.data_frame, file_ref=str(id(self)), wait_on_completion=False, flow_id=-1, node_id=-1\n        )\n        logger.info(\"Caching data in background\")\n        result = edf.get_result()\n        if isinstance(result, pl.LazyFrame):\n            logger.info(\"Data cached\")\n            del self._data_frame\n            self.data_frame = result\n            logger.info(\"Data loaded from cache\")\n        return self\n\n    def collect_external(self):\n        \"\"\"Materializes data from a tracked external source.\n\n        If the `FlowDataEngine` was created from an `ExternalDataSource`, this\n        method will trigger the data retrieval, update the internal `_data_frame`\n        to a `LazyFrame` of the collected data, and reset the schema to be\n        re-evaluated.\n        \"\"\"\n        if self._external_source is not None:\n            logger.info(\"Collecting external source\")\n            if self.external_source.get_pl_df() is not None:\n                self.data_frame = self.external_source.get_pl_df().lazy()\n            else:\n                self.data_frame = pl.LazyFrame(list(self.external_source.get_iter()))\n            self._schema = None  # enforce reset schema\n\n    def get_output_sample(self, n_rows: int = 10) -&gt; list[dict]:\n        \"\"\"Gets a sample of the data as a list of dictionaries.\n\n        This is typically used to display a preview of the data in a UI.\n\n        Args:\n            n_rows: The number of rows to sample.\n\n        Returns:\n            A list of dictionaries, where each dictionary represents a row.\n        \"\"\"\n        if self.number_of_records &gt; n_rows or self.number_of_records &lt; 0:\n            df = self.collect(n_rows)\n        else:\n            df = self.collect()\n        return df.to_dicts()\n\n    def __get_sample__(self, n_rows: int = 100, streamable: bool = True) -&gt; FlowDataEngine:\n        \"\"\"Internal method to get a sample of the data.\"\"\"\n        if not self.lazy:\n            df = self.data_frame.lazy()\n        else:\n            df = self.data_frame\n\n        if streamable:\n            try:\n                df = df.head(n_rows).collect()\n            except Exception as e:\n                logger.warning(f\"Error in getting sample: {e}\")\n                df = df.head(n_rows).collect(engine=\"auto\")\n        else:\n            df = self.collect()\n        return FlowDataEngine(df, number_of_records=len(df), schema=self.schema)\n\n    def get_sample(\n        self,\n        n_rows: int = 100,\n        random: bool = False,\n        shuffle: bool = False,\n        seed: int = None,\n        execution_location: ExecutionLocationsLiteral | None = None,\n    ) -&gt; FlowDataEngine:\n        \"\"\"Gets a sample of rows from the DataFrame.\n\n        Args:\n            n_rows: The number of rows to sample.\n            random: If True, performs random sampling. If False, takes the first n_rows.\n            shuffle: If True (and `random` is True), shuffles the data before sampling.\n            seed: A random seed for reproducibility.\n            execution_location: Location which is used to calculate the size of the dataframe\n        Returns:\n            A new `FlowDataEngine` instance containing the sampled data.\n        \"\"\"\n        logging.info(f\"Getting sample of {n_rows} rows\")\n        if random:\n            if self.lazy and self.external_source is not None:\n                self.collect_external()\n\n            if self.lazy and shuffle:\n                sample_df = self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\").sample(\n                    n_rows, seed=seed, shuffle=shuffle\n                )\n            elif shuffle:\n                sample_df = self.data_frame.sample(n_rows, seed=seed, shuffle=shuffle)\n            else:\n                if execution_location is None:\n                    execution_location = get_global_execution_location()\n                n_rows = min(\n                    n_rows, self.get_number_of_records(calculate_in_worker_process=execution_location == \"remote\")\n                )\n\n                every_n_records = ceil(self.number_of_records / n_rows)\n                sample_df = self.data_frame.gather_every(every_n_records)\n        else:\n            if self.external_source:\n                self.collect(n_rows)\n            sample_df = self.data_frame.head(n_rows)\n\n        return FlowDataEngine(sample_df, schema=self.schema)\n\n    def get_subset(self, n_rows: int = 100) -&gt; FlowDataEngine:\n        \"\"\"Gets the first `n_rows` from the DataFrame.\n\n        Args:\n            n_rows: The number of rows to include in the subset.\n\n        Returns:\n            A new `FlowDataEngine` instance containing the subset of data.\n        \"\"\"\n        if not self.lazy:\n            return FlowDataEngine(self.data_frame.head(n_rows), calculate_schema_stats=True)\n        else:\n            return FlowDataEngine(self.data_frame.head(n_rows), calculate_schema_stats=True)\n\n    def iter_batches(\n        self, batch_size: int = 1000, columns: list | tuple | str = None\n    ) -&gt; Generator[FlowDataEngine, None, None]:\n        \"\"\"Iterates over the DataFrame in batches.\n\n        Args:\n            batch_size: The size of each batch.\n            columns: A list of column names to include in the batches. If None,\n                all columns are included.\n\n        Yields:\n            A `FlowDataEngine` instance for each batch.\n        \"\"\"\n        if columns:\n            self.data_frame = self.data_frame.select(columns)\n        self.lazy = False\n        batches = self.data_frame.iter_slices(batch_size)\n        for batch in batches:\n            yield FlowDataEngine(batch)\n\n    def start_fuzzy_join(\n        self,\n        fuzzy_match_input: transform_schemas.FuzzyMatchInput,\n        other: FlowDataEngine,\n        file_ref: str,\n        flow_id: int = -1,\n        node_id: int | str = -1,\n    ) -&gt; ExternalFuzzyMatchFetcher:\n        \"\"\"Starts a fuzzy join operation in a background process.\n\n        This method prepares the data and initiates the fuzzy matching in a\n        separate process, returning a tracker object immediately.\n\n        Args:\n            fuzzy_match_input: A `FuzzyMatchInput` object with the matching parameters.\n            other: The right `FlowDataEngine` to join with.\n            file_ref: A reference string for temporary files.\n            flow_id: The flow ID for tracking.\n            node_id: The node ID for tracking.\n\n        Returns:\n            An `ExternalFuzzyMatchFetcher` object that can be used to track the\n            progress and retrieve the result of the fuzzy join.\n        \"\"\"\n        fuzzy_match_input_manager = transform_schemas.FuzzyMatchInputManager(fuzzy_match_input)\n        left_df, right_df = prepare_for_fuzzy_match(\n            left=self, right=other, fuzzy_match_input_manager=fuzzy_match_input_manager\n        )\n\n        return ExternalFuzzyMatchFetcher(\n            left_df,\n            right_df,\n            fuzzy_maps=fuzzy_match_input_manager.fuzzy_maps,\n            file_ref=file_ref + \"_fm\",\n            wait_on_completion=False,\n            flow_id=flow_id,\n            node_id=node_id,\n        )\n\n    def fuzzy_join_external(\n        self,\n        fuzzy_match_input: transform_schemas.FuzzyMatchInput,\n        other: FlowDataEngine,\n        file_ref: str = None,\n        flow_id: int = -1,\n        node_id: int = -1,\n    ):\n        if file_ref is None:\n            file_ref = str(id(self)) + \"_\" + str(id(other))\n        fuzzy_match_input_manager = transform_schemas.FuzzyMatchInputManager(fuzzy_match_input)\n\n        left_df, right_df = prepare_for_fuzzy_match(\n            left=self, right=other, fuzzy_match_input_manager=fuzzy_match_input_manager\n        )\n        external_tracker = ExternalFuzzyMatchFetcher(\n            left_df,\n            right_df,\n            fuzzy_maps=fuzzy_match_input_manager.fuzzy_maps,\n            file_ref=file_ref + \"_fm\",\n            wait_on_completion=False,\n            flow_id=flow_id,\n            node_id=node_id,\n        )\n        return FlowDataEngine(external_tracker.get_result())\n\n    def fuzzy_join(\n        self,\n        fuzzy_match_input: transform_schemas.FuzzyMatchInput,\n        other: FlowDataEngine,\n        node_logger: NodeLogger = None,\n    ) -&gt; FlowDataEngine:\n        fuzzy_match_input_manager = transform_schemas.FuzzyMatchInputManager(fuzzy_match_input)\n        left_df, right_df = prepare_for_fuzzy_match(\n            left=self, right=other, fuzzy_match_input_manager=fuzzy_match_input_manager\n        )\n        fuzzy_mappings = [FuzzyMapping(**fm.__dict__) for fm in fuzzy_match_input_manager.fuzzy_maps]\n        return FlowDataEngine(\n            fuzzy_match_dfs(\n                left_df, right_df, fuzzy_maps=fuzzy_mappings, logger=node_logger.logger if node_logger else logger\n            ).lazy()\n        )\n\n    def do_cross_join(\n        self,\n        cross_join_input: transform_schemas.CrossJoinInput,\n        auto_generate_selection: bool,\n        verify_integrity: bool,\n        other: FlowDataEngine,\n    ) -&gt; FlowDataEngine:\n        \"\"\"Performs a cross join with another DataFrame.\n\n        A cross join produces the Cartesian product of the two DataFrames.\n\n        Args:\n            cross_join_input: A `CrossJoinInput` object specifying column selections.\n            auto_generate_selection: If True, automatically renames columns to avoid conflicts.\n            verify_integrity: If True, checks if the resulting join would be too large.\n            other: The right `FlowDataEngine` to join with.\n\n        Returns:\n            A new `FlowDataEngine` with the result of the cross join.\n\n        Raises:\n            Exception: If `verify_integrity` is True and the join would result in\n                an excessively large number of records.\n        \"\"\"\n        self.lazy = True\n        other.lazy = True\n        cross_join_input_manager = transform_schemas.CrossJoinInputManager(cross_join_input)\n        verify_join_select_integrity(\n            cross_join_input_manager.input, left_columns=self.columns, right_columns=other.columns\n        )\n        right_select = [\n            v.old_name\n            for v in cross_join_input_manager.right_select.renames\n            if (v.keep or v.join_key) and v.is_available\n        ]\n        left_select = [\n            v.old_name\n            for v in cross_join_input_manager.left_select.renames\n            if (v.keep or v.join_key) and v.is_available\n        ]\n        cross_join_input_manager.auto_rename(rename_mode=\"suffix\")\n        left = self.data_frame.select(left_select).rename(cross_join_input_manager.left_select.rename_table)\n        right = other.data_frame.select(right_select).rename(cross_join_input_manager.right_select.rename_table)\n\n        joined_df = left.join(right, how=\"cross\")\n\n        cols_to_delete_after = [\n            col.new_name\n            for col in cross_join_input_manager.left_select.renames + cross_join_input_manager.right_select.renames\n            if col.join_key and not col.keep and col.is_available\n        ]\n\n        fl = FlowDataEngine(joined_df.drop(cols_to_delete_after), calculate_schema_stats=False, streamable=False)\n        return fl\n\n    def join(\n        self,\n        join_input: transform_schemas.JoinInput,\n        auto_generate_selection: bool,\n        verify_integrity: bool,\n        other: FlowDataEngine,\n    ) -&gt; FlowDataEngine:\n        \"\"\"Performs a standard SQL-style join with another DataFrame.\"\"\"\n        # Create manager from input\n        join_manager = transform_schemas.JoinInputManager(join_input)\n        ensure_right_unselect_for_semi_and_anti_joins(join_manager.input)\n        for jk in join_manager.join_mapping:\n            if jk.left_col not in {c.old_name for c in join_manager.left_select.renames}:\n                join_manager.left_select.append(transform_schemas.SelectInput(jk.left_col, keep=False))\n            if jk.right_col not in {c.old_name for c in join_manager.right_select.renames}:\n                join_manager.right_select.append(transform_schemas.SelectInput(jk.right_col, keep=False))\n        verify_join_select_integrity(join_manager.input, left_columns=self.columns, right_columns=other.columns)\n        if not verify_join_map_integrity(join_manager.input, left_columns=self.schema, right_columns=other.schema):\n            raise Exception(\"Join is not valid by the data fields\")\n\n        if auto_generate_selection:\n            join_manager.auto_rename()\n\n        # Use manager properties throughout\n        left = self.data_frame.select(join_manager.left_manager.get_select_cols()).rename(\n            join_manager.left_manager.get_rename_table()\n        )\n        right = other.data_frame.select(join_manager.right_manager.get_select_cols()).rename(\n            join_manager.right_manager.get_rename_table()\n        )\n\n        left, right, reverse_join_key_mapping = _handle_duplication_join_keys(left, right, join_manager)\n        left, right = rename_df_table_for_join(left, right, join_manager.get_join_key_renames())\n        if join_manager.how == \"right\":\n            joined_df = right.join(\n                other=left,\n                left_on=join_manager.right_join_keys,\n                right_on=join_manager.left_join_keys,\n                how=\"left\",\n                suffix=\"\",\n            ).rename(reverse_join_key_mapping)\n        else:\n            joined_df = left.join(\n                other=right,\n                left_on=join_manager.left_join_keys,\n                right_on=join_manager.right_join_keys,\n                how=join_manager.how,\n                suffix=\"\",\n            ).rename(reverse_join_key_mapping)\n\n        left_cols_to_delete_after = [\n            get_col_name_to_delete(col, \"left\")\n            for col in join_manager.input.left_select.renames\n            if not col.keep and col.is_available and col.join_key\n        ]\n\n        right_cols_to_delete_after = [\n            get_col_name_to_delete(col, \"right\")\n            for col in join_manager.input.right_select.renames\n            if not col.keep\n            and col.is_available\n            and col.join_key\n            and join_manager.how in (\"left\", \"right\", \"inner\", \"cross\", \"outer\")\n        ]\n\n        if len(right_cols_to_delete_after + left_cols_to_delete_after) &gt; 0:\n            joined_df = joined_df.drop(left_cols_to_delete_after + right_cols_to_delete_after)\n\n        undo_join_key_remapping = get_undo_rename_mapping_join(join_manager)\n        joined_df = joined_df.rename(undo_join_key_remapping)\n\n        return FlowDataEngine(joined_df, calculate_schema_stats=False, number_of_records=0, streamable=False)\n\n    def solve_graph(self, graph_solver_input: transform_schemas.GraphSolverInput) -&gt; FlowDataEngine:\n        \"\"\"Solves a graph problem represented by 'from' and 'to' columns.\n\n        This is used for operations like finding connected components in a graph.\n\n        Args:\n            graph_solver_input: A `GraphSolverInput` object defining the source,\n                destination, and output column names.\n\n        Returns:\n            A new `FlowDataEngine` instance with the solved graph data.\n        \"\"\"\n        lf = self.data_frame.with_columns(\n            graph_solver(graph_solver_input.col_from, graph_solver_input.col_to).alias(\n                graph_solver_input.output_column_name\n            )\n        )\n        return FlowDataEngine(lf)\n\n    def add_new_values(self, values: Iterable, col_name: str = None) -&gt; FlowDataEngine:\n        \"\"\"Adds a new column with the provided values.\n\n        Args:\n            values: An iterable (e.g., list, tuple) of values to add as a new column.\n            col_name: The name for the new column. Defaults to 'new_values'.\n\n        Returns:\n            A new `FlowDataEngine` instance with the added column.\n        \"\"\"\n        if col_name is None:\n            col_name = \"new_values\"\n        return FlowDataEngine(self.data_frame.with_columns(pl.Series(values).alias(col_name)))\n\n    def get_record_count(self) -&gt; FlowDataEngine:\n        \"\"\"Returns a new FlowDataEngine with a single column 'number_of_records'\n        containing the total number of records.\n\n        Returns:\n            A new `FlowDataEngine` instance.\n        \"\"\"\n        return FlowDataEngine(self.data_frame.select(pl.len().alias(\"number_of_records\")))\n\n    def assert_equal(self, other: FlowDataEngine, ordered: bool = True, strict_schema: bool = False):\n        \"\"\"Asserts that this DataFrame is equal to another.\n\n        Useful for testing.\n\n        Args:\n            other: The other `FlowDataEngine` to compare with.\n            ordered: If True, the row order must be identical.\n            strict_schema: If True, the data types of the schemas must be identical.\n\n        Raises:\n            Exception: If the DataFrames are not equal based on the specified criteria.\n        \"\"\"\n        org_laziness = self.lazy, other.lazy\n        self.lazy = False\n        other.lazy = False\n        self.number_of_records = -1\n        other.number_of_records = -1\n        other = other.select_columns(self.columns)\n\n        if self.get_number_of_records_in_process() != other.get_number_of_records_in_process():\n            raise Exception(\"Number of records is not equal\")\n\n        if self.columns != other.columns:\n            raise Exception(\"Schema is not equal\")\n\n        if strict_schema:\n            assert self.data_frame.schema == other.data_frame.schema, \"Data types do not match\"\n\n        if ordered:\n            self_lf = self.data_frame.sort(by=self.columns)\n            other_lf = other.data_frame.sort(by=other.columns)\n        else:\n            self_lf = self.data_frame\n            other_lf = other.data_frame\n\n        self.lazy, other.lazy = org_laziness\n        assert self_lf.equals(other_lf), \"Data is not equal\"\n\n    def initialize_empty_fl(self):\n        \"\"\"Initializes an empty LazyFrame.\"\"\"\n        self.data_frame = pl.LazyFrame()\n        self.number_of_records = 0\n        self._lazy = True\n\n    def _calculate_number_of_records_in_worker(self) -&gt; int:\n        \"\"\"Calculates the number of records in a worker process.\"\"\"\n        number_of_records = ExternalDfFetcher(\n            lf=self.data_frame,\n            operation_type=\"calculate_number_of_records\",\n            flow_id=-1,\n            node_id=-1,\n            wait_on_completion=True,\n        ).result\n        return number_of_records\n\n    def get_number_of_records_in_process(self, force_calculate: bool = False):\n        \"\"\"\n        Get the number of records in the DataFrame in the local process.\n\n        args:\n            force_calculate: If True, forces recalculation even if a value is cached.\n\n        Returns:\n            The total number of records.\n        \"\"\"\n        return self.get_number_of_records(force_calculate=force_calculate)\n\n    def get_number_of_records(\n        self, warn: bool = False, force_calculate: bool = False, calculate_in_worker_process: bool = False\n    ) -&gt; int:\n        \"\"\"Gets the total number of records in the DataFrame.\n\n        For lazy frames, this may trigger a full data scan, which can be expensive.\n\n        Args:\n            warn: If True, logs a warning if a potentially expensive calculation is triggered.\n            force_calculate: If True, forces recalculation even if a value is cached.\n            calculate_in_worker_process: If True, offloads the calculation to a worker process.\n\n        Returns:\n            The total number of records.\n\n        Raises:\n            ValueError: If the number of records could not be determined.\n        \"\"\"\n        if self.is_future and not self.is_collected:\n            return -1\n        if self.number_of_records is None or self.number_of_records &lt; 0 or force_calculate:\n            if self._number_of_records_callback is not None:\n                self._number_of_records_callback(self)\n\n            if self.lazy:\n                if calculate_in_worker_process:\n                    try:\n                        self.number_of_records = self._calculate_number_of_records_in_worker()\n                        return self.number_of_records\n                    except Exception as e:\n                        logger.error(f\"Error: {e}\")\n                if warn:\n                    logger.warning(\"Calculating the number of records this can be expensive on a lazy frame\")\n                try:\n                    self.number_of_records = self.data_frame.select(pl.len()).collect(\n                        engine=\"streaming\" if self._streamable else \"auto\"\n                    )[0, 0]\n                except Exception:\n                    raise ValueError(\"Could not get number of records\")\n            else:\n                self.number_of_records = self.data_frame.__len__()\n        return self.number_of_records\n\n    @property\n    def has_errors(self) -&gt; bool:\n        \"\"\"Checks if there are any errors.\"\"\"\n        return len(self.errors) &gt; 0\n\n    @property\n    def lazy(self) -&gt; bool:\n        \"\"\"Indicates if the DataFrame is in lazy mode.\"\"\"\n        return self._lazy\n\n    @lazy.setter\n    def lazy(self, exec_lazy: bool = False):\n        \"\"\"Sets the laziness of the DataFrame.\n\n        Args:\n            exec_lazy: If True, converts the DataFrame to a LazyFrame. If False,\n                collects the data and converts it to an eager DataFrame.\n        \"\"\"\n        if exec_lazy != self._lazy:\n            if exec_lazy:\n                self.data_frame = self.data_frame.lazy()\n            else:\n                self._lazy = exec_lazy\n                if self.external_source is not None:\n                    df = self.collect()\n                    self.data_frame = df\n                else:\n                    self.data_frame = self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\")\n            self._lazy = exec_lazy\n\n    @property\n    def external_source(self) -&gt; ExternalDataSource:\n        \"\"\"The external data source, if any.\"\"\"\n        return self._external_source\n\n    @property\n    def cols_idx(self) -&gt; dict[str, int]:\n        \"\"\"A dictionary mapping column names to their integer index.\"\"\"\n        if self._col_idx is None:\n            self._col_idx = {c: i for i, c in enumerate(self.columns)}\n        return self._col_idx\n\n    @property\n    def __name__(self) -&gt; str:\n        \"\"\"The name of the table.\"\"\"\n        return self.name\n\n    def get_select_inputs(self) -&gt; transform_schemas.SelectInputs:\n        \"\"\"Gets `SelectInput` specifications for all columns in the current schema.\n\n        Returns:\n            A `SelectInputs` object that can be used to configure selection or\n            transformation operations.\n        \"\"\"\n        return transform_schemas.SelectInputs(\n            [transform_schemas.SelectInput(old_name=c.name, data_type=c.data_type) for c in self.schema]\n        )\n\n    def select_columns(self, list_select: list[str] | tuple[str] | str) -&gt; FlowDataEngine:\n        \"\"\"Selects a subset of columns from the DataFrame.\n\n        Args:\n            list_select: A list, tuple, or single string of column names to select.\n\n        Returns:\n            A new `FlowDataEngine` instance containing only the selected columns.\n        \"\"\"\n        if isinstance(list_select, str):\n            list_select = [list_select]\n\n        idx_to_keep = [self.cols_idx.get(c) for c in list_select]\n        selects = [ls for ls, id_to_keep in zip(list_select, idx_to_keep, strict=False) if id_to_keep is not None]\n        new_schema = [self.schema[i] for i in idx_to_keep if i is not None]\n\n        return FlowDataEngine(\n            self.data_frame.select(selects),\n            number_of_records=self.number_of_records,\n            schema=new_schema,\n            streamable=self._streamable,\n        )\n\n    def drop_columns(self, columns: list[str]) -&gt; FlowDataEngine:\n        \"\"\"Drops specified columns from the DataFrame.\n\n        Args:\n            columns: A list of column names to drop.\n\n        Returns:\n            A new `FlowDataEngine` instance without the dropped columns.\n        \"\"\"\n        cols_for_select = tuple(set(self.columns) - set(columns))\n        idx_to_keep = [self.cols_idx.get(c) for c in cols_for_select]\n        new_schema = [self.schema[i] for i in idx_to_keep]\n\n        return FlowDataEngine(\n            self.data_frame.select(cols_for_select), number_of_records=self.number_of_records, schema=new_schema\n        )\n\n    def reorganize_order(self, column_order: list[str]) -&gt; FlowDataEngine:\n        \"\"\"Reorganizes columns into a specified order.\n\n        Args:\n            column_order: A list of column names in the desired order.\n\n        Returns:\n            A new `FlowDataEngine` instance with the columns reordered.\n        \"\"\"\n        df = self.data_frame.select(column_order)\n        schema = sorted(self.schema, key=lambda x: column_order.index(x.column_name))\n        return FlowDataEngine(df, schema=schema, number_of_records=self.number_of_records)\n\n    def apply_flowfile_formula(\n        self, func: str, col_name: str, output_data_type: pl.DataType = None\n    ) -&gt; FlowDataEngine:\n        \"\"\"Applies a formula to create a new column or transform an existing one.\n\n        Args:\n            func: A string containing a Polars expression formula.\n            col_name: The name of the new or transformed column.\n            output_data_type: The desired Polars data type for the output column.\n\n        Returns:\n            A new `FlowDataEngine` instance with the applied formula.\n        \"\"\"\n        parsed_func = to_expr(func)\n        if output_data_type is not None:\n            df2 = self.data_frame.with_columns(parsed_func.cast(output_data_type).alias(col_name))\n        else:\n            df2 = self.data_frame.with_columns(parsed_func.alias(col_name))\n\n        return FlowDataEngine(df2, number_of_records=self.number_of_records)\n\n    def apply_sql_formula(self, func: str, col_name: str, output_data_type: pl.DataType = None) -&gt; FlowDataEngine:\n        \"\"\"Applies an SQL-style formula using `pl.sql_expr`.\n\n        Args:\n            func: A string containing an SQL expression.\n            col_name: The name of the new or transformed column.\n            output_data_type: The desired Polars data type for the output column.\n\n        Returns:\n            A new `FlowDataEngine` instance with the applied formula.\n        \"\"\"\n        expr = to_expr(func)\n        if output_data_type not in (None, transform_schemas.AUTO_DATA_TYPE):\n            df = self.data_frame.with_columns(expr.cast(output_data_type).alias(col_name))\n        else:\n            df = self.data_frame.with_columns(expr.alias(col_name))\n\n        return FlowDataEngine(df, number_of_records=self.number_of_records)\n\n    def output(\n        self, output_fs: input_schema.OutputSettings, flow_id: int, node_id: int | str, execute_remote: bool = True\n    ) -&gt; FlowDataEngine:\n        \"\"\"Writes the DataFrame to an output file.\n\n        Can execute the write operation locally or in a remote worker process.\n\n        Args:\n            output_fs: An `OutputSettings` object with details about the output file.\n            flow_id: The flow ID for tracking.\n            node_id: The node ID for tracking.\n            execute_remote: If True, executes the write in a worker process.\n\n        Returns:\n            The same `FlowDataEngine` instance for chaining.\n        \"\"\"\n        logger.info(\"Starting to write output\")\n        if execute_remote:\n            status = utils.write_output(\n                self.data_frame,\n                data_type=output_fs.file_type,\n                path=output_fs.abs_file_path,\n                write_mode=output_fs.write_mode,\n                sheet_name=output_fs.sheet_name,\n                delimiter=output_fs.delimiter,\n                flow_id=flow_id,\n                node_id=node_id,\n            )\n            tracker = ExternalExecutorTracker(status)\n            tracker.get_result()\n            logger.info(\"Finished writing output\")\n        else:\n            logger.info(\"Starting to write results locally\")\n            utils.local_write_output(\n                self.data_frame,\n                data_type=output_fs.file_type,\n                path=output_fs.abs_file_path,\n                write_mode=output_fs.write_mode,\n                sheet_name=output_fs.sheet_name,\n                delimiter=output_fs.delimiter,\n                flow_id=flow_id,\n                node_id=node_id,\n            )\n            logger.info(\"Finished writing output\")\n        return self\n\n    def make_unique(self, unique_input: transform_schemas.UniqueInput = None) -&gt; FlowDataEngine:\n        \"\"\"Gets the unique rows from the DataFrame.\n\n        Args:\n            unique_input: A `UniqueInput` object specifying a subset of columns\n                to consider for uniqueness and a strategy for keeping rows.\n\n        Returns:\n            A new `FlowDataEngine` instance with unique rows.\n        \"\"\"\n        if unique_input is None or unique_input.columns is None:\n            return FlowDataEngine(self.data_frame.unique())\n        return FlowDataEngine(self.data_frame.unique(unique_input.columns, keep=unique_input.strategy))\n\n    def concat(self, other: Iterable[FlowDataEngine] | FlowDataEngine) -&gt; FlowDataEngine:\n        \"\"\"Concatenates this DataFrame with one or more other DataFrames.\n\n        Args:\n            other: A single `FlowDataEngine` or an iterable of them.\n\n        Returns:\n            A new `FlowDataEngine` containing the concatenated data.\n        \"\"\"\n        if isinstance(other, FlowDataEngine):\n            other = [other]\n\n        dfs: list[pl.LazyFrame] | list[pl.DataFrame] = [self.data_frame] + [flt.data_frame for flt in other]\n        return FlowDataEngine(pl.concat(dfs, how=\"diagonal_relaxed\"))\n\n    def do_select(self, select_inputs: transform_schemas.SelectInputs, keep_missing: bool = True) -&gt; FlowDataEngine:\n        \"\"\"Performs a complex column selection, renaming, and reordering operation.\n\n        Args:\n            select_inputs: A `SelectInputs` object defining the desired transformations.\n            keep_missing: If True, columns not specified in `select_inputs` are kept.\n                If False, they are dropped.\n\n        Returns:\n            A new `FlowDataEngine` with the transformed selection.\n        \"\"\"\n        new_schema = deepcopy(self.schema)\n        renames = [r for r in select_inputs.renames if r.is_available]\n        if not keep_missing:\n            drop_cols = set(self.data_frame.collect_schema().names()) - set(r.old_name for r in renames).union(\n                set(r.old_name for r in renames if not r.keep)\n            )\n            keep_cols = []\n        else:\n            keep_cols = list(set(self.data_frame.collect_schema().names()) - set(r.old_name for r in renames))\n            drop_cols = set(r.old_name for r in renames if not r.keep)\n\n        if len(drop_cols) &gt; 0:\n            new_schema = [s for s in new_schema if s.name not in drop_cols]\n        new_schema_mapping = {v.name: v for v in new_schema}\n\n        available_renames = []\n        for rename in renames:\n            if (rename.new_name != rename.old_name or rename.new_name not in new_schema_mapping) and rename.keep:\n                schema_entry = new_schema_mapping.get(rename.old_name)\n                if schema_entry is not None:\n                    available_renames.append(rename)\n                    schema_entry.column_name = rename.new_name\n\n        rename_dict = {r.old_name: r.new_name for r in available_renames}\n        fl = self.select_columns(\n            list_select=[col_to_keep.old_name for col_to_keep in renames if col_to_keep.keep] + keep_cols\n        )\n        fl = fl.change_column_types(transforms=[r for r in renames if r.keep])\n        ndf = fl.data_frame.rename(rename_dict)\n        renames.sort(key=lambda r: 0 if r.position is None else r.position)\n        sorted_cols = utils.match_order(\n            ndf.collect_schema().names(), [r.new_name for r in renames] + self.data_frame.collect_schema().names()\n        )\n        output_file = FlowDataEngine(ndf, number_of_records=self.number_of_records)\n        return output_file.reorganize_order(sorted_cols)\n\n    def set_streamable(self, streamable: bool = False):\n        \"\"\"Sets whether DataFrame operations should be streamable.\"\"\"\n        self._streamable = streamable\n\n    def _calculate_schema(self) -&gt; list[dict]:\n        \"\"\"Calculates schema statistics.\"\"\"\n        if self.external_source is not None:\n            self.collect_external()\n        v = utils.calculate_schema(self.data_frame)\n        return v\n\n    def calculate_schema(self):\n        \"\"\"Calculates and returns the schema.\"\"\"\n        self._calculate_schema_stats = True\n        return self.schema\n\n    def count(self) -&gt; int:\n        \"\"\"Gets the total number of records.\"\"\"\n        return self.get_number_of_records()\n\n    @classmethod\n    def create_from_path_worker(cls, received_table: input_schema.ReceivedTable, flow_id: int, node_id: int | str):\n        \"\"\"Creates a FlowDataEngine from a path in a worker process.\"\"\"\n        received_table.set_absolute_filepath()\n\n        external_fetcher = ExternalCreateFetcher(\n            received_table=received_table, file_type=received_table.file_type, flow_id=flow_id, node_id=node_id\n        )\n        return cls(external_fetcher.get_result())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.__name__","title":"<code>__name__</code>  <code>property</code>","text":"<p>The name of the table.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.cols_idx","title":"<code>cols_idx</code>  <code>property</code>","text":"<p>A dictionary mapping column names to their integer index.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.data_frame","title":"<code>data_frame</code>  <code>property</code> <code>writable</code>","text":"<p>The underlying Polars DataFrame or LazyFrame.</p> <p>This property provides access to the Polars object that backs the FlowDataEngine. It handles lazy-loading from external sources if necessary.</p> <p>Returns:</p> Type Description <code>LazyFrame | DataFrame | None</code> <p>The active Polars <code>DataFrame</code> or <code>LazyFrame</code>.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.external_source","title":"<code>external_source</code>  <code>property</code>","text":"<p>The external data source, if any.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.has_errors","title":"<code>has_errors</code>  <code>property</code>","text":"<p>Checks if there are any errors.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.lazy","title":"<code>lazy</code>  <code>property</code> <code>writable</code>","text":"<p>Indicates if the DataFrame is in lazy mode.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.number_of_fields","title":"<code>number_of_fields</code>  <code>property</code>","text":"<p>The number of columns (fields) in the DataFrame.</p> <p>Returns:</p> Type Description <code>int</code> <p>The integer count of columns.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.schema","title":"<code>schema</code>  <code>property</code>","text":"<p>The schema of the DataFrame as a list of <code>FlowfileColumn</code> objects.</p> <p>This property lazily calculates the schema if it hasn't been determined yet.</p> <p>Returns:</p> Type Description <code>list[FlowfileColumn]</code> <p>A list of <code>FlowfileColumn</code> objects describing the schema.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.__call__","title":"<code>__call__()</code>","text":"<p>Makes the class instance callable, returning itself.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def __call__(self) -&gt; FlowDataEngine:\n    \"\"\"Makes the class instance callable, returning itself.\"\"\"\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.__get_sample__","title":"<code>__get_sample__(n_rows=100, streamable=True)</code>","text":"<p>Internal method to get a sample of the data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def __get_sample__(self, n_rows: int = 100, streamable: bool = True) -&gt; FlowDataEngine:\n    \"\"\"Internal method to get a sample of the data.\"\"\"\n    if not self.lazy:\n        df = self.data_frame.lazy()\n    else:\n        df = self.data_frame\n\n    if streamable:\n        try:\n            df = df.head(n_rows).collect()\n        except Exception as e:\n            logger.warning(f\"Error in getting sample: {e}\")\n            df = df.head(n_rows).collect(engine=\"auto\")\n    else:\n        df = self.collect()\n    return FlowDataEngine(df, number_of_records=len(df), schema=self.schema)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.__getitem__","title":"<code>__getitem__(item)</code>","text":"<p>Accesses a specific column or item from the DataFrame.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def __getitem__(self, item):\n    \"\"\"Accesses a specific column or item from the DataFrame.\"\"\"\n    return self.data_frame.select([item])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.__init__","title":"<code>__init__(raw_data=None, path_ref=None, name=None, optimize_memory=True, schema=None, number_of_records=None, calculate_schema_stats=False, streamable=True, number_of_records_callback=None, data_callback=None)</code>","text":"<p>Initializes the FlowDataEngine from various data sources.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data</code> <code>list[dict] | list[Any] | dict[str, Any] | ParquetFile | DataFrame | LazyFrame | RawData</code> <p>The input data. Can be a list of dicts, a Polars DataFrame/LazyFrame, or a <code>RawData</code> schema object.</p> <code>None</code> <code>path_ref</code> <code>str</code> <p>A string path to a Parquet file.</p> <code>None</code> <code>name</code> <code>str</code> <p>An optional name for the data engine instance.</p> <code>None</code> <code>optimize_memory</code> <code>bool</code> <p>If True, prefers lazy operations to conserve memory.</p> <code>True</code> <code>schema</code> <code>list[FlowfileColumn] | list[str] | Schema</code> <p>An optional schema definition. Can be a list of <code>FlowfileColumn</code> objects, a list of column names, or a Polars <code>Schema</code>.</p> <code>None</code> <code>number_of_records</code> <code>int</code> <p>The number of records, if known.</p> <code>None</code> <code>calculate_schema_stats</code> <code>bool</code> <p>If True, computes detailed statistics for each column.</p> <code>False</code> <code>streamable</code> <code>bool</code> <p>If True, allows for streaming operations when possible.</p> <code>True</code> <code>number_of_records_callback</code> <code>Callable</code> <p>A callback function to retrieve the number of records.</p> <code>None</code> <code>data_callback</code> <code>Callable</code> <p>A callback function to retrieve the data.</p> <code>None</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def __init__(\n    self,\n    raw_data: list[dict] | list[Any] | dict[str, Any] | ParquetFile | pl.DataFrame | pl.LazyFrame | input_schema.RawData = None,\n    path_ref: str = None,\n    name: str = None,\n    optimize_memory: bool = True,\n    schema: list[FlowfileColumn] | list[str] | pl.Schema = None,\n    number_of_records: int = None,\n    calculate_schema_stats: bool = False,\n    streamable: bool = True,\n    number_of_records_callback: Callable = None,\n    data_callback: Callable = None,\n):\n    \"\"\"Initializes the FlowDataEngine from various data sources.\n\n    Args:\n        raw_data: The input data. Can be a list of dicts, a Polars DataFrame/LazyFrame,\n            or a `RawData` schema object.\n        path_ref: A string path to a Parquet file.\n        name: An optional name for the data engine instance.\n        optimize_memory: If True, prefers lazy operations to conserve memory.\n        schema: An optional schema definition. Can be a list of `FlowfileColumn` objects,\n            a list of column names, or a Polars `Schema`.\n        number_of_records: The number of records, if known.\n        calculate_schema_stats: If True, computes detailed statistics for each column.\n        streamable: If True, allows for streaming operations when possible.\n        number_of_records_callback: A callback function to retrieve the number of records.\n        data_callback: A callback function to retrieve the data.\n    \"\"\"\n    self._initialize_attributes(number_of_records_callback, data_callback, streamable)\n\n    if raw_data is not None:\n        self._handle_raw_data(raw_data, number_of_records, optimize_memory)\n    elif path_ref:\n        self._handle_path_ref(path_ref, optimize_memory)\n    else:\n        self.initialize_empty_fl()\n    self._finalize_initialization(name, optimize_memory, schema, calculate_schema_stats)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of records in the table.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of records in the table.\"\"\"\n    return self.number_of_records if self.number_of_records &gt;= 0 else self.get_number_of_records()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns a string representation of the FlowDataEngine.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Returns a string representation of the FlowDataEngine.\"\"\"\n    return f\"flow data engine\\n{self.data_frame.__repr__()}\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.add_new_values","title":"<code>add_new_values(values, col_name=None)</code>","text":"<p>Adds a new column with the provided values.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Iterable</code> <p>An iterable (e.g., list, tuple) of values to add as a new column.</p> required <code>col_name</code> <code>str</code> <p>The name for the new column. Defaults to 'new_values'.</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the added column.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def add_new_values(self, values: Iterable, col_name: str = None) -&gt; FlowDataEngine:\n    \"\"\"Adds a new column with the provided values.\n\n    Args:\n        values: An iterable (e.g., list, tuple) of values to add as a new column.\n        col_name: The name for the new column. Defaults to 'new_values'.\n\n    Returns:\n        A new `FlowDataEngine` instance with the added column.\n    \"\"\"\n    if col_name is None:\n        col_name = \"new_values\"\n    return FlowDataEngine(self.data_frame.with_columns(pl.Series(values).alias(col_name)))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.add_record_id","title":"<code>add_record_id(record_id_settings)</code>","text":"<p>Adds a record ID (row number) column to the DataFrame.</p> <p>Can generate a simple sequential ID or a grouped ID that resets for each group.</p> <p>Parameters:</p> Name Type Description Default <code>record_id_settings</code> <code>RecordIdInput</code> <p>A <code>RecordIdInput</code> object specifying the output column name, offset, and optional grouping columns.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the added record ID column.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def add_record_id(self, record_id_settings: transform_schemas.RecordIdInput) -&gt; FlowDataEngine:\n    \"\"\"Adds a record ID (row number) column to the DataFrame.\n\n    Can generate a simple sequential ID or a grouped ID that resets for\n    each group.\n\n    Args:\n        record_id_settings: A `RecordIdInput` object specifying the output\n            column name, offset, and optional grouping columns.\n\n    Returns:\n        A new `FlowDataEngine` instance with the added record ID column.\n    \"\"\"\n    if record_id_settings.group_by and len(record_id_settings.group_by_columns) &gt; 0:\n        return self._add_grouped_record_id(record_id_settings)\n    return self._add_simple_record_id(record_id_settings)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.apply_flowfile_formula","title":"<code>apply_flowfile_formula(func, col_name, output_data_type=None)</code>","text":"<p>Applies a formula to create a new column or transform an existing one.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>str</code> <p>A string containing a Polars expression formula.</p> required <code>col_name</code> <code>str</code> <p>The name of the new or transformed column.</p> required <code>output_data_type</code> <code>DataType</code> <p>The desired Polars data type for the output column.</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the applied formula.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def apply_flowfile_formula(\n    self, func: str, col_name: str, output_data_type: pl.DataType = None\n) -&gt; FlowDataEngine:\n    \"\"\"Applies a formula to create a new column or transform an existing one.\n\n    Args:\n        func: A string containing a Polars expression formula.\n        col_name: The name of the new or transformed column.\n        output_data_type: The desired Polars data type for the output column.\n\n    Returns:\n        A new `FlowDataEngine` instance with the applied formula.\n    \"\"\"\n    parsed_func = to_expr(func)\n    if output_data_type is not None:\n        df2 = self.data_frame.with_columns(parsed_func.cast(output_data_type).alias(col_name))\n    else:\n        df2 = self.data_frame.with_columns(parsed_func.alias(col_name))\n\n    return FlowDataEngine(df2, number_of_records=self.number_of_records)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.apply_sql_formula","title":"<code>apply_sql_formula(func, col_name, output_data_type=None)</code>","text":"<p>Applies an SQL-style formula using <code>pl.sql_expr</code>.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>str</code> <p>A string containing an SQL expression.</p> required <code>col_name</code> <code>str</code> <p>The name of the new or transformed column.</p> required <code>output_data_type</code> <code>DataType</code> <p>The desired Polars data type for the output column.</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the applied formula.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def apply_sql_formula(self, func: str, col_name: str, output_data_type: pl.DataType = None) -&gt; FlowDataEngine:\n    \"\"\"Applies an SQL-style formula using `pl.sql_expr`.\n\n    Args:\n        func: A string containing an SQL expression.\n        col_name: The name of the new or transformed column.\n        output_data_type: The desired Polars data type for the output column.\n\n    Returns:\n        A new `FlowDataEngine` instance with the applied formula.\n    \"\"\"\n    expr = to_expr(func)\n    if output_data_type not in (None, transform_schemas.AUTO_DATA_TYPE):\n        df = self.data_frame.with_columns(expr.cast(output_data_type).alias(col_name))\n    else:\n        df = self.data_frame.with_columns(expr.alias(col_name))\n\n    return FlowDataEngine(df, number_of_records=self.number_of_records)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.assert_equal","title":"<code>assert_equal(other, ordered=True, strict_schema=False)</code>","text":"<p>Asserts that this DataFrame is equal to another.</p> <p>Useful for testing.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>FlowDataEngine</code> <p>The other <code>FlowDataEngine</code> to compare with.</p> required <code>ordered</code> <code>bool</code> <p>If True, the row order must be identical.</p> <code>True</code> <code>strict_schema</code> <code>bool</code> <p>If True, the data types of the schemas must be identical.</p> <code>False</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the DataFrames are not equal based on the specified criteria.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def assert_equal(self, other: FlowDataEngine, ordered: bool = True, strict_schema: bool = False):\n    \"\"\"Asserts that this DataFrame is equal to another.\n\n    Useful for testing.\n\n    Args:\n        other: The other `FlowDataEngine` to compare with.\n        ordered: If True, the row order must be identical.\n        strict_schema: If True, the data types of the schemas must be identical.\n\n    Raises:\n        Exception: If the DataFrames are not equal based on the specified criteria.\n    \"\"\"\n    org_laziness = self.lazy, other.lazy\n    self.lazy = False\n    other.lazy = False\n    self.number_of_records = -1\n    other.number_of_records = -1\n    other = other.select_columns(self.columns)\n\n    if self.get_number_of_records_in_process() != other.get_number_of_records_in_process():\n        raise Exception(\"Number of records is not equal\")\n\n    if self.columns != other.columns:\n        raise Exception(\"Schema is not equal\")\n\n    if strict_schema:\n        assert self.data_frame.schema == other.data_frame.schema, \"Data types do not match\"\n\n    if ordered:\n        self_lf = self.data_frame.sort(by=self.columns)\n        other_lf = other.data_frame.sort(by=other.columns)\n    else:\n        self_lf = self.data_frame\n        other_lf = other.data_frame\n\n    self.lazy, other.lazy = org_laziness\n    assert self_lf.equals(other_lf), \"Data is not equal\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.cache","title":"<code>cache()</code>","text":"<p>Caches the current DataFrame to disk and updates the internal reference.</p> <p>This triggers a background process to write the current LazyFrame's result to a temporary file. Subsequent operations on this <code>FlowDataEngine</code> instance will read from the cached file, which can speed up downstream computations.</p> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>The same <code>FlowDataEngine</code> instance, now backed by the cached data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def cache(self) -&gt; FlowDataEngine:\n    \"\"\"Caches the current DataFrame to disk and updates the internal reference.\n\n    This triggers a background process to write the current LazyFrame's result\n    to a temporary file. Subsequent operations on this `FlowDataEngine` instance\n    will read from the cached file, which can speed up downstream computations.\n\n    Returns:\n        The same `FlowDataEngine` instance, now backed by the cached data.\n    \"\"\"\n    edf = ExternalDfFetcher(\n        lf=self.data_frame, file_ref=str(id(self)), wait_on_completion=False, flow_id=-1, node_id=-1\n    )\n    logger.info(\"Caching data in background\")\n    result = edf.get_result()\n    if isinstance(result, pl.LazyFrame):\n        logger.info(\"Data cached\")\n        del self._data_frame\n        self.data_frame = result\n        logger.info(\"Data loaded from cache\")\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.calculate_schema","title":"<code>calculate_schema()</code>","text":"<p>Calculates and returns the schema.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def calculate_schema(self):\n    \"\"\"Calculates and returns the schema.\"\"\"\n    self._calculate_schema_stats = True\n    return self.schema\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.change_column_types","title":"<code>change_column_types(transforms, calculate_schema=False)</code>","text":"<p>Changes the data type of one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>list[SelectInput]</code> <p>A list of <code>SelectInput</code> objects, where each object specifies the column and its new <code>polars_type</code>.</p> required <code>calculate_schema</code> <code>bool</code> <p>If True, recalculates the schema after the type change.</p> <code>False</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the updated column types.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def change_column_types(\n    self, transforms: list[transform_schemas.SelectInput], calculate_schema: bool = False\n) -&gt; FlowDataEngine:\n    \"\"\"Changes the data type of one or more columns.\n\n    Args:\n        transforms: A list of `SelectInput` objects, where each object specifies\n            the column and its new `polars_type`.\n        calculate_schema: If True, recalculates the schema after the type change.\n\n    Returns:\n        A new `FlowDataEngine` instance with the updated column types.\n    \"\"\"\n    dtypes = [dtype.base_type() for dtype in self.data_frame.collect_schema().dtypes()]\n    idx_mapping = list(\n        (transform.old_name, self.cols_idx.get(transform.old_name), get_polars_type(transform.polars_type))\n        for transform in transforms\n        if transform.data_type is not None\n    )\n\n    actual_transforms = [c for c in idx_mapping if c[2] != dtypes[c[1]]]\n    transformations = [\n        utils.define_pl_col_transformation(col_name=transform[0], col_type=transform[2])\n        for transform in actual_transforms\n    ]\n\n    df = self.data_frame.with_columns(transformations)\n    return FlowDataEngine(\n        df,\n        number_of_records=self.number_of_records,\n        calculate_schema_stats=calculate_schema,\n        streamable=self._streamable,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.collect","title":"<code>collect(n_records=None)</code>","text":"<p>Collects the data and returns it as a Polars DataFrame.</p> <p>This method triggers the execution of the lazy query plan (if applicable) and returns the result. It supports streaming to optimize memory usage for large datasets.</p> <p>Parameters:</p> Name Type Description Default <code>n_records</code> <code>int</code> <p>The maximum number of records to collect. If None, all records are collected.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Polars <code>DataFrame</code> containing the collected data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def collect(self, n_records: int = None) -&gt; pl.DataFrame:\n    \"\"\"Collects the data and returns it as a Polars DataFrame.\n\n    This method triggers the execution of the lazy query plan (if applicable)\n    and returns the result. It supports streaming to optimize memory usage\n    for large datasets.\n\n    Args:\n        n_records: The maximum number of records to collect. If None, all\n            records are collected.\n\n    Returns:\n        A Polars `DataFrame` containing the collected data.\n    \"\"\"\n    if n_records is None:\n        logger.info(f'Fetching all data for Table object \"{id(self)}\". Settings: streaming={self._streamable}')\n    else:\n        logger.info(\n            f'Fetching {n_records} record(s) for Table object \"{id(self)}\". '\n            f\"Settings: streaming={self._streamable}\"\n        )\n\n    if not self.lazy:\n        return self.data_frame\n\n    try:\n        return self._collect_data(n_records)\n    except Exception as e:\n        self.errors = [e]\n        return self._handle_collection_error(n_records)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.collect_external","title":"<code>collect_external()</code>","text":"<p>Materializes data from a tracked external source.</p> <p>If the <code>FlowDataEngine</code> was created from an <code>ExternalDataSource</code>, this method will trigger the data retrieval, update the internal <code>_data_frame</code> to a <code>LazyFrame</code> of the collected data, and reset the schema to be re-evaluated.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def collect_external(self):\n    \"\"\"Materializes data from a tracked external source.\n\n    If the `FlowDataEngine` was created from an `ExternalDataSource`, this\n    method will trigger the data retrieval, update the internal `_data_frame`\n    to a `LazyFrame` of the collected data, and reset the schema to be\n    re-evaluated.\n    \"\"\"\n    if self._external_source is not None:\n        logger.info(\"Collecting external source\")\n        if self.external_source.get_pl_df() is not None:\n            self.data_frame = self.external_source.get_pl_df().lazy()\n        else:\n            self.data_frame = pl.LazyFrame(list(self.external_source.get_iter()))\n        self._schema = None  # enforce reset schema\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.concat","title":"<code>concat(other)</code>","text":"<p>Concatenates this DataFrame with one or more other DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Iterable[FlowDataEngine] | FlowDataEngine</code> <p>A single <code>FlowDataEngine</code> or an iterable of them.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> containing the concatenated data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def concat(self, other: Iterable[FlowDataEngine] | FlowDataEngine) -&gt; FlowDataEngine:\n    \"\"\"Concatenates this DataFrame with one or more other DataFrames.\n\n    Args:\n        other: A single `FlowDataEngine` or an iterable of them.\n\n    Returns:\n        A new `FlowDataEngine` containing the concatenated data.\n    \"\"\"\n    if isinstance(other, FlowDataEngine):\n        other = [other]\n\n    dfs: list[pl.LazyFrame] | list[pl.DataFrame] = [self.data_frame] + [flt.data_frame for flt in other]\n    return FlowDataEngine(pl.concat(dfs, how=\"diagonal_relaxed\"))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.count","title":"<code>count()</code>","text":"<p>Gets the total number of records.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def count(self) -&gt; int:\n    \"\"\"Gets the total number of records.\"\"\"\n    return self.get_number_of_records()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.create_from_external_source","title":"<code>create_from_external_source(external_source)</code>  <code>classmethod</code>","text":"<p>Creates a FlowDataEngine from an external data source.</p> <p>Parameters:</p> Name Type Description Default <code>external_source</code> <code>ExternalDataSource</code> <p>An object that conforms to the <code>ExternalDataSource</code> interface.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef create_from_external_source(cls, external_source: ExternalDataSource) -&gt; FlowDataEngine:\n    \"\"\"Creates a FlowDataEngine from an external data source.\n\n    Args:\n        external_source: An object that conforms to the `ExternalDataSource`\n            interface.\n\n    Returns:\n        A new `FlowDataEngine` instance.\n    \"\"\"\n    if external_source.schema is not None:\n        ff = cls.create_from_schema(external_source.schema)\n    elif external_source.initial_data_getter is not None:\n        ff = cls(raw_data=external_source.initial_data_getter())\n    else:\n        ff = cls()\n    ff._external_source = external_source\n    return ff\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.create_from_path","title":"<code>create_from_path(received_table)</code>  <code>classmethod</code>","text":"<p>Creates a FlowDataEngine from a local file path.</p> <p>Supports various file types like CSV, Parquet, and Excel.</p> <p>Parameters:</p> Name Type Description Default <code>received_table</code> <code>ReceivedTable</code> <p>A <code>ReceivedTableBase</code> object containing the file path and format details.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with data from the file.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef create_from_path(cls, received_table: input_schema.ReceivedTable) -&gt; FlowDataEngine:\n    \"\"\"Creates a FlowDataEngine from a local file path.\n\n    Supports various file types like CSV, Parquet, and Excel.\n\n    Args:\n        received_table: A `ReceivedTableBase` object containing the file path\n            and format details.\n\n    Returns:\n        A new `FlowDataEngine` instance with data from the file.\n    \"\"\"\n    received_table.set_absolute_filepath()\n    file_type_handlers = {\n        \"csv\": create_funcs.create_from_path_csv,\n        \"parquet\": create_funcs.create_from_path_parquet,\n        \"excel\": create_funcs.create_from_path_excel,\n    }\n\n    handler = file_type_handlers.get(received_table.file_type)\n    if not handler:\n        raise Exception(f\"Cannot create from {received_table.file_type}\")\n\n    flow_file = cls(handler(received_table))\n    flow_file._org_path = received_table.abs_file_path\n    return flow_file\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.create_from_path_worker","title":"<code>create_from_path_worker(received_table, flow_id, node_id)</code>  <code>classmethod</code>","text":"<p>Creates a FlowDataEngine from a path in a worker process.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef create_from_path_worker(cls, received_table: input_schema.ReceivedTable, flow_id: int, node_id: int | str):\n    \"\"\"Creates a FlowDataEngine from a path in a worker process.\"\"\"\n    received_table.set_absolute_filepath()\n\n    external_fetcher = ExternalCreateFetcher(\n        received_table=received_table, file_type=received_table.file_type, flow_id=flow_id, node_id=node_id\n    )\n    return cls(external_fetcher.get_result())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.create_from_schema","title":"<code>create_from_schema(schema)</code>  <code>classmethod</code>","text":"<p>Creates an empty FlowDataEngine from a schema definition.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>list[FlowfileColumn]</code> <p>A list of <code>FlowfileColumn</code> objects defining the schema.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new, empty <code>FlowDataEngine</code> instance with the specified schema.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef create_from_schema(cls, schema: list[FlowfileColumn]) -&gt; FlowDataEngine:\n    \"\"\"Creates an empty FlowDataEngine from a schema definition.\n\n    Args:\n        schema: A list of `FlowfileColumn` objects defining the schema.\n\n    Returns:\n        A new, empty `FlowDataEngine` instance with the specified schema.\n    \"\"\"\n    pl_schema = []\n    for i, flow_file_column in enumerate(schema):\n        pl_schema.append((flow_file_column.name, cast_str_to_polars_type(flow_file_column.data_type)))\n        schema[i].col_index = i\n    df = pl.LazyFrame(schema=pl_schema)\n    return cls(df, schema=schema, calculate_schema_stats=False, number_of_records=0)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.create_from_sql","title":"<code>create_from_sql(sql, conn)</code>  <code>classmethod</code>","text":"<p>Creates a FlowDataEngine by executing a SQL query.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL query string to execute.</p> required <code>conn</code> <code>Any</code> <p>A database connection object or connection URI string.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the query result.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef create_from_sql(cls, sql: str, conn: Any) -&gt; FlowDataEngine:\n    \"\"\"Creates a FlowDataEngine by executing a SQL query.\n\n    Args:\n        sql: The SQL query string to execute.\n        conn: A database connection object or connection URI string.\n\n    Returns:\n        A new `FlowDataEngine` instance with the query result.\n    \"\"\"\n    return cls(pl.read_sql(sql, conn))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.create_random","title":"<code>create_random(number_of_records=1000)</code>  <code>classmethod</code>","text":"<p>Creates a FlowDataEngine with randomly generated data.</p> <p>Useful for testing and examples.</p> <p>Parameters:</p> Name Type Description Default <code>number_of_records</code> <code>int</code> <p>The number of random records to generate.</p> <code>1000</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with fake data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef create_random(cls, number_of_records: int = 1000) -&gt; FlowDataEngine:\n    \"\"\"Creates a FlowDataEngine with randomly generated data.\n\n    Useful for testing and examples.\n\n    Args:\n        number_of_records: The number of random records to generate.\n\n    Returns:\n        A new `FlowDataEngine` instance with fake data.\n    \"\"\"\n    return cls(create_fake_data(number_of_records))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.do_cross_join","title":"<code>do_cross_join(cross_join_input, auto_generate_selection, verify_integrity, other)</code>","text":"<p>Performs a cross join with another DataFrame.</p> <p>A cross join produces the Cartesian product of the two DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>cross_join_input</code> <code>CrossJoinInput</code> <p>A <code>CrossJoinInput</code> object specifying column selections.</p> required <code>auto_generate_selection</code> <code>bool</code> <p>If True, automatically renames columns to avoid conflicts.</p> required <code>verify_integrity</code> <code>bool</code> <p>If True, checks if the resulting join would be too large.</p> required <code>other</code> <code>FlowDataEngine</code> <p>The right <code>FlowDataEngine</code> to join with.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> with the result of the cross join.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If <code>verify_integrity</code> is True and the join would result in an excessively large number of records.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def do_cross_join(\n    self,\n    cross_join_input: transform_schemas.CrossJoinInput,\n    auto_generate_selection: bool,\n    verify_integrity: bool,\n    other: FlowDataEngine,\n) -&gt; FlowDataEngine:\n    \"\"\"Performs a cross join with another DataFrame.\n\n    A cross join produces the Cartesian product of the two DataFrames.\n\n    Args:\n        cross_join_input: A `CrossJoinInput` object specifying column selections.\n        auto_generate_selection: If True, automatically renames columns to avoid conflicts.\n        verify_integrity: If True, checks if the resulting join would be too large.\n        other: The right `FlowDataEngine` to join with.\n\n    Returns:\n        A new `FlowDataEngine` with the result of the cross join.\n\n    Raises:\n        Exception: If `verify_integrity` is True and the join would result in\n            an excessively large number of records.\n    \"\"\"\n    self.lazy = True\n    other.lazy = True\n    cross_join_input_manager = transform_schemas.CrossJoinInputManager(cross_join_input)\n    verify_join_select_integrity(\n        cross_join_input_manager.input, left_columns=self.columns, right_columns=other.columns\n    )\n    right_select = [\n        v.old_name\n        for v in cross_join_input_manager.right_select.renames\n        if (v.keep or v.join_key) and v.is_available\n    ]\n    left_select = [\n        v.old_name\n        for v in cross_join_input_manager.left_select.renames\n        if (v.keep or v.join_key) and v.is_available\n    ]\n    cross_join_input_manager.auto_rename(rename_mode=\"suffix\")\n    left = self.data_frame.select(left_select).rename(cross_join_input_manager.left_select.rename_table)\n    right = other.data_frame.select(right_select).rename(cross_join_input_manager.right_select.rename_table)\n\n    joined_df = left.join(right, how=\"cross\")\n\n    cols_to_delete_after = [\n        col.new_name\n        for col in cross_join_input_manager.left_select.renames + cross_join_input_manager.right_select.renames\n        if col.join_key and not col.keep and col.is_available\n    ]\n\n    fl = FlowDataEngine(joined_df.drop(cols_to_delete_after), calculate_schema_stats=False, streamable=False)\n    return fl\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.do_filter","title":"<code>do_filter(predicate)</code>","text":"<p>Filters rows based on a predicate expression.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>str</code> <p>A string containing a Polars expression that evaluates to a boolean value.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance containing only the rows that match</p> <code>FlowDataEngine</code> <p>the predicate.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def do_filter(self, predicate: str) -&gt; FlowDataEngine:\n    \"\"\"Filters rows based on a predicate expression.\n\n    Args:\n        predicate: A string containing a Polars expression that evaluates to\n            a boolean value.\n\n    Returns:\n        A new `FlowDataEngine` instance containing only the rows that match\n        the predicate.\n    \"\"\"\n    try:\n        f = to_expr(predicate)\n    except Exception as e:\n        logger.warning(f\"Error in filter expression: {e}\")\n        f = to_expr(\"False\")\n    df = self.data_frame.filter(f)\n    return FlowDataEngine(df, schema=self.schema, streamable=self._streamable)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.do_group_by","title":"<code>do_group_by(group_by_input, calculate_schema_stats=True)</code>","text":"<p>Performs a group-by operation on the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>group_by_input</code> <code>GroupByInput</code> <p>A <code>GroupByInput</code> object defining the grouping columns and aggregations.</p> required <code>calculate_schema_stats</code> <code>bool</code> <p>If True, calculates schema statistics for the resulting DataFrame.</p> <code>True</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the grouped and aggregated data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def do_group_by(\n    self, group_by_input: transform_schemas.GroupByInput, calculate_schema_stats: bool = True\n) -&gt; FlowDataEngine:\n    \"\"\"Performs a group-by operation on the DataFrame.\n\n    Args:\n        group_by_input: A `GroupByInput` object defining the grouping columns\n            and aggregations.\n        calculate_schema_stats: If True, calculates schema statistics for the\n            resulting DataFrame.\n\n    Returns:\n        A new `FlowDataEngine` instance with the grouped and aggregated data.\n    \"\"\"\n    aggregations = [c for c in group_by_input.agg_cols if c.agg != \"groupby\"]\n    group_columns = [c for c in group_by_input.agg_cols if c.agg == \"groupby\"]\n\n    if len(group_columns) == 0:\n        return FlowDataEngine(\n            self.data_frame.select(ac.agg_func(ac.old_name).alias(ac.new_name) for ac in aggregations),\n            calculate_schema_stats=calculate_schema_stats,\n        )\n\n    df = self.data_frame.rename({c.old_name: c.new_name for c in group_columns})\n    group_by_columns = [n_c.new_name for n_c in group_columns]\n\n    # Handle case where there are no aggregations - just get unique combinations of group columns\n    if len(aggregations) == 0:\n        return FlowDataEngine(\n            df.select(group_by_columns).unique(),\n            calculate_schema_stats=calculate_schema_stats,\n        )\n\n    grouped_df = df.group_by(*group_by_columns)\n    agg_exprs = [ac.agg_func(ac.old_name).alias(ac.new_name) for ac in aggregations]\n    result_df = grouped_df.agg(agg_exprs)\n\n    return FlowDataEngine(\n        result_df,\n        calculate_schema_stats=calculate_schema_stats,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.do_pivot","title":"<code>do_pivot(pivot_input, node_logger=None)</code>","text":"<p>Converts the DataFrame from a long to a wide format, aggregating values.</p> <p>Parameters:</p> Name Type Description Default <code>pivot_input</code> <code>PivotInput</code> <p>A <code>PivotInput</code> object defining the index, pivot, and value columns, along with the aggregation logic.</p> required <code>node_logger</code> <code>NodeLogger</code> <p>An optional logger for reporting warnings, e.g., if the pivot column has too many unique values.</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new, pivoted <code>FlowDataEngine</code> instance.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def do_pivot(self, pivot_input: transform_schemas.PivotInput, node_logger: NodeLogger = None) -&gt; FlowDataEngine:\n    \"\"\"Converts the DataFrame from a long to a wide format, aggregating values.\n\n    Args:\n        pivot_input: A `PivotInput` object defining the index, pivot, and value\n            columns, along with the aggregation logic.\n        node_logger: An optional logger for reporting warnings, e.g., if the\n            pivot column has too many unique values.\n\n    Returns:\n        A new, pivoted `FlowDataEngine` instance.\n    \"\"\"\n    # Get unique values for pivot columns\n    max_unique_vals = 200\n    new_cols_unique = fetch_unique_values(\n        self.data_frame.select(pivot_input.pivot_column)\n        .unique()\n        .sort(pivot_input.pivot_column)\n        .limit(max_unique_vals)\n        .cast(pl.String)\n    )\n    if len(new_cols_unique) &gt;= max_unique_vals:\n        if node_logger:\n            node_logger.warning(\n                \"Pivot column has too many unique values. Please consider using a different column.\"\n                f\" Max unique values: {max_unique_vals}\"\n            )\n\n    if len(pivot_input.index_columns) == 0:\n        no_index_cols = True\n        pivot_input.index_columns = [\"__temp__\"]\n        ff = self.apply_flowfile_formula(\"1\", col_name=\"__temp__\")\n    else:\n        no_index_cols = False\n        ff = self\n\n    # Perform pivot operations\n    index_columns = pivot_input.get_index_columns()\n    grouped_ff = ff.do_group_by(pivot_input.get_group_by_input(), False)\n    pivot_column = pivot_input.get_pivot_column()\n\n    input_df = grouped_ff.data_frame.with_columns(pivot_column.cast(pl.String).alias(pivot_input.pivot_column))\n    number_of_aggregations = len(pivot_input.aggregations)\n    df = (\n        input_df.select(*index_columns, pivot_column, pivot_input.get_values_expr())\n        .group_by(*index_columns)\n        .agg(\n            [\n                (pl.col(\"vals\").filter(pivot_column == new_col_value)).first().alias(new_col_value)\n                for new_col_value in new_cols_unique\n            ]\n        )\n        .select(\n            *index_columns,\n            *[\n                pl.col(new_col)\n                .struct.field(agg)\n                .alias(f'{new_col + \"_\" + agg if number_of_aggregations &gt; 1 else new_col }')\n                for new_col in new_cols_unique\n                for agg in pivot_input.aggregations\n            ],\n        )\n    )\n\n    # Clean up temporary columns if needed\n    if no_index_cols:\n        df = df.drop(\"__temp__\")\n        pivot_input.index_columns = []\n\n    return FlowDataEngine(df, calculate_schema_stats=False)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.do_select","title":"<code>do_select(select_inputs, keep_missing=True)</code>","text":"<p>Performs a complex column selection, renaming, and reordering operation.</p> <p>Parameters:</p> Name Type Description Default <code>select_inputs</code> <code>SelectInputs</code> <p>A <code>SelectInputs</code> object defining the desired transformations.</p> required <code>keep_missing</code> <code>bool</code> <p>If True, columns not specified in <code>select_inputs</code> are kept. If False, they are dropped.</p> <code>True</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> with the transformed selection.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def do_select(self, select_inputs: transform_schemas.SelectInputs, keep_missing: bool = True) -&gt; FlowDataEngine:\n    \"\"\"Performs a complex column selection, renaming, and reordering operation.\n\n    Args:\n        select_inputs: A `SelectInputs` object defining the desired transformations.\n        keep_missing: If True, columns not specified in `select_inputs` are kept.\n            If False, they are dropped.\n\n    Returns:\n        A new `FlowDataEngine` with the transformed selection.\n    \"\"\"\n    new_schema = deepcopy(self.schema)\n    renames = [r for r in select_inputs.renames if r.is_available]\n    if not keep_missing:\n        drop_cols = set(self.data_frame.collect_schema().names()) - set(r.old_name for r in renames).union(\n            set(r.old_name for r in renames if not r.keep)\n        )\n        keep_cols = []\n    else:\n        keep_cols = list(set(self.data_frame.collect_schema().names()) - set(r.old_name for r in renames))\n        drop_cols = set(r.old_name for r in renames if not r.keep)\n\n    if len(drop_cols) &gt; 0:\n        new_schema = [s for s in new_schema if s.name not in drop_cols]\n    new_schema_mapping = {v.name: v for v in new_schema}\n\n    available_renames = []\n    for rename in renames:\n        if (rename.new_name != rename.old_name or rename.new_name not in new_schema_mapping) and rename.keep:\n            schema_entry = new_schema_mapping.get(rename.old_name)\n            if schema_entry is not None:\n                available_renames.append(rename)\n                schema_entry.column_name = rename.new_name\n\n    rename_dict = {r.old_name: r.new_name for r in available_renames}\n    fl = self.select_columns(\n        list_select=[col_to_keep.old_name for col_to_keep in renames if col_to_keep.keep] + keep_cols\n    )\n    fl = fl.change_column_types(transforms=[r for r in renames if r.keep])\n    ndf = fl.data_frame.rename(rename_dict)\n    renames.sort(key=lambda r: 0 if r.position is None else r.position)\n    sorted_cols = utils.match_order(\n        ndf.collect_schema().names(), [r.new_name for r in renames] + self.data_frame.collect_schema().names()\n    )\n    output_file = FlowDataEngine(ndf, number_of_records=self.number_of_records)\n    return output_file.reorganize_order(sorted_cols)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.do_sort","title":"<code>do_sort(sorts)</code>","text":"<p>Sorts the DataFrame by one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>sorts</code> <code>list[SortByInput]</code> <p>A list of <code>SortByInput</code> objects, each specifying a column and sort direction ('asc' or 'desc').</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the sorted data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def do_sort(self, sorts: list[transform_schemas.SortByInput]) -&gt; FlowDataEngine:\n    \"\"\"Sorts the DataFrame by one or more columns.\n\n    Args:\n        sorts: A list of `SortByInput` objects, each specifying a column\n            and sort direction ('asc' or 'desc').\n\n    Returns:\n        A new `FlowDataEngine` instance with the sorted data.\n    \"\"\"\n    if not sorts:\n        return self\n\n    descending = [s.how == \"desc\" or s.how.lower() == \"descending\" for s in sorts]\n    df = self.data_frame.sort([sort_by.column for sort_by in sorts], descending=descending)\n    return FlowDataEngine(df, number_of_records=self.number_of_records, schema=self.schema)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.drop_columns","title":"<code>drop_columns(columns)</code>","text":"<p>Drops specified columns from the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list[str]</code> <p>A list of column names to drop.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance without the dropped columns.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def drop_columns(self, columns: list[str]) -&gt; FlowDataEngine:\n    \"\"\"Drops specified columns from the DataFrame.\n\n    Args:\n        columns: A list of column names to drop.\n\n    Returns:\n        A new `FlowDataEngine` instance without the dropped columns.\n    \"\"\"\n    cols_for_select = tuple(set(self.columns) - set(columns))\n    idx_to_keep = [self.cols_idx.get(c) for c in cols_for_select]\n    new_schema = [self.schema[i] for i in idx_to_keep]\n\n    return FlowDataEngine(\n        self.data_frame.select(cols_for_select), number_of_records=self.number_of_records, schema=new_schema\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.from_cloud_storage_obj","title":"<code>from_cloud_storage_obj(settings)</code>  <code>classmethod</code>","text":"<p>Creates a FlowDataEngine from an object in cloud storage.</p> <p>This method supports reading from various cloud storage providers like AWS S3, Azure Data Lake Storage, and Google Cloud Storage, with support for various authentication methods.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>CloudStorageReadSettingsInternal</code> <p>A <code>CloudStorageReadSettingsInternal</code> object containing connection details, file format, and read options.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance containing the data from cloud storage.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the storage type or file format is not supported.</p> <code>NotImplementedError</code> <p>If a requested file format like \"delta\" or \"iceberg\" is not yet implemented.</p> <code>Exception</code> <p>If reading from cloud storage fails.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef from_cloud_storage_obj(\n    cls, settings: cloud_storage_schemas.CloudStorageReadSettingsInternal\n) -&gt; FlowDataEngine:\n    \"\"\"Creates a FlowDataEngine from an object in cloud storage.\n\n    This method supports reading from various cloud storage providers like AWS S3,\n    Azure Data Lake Storage, and Google Cloud Storage, with support for\n    various authentication methods.\n\n    Args:\n        settings: A `CloudStorageReadSettingsInternal` object containing connection\n            details, file format, and read options.\n\n    Returns:\n        A new `FlowDataEngine` instance containing the data from cloud storage.\n\n    Raises:\n        ValueError: If the storage type or file format is not supported.\n        NotImplementedError: If a requested file format like \"delta\" or \"iceberg\"\n            is not yet implemented.\n        Exception: If reading from cloud storage fails.\n    \"\"\"\n    connection = settings.connection\n    read_settings = settings.read_settings\n\n    logger.info(f\"Reading from {connection.storage_type} storage: {read_settings.resource_path}\")\n    # Get storage options based on connection type\n    storage_options = CloudStorageReader.get_storage_options(connection)\n    # Get credential provider if needed\n    credential_provider = CloudStorageReader.get_credential_provider(connection)\n    if read_settings.file_format == \"parquet\":\n        return cls._read_parquet_from_cloud(\n            read_settings.resource_path,\n            storage_options,\n            credential_provider,\n            read_settings.scan_mode == \"directory\",\n        )\n    elif read_settings.file_format == \"delta\":\n        return cls._read_delta_from_cloud(\n            read_settings.resource_path, storage_options, credential_provider, read_settings\n        )\n    elif read_settings.file_format == \"csv\":\n        return cls._read_csv_from_cloud(\n            read_settings.resource_path, storage_options, credential_provider, read_settings\n        )\n    elif read_settings.file_format == \"json\":\n        return cls._read_json_from_cloud(\n            read_settings.resource_path,\n            storage_options,\n            credential_provider,\n            read_settings.scan_mode == \"directory\",\n        )\n    elif read_settings.file_format == \"iceberg\":\n        return cls._read_iceberg_from_cloud(\n            read_settings.resource_path, storage_options, credential_provider, read_settings\n        )\n\n    elif read_settings.file_format in [\"delta\", \"iceberg\"]:\n        # These would require additional libraries\n        raise NotImplementedError(f\"File format {read_settings.file_format} not yet implemented\")\n    else:\n        raise ValueError(f\"Unsupported file format: {read_settings.file_format}\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.generate_enumerator","title":"<code>generate_enumerator(length=1000, output_name='output_column')</code>  <code>classmethod</code>","text":"<p>Generates a FlowDataEngine with a single column containing a sequence of integers.</p> <p>Parameters:</p> Name Type Description Default <code>length</code> <code>int</code> <p>The number of integers to generate in the sequence.</p> <code>1000</code> <code>output_name</code> <code>str</code> <p>The name of the output column.</p> <code>'output_column'</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef generate_enumerator(cls, length: int = 1000, output_name: str = \"output_column\") -&gt; FlowDataEngine:\n    \"\"\"Generates a FlowDataEngine with a single column containing a sequence of integers.\n\n    Args:\n        length: The number of integers to generate in the sequence.\n        output_name: The name of the output column.\n\n    Returns:\n        A new `FlowDataEngine` instance.\n    \"\"\"\n    if length &gt; 10_000_000:\n        length = 10_000_000\n    return cls(pl.LazyFrame().select((pl.int_range(0, length, dtype=pl.UInt32)).alias(output_name)))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_estimated_file_size","title":"<code>get_estimated_file_size()</code>","text":"<p>Estimates the file size in bytes if the data originated from a local file.</p> <p>This relies on the original path being tracked during file ingestion.</p> <p>Returns:</p> Type Description <code>int</code> <p>The file size in bytes, or 0 if the original path is unknown.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_estimated_file_size(self) -&gt; int:\n    \"\"\"Estimates the file size in bytes if the data originated from a local file.\n\n    This relies on the original path being tracked during file ingestion.\n\n    Returns:\n        The file size in bytes, or 0 if the original path is unknown.\n    \"\"\"\n    if self._org_path is not None:\n        return os.path.getsize(self._org_path)\n    return 0\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_number_of_records","title":"<code>get_number_of_records(warn=False, force_calculate=False, calculate_in_worker_process=False)</code>","text":"<p>Gets the total number of records in the DataFrame.</p> <p>For lazy frames, this may trigger a full data scan, which can be expensive.</p> <p>Parameters:</p> Name Type Description Default <code>warn</code> <code>bool</code> <p>If True, logs a warning if a potentially expensive calculation is triggered.</p> <code>False</code> <code>force_calculate</code> <code>bool</code> <p>If True, forces recalculation even if a value is cached.</p> <code>False</code> <code>calculate_in_worker_process</code> <code>bool</code> <p>If True, offloads the calculation to a worker process.</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>The total number of records.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of records could not be determined.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_number_of_records(\n    self, warn: bool = False, force_calculate: bool = False, calculate_in_worker_process: bool = False\n) -&gt; int:\n    \"\"\"Gets the total number of records in the DataFrame.\n\n    For lazy frames, this may trigger a full data scan, which can be expensive.\n\n    Args:\n        warn: If True, logs a warning if a potentially expensive calculation is triggered.\n        force_calculate: If True, forces recalculation even if a value is cached.\n        calculate_in_worker_process: If True, offloads the calculation to a worker process.\n\n    Returns:\n        The total number of records.\n\n    Raises:\n        ValueError: If the number of records could not be determined.\n    \"\"\"\n    if self.is_future and not self.is_collected:\n        return -1\n    if self.number_of_records is None or self.number_of_records &lt; 0 or force_calculate:\n        if self._number_of_records_callback is not None:\n            self._number_of_records_callback(self)\n\n        if self.lazy:\n            if calculate_in_worker_process:\n                try:\n                    self.number_of_records = self._calculate_number_of_records_in_worker()\n                    return self.number_of_records\n                except Exception as e:\n                    logger.error(f\"Error: {e}\")\n            if warn:\n                logger.warning(\"Calculating the number of records this can be expensive on a lazy frame\")\n            try:\n                self.number_of_records = self.data_frame.select(pl.len()).collect(\n                    engine=\"streaming\" if self._streamable else \"auto\"\n                )[0, 0]\n            except Exception:\n                raise ValueError(\"Could not get number of records\")\n        else:\n            self.number_of_records = self.data_frame.__len__()\n    return self.number_of_records\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_number_of_records_in_process","title":"<code>get_number_of_records_in_process(force_calculate=False)</code>","text":"<p>Get the number of records in the DataFrame in the local process.</p> <p>Parameters:</p> Name Type Description Default <code>force_calculate</code> <code>bool</code> <p>If True, forces recalculation even if a value is cached.</p> <code>False</code> <p>Returns:</p> Type Description <p>The total number of records.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_number_of_records_in_process(self, force_calculate: bool = False):\n    \"\"\"\n    Get the number of records in the DataFrame in the local process.\n\n    args:\n        force_calculate: If True, forces recalculation even if a value is cached.\n\n    Returns:\n        The total number of records.\n    \"\"\"\n    return self.get_number_of_records(force_calculate=force_calculate)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_output_sample","title":"<code>get_output_sample(n_rows=10)</code>","text":"<p>Gets a sample of the data as a list of dictionaries.</p> <p>This is typically used to display a preview of the data in a UI.</p> <p>Parameters:</p> Name Type Description Default <code>n_rows</code> <code>int</code> <p>The number of rows to sample.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of dictionaries, where each dictionary represents a row.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_output_sample(self, n_rows: int = 10) -&gt; list[dict]:\n    \"\"\"Gets a sample of the data as a list of dictionaries.\n\n    This is typically used to display a preview of the data in a UI.\n\n    Args:\n        n_rows: The number of rows to sample.\n\n    Returns:\n        A list of dictionaries, where each dictionary represents a row.\n    \"\"\"\n    if self.number_of_records &gt; n_rows or self.number_of_records &lt; 0:\n        df = self.collect(n_rows)\n    else:\n        df = self.collect()\n    return df.to_dicts()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_record_count","title":"<code>get_record_count()</code>","text":"<p>Returns a new FlowDataEngine with a single column 'number_of_records' containing the total number of records.</p> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_record_count(self) -&gt; FlowDataEngine:\n    \"\"\"Returns a new FlowDataEngine with a single column 'number_of_records'\n    containing the total number of records.\n\n    Returns:\n        A new `FlowDataEngine` instance.\n    \"\"\"\n    return FlowDataEngine(self.data_frame.select(pl.len().alias(\"number_of_records\")))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_sample","title":"<code>get_sample(n_rows=100, random=False, shuffle=False, seed=None, execution_location=None)</code>","text":"<p>Gets a sample of rows from the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>n_rows</code> <code>int</code> <p>The number of rows to sample.</p> <code>100</code> <code>random</code> <code>bool</code> <p>If True, performs random sampling. If False, takes the first n_rows.</p> <code>False</code> <code>shuffle</code> <code>bool</code> <p>If True (and <code>random</code> is True), shuffles the data before sampling.</p> <code>False</code> <code>seed</code> <code>int</code> <p>A random seed for reproducibility.</p> <code>None</code> <code>execution_location</code> <code>ExecutionLocationsLiteral | None</code> <p>Location which is used to calculate the size of the dataframe</p> <code>None</code> <p>Returns:     A new <code>FlowDataEngine</code> instance containing the sampled data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_sample(\n    self,\n    n_rows: int = 100,\n    random: bool = False,\n    shuffle: bool = False,\n    seed: int = None,\n    execution_location: ExecutionLocationsLiteral | None = None,\n) -&gt; FlowDataEngine:\n    \"\"\"Gets a sample of rows from the DataFrame.\n\n    Args:\n        n_rows: The number of rows to sample.\n        random: If True, performs random sampling. If False, takes the first n_rows.\n        shuffle: If True (and `random` is True), shuffles the data before sampling.\n        seed: A random seed for reproducibility.\n        execution_location: Location which is used to calculate the size of the dataframe\n    Returns:\n        A new `FlowDataEngine` instance containing the sampled data.\n    \"\"\"\n    logging.info(f\"Getting sample of {n_rows} rows\")\n    if random:\n        if self.lazy and self.external_source is not None:\n            self.collect_external()\n\n        if self.lazy and shuffle:\n            sample_df = self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\").sample(\n                n_rows, seed=seed, shuffle=shuffle\n            )\n        elif shuffle:\n            sample_df = self.data_frame.sample(n_rows, seed=seed, shuffle=shuffle)\n        else:\n            if execution_location is None:\n                execution_location = get_global_execution_location()\n            n_rows = min(\n                n_rows, self.get_number_of_records(calculate_in_worker_process=execution_location == \"remote\")\n            )\n\n            every_n_records = ceil(self.number_of_records / n_rows)\n            sample_df = self.data_frame.gather_every(every_n_records)\n    else:\n        if self.external_source:\n            self.collect(n_rows)\n        sample_df = self.data_frame.head(n_rows)\n\n    return FlowDataEngine(sample_df, schema=self.schema)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_schema_column","title":"<code>get_schema_column(col_name)</code>","text":"<p>Retrieves the schema information for a single column by its name.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to retrieve.</p> required <p>Returns:</p> Type Description <code>FlowfileColumn</code> <p>A <code>FlowfileColumn</code> object for the specified column, or <code>None</code> if not found.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_schema_column(self, col_name: str) -&gt; FlowfileColumn:\n    \"\"\"Retrieves the schema information for a single column by its name.\n\n    Args:\n        col_name: The name of the column to retrieve.\n\n    Returns:\n        A `FlowfileColumn` object for the specified column, or `None` if not found.\n    \"\"\"\n    for s in self.schema:\n        if s.name == col_name:\n            return s\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_select_inputs","title":"<code>get_select_inputs()</code>","text":"<p>Gets <code>SelectInput</code> specifications for all columns in the current schema.</p> <p>Returns:</p> Type Description <code>SelectInputs</code> <p>A <code>SelectInputs</code> object that can be used to configure selection or</p> <code>SelectInputs</code> <p>transformation operations.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_select_inputs(self) -&gt; transform_schemas.SelectInputs:\n    \"\"\"Gets `SelectInput` specifications for all columns in the current schema.\n\n    Returns:\n        A `SelectInputs` object that can be used to configure selection or\n        transformation operations.\n    \"\"\"\n    return transform_schemas.SelectInputs(\n        [transform_schemas.SelectInput(old_name=c.name, data_type=c.data_type) for c in self.schema]\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_subset","title":"<code>get_subset(n_rows=100)</code>","text":"<p>Gets the first <code>n_rows</code> from the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>n_rows</code> <code>int</code> <p>The number of rows to include in the subset.</p> <code>100</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance containing the subset of data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_subset(self, n_rows: int = 100) -&gt; FlowDataEngine:\n    \"\"\"Gets the first `n_rows` from the DataFrame.\n\n    Args:\n        n_rows: The number of rows to include in the subset.\n\n    Returns:\n        A new `FlowDataEngine` instance containing the subset of data.\n    \"\"\"\n    if not self.lazy:\n        return FlowDataEngine(self.data_frame.head(n_rows), calculate_schema_stats=True)\n    else:\n        return FlowDataEngine(self.data_frame.head(n_rows), calculate_schema_stats=True)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.initialize_empty_fl","title":"<code>initialize_empty_fl()</code>","text":"<p>Initializes an empty LazyFrame.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def initialize_empty_fl(self):\n    \"\"\"Initializes an empty LazyFrame.\"\"\"\n    self.data_frame = pl.LazyFrame()\n    self.number_of_records = 0\n    self._lazy = True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.iter_batches","title":"<code>iter_batches(batch_size=1000, columns=None)</code>","text":"<p>Iterates over the DataFrame in batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The size of each batch.</p> <code>1000</code> <code>columns</code> <code>list | tuple | str</code> <p>A list of column names to include in the batches. If None, all columns are included.</p> <code>None</code> <p>Yields:</p> Type Description <code>FlowDataEngine</code> <p>A <code>FlowDataEngine</code> instance for each batch.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def iter_batches(\n    self, batch_size: int = 1000, columns: list | tuple | str = None\n) -&gt; Generator[FlowDataEngine, None, None]:\n    \"\"\"Iterates over the DataFrame in batches.\n\n    Args:\n        batch_size: The size of each batch.\n        columns: A list of column names to include in the batches. If None,\n            all columns are included.\n\n    Yields:\n        A `FlowDataEngine` instance for each batch.\n    \"\"\"\n    if columns:\n        self.data_frame = self.data_frame.select(columns)\n    self.lazy = False\n    batches = self.data_frame.iter_slices(batch_size)\n    for batch in batches:\n        yield FlowDataEngine(batch)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.join","title":"<code>join(join_input, auto_generate_selection, verify_integrity, other)</code>","text":"<p>Performs a standard SQL-style join with another DataFrame.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def join(\n    self,\n    join_input: transform_schemas.JoinInput,\n    auto_generate_selection: bool,\n    verify_integrity: bool,\n    other: FlowDataEngine,\n) -&gt; FlowDataEngine:\n    \"\"\"Performs a standard SQL-style join with another DataFrame.\"\"\"\n    # Create manager from input\n    join_manager = transform_schemas.JoinInputManager(join_input)\n    ensure_right_unselect_for_semi_and_anti_joins(join_manager.input)\n    for jk in join_manager.join_mapping:\n        if jk.left_col not in {c.old_name for c in join_manager.left_select.renames}:\n            join_manager.left_select.append(transform_schemas.SelectInput(jk.left_col, keep=False))\n        if jk.right_col not in {c.old_name for c in join_manager.right_select.renames}:\n            join_manager.right_select.append(transform_schemas.SelectInput(jk.right_col, keep=False))\n    verify_join_select_integrity(join_manager.input, left_columns=self.columns, right_columns=other.columns)\n    if not verify_join_map_integrity(join_manager.input, left_columns=self.schema, right_columns=other.schema):\n        raise Exception(\"Join is not valid by the data fields\")\n\n    if auto_generate_selection:\n        join_manager.auto_rename()\n\n    # Use manager properties throughout\n    left = self.data_frame.select(join_manager.left_manager.get_select_cols()).rename(\n        join_manager.left_manager.get_rename_table()\n    )\n    right = other.data_frame.select(join_manager.right_manager.get_select_cols()).rename(\n        join_manager.right_manager.get_rename_table()\n    )\n\n    left, right, reverse_join_key_mapping = _handle_duplication_join_keys(left, right, join_manager)\n    left, right = rename_df_table_for_join(left, right, join_manager.get_join_key_renames())\n    if join_manager.how == \"right\":\n        joined_df = right.join(\n            other=left,\n            left_on=join_manager.right_join_keys,\n            right_on=join_manager.left_join_keys,\n            how=\"left\",\n            suffix=\"\",\n        ).rename(reverse_join_key_mapping)\n    else:\n        joined_df = left.join(\n            other=right,\n            left_on=join_manager.left_join_keys,\n            right_on=join_manager.right_join_keys,\n            how=join_manager.how,\n            suffix=\"\",\n        ).rename(reverse_join_key_mapping)\n\n    left_cols_to_delete_after = [\n        get_col_name_to_delete(col, \"left\")\n        for col in join_manager.input.left_select.renames\n        if not col.keep and col.is_available and col.join_key\n    ]\n\n    right_cols_to_delete_after = [\n        get_col_name_to_delete(col, \"right\")\n        for col in join_manager.input.right_select.renames\n        if not col.keep\n        and col.is_available\n        and col.join_key\n        and join_manager.how in (\"left\", \"right\", \"inner\", \"cross\", \"outer\")\n    ]\n\n    if len(right_cols_to_delete_after + left_cols_to_delete_after) &gt; 0:\n        joined_df = joined_df.drop(left_cols_to_delete_after + right_cols_to_delete_after)\n\n    undo_join_key_remapping = get_undo_rename_mapping_join(join_manager)\n    joined_df = joined_df.rename(undo_join_key_remapping)\n\n    return FlowDataEngine(joined_df, calculate_schema_stats=False, number_of_records=0, streamable=False)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.make_unique","title":"<code>make_unique(unique_input=None)</code>","text":"<p>Gets the unique rows from the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>unique_input</code> <code>UniqueInput</code> <p>A <code>UniqueInput</code> object specifying a subset of columns to consider for uniqueness and a strategy for keeping rows.</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with unique rows.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def make_unique(self, unique_input: transform_schemas.UniqueInput = None) -&gt; FlowDataEngine:\n    \"\"\"Gets the unique rows from the DataFrame.\n\n    Args:\n        unique_input: A `UniqueInput` object specifying a subset of columns\n            to consider for uniqueness and a strategy for keeping rows.\n\n    Returns:\n        A new `FlowDataEngine` instance with unique rows.\n    \"\"\"\n    if unique_input is None or unique_input.columns is None:\n        return FlowDataEngine(self.data_frame.unique())\n    return FlowDataEngine(self.data_frame.unique(unique_input.columns, keep=unique_input.strategy))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.output","title":"<code>output(output_fs, flow_id, node_id, execute_remote=True)</code>","text":"<p>Writes the DataFrame to an output file.</p> <p>Can execute the write operation locally or in a remote worker process.</p> <p>Parameters:</p> Name Type Description Default <code>output_fs</code> <code>OutputSettings</code> <p>An <code>OutputSettings</code> object with details about the output file.</p> required <code>flow_id</code> <code>int</code> <p>The flow ID for tracking.</p> required <code>node_id</code> <code>int | str</code> <p>The node ID for tracking.</p> required <code>execute_remote</code> <code>bool</code> <p>If True, executes the write in a worker process.</p> <code>True</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>The same <code>FlowDataEngine</code> instance for chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def output(\n    self, output_fs: input_schema.OutputSettings, flow_id: int, node_id: int | str, execute_remote: bool = True\n) -&gt; FlowDataEngine:\n    \"\"\"Writes the DataFrame to an output file.\n\n    Can execute the write operation locally or in a remote worker process.\n\n    Args:\n        output_fs: An `OutputSettings` object with details about the output file.\n        flow_id: The flow ID for tracking.\n        node_id: The node ID for tracking.\n        execute_remote: If True, executes the write in a worker process.\n\n    Returns:\n        The same `FlowDataEngine` instance for chaining.\n    \"\"\"\n    logger.info(\"Starting to write output\")\n    if execute_remote:\n        status = utils.write_output(\n            self.data_frame,\n            data_type=output_fs.file_type,\n            path=output_fs.abs_file_path,\n            write_mode=output_fs.write_mode,\n            sheet_name=output_fs.sheet_name,\n            delimiter=output_fs.delimiter,\n            flow_id=flow_id,\n            node_id=node_id,\n        )\n        tracker = ExternalExecutorTracker(status)\n        tracker.get_result()\n        logger.info(\"Finished writing output\")\n    else:\n        logger.info(\"Starting to write results locally\")\n        utils.local_write_output(\n            self.data_frame,\n            data_type=output_fs.file_type,\n            path=output_fs.abs_file_path,\n            write_mode=output_fs.write_mode,\n            sheet_name=output_fs.sheet_name,\n            delimiter=output_fs.delimiter,\n            flow_id=flow_id,\n            node_id=node_id,\n        )\n        logger.info(\"Finished writing output\")\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.reorganize_order","title":"<code>reorganize_order(column_order)</code>","text":"<p>Reorganizes columns into a specified order.</p> <p>Parameters:</p> Name Type Description Default <code>column_order</code> <code>list[str]</code> <p>A list of column names in the desired order.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the columns reordered.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def reorganize_order(self, column_order: list[str]) -&gt; FlowDataEngine:\n    \"\"\"Reorganizes columns into a specified order.\n\n    Args:\n        column_order: A list of column names in the desired order.\n\n    Returns:\n        A new `FlowDataEngine` instance with the columns reordered.\n    \"\"\"\n    df = self.data_frame.select(column_order)\n    schema = sorted(self.schema, key=lambda x: column_order.index(x.column_name))\n    return FlowDataEngine(df, schema=schema, number_of_records=self.number_of_records)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.save","title":"<code>save(path, data_type='parquet')</code>","text":"<p>Saves the DataFrame to a file in a separate thread.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to save to.</p> required <code>data_type</code> <code>str</code> <p>The format to save in (e.g., 'parquet', 'csv').</p> <code>'parquet'</code> <p>Returns:</p> Type Description <code>Future</code> <p>A <code>loky.Future</code> object representing the asynchronous save operation.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def save(self, path: str, data_type: str = \"parquet\") -&gt; Future:\n    \"\"\"Saves the DataFrame to a file in a separate thread.\n\n    Args:\n        path: The file path to save to.\n        data_type: The format to save in (e.g., 'parquet', 'csv').\n\n    Returns:\n        A `loky.Future` object representing the asynchronous save operation.\n    \"\"\"\n    estimated_size = deepcopy(self.get_estimated_file_size() * 4)\n    df = deepcopy(self.data_frame)\n    return write_threaded(_df=df, path=path, data_type=data_type, estimated_size=estimated_size)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.select_columns","title":"<code>select_columns(list_select)</code>","text":"<p>Selects a subset of columns from the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>list_select</code> <code>list[str] | tuple[str] | str</code> <p>A list, tuple, or single string of column names to select.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance containing only the selected columns.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def select_columns(self, list_select: list[str] | tuple[str] | str) -&gt; FlowDataEngine:\n    \"\"\"Selects a subset of columns from the DataFrame.\n\n    Args:\n        list_select: A list, tuple, or single string of column names to select.\n\n    Returns:\n        A new `FlowDataEngine` instance containing only the selected columns.\n    \"\"\"\n    if isinstance(list_select, str):\n        list_select = [list_select]\n\n    idx_to_keep = [self.cols_idx.get(c) for c in list_select]\n    selects = [ls for ls, id_to_keep in zip(list_select, idx_to_keep, strict=False) if id_to_keep is not None]\n    new_schema = [self.schema[i] for i in idx_to_keep if i is not None]\n\n    return FlowDataEngine(\n        self.data_frame.select(selects),\n        number_of_records=self.number_of_records,\n        schema=new_schema,\n        streamable=self._streamable,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.set_streamable","title":"<code>set_streamable(streamable=False)</code>","text":"<p>Sets whether DataFrame operations should be streamable.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def set_streamable(self, streamable: bool = False):\n    \"\"\"Sets whether DataFrame operations should be streamable.\"\"\"\n    self._streamable = streamable\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.solve_graph","title":"<code>solve_graph(graph_solver_input)</code>","text":"<p>Solves a graph problem represented by 'from' and 'to' columns.</p> <p>This is used for operations like finding connected components in a graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph_solver_input</code> <code>GraphSolverInput</code> <p>A <code>GraphSolverInput</code> object defining the source, destination, and output column names.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the solved graph data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def solve_graph(self, graph_solver_input: transform_schemas.GraphSolverInput) -&gt; FlowDataEngine:\n    \"\"\"Solves a graph problem represented by 'from' and 'to' columns.\n\n    This is used for operations like finding connected components in a graph.\n\n    Args:\n        graph_solver_input: A `GraphSolverInput` object defining the source,\n            destination, and output column names.\n\n    Returns:\n        A new `FlowDataEngine` instance with the solved graph data.\n    \"\"\"\n    lf = self.data_frame.with_columns(\n        graph_solver(graph_solver_input.col_from, graph_solver_input.col_to).alias(\n            graph_solver_input.output_column_name\n        )\n    )\n    return FlowDataEngine(lf)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.split","title":"<code>split(split_input)</code>","text":"<p>Splits a column's text values into multiple rows based on a delimiter.</p> <p>This operation is often referred to as \"exploding\" the DataFrame, as it increases the number of rows.</p> <p>Parameters:</p> Name Type Description Default <code>split_input</code> <code>TextToRowsInput</code> <p>A <code>TextToRowsInput</code> object specifying the column to split, the delimiter, and the output column name.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the exploded rows.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def split(self, split_input: transform_schemas.TextToRowsInput) -&gt; FlowDataEngine:\n    \"\"\"Splits a column's text values into multiple rows based on a delimiter.\n\n    This operation is often referred to as \"exploding\" the DataFrame, as it\n    increases the number of rows.\n\n    Args:\n        split_input: A `TextToRowsInput` object specifying the column to split,\n            the delimiter, and the output column name.\n\n    Returns:\n        A new `FlowDataEngine` instance with the exploded rows.\n    \"\"\"\n    output_column_name = (\n        split_input.output_column_name if split_input.output_column_name else split_input.column_to_split\n    )\n\n    split_value = (\n        split_input.split_fixed_value if split_input.split_by_fixed_value else pl.col(split_input.split_by_column)\n    )\n\n    df = self.data_frame.with_columns(\n        pl.col(split_input.column_to_split).str.split(by=split_value).alias(output_column_name)\n    ).explode(output_column_name)\n\n    return FlowDataEngine(df)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.start_fuzzy_join","title":"<code>start_fuzzy_join(fuzzy_match_input, other, file_ref, flow_id=-1, node_id=-1)</code>","text":"<p>Starts a fuzzy join operation in a background process.</p> <p>This method prepares the data and initiates the fuzzy matching in a separate process, returning a tracker object immediately.</p> <p>Parameters:</p> Name Type Description Default <code>fuzzy_match_input</code> <code>FuzzyMatchInput</code> <p>A <code>FuzzyMatchInput</code> object with the matching parameters.</p> required <code>other</code> <code>FlowDataEngine</code> <p>The right <code>FlowDataEngine</code> to join with.</p> required <code>file_ref</code> <code>str</code> <p>A reference string for temporary files.</p> required <code>flow_id</code> <code>int</code> <p>The flow ID for tracking.</p> <code>-1</code> <code>node_id</code> <code>int | str</code> <p>The node ID for tracking.</p> <code>-1</code> <p>Returns:</p> Type Description <code>ExternalFuzzyMatchFetcher</code> <p>An <code>ExternalFuzzyMatchFetcher</code> object that can be used to track the</p> <code>ExternalFuzzyMatchFetcher</code> <p>progress and retrieve the result of the fuzzy join.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def start_fuzzy_join(\n    self,\n    fuzzy_match_input: transform_schemas.FuzzyMatchInput,\n    other: FlowDataEngine,\n    file_ref: str,\n    flow_id: int = -1,\n    node_id: int | str = -1,\n) -&gt; ExternalFuzzyMatchFetcher:\n    \"\"\"Starts a fuzzy join operation in a background process.\n\n    This method prepares the data and initiates the fuzzy matching in a\n    separate process, returning a tracker object immediately.\n\n    Args:\n        fuzzy_match_input: A `FuzzyMatchInput` object with the matching parameters.\n        other: The right `FlowDataEngine` to join with.\n        file_ref: A reference string for temporary files.\n        flow_id: The flow ID for tracking.\n        node_id: The node ID for tracking.\n\n    Returns:\n        An `ExternalFuzzyMatchFetcher` object that can be used to track the\n        progress and retrieve the result of the fuzzy join.\n    \"\"\"\n    fuzzy_match_input_manager = transform_schemas.FuzzyMatchInputManager(fuzzy_match_input)\n    left_df, right_df = prepare_for_fuzzy_match(\n        left=self, right=other, fuzzy_match_input_manager=fuzzy_match_input_manager\n    )\n\n    return ExternalFuzzyMatchFetcher(\n        left_df,\n        right_df,\n        fuzzy_maps=fuzzy_match_input_manager.fuzzy_maps,\n        file_ref=file_ref + \"_fm\",\n        wait_on_completion=False,\n        flow_id=flow_id,\n        node_id=node_id,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.to_arrow","title":"<code>to_arrow()</code>","text":"<p>Converts the DataFrame to a PyArrow Table.</p> <p>This method triggers a <code>.collect()</code> call if the data is lazy, then converts the resulting eager DataFrame into a <code>pyarrow.Table</code>.</p> <p>Returns:</p> Type Description <code>Table</code> <p>A <code>pyarrow.Table</code> instance representing the data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def to_arrow(self) -&gt; PaTable:\n    \"\"\"Converts the DataFrame to a PyArrow Table.\n\n    This method triggers a `.collect()` call if the data is lazy,\n    then converts the resulting eager DataFrame into a `pyarrow.Table`.\n\n    Returns:\n        A `pyarrow.Table` instance representing the data.\n    \"\"\"\n    if self.lazy:\n        return self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\").to_arrow()\n    else:\n        return self.data_frame.to_arrow()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.to_cloud_storage_obj","title":"<code>to_cloud_storage_obj(settings)</code>","text":"<p>Writes the DataFrame to an object in cloud storage.</p> <p>This method supports writing to various cloud storage providers like AWS S3, Azure Data Lake Storage, and Google Cloud Storage.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>CloudStorageWriteSettingsInternal</code> <p>A <code>CloudStorageWriteSettingsInternal</code> object containing connection details, file format, and write options.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified file format is not supported for writing.</p> <code>NotImplementedError</code> <p>If the 'append' write mode is used with an unsupported format.</p> <code>Exception</code> <p>If the write operation to cloud storage fails for any reason.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def to_cloud_storage_obj(self, settings: cloud_storage_schemas.CloudStorageWriteSettingsInternal):\n    \"\"\"Writes the DataFrame to an object in cloud storage.\n\n    This method supports writing to various cloud storage providers like AWS S3,\n    Azure Data Lake Storage, and Google Cloud Storage.\n\n    Args:\n        settings: A `CloudStorageWriteSettingsInternal` object containing connection\n            details, file format, and write options.\n\n    Raises:\n        ValueError: If the specified file format is not supported for writing.\n        NotImplementedError: If the 'append' write mode is used with an unsupported format.\n        Exception: If the write operation to cloud storage fails for any reason.\n    \"\"\"\n    connection = settings.connection\n    write_settings = settings.write_settings\n\n    logger.info(f\"Writing to {connection.storage_type} storage: {write_settings.resource_path}\")\n\n    if write_settings.write_mode == \"append\" and write_settings.file_format != \"delta\":\n        raise NotImplementedError(\"The 'append' write mode is not yet supported for this destination.\")\n    storage_options = CloudStorageReader.get_storage_options(connection)\n    credential_provider = CloudStorageReader.get_credential_provider(connection)\n    # Dispatch to the correct writer based on file format\n    if write_settings.file_format == \"parquet\":\n        self._write_parquet_to_cloud(\n            write_settings.resource_path, storage_options, credential_provider, write_settings\n        )\n    elif write_settings.file_format == \"delta\":\n        self._write_delta_to_cloud(\n            write_settings.resource_path, storage_options, credential_provider, write_settings\n        )\n    elif write_settings.file_format == \"csv\":\n        self._write_csv_to_cloud(write_settings.resource_path, storage_options, credential_provider, write_settings)\n    elif write_settings.file_format == \"json\":\n        self._write_json_to_cloud(\n            write_settings.resource_path, storage_options, credential_provider, write_settings\n        )\n    else:\n        raise ValueError(f\"Unsupported file format for writing: {write_settings.file_format}\")\n\n    logger.info(f\"Successfully wrote data to {write_settings.resource_path}\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.to_dict","title":"<code>to_dict()</code>","text":"<p>Converts the DataFrame to a Python dictionary of columns.</p> <p>Each key in the dictionary is a column name, and the corresponding value is a list of the data in that column.</p> <p>Returns:</p> Type Description <code>dict[str, list]</code> <p>A dictionary mapping column names to lists of their values.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def to_dict(self) -&gt; dict[str, list]:\n    \"\"\"Converts the DataFrame to a Python dictionary of columns.\n\n    Each key in the dictionary is a column name, and the corresponding value\n    is a list of the data in that column.\n\n    Returns:\n        A dictionary mapping column names to lists of their values.\n    \"\"\"\n    if self.lazy:\n        return self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\").to_dict(as_series=False)\n    else:\n        return self.data_frame.to_dict(as_series=False)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.to_pylist","title":"<code>to_pylist()</code>","text":"<p>Converts the DataFrame to a list of Python dictionaries.</p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list where each item is a dictionary representing a row.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def to_pylist(self) -&gt; list[dict]:\n    \"\"\"Converts the DataFrame to a list of Python dictionaries.\n\n    Returns:\n        A list where each item is a dictionary representing a row.\n    \"\"\"\n    if self.lazy:\n        return self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\").to_dicts()\n    return self.data_frame.to_dicts()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.to_raw_data","title":"<code>to_raw_data()</code>","text":"<p>Converts the DataFrame to a <code>RawData</code> schema object.</p> <p>Returns:</p> Type Description <code>RawData</code> <p>An <code>input_schema.RawData</code> object containing the schema and data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def to_raw_data(self) -&gt; input_schema.RawData:\n    \"\"\"Converts the DataFrame to a `RawData` schema object.\n\n    Returns:\n        An `input_schema.RawData` object containing the schema and data.\n    \"\"\"\n    columns = [c.get_minimal_field_info() for c in self.schema]\n    data = list(self.to_dict().values())\n    return input_schema.RawData(columns=columns, data=data)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.unpivot","title":"<code>unpivot(unpivot_input)</code>","text":"<p>Converts the DataFrame from a wide to a long format.</p> <p>This is the inverse of a pivot operation, taking columns and transforming them into <code>variable</code> and <code>value</code> rows.</p> <p>Parameters:</p> Name Type Description Default <code>unpivot_input</code> <code>UnpivotInput</code> <p>An <code>UnpivotInput</code> object specifying which columns to unpivot and which to keep as index columns.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new, unpivoted <code>FlowDataEngine</code> instance.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def unpivot(self, unpivot_input: transform_schemas.UnpivotInput) -&gt; FlowDataEngine:\n    \"\"\"Converts the DataFrame from a wide to a long format.\n\n    This is the inverse of a pivot operation, taking columns and transforming\n    them into `variable` and `value` rows.\n\n    Args:\n        unpivot_input: An `UnpivotInput` object specifying which columns to\n            unpivot and which to keep as index columns.\n\n    Returns:\n        A new, unpivoted `FlowDataEngine` instance.\n    \"\"\"\n    lf = self.data_frame\n\n    if unpivot_input.data_type_selector_expr is not None:\n        result = lf.unpivot(on=unpivot_input.data_type_selector_expr(), index=unpivot_input.index_columns)\n    elif unpivot_input.value_columns is not None:\n        result = lf.unpivot(on=unpivot_input.value_columns, index=unpivot_input.index_columns)\n    else:\n        result = lf.unpivot()\n\n    return FlowDataEngine(result)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfilecolumn","title":"FlowfileColumn","text":""},{"location":"for-developers/python-api-reference.html#flowfilecolumn_1","title":"<code>FlowfileColumn</code>","text":"<p>The <code>FlowfileColumn</code> is a data class that holds the schema and rich metadata for a single column managed by the <code>FlowDataEngine</code>.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_file_column.main.FlowfileColumn","title":"<code>flowfile_core.flowfile.flow_data_engine.flow_file_column.main.FlowfileColumn</code>  <code>dataclass</code>","text":"<p>Methods:</p> Name Description <code>__repr__</code> <p>Provides a concise, developer-friendly representation of the object.</p> <code>__str__</code> <p>Provides a detailed, readable summary of the column's metadata.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_file_column/main.py</code> <pre><code>@dataclass\nclass FlowfileColumn:\n    column_name: str\n    data_type: str\n    size: int\n    max_value: str\n    min_value: str\n    col_index: int\n    number_of_empty_values: int\n    number_of_unique_values: int\n    example_values: str\n    data_type_group: ReadableDataTypeGroup\n    __sql_type: Any | None\n    __is_unique: bool | None\n    __nullable: bool | None\n    __has_values: bool | None\n    average_value: str | None\n    __perc_unique: float | None\n\n    def __init__(self, polars_type: PlType):\n        self.data_type = convert_pl_type_to_string(polars_type.pl_datatype)\n        self.size = polars_type.count - polars_type.null_count\n        self.max_value = polars_type.max\n        self.min_value = polars_type.min\n        self.number_of_unique_values = polars_type.n_unique\n        self.number_of_empty_values = polars_type.null_count\n        self.example_values = polars_type.examples\n        self.column_name = polars_type.column_name\n        self.average_value = polars_type.mean\n        self.col_index = polars_type.col_index\n        self.__has_values = None\n        self.__nullable = None\n        self.__is_unique = None\n        self.__sql_type = None\n        self.__perc_unique = None\n        self.data_type_group = self.get_readable_datatype_group()\n\n    def __repr__(self):\n        \"\"\"\n        Provides a concise, developer-friendly representation of the object.\n        Ideal for debugging and console inspection.\n        \"\"\"\n        return (\n            f\"FlowfileColumn(name='{self.column_name}', \"\n            f\"type={self.data_type}, \"\n            f\"size={self.size}, \"\n            f\"nulls={self.number_of_empty_values})\"\n        )\n\n    def __str__(self):\n        \"\"\"\n        Provides a detailed, readable summary of the column's metadata.\n        It conditionally omits any attribute that is None, ensuring a clean output.\n        \"\"\"\n        # --- Header (Always Shown) ---\n        header = f\"&lt;FlowfileColumn: '{self.column_name}'&gt;\"\n        lines = []\n\n        # --- Core Attributes (Conditionally Shown) ---\n        if self.data_type is not None:\n            lines.append(f\"  Type: {self.data_type}\")\n        if self.size is not None:\n            lines.append(f\"  Non-Nulls: {self.size}\")\n\n        # Calculate and display nulls if possible\n        if self.size is not None and self.number_of_empty_values is not None:\n            total_entries = self.size + self.number_of_empty_values\n            if total_entries &gt; 0:\n                null_perc = (self.number_of_empty_values / total_entries) * 100\n                null_info = f\"{self.number_of_empty_values} ({null_perc:.1f}%)\"\n            else:\n                null_info = \"0 (0.0%)\"\n            lines.append(f\"  Nulls: {null_info}\")\n\n        if self.number_of_unique_values is not None:\n            lines.append(f\"  Unique: {self.number_of_unique_values}\")\n\n        # --- Conditional Stats Section ---\n        stats = []\n        if self.min_value is not None:\n            stats.append(f\"    Min: {self.min_value}\")\n        if self.max_value is not None:\n            stats.append(f\"    Max: {self.max_value}\")\n        if self.average_value is not None:\n            stats.append(f\"    Mean: {self.average_value}\")\n\n        if stats:\n            lines.append(\"  Stats:\")\n            lines.extend(stats)\n\n        # --- Conditional Examples Section ---\n        if self.example_values:\n            example_str = str(self.example_values)\n            # Truncate long example strings for cleaner display\n            if len(example_str) &gt; 70:\n                example_str = example_str[:67] + \"...\"\n            lines.append(f\"  Examples: {example_str}\")\n\n        return f\"{header}\\n\" + \"\\n\".join(lines)\n\n    @classmethod\n    def create_from_polars_type(cls, polars_type: PlType, **kwargs) -&gt; \"FlowfileColumn\":\n        for k, v in kwargs.items():\n            if hasattr(polars_type, k):\n                setattr(polars_type, k, v)\n        return cls(polars_type)\n\n    @classmethod\n    def from_input(cls, column_name: str, data_type: str, **kwargs) -&gt; \"FlowfileColumn\":\n        pl_type = cast_str_to_polars_type(data_type)\n        if pl_type is not None:\n            data_type = pl_type\n        return cls(PlType(column_name=column_name, pl_datatype=data_type, **kwargs))\n\n    @classmethod\n    def create_from_polars_dtype(cls, column_name: str, data_type: pl.DataType, **kwargs):\n        return cls(PlType(column_name=column_name, pl_datatype=data_type, **kwargs))\n\n    def get_minimal_field_info(self) -&gt; input_schema.MinimalFieldInfo:\n        return input_schema.MinimalFieldInfo(name=self.column_name, data_type=self.data_type)\n\n    @classmethod\n    def create_from_minimal_field_info(cls, minimal_field_info: input_schema.MinimalFieldInfo) -&gt; \"FlowfileColumn\":\n        return cls.from_input(column_name=minimal_field_info.name, data_type=minimal_field_info.data_type)\n\n    @property\n    def is_unique(self) -&gt; bool:\n        if self.__is_unique is None:\n            if self.has_values:\n                self.__is_unique = self.number_of_unique_values == self.number_of_filled_values\n            else:\n                self.__is_unique = False\n        return self.__is_unique\n\n    @property\n    def perc_unique(self) -&gt; float:\n        if self.__perc_unique is None:\n            self.__perc_unique = self.number_of_unique_values / self.number_of_filled_values\n        return self.__perc_unique\n\n    @property\n    def has_values(self) -&gt; bool:\n        if not self.__has_values:\n            self.__has_values = self.number_of_unique_values &gt; 0\n        return self.__has_values\n\n    @property\n    def number_of_filled_values(self):\n        return self.size\n\n    @property\n    def nullable(self):\n        if self.__nullable is None:\n            self.__nullable = self.number_of_empty_values &gt; 0\n        return self.__nullable\n\n    @property\n    def name(self):\n        return self.column_name\n\n    def get_column_repr(self):\n        return dict(\n            name=self.name,\n            size=self.size,\n            data_type=str(self.data_type),\n            has_values=self.has_values,\n            is_unique=self.is_unique,\n            max_value=str(self.max_value),\n            min_value=str(self.min_value),\n            number_of_unique_values=self.number_of_unique_values,\n            number_of_filled_values=self.number_of_filled_values,\n            number_of_empty_values=self.number_of_empty_values,\n            average_size=self.average_value,\n        )\n\n    def generic_datatype(self) -&gt; DataTypeGroup:\n        if self.data_type in (\"Utf8\", \"VARCHAR\", \"CHAR\", \"NVARCHAR\", \"String\"):\n            return \"str\"\n        elif self.data_type in (\n            \"fixed_decimal\",\n            \"decimal\",\n            \"float\",\n            \"integer\",\n            \"boolean\",\n            \"double\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"Binary\",\n            \"Boolean\",\n            \"Uint8\",\n            \"Uint16\",\n            \"Uint32\",\n            \"Uint64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n        ):\n            return \"numeric\"\n        elif self.data_type in (\"datetime\", \"date\", \"Date\", \"Datetime\", \"Time\"):\n            return \"date\"\n        else:\n            return \"str\"\n\n    def get_readable_datatype_group(self) -&gt; ReadableDataTypeGroup:\n        if self.data_type in (\"Utf8\", \"VARCHAR\", \"CHAR\", \"NVARCHAR\", \"String\"):\n            return \"String\"\n        elif self.data_type in (\n            \"fixed_decimal\",\n            \"decimal\",\n            \"float\",\n            \"integer\",\n            \"boolean\",\n            \"double\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"Binary\",\n            \"Boolean\",\n            \"Uint8\",\n            \"Uint16\",\n            \"Uint32\",\n            \"Uint64\",\n        ):\n            return \"Numeric\"\n        elif self.data_type in (\"datetime\", \"date\", \"Date\", \"Datetime\", \"Time\"):\n            return \"Date\"\n        else:\n            return \"Other\"\n\n    def get_polars_type(self) -&gt; PlType:\n        pl_datatype = cast_str_to_polars_type(self.data_type)\n        pl_type = PlType(pl_datatype=pl_datatype, **self.__dict__)\n        return pl_type\n\n    def update_type_from_polars_type(self, pl_type: PlType):\n        self.data_type = str(pl_type.pl_datatype.base_type())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_file_column.main.FlowfileColumn.__repr__","title":"<code>__repr__()</code>","text":"<p>Provides a concise, developer-friendly representation of the object. Ideal for debugging and console inspection.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_file_column/main.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    Provides a concise, developer-friendly representation of the object.\n    Ideal for debugging and console inspection.\n    \"\"\"\n    return (\n        f\"FlowfileColumn(name='{self.column_name}', \"\n        f\"type={self.data_type}, \"\n        f\"size={self.size}, \"\n        f\"nulls={self.number_of_empty_values})\"\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_file_column.main.FlowfileColumn.__str__","title":"<code>__str__()</code>","text":"<p>Provides a detailed, readable summary of the column's metadata. It conditionally omits any attribute that is None, ensuring a clean output.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_file_column/main.py</code> <pre><code>def __str__(self):\n    \"\"\"\n    Provides a detailed, readable summary of the column's metadata.\n    It conditionally omits any attribute that is None, ensuring a clean output.\n    \"\"\"\n    # --- Header (Always Shown) ---\n    header = f\"&lt;FlowfileColumn: '{self.column_name}'&gt;\"\n    lines = []\n\n    # --- Core Attributes (Conditionally Shown) ---\n    if self.data_type is not None:\n        lines.append(f\"  Type: {self.data_type}\")\n    if self.size is not None:\n        lines.append(f\"  Non-Nulls: {self.size}\")\n\n    # Calculate and display nulls if possible\n    if self.size is not None and self.number_of_empty_values is not None:\n        total_entries = self.size + self.number_of_empty_values\n        if total_entries &gt; 0:\n            null_perc = (self.number_of_empty_values / total_entries) * 100\n            null_info = f\"{self.number_of_empty_values} ({null_perc:.1f}%)\"\n        else:\n            null_info = \"0 (0.0%)\"\n        lines.append(f\"  Nulls: {null_info}\")\n\n    if self.number_of_unique_values is not None:\n        lines.append(f\"  Unique: {self.number_of_unique_values}\")\n\n    # --- Conditional Stats Section ---\n    stats = []\n    if self.min_value is not None:\n        stats.append(f\"    Min: {self.min_value}\")\n    if self.max_value is not None:\n        stats.append(f\"    Max: {self.max_value}\")\n    if self.average_value is not None:\n        stats.append(f\"    Mean: {self.average_value}\")\n\n    if stats:\n        lines.append(\"  Stats:\")\n        lines.extend(stats)\n\n    # --- Conditional Examples Section ---\n    if self.example_values:\n        example_str = str(self.example_values)\n        # Truncate long example strings for cleaner display\n        if len(example_str) &gt; 70:\n            example_str = example_str[:67] + \"...\"\n        lines.append(f\"  Examples: {example_str}\")\n\n    return f\"{header}\\n\" + \"\\n\".join(lines)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#data-modeling-schemas","title":"Data Modeling (Schemas)","text":"<p>This section documents the Pydantic models that define the structure of settings and data.</p>"},{"location":"for-developers/python-api-reference.html#schemas","title":"<code>schemas</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas","title":"<code>flowfile_core.schemas.schemas</code>","text":"<p>Classes:</p> Name Description <code>FlowGraphConfig</code> <p>Configuration model for a flow graph's basic properties.</p> <code>FlowInformation</code> <p>Represents the complete state of a flow, including settings, nodes, and connections.</p> <code>FlowSettings</code> <p>Extends FlowGraphConfig with additional operational settings for a flow.</p> <code>FlowfileData</code> <p>Root model for flowfile serialization (YAML/JSON).</p> <code>FlowfileNode</code> <p>Node representation for flowfile serialization (YAML/JSON).</p> <code>FlowfileSettings</code> <p>Settings for flowfile serialization (YAML/JSON).</p> <code>NodeConnection</code> <p>Represents a connection between two nodes in the flow.</p> <code>NodeDefault</code> <p>Defines default properties for a node type.</p> <code>NodeEdge</code> <p>Represents a connection (edge) between two nodes in the frontend.</p> <code>NodeInformation</code> <p>Stores the state and configuration of a specific node instance within a flow.</p> <code>NodeInput</code> <p>Represents a node as it is received from the frontend, including position.</p> <code>NodeTemplate</code> <p>Defines the template for a node type, specifying its UI and functional characteristics.</p> <code>RawLogInput</code> <p>Schema for a raw log message.</p> <code>VueFlowInput</code> <p>Represents the complete graph structure from the Vue-based frontend.</p> <p>Functions:</p> Name Description <code>get_global_execution_location</code> <p>Calculates the default execution location based on the global settings</p> <code>get_settings_class_for_node_type</code> <p>Get the settings class for a node type, supporting both standard and user-defined nodes.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowGraphConfig","title":"<code>FlowGraphConfig</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration model for a flow graph's basic properties.</p> <p>Attributes:</p> Name Type Description <code>flow_id</code> <code>int</code> <p>Unique identifier for the flow.</p> <code>description</code> <code>Optional[str]</code> <p>A description of the flow.</p> <code>save_location</code> <code>Optional[str]</code> <p>The location where the flow is saved.</p> <code>name</code> <code>str</code> <p>The name of the flow.</p> <code>path</code> <code>str</code> <p>The file path associated with the flow.</p> <code>execution_mode</code> <code>ExecutionModeLiteral</code> <p>The mode of execution ('Development' or 'Performance').</p> <code>execution_location</code> <code>ExecutionLocationsLiteral</code> <p>The location for execution ('local', 'remote').</p> <code>max_parallel_workers</code> <code>int</code> <p>Maximum number of threads used for parallel node execution within a stage. Set to 1 to disable parallelism. Defaults to 4.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Configuration model for a flow graph's basic properties.\\n\\nAttributes:\\n    flow_id (int): Unique identifier for the flow.\\n    description (Optional[str]): A description of the flow.\\n    save_location (Optional[str]): The location where the flow is saved.\\n    name (str): The name of the flow.\\n    path (str): The file path associated with the flow.\\n    execution_mode (ExecutionModeLiteral): The mode of execution ('Development' or 'Performance').\\n    execution_location (ExecutionLocationsLiteral): The location for execution ('local', 'remote').\\n    max_parallel_workers (int): Maximum number of threads used for parallel node execution within a\\n        stage. Set to 1 to disable parallelism. Defaults to 4.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"description\": \"Unique identifier for the flow.\",\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Description\"\n    },\n    \"save_location\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Save Location\"\n    },\n    \"name\": {\n      \"default\": \"\",\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"path\": {\n      \"default\": \"\",\n      \"title\": \"Path\",\n      \"type\": \"string\"\n    },\n    \"source_registration_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Catalog registration ID when running a registered flow.\",\n      \"title\": \"Source Registration Id\"\n    },\n    \"execution_mode\": {\n      \"default\": \"Performance\",\n      \"enum\": [\n        \"Development\",\n        \"Performance\"\n      ],\n      \"title\": \"Execution Mode\",\n      \"type\": \"string\"\n    },\n    \"execution_location\": {\n      \"enum\": [\n        \"local\",\n        \"remote\"\n      ],\n      \"title\": \"Execution Location\",\n      \"type\": \"string\"\n    },\n    \"max_parallel_workers\": {\n      \"default\": 4,\n      \"description\": \"Max threads for parallel node execution.\",\n      \"minimum\": 1,\n      \"title\": \"Max Parallel Workers\",\n      \"type\": \"integer\"\n    }\n  },\n  \"title\": \"FlowGraphConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>save_location</code>                 (<code>str | None</code>)             </li> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>path</code>                 (<code>str</code>)             </li> <li> <code>source_registration_id</code>                 (<code>int | None</code>)             </li> <li> <code>execution_mode</code>                 (<code>ExecutionModeLiteral</code>)             </li> <li> <code>execution_location</code>                 (<code>ExecutionLocationsLiteral</code>)             </li> <li> <code>max_parallel_workers</code>                 (<code>int</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_and_set_execution_location</code>                 \u2192                   <code>execution_location</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class FlowGraphConfig(BaseModel):\n    \"\"\"\n    Configuration model for a flow graph's basic properties.\n\n    Attributes:\n        flow_id (int): Unique identifier for the flow.\n        description (Optional[str]): A description of the flow.\n        save_location (Optional[str]): The location where the flow is saved.\n        name (str): The name of the flow.\n        path (str): The file path associated with the flow.\n        execution_mode (ExecutionModeLiteral): The mode of execution ('Development' or 'Performance').\n        execution_location (ExecutionLocationsLiteral): The location for execution ('local', 'remote').\n        max_parallel_workers (int): Maximum number of threads used for parallel node execution within a\n            stage. Set to 1 to disable parallelism. Defaults to 4.\n    \"\"\"\n\n    flow_id: int = Field(default_factory=create_unique_id, description=\"Unique identifier for the flow.\")\n    description: str | None = None\n    save_location: str | None = None\n    name: str = \"\"\n    path: str = \"\"\n    source_registration_id: int | None = Field(\n        default=None,\n        description=\"Catalog registration ID when running a registered flow.\",\n    )\n    execution_mode: ExecutionModeLiteral = \"Performance\"\n    execution_location: ExecutionLocationsLiteral = Field(default_factory=get_global_execution_location)\n    max_parallel_workers: int = Field(default=4, ge=1, description=\"Max threads for parallel node execution.\")\n\n    @field_validator(\"execution_location\", mode=\"before\")\n    def validate_and_set_execution_location(cls, v: ExecutionLocationsLiteral | None) -&gt; ExecutionLocationsLiteral:\n        \"\"\"\n        Validates and sets the execution location.\n        1.  **If `None` is provided**: It defaults to the location determined by global settings.\n        2.  **If a value is provided**: It checks if the value is compatible with the global\n            settings. If not (e.g., requesting 'remote' when only 'local' is possible),\n            it corrects the value to a compatible one.\n        \"\"\"\n        if v is None:\n            return get_global_execution_location()\n        if v == \"auto\":\n            return get_global_execution_location()\n\n        return get_prio_execution_location(v, get_global_execution_location())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowGraphConfig.flow_id","title":"<code>flow_id</code>  <code>pydantic-field</code>","text":"<p>Unique identifier for the flow.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowGraphConfig.max_parallel_workers","title":"<code>max_parallel_workers = 4</code>  <code>pydantic-field</code>","text":"<p>Max threads for parallel node execution.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowGraphConfig.source_registration_id","title":"<code>source_registration_id = None</code>  <code>pydantic-field</code>","text":"<p>Catalog registration ID when running a registered flow.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowGraphConfig.validate_and_set_execution_location","title":"<code>validate_and_set_execution_location(v)</code>  <code>pydantic-validator</code>","text":"<p>Validates and sets the execution location. 1.  If <code>None</code> is provided: It defaults to the location determined by global settings. 2.  If a value is provided: It checks if the value is compatible with the global     settings. If not (e.g., requesting 'remote' when only 'local' is possible),     it corrects the value to a compatible one.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>@field_validator(\"execution_location\", mode=\"before\")\ndef validate_and_set_execution_location(cls, v: ExecutionLocationsLiteral | None) -&gt; ExecutionLocationsLiteral:\n    \"\"\"\n    Validates and sets the execution location.\n    1.  **If `None` is provided**: It defaults to the location determined by global settings.\n    2.  **If a value is provided**: It checks if the value is compatible with the global\n        settings. If not (e.g., requesting 'remote' when only 'local' is possible),\n        it corrects the value to a compatible one.\n    \"\"\"\n    if v is None:\n        return get_global_execution_location()\n    if v == \"auto\":\n        return get_global_execution_location()\n\n    return get_prio_execution_location(v, get_global_execution_location())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowInformation","title":"<code>FlowInformation</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the complete state of a flow, including settings, nodes, and connections.</p> <p>Attributes:</p> Name Type Description <code>flow_id</code> <code>int</code> <p>The unique ID of the flow.</p> <code>flow_name</code> <code>Optional[str]</code> <p>The name of the flow.</p> <code>flow_settings</code> <code>FlowSettings</code> <p>The settings for the flow.</p> <code>data</code> <code>Dict[int, NodeInformation]</code> <p>A dictionary mapping node IDs to their information.</p> <code>node_starts</code> <code>List[int]</code> <p>A list of starting node IDs.</p> <code>node_connections</code> <code>List[Tuple[int, int]]</code> <p>A list of tuples representing connections between nodes.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FlowSettings\": {\n      \"description\": \"Extends FlowGraphConfig with additional operational settings for a flow.\\n\\nAttributes:\\n    auto_save (bool): Flag to enable or disable automatic saving.\\n    modified_on (Optional[float]): Timestamp of the last modification.\\n    show_detailed_progress (bool): Flag to show detailed progress during execution.\\n    is_running (bool): Indicates if the flow is currently running.\\n    is_canceled (bool): Indicates if the flow execution has been canceled.\\n    track_history (bool): Flag to enable or disable undo/redo history tracking.\",\n      \"properties\": {\n        \"flow_id\": {\n          \"description\": \"Unique identifier for the flow.\",\n          \"title\": \"Flow Id\",\n          \"type\": \"integer\"\n        },\n        \"description\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Description\"\n        },\n        \"save_location\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Save Location\"\n        },\n        \"name\": {\n          \"default\": \"\",\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"path\": {\n          \"default\": \"\",\n          \"title\": \"Path\",\n          \"type\": \"string\"\n        },\n        \"source_registration_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Catalog registration ID when running a registered flow.\",\n          \"title\": \"Source Registration Id\"\n        },\n        \"execution_mode\": {\n          \"default\": \"Performance\",\n          \"enum\": [\n            \"Development\",\n            \"Performance\"\n          ],\n          \"title\": \"Execution Mode\",\n          \"type\": \"string\"\n        },\n        \"execution_location\": {\n          \"enum\": [\n            \"local\",\n            \"remote\"\n          ],\n          \"title\": \"Execution Location\",\n          \"type\": \"string\"\n        },\n        \"max_parallel_workers\": {\n          \"default\": 4,\n          \"description\": \"Max threads for parallel node execution.\",\n          \"minimum\": 1,\n          \"title\": \"Max Parallel Workers\",\n          \"type\": \"integer\"\n        },\n        \"auto_save\": {\n          \"default\": false,\n          \"title\": \"Auto Save\",\n          \"type\": \"boolean\"\n        },\n        \"modified_on\": {\n          \"anyOf\": [\n            {\n              \"type\": \"number\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Modified On\"\n        },\n        \"show_detailed_progress\": {\n          \"default\": true,\n          \"title\": \"Show Detailed Progress\",\n          \"type\": \"boolean\"\n        },\n        \"is_running\": {\n          \"default\": false,\n          \"title\": \"Is Running\",\n          \"type\": \"boolean\"\n        },\n        \"is_canceled\": {\n          \"default\": false,\n          \"title\": \"Is Canceled\",\n          \"type\": \"boolean\"\n        },\n        \"track_history\": {\n          \"default\": true,\n          \"title\": \"Track History\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"FlowSettings\",\n      \"type\": \"object\"\n    },\n    \"NodeInformation\": {\n      \"description\": \"Stores the state and configuration of a specific node instance within a flow.\",\n      \"properties\": {\n        \"id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Id\"\n        },\n        \"type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Type\"\n        },\n        \"is_setup\": {\n          \"anyOf\": [\n            {\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Is Setup\"\n        },\n        \"is_start_node\": {\n          \"default\": false,\n          \"title\": \"Is Start Node\",\n          \"type\": \"boolean\"\n        },\n        \"description\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": \"\",\n          \"title\": \"Description\"\n        },\n        \"node_reference\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Node Reference\"\n        },\n        \"x_position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": 0,\n          \"title\": \"X Position\"\n        },\n        \"y_position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": 0,\n          \"title\": \"Y Position\"\n        },\n        \"left_input_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Left Input Id\"\n        },\n        \"right_input_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Right Input Id\"\n        },\n        \"input_ids\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"integer\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"title\": \"Input Ids\"\n        },\n        \"outputs\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"integer\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"title\": \"Outputs\"\n        },\n        \"setting_input\": {\n          \"anyOf\": [\n            {},\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Setting Input\"\n        }\n      },\n      \"title\": \"NodeInformation\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Represents the complete state of a flow, including settings, nodes, and connections.\\n\\nAttributes:\\n    flow_id (int): The unique ID of the flow.\\n    flow_name (Optional[str]): The name of the flow.\\n    flow_settings (FlowSettings): The settings for the flow.\\n    data (Dict[int, NodeInformation]): A dictionary mapping node IDs to their information.\\n    node_starts (List[int]): A list of starting node IDs.\\n    node_connections (List[Tuple[int, int]]): A list of tuples representing connections between nodes.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"flow_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Flow Name\"\n    },\n    \"flow_settings\": {\n      \"$ref\": \"#/$defs/FlowSettings\"\n    },\n    \"data\": {\n      \"additionalProperties\": {\n        \"$ref\": \"#/$defs/NodeInformation\"\n      },\n      \"default\": {},\n      \"title\": \"Data\",\n      \"type\": \"object\"\n    },\n    \"node_starts\": {\n      \"items\": {\n        \"type\": \"integer\"\n      },\n      \"title\": \"Node Starts\",\n      \"type\": \"array\"\n    },\n    \"node_connections\": {\n      \"default\": [],\n      \"items\": {\n        \"maxItems\": 2,\n        \"minItems\": 2,\n        \"prefixItems\": [\n          {\n            \"type\": \"integer\"\n          },\n          {\n            \"type\": \"integer\"\n          }\n        ],\n        \"type\": \"array\"\n      },\n      \"title\": \"Node Connections\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"flow_settings\",\n    \"node_starts\"\n  ],\n  \"title\": \"FlowInformation\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>flow_name</code>                 (<code>str | None</code>)             </li> <li> <code>flow_settings</code>                 (<code>FlowSettings</code>)             </li> <li> <code>data</code>                 (<code>dict[int, NodeInformation]</code>)             </li> <li> <code>node_starts</code>                 (<code>list[int]</code>)             </li> <li> <code>node_connections</code>                 (<code>list[tuple[int, int]]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>ensure_string</code>                 \u2192                   <code>flow_name</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class FlowInformation(BaseModel):\n    \"\"\"\n    Represents the complete state of a flow, including settings, nodes, and connections.\n\n    Attributes:\n        flow_id (int): The unique ID of the flow.\n        flow_name (Optional[str]): The name of the flow.\n        flow_settings (FlowSettings): The settings for the flow.\n        data (Dict[int, NodeInformation]): A dictionary mapping node IDs to their information.\n        node_starts (List[int]): A list of starting node IDs.\n        node_connections (List[Tuple[int, int]]): A list of tuples representing connections between nodes.\n    \"\"\"\n\n    flow_id: int\n    flow_name: str | None = \"\"\n    flow_settings: FlowSettings\n    data: dict[int, NodeInformation] = {}\n    node_starts: list[int]\n    node_connections: list[tuple[int, int]] = []\n\n    @field_validator(\"flow_name\", mode=\"before\")\n    def ensure_string(cls, v):\n        \"\"\"\n        Validator to ensure the flow_name is always a string.\n        :param v: The value to validate.\n        :return: The value as a string, or an empty string if it's None.\n        \"\"\"\n        return str(v) if v is not None else \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowInformation.ensure_string","title":"<code>ensure_string(v)</code>  <code>pydantic-validator</code>","text":"<p>Validator to ensure the flow_name is always a string. :param v: The value to validate. :return: The value as a string, or an empty string if it's None.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>@field_validator(\"flow_name\", mode=\"before\")\ndef ensure_string(cls, v):\n    \"\"\"\n    Validator to ensure the flow_name is always a string.\n    :param v: The value to validate.\n    :return: The value as a string, or an empty string if it's None.\n    \"\"\"\n    return str(v) if v is not None else \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowSettings","title":"<code>FlowSettings</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>FlowGraphConfig</code></p> <p>Extends FlowGraphConfig with additional operational settings for a flow.</p> <p>Attributes:</p> Name Type Description <code>auto_save</code> <code>bool</code> <p>Flag to enable or disable automatic saving.</p> <code>modified_on</code> <code>Optional[float]</code> <p>Timestamp of the last modification.</p> <code>show_detailed_progress</code> <code>bool</code> <p>Flag to show detailed progress during execution.</p> <code>is_running</code> <code>bool</code> <p>Indicates if the flow is currently running.</p> <code>is_canceled</code> <code>bool</code> <p>Indicates if the flow execution has been canceled.</p> <code>track_history</code> <code>bool</code> <p>Flag to enable or disable undo/redo history tracking.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Extends FlowGraphConfig with additional operational settings for a flow.\\n\\nAttributes:\\n    auto_save (bool): Flag to enable or disable automatic saving.\\n    modified_on (Optional[float]): Timestamp of the last modification.\\n    show_detailed_progress (bool): Flag to show detailed progress during execution.\\n    is_running (bool): Indicates if the flow is currently running.\\n    is_canceled (bool): Indicates if the flow execution has been canceled.\\n    track_history (bool): Flag to enable or disable undo/redo history tracking.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"description\": \"Unique identifier for the flow.\",\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Description\"\n    },\n    \"save_location\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Save Location\"\n    },\n    \"name\": {\n      \"default\": \"\",\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"path\": {\n      \"default\": \"\",\n      \"title\": \"Path\",\n      \"type\": \"string\"\n    },\n    \"source_registration_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Catalog registration ID when running a registered flow.\",\n      \"title\": \"Source Registration Id\"\n    },\n    \"execution_mode\": {\n      \"default\": \"Performance\",\n      \"enum\": [\n        \"Development\",\n        \"Performance\"\n      ],\n      \"title\": \"Execution Mode\",\n      \"type\": \"string\"\n    },\n    \"execution_location\": {\n      \"enum\": [\n        \"local\",\n        \"remote\"\n      ],\n      \"title\": \"Execution Location\",\n      \"type\": \"string\"\n    },\n    \"max_parallel_workers\": {\n      \"default\": 4,\n      \"description\": \"Max threads for parallel node execution.\",\n      \"minimum\": 1,\n      \"title\": \"Max Parallel Workers\",\n      \"type\": \"integer\"\n    },\n    \"auto_save\": {\n      \"default\": false,\n      \"title\": \"Auto Save\",\n      \"type\": \"boolean\"\n    },\n    \"modified_on\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Modified On\"\n    },\n    \"show_detailed_progress\": {\n      \"default\": true,\n      \"title\": \"Show Detailed Progress\",\n      \"type\": \"boolean\"\n    },\n    \"is_running\": {\n      \"default\": false,\n      \"title\": \"Is Running\",\n      \"type\": \"boolean\"\n    },\n    \"is_canceled\": {\n      \"default\": false,\n      \"title\": \"Is Canceled\",\n      \"type\": \"boolean\"\n    },\n    \"track_history\": {\n      \"default\": true,\n      \"title\": \"Track History\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"title\": \"FlowSettings\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>save_location</code>                 (<code>str | None</code>)             </li> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>path</code>                 (<code>str</code>)             </li> <li> <code>source_registration_id</code>                 (<code>int | None</code>)             </li> <li> <code>execution_mode</code>                 (<code>ExecutionModeLiteral</code>)             </li> <li> <code>execution_location</code>                 (<code>ExecutionLocationsLiteral</code>)             </li> <li> <code>max_parallel_workers</code>                 (<code>int</code>)             </li> <li> <code>auto_save</code>                 (<code>bool</code>)             </li> <li> <code>modified_on</code>                 (<code>float | None</code>)             </li> <li> <code>show_detailed_progress</code>                 (<code>bool</code>)             </li> <li> <code>is_running</code>                 (<code>bool</code>)             </li> <li> <code>is_canceled</code>                 (<code>bool</code>)             </li> <li> <code>track_history</code>                 (<code>bool</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_and_set_execution_location</code>                 \u2192                   <code>execution_location</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class FlowSettings(FlowGraphConfig):\n    \"\"\"\n    Extends FlowGraphConfig with additional operational settings for a flow.\n\n    Attributes:\n        auto_save (bool): Flag to enable or disable automatic saving.\n        modified_on (Optional[float]): Timestamp of the last modification.\n        show_detailed_progress (bool): Flag to show detailed progress during execution.\n        is_running (bool): Indicates if the flow is currently running.\n        is_canceled (bool): Indicates if the flow execution has been canceled.\n        track_history (bool): Flag to enable or disable undo/redo history tracking.\n    \"\"\"\n\n    auto_save: bool = False\n    modified_on: float | None = None\n    show_detailed_progress: bool = True\n    is_running: bool = False\n    is_canceled: bool = False\n    track_history: bool = True\n\n    @classmethod\n    def from_flow_settings_input(cls, flow_graph_config: FlowGraphConfig):\n        \"\"\"\n        Creates a FlowSettings instance from a FlowGraphConfig instance.\n\n        :param flow_graph_config: The base flow graph configuration.\n        :return: A new instance of FlowSettings with data from flow_graph_config.\n        \"\"\"\n        return cls.model_validate(flow_graph_config.model_dump())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowSettings.from_flow_settings_input","title":"<code>from_flow_settings_input(flow_graph_config)</code>  <code>classmethod</code>","text":"<p>Creates a FlowSettings instance from a FlowGraphConfig instance.</p> <p>:param flow_graph_config: The base flow graph configuration. :return: A new instance of FlowSettings with data from flow_graph_config.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>@classmethod\ndef from_flow_settings_input(cls, flow_graph_config: FlowGraphConfig):\n    \"\"\"\n    Creates a FlowSettings instance from a FlowGraphConfig instance.\n\n    :param flow_graph_config: The base flow graph configuration.\n    :return: A new instance of FlowSettings with data from flow_graph_config.\n    \"\"\"\n    return cls.model_validate(flow_graph_config.model_dump())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowfileData","title":"<code>FlowfileData</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Root model for flowfile serialization (YAML/JSON).</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FlowfileNode\": {\n      \"description\": \"Node representation for flowfile serialization (YAML/JSON).\",\n      \"properties\": {\n        \"id\": {\n          \"title\": \"Id\",\n          \"type\": \"integer\"\n        },\n        \"type\": {\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"is_start_node\": {\n          \"default\": false,\n          \"title\": \"Is Start Node\",\n          \"type\": \"boolean\"\n        },\n        \"description\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": \"\",\n          \"title\": \"Description\"\n        },\n        \"node_reference\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Node Reference\"\n        },\n        \"x_position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": 0,\n          \"title\": \"X Position\"\n        },\n        \"y_position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": 0,\n          \"title\": \"Y Position\"\n        },\n        \"left_input_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Left Input Id\"\n        },\n        \"right_input_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Right Input Id\"\n        },\n        \"input_ids\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"integer\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"title\": \"Input Ids\"\n        },\n        \"outputs\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"integer\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"title\": \"Outputs\"\n        },\n        \"setting_input\": {\n          \"anyOf\": [\n            {},\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Setting Input\"\n        }\n      },\n      \"required\": [\n        \"id\",\n        \"type\"\n      ],\n      \"title\": \"FlowfileNode\",\n      \"type\": \"object\"\n    },\n    \"FlowfileSettings\": {\n      \"description\": \"Settings for flowfile serialization (YAML/JSON).\\n\\nExcludes runtime state fields like is_running, is_canceled, modified_on.\",\n      \"properties\": {\n        \"description\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Description\"\n        },\n        \"execution_mode\": {\n          \"default\": \"Performance\",\n          \"enum\": [\n            \"Development\",\n            \"Performance\"\n          ],\n          \"title\": \"Execution Mode\",\n          \"type\": \"string\"\n        },\n        \"execution_location\": {\n          \"default\": \"local\",\n          \"enum\": [\n            \"local\",\n            \"remote\"\n          ],\n          \"title\": \"Execution Location\",\n          \"type\": \"string\"\n        },\n        \"auto_save\": {\n          \"default\": false,\n          \"title\": \"Auto Save\",\n          \"type\": \"boolean\"\n        },\n        \"show_detailed_progress\": {\n          \"default\": true,\n          \"title\": \"Show Detailed Progress\",\n          \"type\": \"boolean\"\n        },\n        \"max_parallel_workers\": {\n          \"default\": 4,\n          \"minimum\": 1,\n          \"title\": \"Max Parallel Workers\",\n          \"type\": \"integer\"\n        },\n        \"source_registration_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Source Registration Id\"\n        }\n      },\n      \"title\": \"FlowfileSettings\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Root model for flowfile serialization (YAML/JSON).\",\n  \"properties\": {\n    \"flowfile_version\": {\n      \"title\": \"Flowfile Version\",\n      \"type\": \"string\"\n    },\n    \"flowfile_id\": {\n      \"title\": \"Flowfile Id\",\n      \"type\": \"integer\"\n    },\n    \"flowfile_name\": {\n      \"title\": \"Flowfile Name\",\n      \"type\": \"string\"\n    },\n    \"flowfile_settings\": {\n      \"$ref\": \"#/$defs/FlowfileSettings\"\n    },\n    \"nodes\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/FlowfileNode\"\n      },\n      \"title\": \"Nodes\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"flowfile_version\",\n    \"flowfile_id\",\n    \"flowfile_name\",\n    \"flowfile_settings\",\n    \"nodes\"\n  ],\n  \"title\": \"FlowfileData\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flowfile_version</code>                 (<code>str</code>)             </li> <li> <code>flowfile_id</code>                 (<code>int</code>)             </li> <li> <code>flowfile_name</code>                 (<code>str</code>)             </li> <li> <code>flowfile_settings</code>                 (<code>FlowfileSettings</code>)             </li> <li> <code>nodes</code>                 (<code>list[FlowfileNode]</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class FlowfileData(BaseModel):\n    \"\"\"Root model for flowfile serialization (YAML/JSON).\"\"\"\n\n    flowfile_version: str\n    flowfile_id: int\n    flowfile_name: str\n    flowfile_settings: FlowfileSettings\n    nodes: list[FlowfileNode]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowfileNode","title":"<code>FlowfileNode</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Node representation for flowfile serialization (YAML/JSON).</p> Show JSON schema: <pre><code>{\n  \"description\": \"Node representation for flowfile serialization (YAML/JSON).\",\n  \"properties\": {\n    \"id\": {\n      \"title\": \"Id\",\n      \"type\": \"integer\"\n    },\n    \"type\": {\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"is_start_node\": {\n      \"default\": false,\n      \"title\": \"Is Start Node\",\n      \"type\": \"boolean\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"x_position\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"X Position\"\n    },\n    \"y_position\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Y Position\"\n    },\n    \"left_input_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Left Input Id\"\n    },\n    \"right_input_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Right Input Id\"\n    },\n    \"input_ids\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"integer\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Input Ids\"\n    },\n    \"outputs\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"integer\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Outputs\"\n    },\n    \"setting_input\": {\n      \"anyOf\": [\n        {},\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Setting Input\"\n    }\n  },\n  \"required\": [\n    \"id\",\n    \"type\"\n  ],\n  \"title\": \"FlowfileNode\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>int</code>)             </li> <li> <code>type</code>                 (<code>str</code>)             </li> <li> <code>is_start_node</code>                 (<code>bool</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>x_position</code>                 (<code>int | None</code>)             </li> <li> <code>y_position</code>                 (<code>int | None</code>)             </li> <li> <code>left_input_id</code>                 (<code>int | None</code>)             </li> <li> <code>right_input_id</code>                 (<code>int | None</code>)             </li> <li> <code>input_ids</code>                 (<code>list[int] | None</code>)             </li> <li> <code>outputs</code>                 (<code>list[int] | None</code>)             </li> <li> <code>setting_input</code>                 (<code>Any | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class FlowfileNode(BaseModel):\n    \"\"\"Node representation for flowfile serialization (YAML/JSON).\"\"\"\n\n    id: int\n    type: str\n    is_start_node: bool = False\n    description: str | None = \"\"\n    node_reference: str | None = None  # Unique reference identifier for code generation\n    x_position: int | None = 0\n    y_position: int | None = 0\n    left_input_id: int | None = None\n    right_input_id: int | None = None\n    input_ids: list[int] | None = Field(default_factory=list)\n    outputs: list[int] | None = Field(default_factory=list)\n    setting_input: Any | None = None\n\n    _setting_input_exclude: ClassVar[set] = {\n        \"flow_id\",\n        \"node_id\",\n        \"pos_x\",\n        \"pos_y\",\n        \"is_setup\",\n        \"description\",\n        \"node_reference\",\n        \"user_id\",\n        \"is_flow_output\",\n        \"is_user_defined\",\n        \"depending_on_id\",\n        \"depending_on_ids\",\n    }\n\n    @field_serializer(\"setting_input\")\n    def serialize_setting_input(self, value, _info):\n        if value is None:\n            return None\n        if isinstance(value, input_schema.NodePromise):\n            return None\n        if hasattr(value, \"to_yaml_dict\"):\n            return value.to_yaml_dict()\n        if hasattr(value, \"to_yaml_dict\"):\n            return value.to_yaml_dict()\n        return value.model_dump(exclude=self._setting_input_exclude)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowfileSettings","title":"<code>FlowfileSettings</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Settings for flowfile serialization (YAML/JSON).</p> <p>Excludes runtime state fields like is_running, is_canceled, modified_on.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Settings for flowfile serialization (YAML/JSON).\\n\\nExcludes runtime state fields like is_running, is_canceled, modified_on.\",\n  \"properties\": {\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Description\"\n    },\n    \"execution_mode\": {\n      \"default\": \"Performance\",\n      \"enum\": [\n        \"Development\",\n        \"Performance\"\n      ],\n      \"title\": \"Execution Mode\",\n      \"type\": \"string\"\n    },\n    \"execution_location\": {\n      \"default\": \"local\",\n      \"enum\": [\n        \"local\",\n        \"remote\"\n      ],\n      \"title\": \"Execution Location\",\n      \"type\": \"string\"\n    },\n    \"auto_save\": {\n      \"default\": false,\n      \"title\": \"Auto Save\",\n      \"type\": \"boolean\"\n    },\n    \"show_detailed_progress\": {\n      \"default\": true,\n      \"title\": \"Show Detailed Progress\",\n      \"type\": \"boolean\"\n    },\n    \"max_parallel_workers\": {\n      \"default\": 4,\n      \"minimum\": 1,\n      \"title\": \"Max Parallel Workers\",\n      \"type\": \"integer\"\n    },\n    \"source_registration_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Source Registration Id\"\n    }\n  },\n  \"title\": \"FlowfileSettings\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>execution_mode</code>                 (<code>ExecutionModeLiteral</code>)             </li> <li> <code>execution_location</code>                 (<code>ExecutionLocationsLiteral</code>)             </li> <li> <code>auto_save</code>                 (<code>bool</code>)             </li> <li> <code>show_detailed_progress</code>                 (<code>bool</code>)             </li> <li> <code>max_parallel_workers</code>                 (<code>int</code>)             </li> <li> <code>source_registration_id</code>                 (<code>int | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class FlowfileSettings(BaseModel):\n    \"\"\"Settings for flowfile serialization (YAML/JSON).\n\n    Excludes runtime state fields like is_running, is_canceled, modified_on.\n    \"\"\"\n\n    description: str | None = None\n    execution_mode: ExecutionModeLiteral = \"Performance\"\n    execution_location: ExecutionLocationsLiteral = \"local\"\n    auto_save: bool = False\n    show_detailed_progress: bool = True\n    max_parallel_workers: int = Field(default=4, ge=1)\n    source_registration_id: int | None = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.NodeConnection","title":"<code>NodeConnection</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a connection between two nodes in the flow.</p> <p>Attributes:</p> Name Type Description <code>from_node_id</code> <code>int</code> <p>The ID of the source node.</p> <code>to_node_id</code> <code>int</code> <p>The ID of the target node.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Represents a connection between two nodes in the flow.\\n\\nAttributes:\\n    from_node_id (int): The ID of the source node.\\n    to_node_id (int): The ID of the target node.\",\n  \"properties\": {\n    \"from_node_id\": {\n      \"title\": \"From Node Id\",\n      \"type\": \"integer\"\n    },\n    \"to_node_id\": {\n      \"title\": \"To Node Id\",\n      \"type\": \"integer\"\n    }\n  },\n  \"required\": [\n    \"from_node_id\",\n    \"to_node_id\"\n  ],\n  \"title\": \"NodeConnection\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>frozen</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>from_node_id</code>                 (<code>int</code>)             </li> <li> <code>to_node_id</code>                 (<code>int</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class NodeConnection(BaseModel):\n    \"\"\"\n    Represents a connection between two nodes in the flow.\n\n    Attributes:\n        from_node_id (int): The ID of the source node.\n        to_node_id (int): The ID of the target node.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    from_node_id: int\n    to_node_id: int\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.NodeDefault","title":"<code>NodeDefault</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines default properties for a node type.</p> <p>Attributes:</p> Name Type Description <code>node_name</code> <code>str</code> <p>The name of the node.</p> <code>node_type</code> <code>NodeTypeLiteral</code> <p>The functional type of the node ('input', 'output', 'process').</p> <code>transform_type</code> <code>TransformTypeLiteral</code> <p>The data transformation behavior ('narrow', 'wide', 'other').</p> <code>has_default_settings</code> <code>Optional[Any]</code> <p>Indicates if the node has predefined default settings.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines default properties for a node type.\\n\\nAttributes:\\n    node_name (str): The name of the node.\\n    node_type (NodeTypeLiteral): The functional type of the node ('input', 'output', 'process').\\n    transform_type (TransformTypeLiteral): The data transformation behavior ('narrow', 'wide', 'other').\\n    has_default_settings (Optional[Any]): Indicates if the node has predefined default settings.\",\n  \"properties\": {\n    \"node_name\": {\n      \"title\": \"Node Name\",\n      \"type\": \"string\"\n    },\n    \"node_type\": {\n      \"enum\": [\n        \"input\",\n        \"output\",\n        \"process\"\n      ],\n      \"title\": \"Node Type\",\n      \"type\": \"string\"\n    },\n    \"transform_type\": {\n      \"enum\": [\n        \"narrow\",\n        \"wide\",\n        \"other\"\n      ],\n      \"title\": \"Transform Type\",\n      \"type\": \"string\"\n    },\n    \"has_default_settings\": {\n      \"anyOf\": [\n        {},\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Has Default Settings\"\n    }\n  },\n  \"required\": [\n    \"node_name\",\n    \"node_type\",\n    \"transform_type\"\n  ],\n  \"title\": \"NodeDefault\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>node_name</code>                 (<code>str</code>)             </li> <li> <code>node_type</code>                 (<code>NodeTypeLiteral</code>)             </li> <li> <code>transform_type</code>                 (<code>TransformTypeLiteral</code>)             </li> <li> <code>has_default_settings</code>                 (<code>Any | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class NodeDefault(BaseModel):\n    \"\"\"\n    Defines default properties for a node type.\n\n    Attributes:\n        node_name (str): The name of the node.\n        node_type (NodeTypeLiteral): The functional type of the node ('input', 'output', 'process').\n        transform_type (TransformTypeLiteral): The data transformation behavior ('narrow', 'wide', 'other').\n        has_default_settings (Optional[Any]): Indicates if the node has predefined default settings.\n    \"\"\"\n\n    node_name: str\n    node_type: NodeTypeLiteral\n    transform_type: TransformTypeLiteral\n    has_default_settings: Any | None = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.NodeEdge","title":"<code>NodeEdge</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a connection (edge) between two nodes in the frontend.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>A unique identifier for the edge.</p> <code>source</code> <code>str</code> <p>The ID of the source node.</p> <code>target</code> <code>str</code> <p>The ID of the target node.</p> <code>targetHandle</code> <code>str</code> <p>The specific input handle on the target node.</p> <code>sourceHandle</code> <code>str</code> <p>The specific output handle on the source node.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Represents a connection (edge) between two nodes in the frontend.\\n\\nAttributes:\\n    id (str): A unique identifier for the edge.\\n    source (str): The ID of the source node.\\n    target (str): The ID of the target node.\\n    targetHandle (str): The specific input handle on the target node.\\n    sourceHandle (str): The specific output handle on the source node.\",\n  \"properties\": {\n    \"id\": {\n      \"title\": \"Id\",\n      \"type\": \"string\"\n    },\n    \"source\": {\n      \"title\": \"Source\",\n      \"type\": \"string\"\n    },\n    \"target\": {\n      \"title\": \"Target\",\n      \"type\": \"string\"\n    },\n    \"targetHandle\": {\n      \"title\": \"Targethandle\",\n      \"type\": \"string\"\n    },\n    \"sourceHandle\": {\n      \"title\": \"Sourcehandle\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"id\",\n    \"source\",\n    \"target\",\n    \"targetHandle\",\n    \"sourceHandle\"\n  ],\n  \"title\": \"NodeEdge\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>coerce_numbers_to_str</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>str</code>)             </li> <li> <code>source</code>                 (<code>str</code>)             </li> <li> <code>target</code>                 (<code>str</code>)             </li> <li> <code>targetHandle</code>                 (<code>str</code>)             </li> <li> <code>sourceHandle</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class NodeEdge(BaseModel):\n    \"\"\"\n    Represents a connection (edge) between two nodes in the frontend.\n\n    Attributes:\n        id (str): A unique identifier for the edge.\n        source (str): The ID of the source node.\n        target (str): The ID of the target node.\n        targetHandle (str): The specific input handle on the target node.\n        sourceHandle (str): The specific output handle on the source node.\n    \"\"\"\n\n    model_config = ConfigDict(coerce_numbers_to_str=True)\n    id: str\n    source: str\n    target: str\n    targetHandle: str\n    sourceHandle: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.NodeInformation","title":"<code>NodeInformation</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Stores the state and configuration of a specific node instance within a flow.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Stores the state and configuration of a specific node instance within a flow.\",\n  \"properties\": {\n    \"id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Id\"\n    },\n    \"type\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Type\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Is Setup\"\n    },\n    \"is_start_node\": {\n      \"default\": false,\n      \"title\": \"Is Start Node\",\n      \"type\": \"boolean\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"x_position\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"X Position\"\n    },\n    \"y_position\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Y Position\"\n    },\n    \"left_input_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Left Input Id\"\n    },\n    \"right_input_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Right Input Id\"\n    },\n    \"input_ids\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"integer\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Input Ids\"\n    },\n    \"outputs\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"integer\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Outputs\"\n    },\n    \"setting_input\": {\n      \"anyOf\": [\n        {},\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Setting Input\"\n    }\n  },\n  \"title\": \"NodeInformation\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>int | None</code>)             </li> <li> <code>type</code>                 (<code>str | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>is_start_node</code>                 (<code>bool</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>x_position</code>                 (<code>int | None</code>)             </li> <li> <code>y_position</code>                 (<code>int | None</code>)             </li> <li> <code>left_input_id</code>                 (<code>int | None</code>)             </li> <li> <code>right_input_id</code>                 (<code>int | None</code>)             </li> <li> <code>input_ids</code>                 (<code>list[int] | None</code>)             </li> <li> <code>outputs</code>                 (<code>list[int] | None</code>)             </li> <li> <code>setting_input</code>                 (<code>Any | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_setting_input</code>                 \u2192                   <code>setting_input</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class NodeInformation(BaseModel):\n    \"\"\"\n    Stores the state and configuration of a specific node instance within a flow.\n    \"\"\"\n\n    id: int | None = None\n    type: str | None = None\n    is_setup: bool | None = None\n    is_start_node: bool = False\n    description: str | None = \"\"\n    node_reference: str | None = None  # Unique reference identifier for code generation\n    x_position: int | None = 0\n    y_position: int | None = 0\n    left_input_id: int | None = None\n    right_input_id: int | None = None\n    input_ids: list[int] | None = Field(default_factory=list)\n    outputs: list[int] | None = Field(default_factory=list)\n    setting_input: Any | None = None\n\n    @property\n    def data(self) -&gt; Any:\n        return self.setting_input\n\n    @property\n    def main_input_ids(self) -&gt; list[int] | None:\n        return self.input_ids\n\n    @field_validator(\"setting_input\", mode=\"before\")\n    @classmethod\n    def validate_setting_input(cls, v, info: ValidationInfo):\n        if v is None:\n            return None\n        if isinstance(v, BaseModel):\n            return v\n\n        node_type = info.data.get(\"type\")\n        model_class = get_settings_class_for_node_type(node_type)\n\n        if model_class is None:\n            raise ValueError(f\"Unknown node type: {node_type}\")\n\n        if isinstance(v, model_class):\n            return v\n\n        return model_class.model_validate(v)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.NodeInput","title":"<code>NodeInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeTemplate</code></p> <p>Represents a node as it is received from the frontend, including position.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>The unique ID of the node instance.</p> <code>pos_x</code> <code>float</code> <p>The x-coordinate on the canvas.</p> <code>pos_y</code> <code>float</code> <p>The y-coordinate on the canvas.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Represents a node as it is received from the frontend, including position.\\n\\nAttributes:\\n    id (int): The unique ID of the node instance.\\n    pos_x (float): The x-coordinate on the canvas.\\n    pos_y (float): The y-coordinate on the canvas.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"item\": {\n      \"title\": \"Item\",\n      \"type\": \"string\"\n    },\n    \"input\": {\n      \"title\": \"Input\",\n      \"type\": \"integer\"\n    },\n    \"output\": {\n      \"title\": \"Output\",\n      \"type\": \"integer\"\n    },\n    \"image\": {\n      \"title\": \"Image\",\n      \"type\": \"string\"\n    },\n    \"multi\": {\n      \"default\": false,\n      \"title\": \"Multi\",\n      \"type\": \"boolean\"\n    },\n    \"node_type\": {\n      \"enum\": [\n        \"input\",\n        \"output\",\n        \"process\"\n      ],\n      \"title\": \"Node Type\",\n      \"type\": \"string\"\n    },\n    \"transform_type\": {\n      \"enum\": [\n        \"narrow\",\n        \"wide\",\n        \"other\"\n      ],\n      \"title\": \"Transform Type\",\n      \"type\": \"string\"\n    },\n    \"node_group\": {\n      \"title\": \"Node Group\",\n      \"type\": \"string\"\n    },\n    \"prod_ready\": {\n      \"default\": true,\n      \"title\": \"Prod Ready\",\n      \"type\": \"boolean\"\n    },\n    \"can_be_start\": {\n      \"default\": false,\n      \"title\": \"Can Be Start\",\n      \"type\": \"boolean\"\n    },\n    \"drawer_title\": {\n      \"default\": \"Node title\",\n      \"title\": \"Drawer Title\",\n      \"type\": \"string\"\n    },\n    \"drawer_intro\": {\n      \"default\": \"Drawer into\",\n      \"title\": \"Drawer Intro\",\n      \"type\": \"string\"\n    },\n    \"custom_node\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Custom Node\"\n    },\n    \"id\": {\n      \"title\": \"Id\",\n      \"type\": \"integer\"\n    },\n    \"pos_x\": {\n      \"title\": \"Pos X\",\n      \"type\": \"number\"\n    },\n    \"pos_y\": {\n      \"title\": \"Pos Y\",\n      \"type\": \"number\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"item\",\n    \"input\",\n    \"output\",\n    \"image\",\n    \"node_type\",\n    \"transform_type\",\n    \"node_group\",\n    \"id\",\n    \"pos_x\",\n    \"pos_y\"\n  ],\n  \"title\": \"NodeInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>item</code>                 (<code>str</code>)             </li> <li> <code>input</code>                 (<code>int</code>)             </li> <li> <code>output</code>                 (<code>int</code>)             </li> <li> <code>image</code>                 (<code>str</code>)             </li> <li> <code>multi</code>                 (<code>bool</code>)             </li> <li> <code>node_type</code>                 (<code>NodeTypeLiteral</code>)             </li> <li> <code>transform_type</code>                 (<code>TransformTypeLiteral</code>)             </li> <li> <code>node_group</code>                 (<code>str</code>)             </li> <li> <code>prod_ready</code>                 (<code>bool</code>)             </li> <li> <code>can_be_start</code>                 (<code>bool</code>)             </li> <li> <code>drawer_title</code>                 (<code>str</code>)             </li> <li> <code>drawer_intro</code>                 (<code>str</code>)             </li> <li> <code>custom_node</code>                 (<code>bool | None</code>)             </li> <li> <code>id</code>                 (<code>int</code>)             </li> <li> <code>pos_x</code>                 (<code>float</code>)             </li> <li> <code>pos_y</code>                 (<code>float</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class NodeInput(NodeTemplate):\n    \"\"\"\n    Represents a node as it is received from the frontend, including position.\n\n    Attributes:\n        id (int): The unique ID of the node instance.\n        pos_x (float): The x-coordinate on the canvas.\n        pos_y (float): The y-coordinate on the canvas.\n    \"\"\"\n\n    id: int\n    pos_x: float\n    pos_y: float\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.NodeTemplate","title":"<code>NodeTemplate</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the template for a node type, specifying its UI and functional characteristics.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The display name of the node.</p> <code>item</code> <code>str</code> <p>The unique identifier for the node type.</p> <code>input</code> <code>int</code> <p>The number of required input connections.</p> <code>output</code> <code>int</code> <p>The number of output connections.</p> <code>image</code> <code>str</code> <p>The filename of the icon for the node.</p> <code>multi</code> <code>bool</code> <p>Whether the node accepts multiple main input connections.</p> <code>node_group</code> <code>str</code> <p>The category group the node belongs to (e.g., 'input', 'transform').</p> <code>prod_ready</code> <code>bool</code> <p>Whether the node is considered production-ready.</p> <code>can_be_start</code> <code>bool</code> <p>Whether the node can be a starting point in a flow.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines the template for a node type, specifying its UI and functional characteristics.\\n\\nAttributes:\\n    name (str): The display name of the node.\\n    item (str): The unique identifier for the node type.\\n    input (int): The number of required input connections.\\n    output (int): The number of output connections.\\n    image (str): The filename of the icon for the node.\\n    multi (bool): Whether the node accepts multiple main input connections.\\n    node_group (str): The category group the node belongs to (e.g., 'input', 'transform').\\n    prod_ready (bool): Whether the node is considered production-ready.\\n    can_be_start (bool): Whether the node can be a starting point in a flow.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"item\": {\n      \"title\": \"Item\",\n      \"type\": \"string\"\n    },\n    \"input\": {\n      \"title\": \"Input\",\n      \"type\": \"integer\"\n    },\n    \"output\": {\n      \"title\": \"Output\",\n      \"type\": \"integer\"\n    },\n    \"image\": {\n      \"title\": \"Image\",\n      \"type\": \"string\"\n    },\n    \"multi\": {\n      \"default\": false,\n      \"title\": \"Multi\",\n      \"type\": \"boolean\"\n    },\n    \"node_type\": {\n      \"enum\": [\n        \"input\",\n        \"output\",\n        \"process\"\n      ],\n      \"title\": \"Node Type\",\n      \"type\": \"string\"\n    },\n    \"transform_type\": {\n      \"enum\": [\n        \"narrow\",\n        \"wide\",\n        \"other\"\n      ],\n      \"title\": \"Transform Type\",\n      \"type\": \"string\"\n    },\n    \"node_group\": {\n      \"title\": \"Node Group\",\n      \"type\": \"string\"\n    },\n    \"prod_ready\": {\n      \"default\": true,\n      \"title\": \"Prod Ready\",\n      \"type\": \"boolean\"\n    },\n    \"can_be_start\": {\n      \"default\": false,\n      \"title\": \"Can Be Start\",\n      \"type\": \"boolean\"\n    },\n    \"drawer_title\": {\n      \"default\": \"Node title\",\n      \"title\": \"Drawer Title\",\n      \"type\": \"string\"\n    },\n    \"drawer_intro\": {\n      \"default\": \"Drawer into\",\n      \"title\": \"Drawer Intro\",\n      \"type\": \"string\"\n    },\n    \"custom_node\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Custom Node\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"item\",\n    \"input\",\n    \"output\",\n    \"image\",\n    \"node_type\",\n    \"transform_type\",\n    \"node_group\"\n  ],\n  \"title\": \"NodeTemplate\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>item</code>                 (<code>str</code>)             </li> <li> <code>input</code>                 (<code>int</code>)             </li> <li> <code>output</code>                 (<code>int</code>)             </li> <li> <code>image</code>                 (<code>str</code>)             </li> <li> <code>multi</code>                 (<code>bool</code>)             </li> <li> <code>node_type</code>                 (<code>NodeTypeLiteral</code>)             </li> <li> <code>transform_type</code>                 (<code>TransformTypeLiteral</code>)             </li> <li> <code>node_group</code>                 (<code>str</code>)             </li> <li> <code>prod_ready</code>                 (<code>bool</code>)             </li> <li> <code>can_be_start</code>                 (<code>bool</code>)             </li> <li> <code>drawer_title</code>                 (<code>str</code>)             </li> <li> <code>drawer_intro</code>                 (<code>str</code>)             </li> <li> <code>custom_node</code>                 (<code>bool | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class NodeTemplate(BaseModel):\n    \"\"\"\n    Defines the template for a node type, specifying its UI and functional characteristics.\n\n    Attributes:\n        name (str): The display name of the node.\n        item (str): The unique identifier for the node type.\n        input (int): The number of required input connections.\n        output (int): The number of output connections.\n        image (str): The filename of the icon for the node.\n        multi (bool): Whether the node accepts multiple main input connections.\n        node_group (str): The category group the node belongs to (e.g., 'input', 'transform').\n        prod_ready (bool): Whether the node is considered production-ready.\n        can_be_start (bool): Whether the node can be a starting point in a flow.\n    \"\"\"\n\n    name: str\n    item: str\n    input: int\n    output: int\n    image: str\n    multi: bool = False\n    node_type: NodeTypeLiteral\n    transform_type: TransformTypeLiteral\n    node_group: str\n    prod_ready: bool = True\n    can_be_start: bool = False\n    drawer_title: str = \"Node title\"\n    drawer_intro: str = \"Drawer into\"\n    custom_node: bool | None = False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.RawLogInput","title":"<code>RawLogInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for a raw log message.</p> <p>Attributes:</p> Name Type Description <code>flowfile_flow_id</code> <code>int</code> <p>The ID of the flow that generated the log.</p> <code>log_message</code> <code>str</code> <p>The content of the log message.</p> <code>log_type</code> <code>Literal['INFO', 'WARNING', 'ERROR']</code> <p>The type of log.</p> <code>node_id</code> <code>int | None</code> <p>Optional node ID to attribute the log to.</p> <code>extra</code> <code>Optional[dict]</code> <p>Extra context data for the log.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Schema for a raw log message.\\n\\nAttributes:\\n    flowfile_flow_id (int): The ID of the flow that generated the log.\\n    log_message (str): The content of the log message.\\n    log_type (Literal[\\\"INFO\\\", \\\"WARNING\\\", \\\"ERROR\\\"]): The type of log.\\n    node_id (int | None): Optional node ID to attribute the log to.\\n    extra (Optional[dict]): Extra context data for the log.\",\n  \"properties\": {\n    \"flowfile_flow_id\": {\n      \"title\": \"Flowfile Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"log_message\": {\n      \"title\": \"Log Message\",\n      \"type\": \"string\"\n    },\n    \"log_type\": {\n      \"enum\": [\n        \"INFO\",\n        \"WARNING\",\n        \"ERROR\"\n      ],\n      \"title\": \"Log Type\",\n      \"type\": \"string\"\n    },\n    \"node_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Id\"\n    },\n    \"extra\": {\n      \"anyOf\": [\n        {\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Extra\"\n    }\n  },\n  \"required\": [\n    \"flowfile_flow_id\",\n    \"log_message\",\n    \"log_type\"\n  ],\n  \"title\": \"RawLogInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flowfile_flow_id</code>                 (<code>int</code>)             </li> <li> <code>log_message</code>                 (<code>str</code>)             </li> <li> <code>log_type</code>                 (<code>Literal['INFO', 'WARNING', 'ERROR']</code>)             </li> <li> <code>node_id</code>                 (<code>int | None</code>)             </li> <li> <code>extra</code>                 (<code>dict | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class RawLogInput(BaseModel):\n    \"\"\"\n    Schema for a raw log message.\n\n    Attributes:\n        flowfile_flow_id (int): The ID of the flow that generated the log.\n        log_message (str): The content of the log message.\n        log_type (Literal[\"INFO\", \"WARNING\", \"ERROR\"]): The type of log.\n        node_id (int | None): Optional node ID to attribute the log to.\n        extra (Optional[dict]): Extra context data for the log.\n    \"\"\"\n\n    flowfile_flow_id: int\n    log_message: str\n    log_type: Literal[\"INFO\", \"WARNING\", \"ERROR\"]\n    node_id: int | None = None\n    extra: dict | None = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.VueFlowInput","title":"<code>VueFlowInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the complete graph structure from the Vue-based frontend.</p> <p>Attributes:</p> Name Type Description <code>node_edges</code> <code>List[NodeEdge]</code> <p>A list of all edges in the graph.</p> <code>node_inputs</code> <code>List[NodeInput]</code> <p>A list of all nodes in the graph.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"NodeEdge\": {\n      \"description\": \"Represents a connection (edge) between two nodes in the frontend.\\n\\nAttributes:\\n    id (str): A unique identifier for the edge.\\n    source (str): The ID of the source node.\\n    target (str): The ID of the target node.\\n    targetHandle (str): The specific input handle on the target node.\\n    sourceHandle (str): The specific output handle on the source node.\",\n      \"properties\": {\n        \"id\": {\n          \"title\": \"Id\",\n          \"type\": \"string\"\n        },\n        \"source\": {\n          \"title\": \"Source\",\n          \"type\": \"string\"\n        },\n        \"target\": {\n          \"title\": \"Target\",\n          \"type\": \"string\"\n        },\n        \"targetHandle\": {\n          \"title\": \"Targethandle\",\n          \"type\": \"string\"\n        },\n        \"sourceHandle\": {\n          \"title\": \"Sourcehandle\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"id\",\n        \"source\",\n        \"target\",\n        \"targetHandle\",\n        \"sourceHandle\"\n      ],\n      \"title\": \"NodeEdge\",\n      \"type\": \"object\"\n    },\n    \"NodeInput\": {\n      \"description\": \"Represents a node as it is received from the frontend, including position.\\n\\nAttributes:\\n    id (int): The unique ID of the node instance.\\n    pos_x (float): The x-coordinate on the canvas.\\n    pos_y (float): The y-coordinate on the canvas.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"item\": {\n          \"title\": \"Item\",\n          \"type\": \"string\"\n        },\n        \"input\": {\n          \"title\": \"Input\",\n          \"type\": \"integer\"\n        },\n        \"output\": {\n          \"title\": \"Output\",\n          \"type\": \"integer\"\n        },\n        \"image\": {\n          \"title\": \"Image\",\n          \"type\": \"string\"\n        },\n        \"multi\": {\n          \"default\": false,\n          \"title\": \"Multi\",\n          \"type\": \"boolean\"\n        },\n        \"node_type\": {\n          \"enum\": [\n            \"input\",\n            \"output\",\n            \"process\"\n          ],\n          \"title\": \"Node Type\",\n          \"type\": \"string\"\n        },\n        \"transform_type\": {\n          \"enum\": [\n            \"narrow\",\n            \"wide\",\n            \"other\"\n          ],\n          \"title\": \"Transform Type\",\n          \"type\": \"string\"\n        },\n        \"node_group\": {\n          \"title\": \"Node Group\",\n          \"type\": \"string\"\n        },\n        \"prod_ready\": {\n          \"default\": true,\n          \"title\": \"Prod Ready\",\n          \"type\": \"boolean\"\n        },\n        \"can_be_start\": {\n          \"default\": false,\n          \"title\": \"Can Be Start\",\n          \"type\": \"boolean\"\n        },\n        \"drawer_title\": {\n          \"default\": \"Node title\",\n          \"title\": \"Drawer Title\",\n          \"type\": \"string\"\n        },\n        \"drawer_intro\": {\n          \"default\": \"Drawer into\",\n          \"title\": \"Drawer Intro\",\n          \"type\": \"string\"\n        },\n        \"custom_node\": {\n          \"anyOf\": [\n            {\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": false,\n          \"title\": \"Custom Node\"\n        },\n        \"id\": {\n          \"title\": \"Id\",\n          \"type\": \"integer\"\n        },\n        \"pos_x\": {\n          \"title\": \"Pos X\",\n          \"type\": \"number\"\n        },\n        \"pos_y\": {\n          \"title\": \"Pos Y\",\n          \"type\": \"number\"\n        }\n      },\n      \"required\": [\n        \"name\",\n        \"item\",\n        \"input\",\n        \"output\",\n        \"image\",\n        \"node_type\",\n        \"transform_type\",\n        \"node_group\",\n        \"id\",\n        \"pos_x\",\n        \"pos_y\"\n      ],\n      \"title\": \"NodeInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Represents the complete graph structure from the Vue-based frontend.\\n\\nAttributes:\\n    node_edges (List[NodeEdge]): A list of all edges in the graph.\\n    node_inputs (List[NodeInput]): A list of all nodes in the graph.\",\n  \"properties\": {\n    \"node_edges\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/NodeEdge\"\n      },\n      \"title\": \"Node Edges\",\n      \"type\": \"array\"\n    },\n    \"node_inputs\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/NodeInput\"\n      },\n      \"title\": \"Node Inputs\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"node_edges\",\n    \"node_inputs\"\n  ],\n  \"title\": \"VueFlowInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>node_edges</code>                 (<code>list[NodeEdge]</code>)             </li> <li> <code>node_inputs</code>                 (<code>list[NodeInput]</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class VueFlowInput(BaseModel):\n    \"\"\"\n\n    Represents the complete graph structure from the Vue-based frontend.\n\n    Attributes:\n        node_edges (List[NodeEdge]): A list of all edges in the graph.\n        node_inputs (List[NodeInput]): A list of all nodes in the graph.\n    \"\"\"\n\n    node_edges: list[NodeEdge]\n    node_inputs: list[NodeInput]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.get_global_execution_location","title":"<code>get_global_execution_location()</code>","text":"<p>Calculates the default execution location based on the global settings Returns</p> <p>ExecutionLocationsLiteral where the current</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>def get_global_execution_location() -&gt; ExecutionLocationsLiteral:\n    \"\"\"\n    Calculates the default execution location based on the global settings\n    Returns\n    -------\n    ExecutionLocationsLiteral where the current\n    \"\"\"\n    if OFFLOAD_TO_WORKER:\n        return \"remote\"\n    return \"local\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.get_settings_class_for_node_type","title":"<code>get_settings_class_for_node_type(node_type)</code>","text":"<p>Get the settings class for a node type, supporting both standard and user-defined nodes.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>def get_settings_class_for_node_type(node_type: str):\n    \"\"\"Get the settings class for a node type, supporting both standard and user-defined nodes.\"\"\"\n    model_class = NODE_TYPE_TO_SETTINGS_CLASS.get(node_type)\n    if model_class is None:\n        if node_type in _get_custom_node_store():\n            return input_schema.UserDefinedNode\n        return None\n    return model_class\n</code></pre>"},{"location":"for-developers/python-api-reference.html#input_schema","title":"<code>input_schema</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema","title":"<code>flowfile_core.schemas.input_schema</code>","text":"<p>Classes:</p> Name Description <code>DatabaseConnection</code> <p>Defines the connection parameters for a database.</p> <code>DatabaseSettings</code> <p>Defines settings for reading from a database, either via table or query.</p> <code>DatabaseWriteSettings</code> <p>Defines settings for writing data to a database table.</p> <code>ExternalSource</code> <p>Base model for data coming from a predefined external source.</p> <code>FullDatabaseConnection</code> <p>A complete database connection model including the secret password.</p> <code>FullDatabaseConnectionInterface</code> <p>A database connection model intended for UI display, omitting the password.</p> <code>InputCsvTable</code> <p>Defines settings for reading a CSV file.</p> <code>InputExcelTable</code> <p>Defines settings for reading an Excel file.</p> <code>InputJsonTable</code> <p>Defines settings for reading a JSON file.</p> <code>InputParquetTable</code> <p>Defines settings for reading a Parquet file.</p> <code>InputTableBase</code> <p>Base settings for input file operations.</p> <code>MinimalFieldInfo</code> <p>Represents the most basic information about a data field (column).</p> <code>NewDirectory</code> <p>Defines the information required to create a new directory.</p> <code>NodeBase</code> <p>Base model for all nodes in a FlowGraph. Contains common metadata.</p> <code>NodeCloudStorageReader</code> <p>Settings for a node that reads from a cloud storage service (S3, GCS, etc.).</p> <code>NodeCloudStorageWriter</code> <p>Settings for a node that writes to a cloud storage service.</p> <code>NodeConnection</code> <p>Represents a connection (edge) between two nodes in the graph.</p> <code>NodeCrossJoin</code> <p>Settings for a node that performs a cross join.</p> <code>NodeDatabaseReader</code> <p>Settings for a node that reads from a database.</p> <code>NodeDatabaseWriter</code> <p>Settings for a node that writes data to a database.</p> <code>NodeDatasource</code> <p>Base settings for a node that acts as a data source.</p> <code>NodeDescription</code> <p>A simple model for updating a node's description text.</p> <code>NodeExploreData</code> <p>Settings for a node that provides an interactive data exploration interface.</p> <code>NodeExternalSource</code> <p>Settings for a node that connects to a registered external data source.</p> <code>NodeFilter</code> <p>Settings for a node that filters rows based on a condition.</p> <code>NodeFormula</code> <p>Settings for a node that applies a formula to create/modify a column.</p> <code>NodeFuzzyMatch</code> <p>Settings for a node that performs a fuzzy join based on string similarity.</p> <code>NodeGraphSolver</code> <p>Settings for a node that solves graph-based problems (e.g., connected components).</p> <code>NodeGroupBy</code> <p>Settings for a node that performs a group-by and aggregation operation.</p> <code>NodeInputConnection</code> <p>Represents the input side of a connection between two nodes.</p> <code>NodeJoin</code> <p>Settings for a node that performs a standard SQL-style join.</p> <code>NodeManualInput</code> <p>Settings for a node that allows direct data entry in the UI.</p> <code>NodeMultiInput</code> <p>A base model for any node that takes multiple data inputs.</p> <code>NodeOutput</code> <p>Settings for a node that writes its input to a file.</p> <code>NodeOutputConnection</code> <p>Represents the output side of a connection between two nodes.</p> <code>NodePivot</code> <p>Settings for a node that pivots data from a long to a wide format.</p> <code>NodePolarsCode</code> <p>Settings for a node that executes arbitrary user-provided Polars code.</p> <code>NodePromise</code> <p>A placeholder node for an operation that has not yet been configured.</p> <code>NodePythonScript</code> <p>Node that executes Python code on a kernel container.</p> <code>NodeRead</code> <p>Settings for a node that reads data from a file.</p> <code>NodeRecordCount</code> <p>Settings for a node that counts the number of records.</p> <code>NodeRecordId</code> <p>Settings for a node that adds a unique record ID column.</p> <code>NodeSample</code> <p>Settings for a node that samples a subset of the data.</p> <code>NodeSelect</code> <p>Settings for a node that selects, renames, and reorders columns.</p> <code>NodeSingleInput</code> <p>A base model for any node that takes a single data input.</p> <code>NodeSort</code> <p>Settings for a node that sorts the data by one or more columns.</p> <code>NodeTextToRows</code> <p>Settings for a node that splits a text column into multiple rows.</p> <code>NodeUnion</code> <p>Settings for a node that concatenates multiple data inputs.</p> <code>NodeUnique</code> <p>Settings for a node that returns the unique rows from the data.</p> <code>NodeUnpivot</code> <p>Settings for a node that unpivots data from a wide to a long format.</p> <code>NotebookCell</code> <p>A single cell in the notebook editor.</p> <code>OutputCsvTable</code> <p>Defines settings for writing a CSV file.</p> <code>OutputExcelTable</code> <p>Defines settings for writing an Excel file.</p> <code>OutputFieldConfig</code> <p>Configuration for output field validation and transformation behavior.</p> <code>OutputFieldInfo</code> <p>Field information with optional default value for output field configuration.</p> <code>OutputParquetTable</code> <p>Defines settings for writing a Parquet file.</p> <code>OutputSettings</code> <p>Defines the complete settings for an output node.</p> <code>PythonScriptInput</code> <p>Settings for Python code execution on a kernel.</p> <code>RawData</code> <p>Represents data in a raw, columnar format for manual input.</p> <code>ReceivedTable</code> <p>Model for defining a table received from an external source.</p> <code>RemoveItem</code> <p>Represents a single item to be removed from a directory or list.</p> <code>RemoveItemsInput</code> <p>Defines a list of items to be removed.</p> <code>SampleUsers</code> <p>Settings for generating a sample dataset of users.</p> <code>UserDefinedNode</code> <p>Settings for a node that contains the user defined node information</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.DatabaseConnection","title":"<code>DatabaseConnection</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the connection parameters for a database.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines the connection parameters for a database.\",\n  \"properties\": {\n    \"database_type\": {\n      \"default\": \"postgresql\",\n      \"title\": \"Database Type\",\n      \"type\": \"string\"\n    },\n    \"username\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Username\"\n    },\n    \"password_ref\": {\n      \"anyOf\": [\n        {\n          \"description\": \"An ID referencing an encrypted secret.\",\n          \"maxLength\": 100,\n          \"minLength\": 1,\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Password Ref\"\n    },\n    \"host\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Host\"\n    },\n    \"port\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Port\"\n    },\n    \"database\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Database\"\n    },\n    \"url\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Url\"\n    }\n  },\n  \"title\": \"DatabaseConnection\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>database_type</code>                 (<code>str</code>)             </li> <li> <code>username</code>                 (<code>str | None</code>)             </li> <li> <code>password_ref</code>                 (<code>SecretRef | None</code>)             </li> <li> <code>host</code>                 (<code>str | None</code>)             </li> <li> <code>port</code>                 (<code>int | None</code>)             </li> <li> <code>database</code>                 (<code>str | None</code>)             </li> <li> <code>url</code>                 (<code>str | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class DatabaseConnection(BaseModel):\n    \"\"\"Defines the connection parameters for a database.\"\"\"\n\n    database_type: str = \"postgresql\"\n    username: str | None = None\n    password_ref: SecretRef | None = None\n    host: str | None = None\n    port: int | None = None\n    database: str | None = None\n    url: str | None = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.DatabaseSettings","title":"<code>DatabaseSettings</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for reading from a database, either via table or query.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"DatabaseConnection\": {\n      \"description\": \"Defines the connection parameters for a database.\",\n      \"properties\": {\n        \"database_type\": {\n          \"default\": \"postgresql\",\n          \"title\": \"Database Type\",\n          \"type\": \"string\"\n        },\n        \"username\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Username\"\n        },\n        \"password_ref\": {\n          \"anyOf\": [\n            {\n              \"description\": \"An ID referencing an encrypted secret.\",\n              \"maxLength\": 100,\n              \"minLength\": 1,\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Password Ref\"\n        },\n        \"host\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Host\"\n        },\n        \"port\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Port\"\n        },\n        \"database\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Database\"\n        },\n        \"url\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Url\"\n        }\n      },\n      \"title\": \"DatabaseConnection\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Defines settings for reading from a database, either via table or query.\",\n  \"properties\": {\n    \"connection_mode\": {\n      \"anyOf\": [\n        {\n          \"enum\": [\n            \"inline\",\n            \"reference\"\n          ],\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"inline\",\n      \"title\": \"Connection Mode\"\n    },\n    \"database_connection\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/DatabaseConnection\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"database_connection_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Database Connection Name\"\n    },\n    \"schema_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Schema Name\"\n    },\n    \"table_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Table Name\"\n    },\n    \"query\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Query\"\n    },\n    \"query_mode\": {\n      \"default\": \"table\",\n      \"enum\": [\n        \"query\",\n        \"table\",\n        \"reference\"\n      ],\n      \"title\": \"Query Mode\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"DatabaseSettings\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>connection_mode</code>                 (<code>Literal['inline', 'reference'] | None</code>)             </li> <li> <code>database_connection</code>                 (<code>DatabaseConnection | None</code>)             </li> <li> <code>database_connection_name</code>                 (<code>str | None</code>)             </li> <li> <code>schema_name</code>                 (<code>str | None</code>)             </li> <li> <code>table_name</code>                 (<code>str | None</code>)             </li> <li> <code>query</code>                 (<code>str | None</code>)             </li> <li> <code>query_mode</code>                 (<code>Literal['query', 'table', 'reference']</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_sql_identifier</code>                 \u2192                   <code>table_name</code>,                   <code>schema_name</code> </li> <li> <code>validate_table_or_query</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class DatabaseSettings(BaseModel):\n    \"\"\"Defines settings for reading from a database, either via table or query.\"\"\"\n\n    connection_mode: Literal[\"inline\", \"reference\"] | None = \"inline\"\n    database_connection: DatabaseConnection | None = None\n    database_connection_name: str | None = None\n    schema_name: str | None = None\n    table_name: str | None = None\n    query: str | None = None\n    query_mode: Literal[\"query\", \"table\", \"reference\"] = \"table\"\n\n    @field_validator(\"table_name\", \"schema_name\", mode=\"before\")\n    @classmethod\n    def validate_sql_identifier(cls, v):\n        if v is not None and v != \"\":\n            parts = v.split(\".\")\n            for part in parts:\n                if not part or not re.match(r\"^[a-zA-Z_][a-zA-Z0-9_]*$\", part):\n                    raise ValueError(\n                        f\"Invalid SQL identifier: '{v}'. \"\n                        f\"Only letters, numbers, and underscores are allowed.\"\n                    )\n        return v\n\n    @model_validator(mode=\"after\")\n    def validate_table_or_query(self):\n        # Validate that either table_name or query is provided\n        if (not self.table_name and not self.query) and self.query_mode == \"inline\":\n            raise ValueError(\"Either 'table_name' or 'query' must be provided\")\n\n        # Validate correct connection information based on connection_mode\n        if self.connection_mode == \"inline\" and self.database_connection is None:\n            raise ValueError(\"When 'connection_mode' is 'inline', 'database_connection' must be provided\")\n\n        if self.connection_mode == \"reference\" and not self.database_connection_name:\n            raise ValueError(\"When 'connection_mode' is 'reference', 'database_connection_name' must be provided\")\n\n        return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.DatabaseWriteSettings","title":"<code>DatabaseWriteSettings</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for writing data to a database table.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"DatabaseConnection\": {\n      \"description\": \"Defines the connection parameters for a database.\",\n      \"properties\": {\n        \"database_type\": {\n          \"default\": \"postgresql\",\n          \"title\": \"Database Type\",\n          \"type\": \"string\"\n        },\n        \"username\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Username\"\n        },\n        \"password_ref\": {\n          \"anyOf\": [\n            {\n              \"description\": \"An ID referencing an encrypted secret.\",\n              \"maxLength\": 100,\n              \"minLength\": 1,\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Password Ref\"\n        },\n        \"host\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Host\"\n        },\n        \"port\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Port\"\n        },\n        \"database\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Database\"\n        },\n        \"url\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Url\"\n        }\n      },\n      \"title\": \"DatabaseConnection\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Defines settings for writing data to a database table.\",\n  \"properties\": {\n    \"connection_mode\": {\n      \"anyOf\": [\n        {\n          \"enum\": [\n            \"inline\",\n            \"reference\"\n          ],\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"inline\",\n      \"title\": \"Connection Mode\"\n    },\n    \"database_connection\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/DatabaseConnection\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"database_connection_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Database Connection Name\"\n    },\n    \"table_name\": {\n      \"title\": \"Table Name\",\n      \"type\": \"string\"\n    },\n    \"schema_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Schema Name\"\n    },\n    \"if_exists\": {\n      \"anyOf\": [\n        {\n          \"enum\": [\n            \"append\",\n            \"replace\",\n            \"fail\"\n          ],\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"append\",\n      \"title\": \"If Exists\"\n    }\n  },\n  \"required\": [\n    \"table_name\"\n  ],\n  \"title\": \"DatabaseWriteSettings\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>connection_mode</code>                 (<code>Literal['inline', 'reference'] | None</code>)             </li> <li> <code>database_connection</code>                 (<code>DatabaseConnection | None</code>)             </li> <li> <code>database_connection_name</code>                 (<code>str | None</code>)             </li> <li> <code>table_name</code>                 (<code>str</code>)             </li> <li> <code>schema_name</code>                 (<code>str | None</code>)             </li> <li> <code>if_exists</code>                 (<code>Literal['append', 'replace', 'fail'] | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_sql_identifier</code>                 \u2192                   <code>table_name</code>,                   <code>schema_name</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class DatabaseWriteSettings(BaseModel):\n    \"\"\"Defines settings for writing data to a database table.\"\"\"\n\n    connection_mode: Literal[\"inline\", \"reference\"] | None = \"inline\"\n    database_connection: DatabaseConnection | None = None\n    database_connection_name: str | None = None\n    table_name: str\n    schema_name: str | None = None\n    if_exists: Literal[\"append\", \"replace\", \"fail\"] | None = \"append\"\n\n    @field_validator(\"table_name\", \"schema_name\", mode=\"before\")\n    @classmethod\n    def validate_sql_identifier(cls, v):\n        if v is not None and v != \"\":\n            parts = v.split(\".\")\n            for part in parts:\n                if not part or not re.match(r\"^[a-zA-Z_][a-zA-Z0-9_]*$\", part):\n                    raise ValueError(\n                        f\"Invalid SQL identifier: '{v}'. \"\n                        f\"Only letters, numbers, and underscores are allowed.\"\n                    )\n        return v\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ExternalSource","title":"<code>ExternalSource</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model for data coming from a predefined external source.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"MinimalFieldInfo\": {\n      \"description\": \"Represents the most basic information about a data field (column).\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"MinimalFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Base model for data coming from a predefined external source.\",\n  \"properties\": {\n    \"orientation\": {\n      \"default\": \"row\",\n      \"title\": \"Orientation\",\n      \"type\": \"string\"\n    },\n    \"fields\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/MinimalFieldInfo\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Fields\"\n    }\n  },\n  \"title\": \"ExternalSource\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>orientation</code>                 (<code>str</code>)             </li> <li> <code>fields</code>                 (<code>list[MinimalFieldInfo] | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class ExternalSource(BaseModel):\n    \"\"\"Base model for data coming from a predefined external source.\"\"\"\n\n    orientation: str = \"row\"\n    fields: list[MinimalFieldInfo] | None = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.FullDatabaseConnection","title":"<code>FullDatabaseConnection</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A complete database connection model including the secret password.</p> Show JSON schema: <pre><code>{\n  \"description\": \"A complete database connection model including the secret password.\",\n  \"properties\": {\n    \"connection_name\": {\n      \"title\": \"Connection Name\",\n      \"type\": \"string\"\n    },\n    \"database_type\": {\n      \"default\": \"postgresql\",\n      \"title\": \"Database Type\",\n      \"type\": \"string\"\n    },\n    \"username\": {\n      \"title\": \"Username\",\n      \"type\": \"string\"\n    },\n    \"password\": {\n      \"format\": \"password\",\n      \"title\": \"Password\",\n      \"type\": \"string\",\n      \"writeOnly\": true\n    },\n    \"host\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Host\"\n    },\n    \"port\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Port\"\n    },\n    \"database\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Database\"\n    },\n    \"ssl_enabled\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Ssl Enabled\"\n    },\n    \"url\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Url\"\n    }\n  },\n  \"required\": [\n    \"connection_name\",\n    \"username\",\n    \"password\"\n  ],\n  \"title\": \"FullDatabaseConnection\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>connection_name</code>                 (<code>str</code>)             </li> <li> <code>database_type</code>                 (<code>str</code>)             </li> <li> <code>username</code>                 (<code>str</code>)             </li> <li> <code>password</code>                 (<code>SecretStr</code>)             </li> <li> <code>host</code>                 (<code>str | None</code>)             </li> <li> <code>port</code>                 (<code>int | None</code>)             </li> <li> <code>database</code>                 (<code>str | None</code>)             </li> <li> <code>ssl_enabled</code>                 (<code>bool | None</code>)             </li> <li> <code>url</code>                 (<code>str | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class FullDatabaseConnection(BaseModel):\n    \"\"\"A complete database connection model including the secret password.\"\"\"\n\n    connection_name: str\n    database_type: str = \"postgresql\"\n    username: str\n    password: SecretStr\n    host: str | None = None\n    port: int | None = None\n    database: str | None = None\n    ssl_enabled: bool | None = False\n    url: str | None = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.FullDatabaseConnectionInterface","title":"<code>FullDatabaseConnectionInterface</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A database connection model intended for UI display, omitting the password.</p> Show JSON schema: <pre><code>{\n  \"description\": \"A database connection model intended for UI display, omitting the password.\",\n  \"properties\": {\n    \"connection_name\": {\n      \"title\": \"Connection Name\",\n      \"type\": \"string\"\n    },\n    \"database_type\": {\n      \"default\": \"postgresql\",\n      \"title\": \"Database Type\",\n      \"type\": \"string\"\n    },\n    \"username\": {\n      \"title\": \"Username\",\n      \"type\": \"string\"\n    },\n    \"host\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Host\"\n    },\n    \"port\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Port\"\n    },\n    \"database\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Database\"\n    },\n    \"ssl_enabled\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Ssl Enabled\"\n    },\n    \"url\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Url\"\n    }\n  },\n  \"required\": [\n    \"connection_name\",\n    \"username\"\n  ],\n  \"title\": \"FullDatabaseConnectionInterface\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>connection_name</code>                 (<code>str</code>)             </li> <li> <code>database_type</code>                 (<code>str</code>)             </li> <li> <code>username</code>                 (<code>str</code>)             </li> <li> <code>host</code>                 (<code>str | None</code>)             </li> <li> <code>port</code>                 (<code>int | None</code>)             </li> <li> <code>database</code>                 (<code>str | None</code>)             </li> <li> <code>ssl_enabled</code>                 (<code>bool | None</code>)             </li> <li> <code>url</code>                 (<code>str | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class FullDatabaseConnectionInterface(BaseModel):\n    \"\"\"A database connection model intended for UI display, omitting the password.\"\"\"\n\n    connection_name: str\n    database_type: str = \"postgresql\"\n    username: str\n    host: str | None = None\n    port: int | None = None\n    database: str | None = None\n    ssl_enabled: bool | None = False\n    url: str | None = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.InputCsvTable","title":"<code>InputCsvTable</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>InputTableBase</code></p> <p>Defines settings for reading a CSV file.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines settings for reading a CSV file.\",\n  \"properties\": {\n    \"file_type\": {\n      \"const\": \"csv\",\n      \"default\": \"csv\",\n      \"enum\": [\n        \"csv\"\n      ],\n      \"title\": \"File Type\",\n      \"type\": \"string\"\n    },\n    \"reference\": {\n      \"default\": \"\",\n      \"title\": \"Reference\",\n      \"type\": \"string\"\n    },\n    \"starting_from_line\": {\n      \"default\": 0,\n      \"title\": \"Starting From Line\",\n      \"type\": \"integer\"\n    },\n    \"delimiter\": {\n      \"default\": \",\",\n      \"title\": \"Delimiter\",\n      \"type\": \"string\"\n    },\n    \"has_headers\": {\n      \"default\": true,\n      \"title\": \"Has Headers\",\n      \"type\": \"boolean\"\n    },\n    \"encoding\": {\n      \"default\": \"utf-8\",\n      \"title\": \"Encoding\",\n      \"type\": \"string\"\n    },\n    \"parquet_ref\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Parquet Ref\"\n    },\n    \"row_delimiter\": {\n      \"default\": \"\\n\",\n      \"title\": \"Row Delimiter\",\n      \"type\": \"string\"\n    },\n    \"quote_char\": {\n      \"default\": \"\\\"\",\n      \"title\": \"Quote Char\",\n      \"type\": \"string\"\n    },\n    \"infer_schema_length\": {\n      \"default\": 10000,\n      \"title\": \"Infer Schema Length\",\n      \"type\": \"integer\"\n    },\n    \"truncate_ragged_lines\": {\n      \"default\": false,\n      \"title\": \"Truncate Ragged Lines\",\n      \"type\": \"boolean\"\n    },\n    \"ignore_errors\": {\n      \"default\": false,\n      \"title\": \"Ignore Errors\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"title\": \"InputCsvTable\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>file_type</code>                 (<code>Literal['csv']</code>)             </li> <li> <code>reference</code>                 (<code>str</code>)             </li> <li> <code>starting_from_line</code>                 (<code>int</code>)             </li> <li> <code>delimiter</code>                 (<code>str</code>)             </li> <li> <code>has_headers</code>                 (<code>bool</code>)             </li> <li> <code>encoding</code>                 (<code>str</code>)             </li> <li> <code>parquet_ref</code>                 (<code>str | None</code>)             </li> <li> <code>row_delimiter</code>                 (<code>str</code>)             </li> <li> <code>quote_char</code>                 (<code>str</code>)             </li> <li> <code>infer_schema_length</code>                 (<code>int</code>)             </li> <li> <code>truncate_ragged_lines</code>                 (<code>bool</code>)             </li> <li> <code>ignore_errors</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class InputCsvTable(InputTableBase):\n    \"\"\"Defines settings for reading a CSV file.\"\"\"\n\n    file_type: Literal[\"csv\"] = \"csv\"\n    reference: str = \"\"\n    starting_from_line: int = 0\n    delimiter: str = \",\"\n    has_headers: bool = True\n    encoding: str = \"utf-8\"\n    parquet_ref: str | None = None\n    row_delimiter: str = \"\\n\"\n    quote_char: str = '\"'\n    infer_schema_length: int = 10_000\n    truncate_ragged_lines: bool = False\n    ignore_errors: bool = False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.InputExcelTable","title":"<code>InputExcelTable</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>InputTableBase</code></p> <p>Defines settings for reading an Excel file.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines settings for reading an Excel file.\",\n  \"properties\": {\n    \"file_type\": {\n      \"const\": \"excel\",\n      \"default\": \"excel\",\n      \"enum\": [\n        \"excel\"\n      ],\n      \"title\": \"File Type\",\n      \"type\": \"string\"\n    },\n    \"sheet_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Sheet Name\"\n    },\n    \"start_row\": {\n      \"default\": 0,\n      \"title\": \"Start Row\",\n      \"type\": \"integer\"\n    },\n    \"start_column\": {\n      \"default\": 0,\n      \"title\": \"Start Column\",\n      \"type\": \"integer\"\n    },\n    \"end_row\": {\n      \"default\": 0,\n      \"title\": \"End Row\",\n      \"type\": \"integer\"\n    },\n    \"end_column\": {\n      \"default\": 0,\n      \"title\": \"End Column\",\n      \"type\": \"integer\"\n    },\n    \"has_headers\": {\n      \"default\": true,\n      \"title\": \"Has Headers\",\n      \"type\": \"boolean\"\n    },\n    \"type_inference\": {\n      \"default\": false,\n      \"title\": \"Type Inference\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"title\": \"InputExcelTable\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>file_type</code>                 (<code>Literal['excel']</code>)             </li> <li> <code>sheet_name</code>                 (<code>str | None</code>)             </li> <li> <code>start_row</code>                 (<code>int</code>)             </li> <li> <code>start_column</code>                 (<code>int</code>)             </li> <li> <code>end_row</code>                 (<code>int</code>)             </li> <li> <code>end_column</code>                 (<code>int</code>)             </li> <li> <code>has_headers</code>                 (<code>bool</code>)             </li> <li> <code>type_inference</code>                 (<code>bool</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_range_values</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class InputExcelTable(InputTableBase):\n    \"\"\"Defines settings for reading an Excel file.\"\"\"\n\n    file_type: Literal[\"excel\"] = \"excel\"\n    sheet_name: str | None = None\n    start_row: int = 0\n    start_column: int = 0\n    end_row: int = 0\n    end_column: int = 0\n    has_headers: bool = True\n    type_inference: bool = False\n\n    @model_validator(mode=\"after\")\n    def validate_range_values(self):\n        \"\"\"Validates that the Excel cell range is logical.\"\"\"\n        for attribute in [self.start_row, self.start_column, self.end_row, self.end_column]:\n            if not isinstance(attribute, int) or attribute &lt; 0:\n                raise ValueError(\"Row and column indices must be non-negative integers\")\n        if (self.end_row &gt; 0 and self.start_row &gt; self.end_row) or (\n            self.end_column &gt; 0 and self.start_column &gt; self.end_column\n        ):\n            raise ValueError(\"Start row/column must not be greater than end row/column\")\n        return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.InputExcelTable.validate_range_values","title":"<code>validate_range_values()</code>  <code>pydantic-validator</code>","text":"<p>Validates that the Excel cell range is logical.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_range_values(self):\n    \"\"\"Validates that the Excel cell range is logical.\"\"\"\n    for attribute in [self.start_row, self.start_column, self.end_row, self.end_column]:\n        if not isinstance(attribute, int) or attribute &lt; 0:\n            raise ValueError(\"Row and column indices must be non-negative integers\")\n    if (self.end_row &gt; 0 and self.start_row &gt; self.end_row) or (\n        self.end_column &gt; 0 and self.start_column &gt; self.end_column\n    ):\n        raise ValueError(\"Start row/column must not be greater than end row/column\")\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.InputJsonTable","title":"<code>InputJsonTable</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>InputCsvTable</code></p> <p>Defines settings for reading a JSON file.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines settings for reading a JSON file.\",\n  \"properties\": {\n    \"file_type\": {\n      \"const\": \"json\",\n      \"default\": \"json\",\n      \"enum\": [\n        \"json\"\n      ],\n      \"title\": \"File Type\",\n      \"type\": \"string\"\n    },\n    \"reference\": {\n      \"default\": \"\",\n      \"title\": \"Reference\",\n      \"type\": \"string\"\n    },\n    \"starting_from_line\": {\n      \"default\": 0,\n      \"title\": \"Starting From Line\",\n      \"type\": \"integer\"\n    },\n    \"delimiter\": {\n      \"default\": \",\",\n      \"title\": \"Delimiter\",\n      \"type\": \"string\"\n    },\n    \"has_headers\": {\n      \"default\": true,\n      \"title\": \"Has Headers\",\n      \"type\": \"boolean\"\n    },\n    \"encoding\": {\n      \"default\": \"utf-8\",\n      \"title\": \"Encoding\",\n      \"type\": \"string\"\n    },\n    \"parquet_ref\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Parquet Ref\"\n    },\n    \"row_delimiter\": {\n      \"default\": \"\\n\",\n      \"title\": \"Row Delimiter\",\n      \"type\": \"string\"\n    },\n    \"quote_char\": {\n      \"default\": \"\\\"\",\n      \"title\": \"Quote Char\",\n      \"type\": \"string\"\n    },\n    \"infer_schema_length\": {\n      \"default\": 10000,\n      \"title\": \"Infer Schema Length\",\n      \"type\": \"integer\"\n    },\n    \"truncate_ragged_lines\": {\n      \"default\": false,\n      \"title\": \"Truncate Ragged Lines\",\n      \"type\": \"boolean\"\n    },\n    \"ignore_errors\": {\n      \"default\": false,\n      \"title\": \"Ignore Errors\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"title\": \"InputJsonTable\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>reference</code>                 (<code>str</code>)             </li> <li> <code>starting_from_line</code>                 (<code>int</code>)             </li> <li> <code>delimiter</code>                 (<code>str</code>)             </li> <li> <code>has_headers</code>                 (<code>bool</code>)             </li> <li> <code>encoding</code>                 (<code>str</code>)             </li> <li> <code>parquet_ref</code>                 (<code>str | None</code>)             </li> <li> <code>row_delimiter</code>                 (<code>str</code>)             </li> <li> <code>quote_char</code>                 (<code>str</code>)             </li> <li> <code>infer_schema_length</code>                 (<code>int</code>)             </li> <li> <code>truncate_ragged_lines</code>                 (<code>bool</code>)             </li> <li> <code>ignore_errors</code>                 (<code>bool</code>)             </li> <li> <code>file_type</code>                 (<code>Literal['json']</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class InputJsonTable(InputCsvTable):\n    \"\"\"Defines settings for reading a JSON file.\"\"\"\n\n    file_type: Literal[\"json\"] = \"json\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.InputParquetTable","title":"<code>InputParquetTable</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>InputTableBase</code></p> <p>Defines settings for reading a Parquet file.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines settings for reading a Parquet file.\",\n  \"properties\": {\n    \"file_type\": {\n      \"const\": \"parquet\",\n      \"default\": \"parquet\",\n      \"enum\": [\n        \"parquet\"\n      ],\n      \"title\": \"File Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"InputParquetTable\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>file_type</code>                 (<code>Literal['parquet']</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class InputParquetTable(InputTableBase):\n    \"\"\"Defines settings for reading a Parquet file.\"\"\"\n\n    file_type: Literal[\"parquet\"] = \"parquet\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.InputTableBase","title":"<code>InputTableBase</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base settings for input file operations.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Base settings for input file operations.\",\n  \"properties\": {\n    \"file_type\": {\n      \"title\": \"File Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"file_type\"\n  ],\n  \"title\": \"InputTableBase\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>file_type</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class InputTableBase(BaseModel):\n    \"\"\"Base settings for input file operations.\"\"\"\n\n    file_type: str  # Will be overridden with Literal in subclasses\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.MinimalFieldInfo","title":"<code>MinimalFieldInfo</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the most basic information about a data field (column).</p> Show JSON schema: <pre><code>{\n  \"description\": \"Represents the most basic information about a data field (column).\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"data_type\": {\n      \"default\": \"String\",\n      \"title\": \"Data Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"name\"\n  ],\n  \"title\": \"MinimalFieldInfo\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>data_type</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class MinimalFieldInfo(BaseModel):\n    \"\"\"Represents the most basic information about a data field (column).\"\"\"\n\n    name: str\n    data_type: str = \"String\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NewDirectory","title":"<code>NewDirectory</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the information required to create a new directory.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines the information required to create a new directory.\",\n  \"properties\": {\n    \"source_path\": {\n      \"title\": \"Source Path\",\n      \"type\": \"string\"\n    },\n    \"dir_name\": {\n      \"title\": \"Dir Name\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"source_path\",\n    \"dir_name\"\n  ],\n  \"title\": \"NewDirectory\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>source_path</code>                 (<code>str</code>)             </li> <li> <code>dir_name</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NewDirectory(BaseModel):\n    \"\"\"Defines the information required to create a new directory.\"\"\"\n\n    source_path: str\n    dir_name: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeBase","title":"<code>NodeBase</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model for all nodes in a FlowGraph. Contains common metadata.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Base model for all nodes in a FlowGraph. Contains common metadata.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodeBase\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeBase(BaseModel):\n    \"\"\"Base model for all nodes in a FlowGraph. Contains common metadata.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    flow_id: int\n    node_id: int\n    cache_results: bool | None = False\n    pos_x: float | None = 0\n    pos_y: float | None = 0\n    is_setup: bool | None = True\n    description: str | None = \"\"\n    node_reference: str | None = None  # Unique reference identifier for code generation (lowercase, no spaces)\n    user_id: int | None = None\n    is_flow_output: bool | None = False\n    is_user_defined: bool | None = False  # Indicator if the node is a user defined node\n    output_field_config: OutputFieldConfig | None = None\n\n    @field_validator(\"node_reference\", mode=\"before\")\n    @classmethod\n    def validate_node_reference(cls, v):\n        \"\"\"Validates that node_reference is lowercase and contains no spaces.\"\"\"\n        if v is None or v == \"\":\n            return None\n        if not isinstance(v, str):\n            raise ValueError(\"node_reference must be a string\")\n        if \" \" in v:\n            raise ValueError(\"node_reference cannot contain spaces\")\n        if v != v.lower():\n            raise ValueError(\"node_reference must be lowercase\")\n        return v\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Generates a human-readable description based on the node's configured content.\n\n        Subclasses override this to provide meaningful descriptions.\n        Returns an empty string by default.\n        \"\"\"\n        return \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeBase.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Generates a human-readable description based on the node's configured content.</p> <p>Subclasses override this to provide meaningful descriptions. Returns an empty string by default.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Generates a human-readable description based on the node's configured content.\n\n    Subclasses override this to provide meaningful descriptions.\n    Returns an empty string by default.\n    \"\"\"\n    return \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeBase.validate_node_reference","title":"<code>validate_node_reference(v)</code>  <code>pydantic-validator</code>","text":"<p>Validates that node_reference is lowercase and contains no spaces.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@field_validator(\"node_reference\", mode=\"before\")\n@classmethod\ndef validate_node_reference(cls, v):\n    \"\"\"Validates that node_reference is lowercase and contains no spaces.\"\"\"\n    if v is None or v == \"\":\n        return None\n    if not isinstance(v, str):\n        raise ValueError(\"node_reference must be a string\")\n    if \" \" in v:\n        raise ValueError(\"node_reference cannot contain spaces\")\n    if v != v.lower():\n        raise ValueError(\"node_reference must be lowercase\")\n    return v\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeCloudStorageReader","title":"<code>NodeCloudStorageReader</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>Settings for a node that reads from a cloud storage service (S3, GCS, etc.).</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"CloudStorageReadSettings\": {\n      \"description\": \"Settings for reading from cloud storage\",\n      \"properties\": {\n        \"auth_mode\": {\n          \"default\": \"auto\",\n          \"enum\": [\n            \"access_key\",\n            \"iam_role\",\n            \"service_principal\",\n            \"managed_identity\",\n            \"sas_token\",\n            \"aws-cli\",\n            \"env_vars\"\n          ],\n          \"title\": \"Auth Mode\",\n          \"type\": \"string\"\n        },\n        \"connection_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Connection Name\"\n        },\n        \"resource_path\": {\n          \"title\": \"Resource Path\",\n          \"type\": \"string\"\n        },\n        \"scan_mode\": {\n          \"default\": \"single_file\",\n          \"enum\": [\n            \"single_file\",\n            \"directory\"\n          ],\n          \"title\": \"Scan Mode\",\n          \"type\": \"string\"\n        },\n        \"file_format\": {\n          \"default\": \"parquet\",\n          \"enum\": [\n            \"csv\",\n            \"parquet\",\n            \"json\",\n            \"delta\",\n            \"iceberg\"\n          ],\n          \"title\": \"File Format\",\n          \"type\": \"string\"\n        },\n        \"csv_has_header\": {\n          \"anyOf\": [\n            {\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": true,\n          \"title\": \"Csv Has Header\"\n        },\n        \"csv_delimiter\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": \",\",\n          \"title\": \"Csv Delimiter\"\n        },\n        \"csv_encoding\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": \"utf8\",\n          \"title\": \"Csv Encoding\"\n        },\n        \"delta_version\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Delta Version\"\n        }\n      },\n      \"required\": [\n        \"resource_path\"\n      ],\n      \"title\": \"CloudStorageReadSettings\",\n      \"type\": \"object\"\n    },\n    \"MinimalFieldInfo\": {\n      \"description\": \"Represents the most basic information about a data field (column).\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"MinimalFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that reads from a cloud storage service (S3, GCS, etc.).\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"cloud_storage_settings\": {\n      \"$ref\": \"#/$defs/CloudStorageReadSettings\"\n    },\n    \"fields\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/MinimalFieldInfo\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Fields\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"cloud_storage_settings\"\n  ],\n  \"title\": \"NodeCloudStorageReader\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>cloud_storage_settings</code>                 (<code>CloudStorageReadSettings</code>)             </li> <li> <code>fields</code>                 (<code>list[MinimalFieldInfo] | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeCloudStorageReader(NodeBase):\n    \"\"\"Settings for a node that reads from a cloud storage service (S3, GCS, etc.).\"\"\"\n\n    cloud_storage_settings: CloudStorageReadSettings\n    fields: list[MinimalFieldInfo] | None = None\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the cloud storage source.\"\"\"\n        cs = self.cloud_storage_settings\n        return f\"Read {cs.resource_path} ({cs.file_format})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeCloudStorageReader.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the cloud storage source.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the cloud storage source.\"\"\"\n    cs = self.cloud_storage_settings\n    return f\"Read {cs.resource_path} ({cs.file_format})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeCloudStorageWriter","title":"<code>NodeCloudStorageWriter</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that writes to a cloud storage service.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"CloudStorageWriteSettings\": {\n      \"description\": \"Settings for writing to cloud storage\",\n      \"properties\": {\n        \"resource_path\": {\n          \"title\": \"Resource Path\",\n          \"type\": \"string\"\n        },\n        \"write_mode\": {\n          \"default\": \"overwrite\",\n          \"enum\": [\n            \"overwrite\",\n            \"append\"\n          ],\n          \"title\": \"Write Mode\",\n          \"type\": \"string\"\n        },\n        \"file_format\": {\n          \"default\": \"parquet\",\n          \"enum\": [\n            \"csv\",\n            \"parquet\",\n            \"json\",\n            \"delta\"\n          ],\n          \"title\": \"File Format\",\n          \"type\": \"string\"\n        },\n        \"parquet_compression\": {\n          \"default\": \"snappy\",\n          \"enum\": [\n            \"snappy\",\n            \"gzip\",\n            \"brotli\",\n            \"lz4\",\n            \"zstd\"\n          ],\n          \"title\": \"Parquet Compression\",\n          \"type\": \"string\"\n        },\n        \"csv_delimiter\": {\n          \"default\": \",\",\n          \"title\": \"Csv Delimiter\",\n          \"type\": \"string\"\n        },\n        \"csv_encoding\": {\n          \"default\": \"utf8\",\n          \"title\": \"Csv Encoding\",\n          \"type\": \"string\"\n        },\n        \"auth_mode\": {\n          \"default\": \"auto\",\n          \"enum\": [\n            \"access_key\",\n            \"iam_role\",\n            \"service_principal\",\n            \"managed_identity\",\n            \"sas_token\",\n            \"aws-cli\",\n            \"env_vars\"\n          ],\n          \"title\": \"Auth Mode\",\n          \"type\": \"string\"\n        },\n        \"connection_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Connection Name\"\n        }\n      },\n      \"required\": [\n        \"resource_path\"\n      ],\n      \"title\": \"CloudStorageWriteSettings\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that writes to a cloud storage service.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"cloud_storage_settings\": {\n      \"$ref\": \"#/$defs/CloudStorageWriteSettings\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"cloud_storage_settings\"\n  ],\n  \"title\": \"NodeCloudStorageWriter\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>cloud_storage_settings</code>                 (<code>CloudStorageWriteSettings</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeCloudStorageWriter(NodeSingleInput):\n    \"\"\"Settings for a node that writes to a cloud storage service.\"\"\"\n\n    cloud_storage_settings: CloudStorageWriteSettings\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the cloud storage write target.\"\"\"\n        cs = self.cloud_storage_settings\n        return f\"Write to {cs.resource_path} ({cs.file_format})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeCloudStorageWriter.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the cloud storage write target.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the cloud storage write target.\"\"\"\n    cs = self.cloud_storage_settings\n    return f\"Write to {cs.resource_path} ({cs.file_format})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeConnection","title":"<code>NodeConnection</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a connection (edge) between two nodes in the graph.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"NodeInputConnection\": {\n      \"description\": \"Represents the input side of a connection between two nodes.\",\n      \"properties\": {\n        \"node_id\": {\n          \"title\": \"Node Id\",\n          \"type\": \"integer\"\n        },\n        \"connection_class\": {\n          \"enum\": [\n            \"input-0\",\n            \"input-1\",\n            \"input-2\",\n            \"input-3\",\n            \"input-4\",\n            \"input-5\",\n            \"input-6\",\n            \"input-7\",\n            \"input-8\",\n            \"input-9\"\n          ],\n          \"title\": \"Connection Class\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"node_id\",\n        \"connection_class\"\n      ],\n      \"title\": \"NodeInputConnection\",\n      \"type\": \"object\"\n    },\n    \"NodeOutputConnection\": {\n      \"description\": \"Represents the output side of a connection between two nodes.\",\n      \"properties\": {\n        \"node_id\": {\n          \"title\": \"Node Id\",\n          \"type\": \"integer\"\n        },\n        \"connection_class\": {\n          \"enum\": [\n            \"output-0\",\n            \"output-1\",\n            \"output-2\",\n            \"output-3\",\n            \"output-4\",\n            \"output-5\",\n            \"output-6\",\n            \"output-7\",\n            \"output-8\",\n            \"output-9\"\n          ],\n          \"title\": \"Connection Class\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"node_id\",\n        \"connection_class\"\n      ],\n      \"title\": \"NodeOutputConnection\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Represents a connection (edge) between two nodes in the graph.\",\n  \"properties\": {\n    \"input_connection\": {\n      \"$ref\": \"#/$defs/NodeInputConnection\"\n    },\n    \"output_connection\": {\n      \"$ref\": \"#/$defs/NodeOutputConnection\"\n    }\n  },\n  \"required\": [\n    \"input_connection\",\n    \"output_connection\"\n  ],\n  \"title\": \"NodeConnection\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>input_connection</code>                 (<code>NodeInputConnection</code>)             </li> <li> <code>output_connection</code>                 (<code>NodeOutputConnection</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeConnection(BaseModel):\n    \"\"\"Represents a connection (edge) between two nodes in the graph.\"\"\"\n\n    input_connection: NodeInputConnection\n    output_connection: NodeOutputConnection\n\n    @classmethod\n    def create_from_simple_input(cls, from_id: int, to_id: int, input_type: InputType = \"input-0\"):\n        \"\"\"Creates a standard connection between two nodes.\"\"\"\n        match input_type:\n            case \"main\":\n                connection_class: InputConnectionClass = \"input-0\"\n            case \"right\":\n                connection_class: InputConnectionClass = \"input-1\"\n            case \"left\":\n                connection_class: InputConnectionClass = \"input-2\"\n            case _:\n                connection_class: InputConnectionClass = \"input-0\"\n        node_input = NodeInputConnection(node_id=to_id, connection_class=connection_class)\n        node_output = NodeOutputConnection(node_id=from_id, connection_class=\"output-0\")\n        return cls(input_connection=node_input, output_connection=node_output)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeConnection.create_from_simple_input","title":"<code>create_from_simple_input(from_id, to_id, input_type='input-0')</code>  <code>classmethod</code>","text":"<p>Creates a standard connection between two nodes.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@classmethod\ndef create_from_simple_input(cls, from_id: int, to_id: int, input_type: InputType = \"input-0\"):\n    \"\"\"Creates a standard connection between two nodes.\"\"\"\n    match input_type:\n        case \"main\":\n            connection_class: InputConnectionClass = \"input-0\"\n        case \"right\":\n            connection_class: InputConnectionClass = \"input-1\"\n        case \"left\":\n            connection_class: InputConnectionClass = \"input-2\"\n        case _:\n            connection_class: InputConnectionClass = \"input-0\"\n    node_input = NodeInputConnection(node_id=to_id, connection_class=connection_class)\n    node_output = NodeOutputConnection(node_id=from_id, connection_class=\"output-0\")\n    return cls(input_connection=node_input, output_connection=node_output)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeCrossJoin","title":"<code>NodeCrossJoin</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeMultiInput</code></p> <p>Settings for a node that performs a cross join.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"CrossJoinInput\": {\n      \"description\": \"Data model for cross join operations.\",\n      \"properties\": {\n        \"left_select\": {\n          \"$ref\": \"#/$defs/JoinInputs\"\n        },\n        \"right_select\": {\n          \"$ref\": \"#/$defs/JoinInputs\"\n        }\n      },\n      \"required\": [\n        \"left_select\",\n        \"right_select\"\n      ],\n      \"title\": \"CrossJoinInput\",\n      \"type\": \"object\"\n    },\n    \"JoinInputs\": {\n      \"description\": \"Data model for join-specific select inputs (extends SelectInputs).\",\n      \"properties\": {\n        \"renames\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/SelectInput\"\n          },\n          \"title\": \"Renames\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"JoinInputs\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"SelectInput\": {\n      \"description\": \"Defines how a single column should be selected, renamed, or type-cast.\\n\\nThis is a core building block for any operation that involves column manipulation.\\nIt holds all the configuration for a single field in a selection operation.\",\n      \"properties\": {\n        \"old_name\": {\n          \"title\": \"Old Name\",\n          \"type\": \"string\"\n        },\n        \"original_position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Original Position\"\n        },\n        \"new_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"New Name\"\n        },\n        \"data_type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Data Type\"\n        },\n        \"data_type_change\": {\n          \"default\": false,\n          \"title\": \"Data Type Change\",\n          \"type\": \"boolean\"\n        },\n        \"join_key\": {\n          \"default\": false,\n          \"title\": \"Join Key\",\n          \"type\": \"boolean\"\n        },\n        \"is_altered\": {\n          \"default\": false,\n          \"title\": \"Is Altered\",\n          \"type\": \"boolean\"\n        },\n        \"position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Position\"\n        },\n        \"is_available\": {\n          \"default\": true,\n          \"title\": \"Is Available\",\n          \"type\": \"boolean\"\n        },\n        \"keep\": {\n          \"default\": true,\n          \"title\": \"Keep\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"old_name\"\n      ],\n      \"title\": \"SelectInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that performs a cross join.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_ids\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"integer\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Depending On Ids\"\n    },\n    \"auto_generate_selection\": {\n      \"default\": true,\n      \"title\": \"Auto Generate Selection\",\n      \"type\": \"boolean\"\n    },\n    \"verify_integrity\": {\n      \"default\": true,\n      \"title\": \"Verify Integrity\",\n      \"type\": \"boolean\"\n    },\n    \"cross_join_input\": {\n      \"$ref\": \"#/$defs/CrossJoinInput\"\n    },\n    \"auto_keep_all\": {\n      \"default\": true,\n      \"title\": \"Auto Keep All\",\n      \"type\": \"boolean\"\n    },\n    \"auto_keep_right\": {\n      \"default\": true,\n      \"title\": \"Auto Keep Right\",\n      \"type\": \"boolean\"\n    },\n    \"auto_keep_left\": {\n      \"default\": true,\n      \"title\": \"Auto Keep Left\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"cross_join_input\"\n  ],\n  \"title\": \"NodeCrossJoin\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_ids</code>                 (<code>list[int] | None</code>)             </li> <li> <code>auto_generate_selection</code>                 (<code>bool</code>)             </li> <li> <code>verify_integrity</code>                 (<code>bool</code>)             </li> <li> <code>cross_join_input</code>                 (<code>CrossJoinInput</code>)             </li> <li> <code>auto_keep_all</code>                 (<code>bool</code>)             </li> <li> <code>auto_keep_right</code>                 (<code>bool</code>)             </li> <li> <code>auto_keep_left</code>                 (<code>bool</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeCrossJoin(NodeMultiInput):\n    \"\"\"Settings for a node that performs a cross join.\"\"\"\n\n    auto_generate_selection: bool = True\n    verify_integrity: bool = True\n    cross_join_input: transform_schema.CrossJoinInput\n    auto_keep_all: bool = True\n    auto_keep_right: bool = True\n    auto_keep_left: bool = True\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the cross join.\"\"\"\n        return \"Cross join\"\n\n    def to_yaml_dict(self) -&gt; NodeCrossJoinYaml:\n        \"\"\"Converts the cross join node settings to a dictionary for YAML serialization.\"\"\"\n        result: NodeCrossJoinYaml = {\n            \"cache_results\": self.cache_results,\n            \"auto_generate_selection\": self.auto_generate_selection,\n            \"verify_integrity\": self.verify_integrity,\n            \"cross_join_input\": self.cross_join_input.to_yaml_dict(),\n            \"auto_keep_all\": self.auto_keep_all,\n            \"auto_keep_right\": self.auto_keep_right,\n            \"auto_keep_left\": self.auto_keep_left,\n        }\n        if self.output_field_config:\n            result[\"output_field_config\"] = {\n                \"enabled\": self.output_field_config.enabled,\n                \"validation_mode_behavior\": self.output_field_config.validation_mode_behavior,\n                \"validate_data_types\": self.output_field_config.validate_data_types,\n                \"fields\": [\n                    {\n                        \"name\": f.name,\n                        \"data_type\": f.data_type,\n                        \"default_value\": f.default_value,\n                    }\n                    for f in self.output_field_config.fields\n                ],\n            }\n        return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeCrossJoin.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the cross join.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the cross join.\"\"\"\n    return \"Cross join\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeCrossJoin.to_yaml_dict","title":"<code>to_yaml_dict()</code>","text":"<p>Converts the cross join node settings to a dictionary for YAML serialization.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def to_yaml_dict(self) -&gt; NodeCrossJoinYaml:\n    \"\"\"Converts the cross join node settings to a dictionary for YAML serialization.\"\"\"\n    result: NodeCrossJoinYaml = {\n        \"cache_results\": self.cache_results,\n        \"auto_generate_selection\": self.auto_generate_selection,\n        \"verify_integrity\": self.verify_integrity,\n        \"cross_join_input\": self.cross_join_input.to_yaml_dict(),\n        \"auto_keep_all\": self.auto_keep_all,\n        \"auto_keep_right\": self.auto_keep_right,\n        \"auto_keep_left\": self.auto_keep_left,\n    }\n    if self.output_field_config:\n        result[\"output_field_config\"] = {\n            \"enabled\": self.output_field_config.enabled,\n            \"validation_mode_behavior\": self.output_field_config.validation_mode_behavior,\n            \"validate_data_types\": self.output_field_config.validate_data_types,\n            \"fields\": [\n                {\n                    \"name\": f.name,\n                    \"data_type\": f.data_type,\n                    \"default_value\": f.default_value,\n                }\n                for f in self.output_field_config.fields\n            ],\n        }\n    return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeDatabaseReader","title":"<code>NodeDatabaseReader</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>Settings for a node that reads from a database.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"DatabaseConnection\": {\n      \"description\": \"Defines the connection parameters for a database.\",\n      \"properties\": {\n        \"database_type\": {\n          \"default\": \"postgresql\",\n          \"title\": \"Database Type\",\n          \"type\": \"string\"\n        },\n        \"username\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Username\"\n        },\n        \"password_ref\": {\n          \"anyOf\": [\n            {\n              \"description\": \"An ID referencing an encrypted secret.\",\n              \"maxLength\": 100,\n              \"minLength\": 1,\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Password Ref\"\n        },\n        \"host\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Host\"\n        },\n        \"port\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Port\"\n        },\n        \"database\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Database\"\n        },\n        \"url\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Url\"\n        }\n      },\n      \"title\": \"DatabaseConnection\",\n      \"type\": \"object\"\n    },\n    \"DatabaseSettings\": {\n      \"description\": \"Defines settings for reading from a database, either via table or query.\",\n      \"properties\": {\n        \"connection_mode\": {\n          \"anyOf\": [\n            {\n              \"enum\": [\n                \"inline\",\n                \"reference\"\n              ],\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": \"inline\",\n          \"title\": \"Connection Mode\"\n        },\n        \"database_connection\": {\n          \"anyOf\": [\n            {\n              \"$ref\": \"#/$defs/DatabaseConnection\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null\n        },\n        \"database_connection_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Database Connection Name\"\n        },\n        \"schema_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Schema Name\"\n        },\n        \"table_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Table Name\"\n        },\n        \"query\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Query\"\n        },\n        \"query_mode\": {\n          \"default\": \"table\",\n          \"enum\": [\n            \"query\",\n            \"table\",\n            \"reference\"\n          ],\n          \"title\": \"Query Mode\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"DatabaseSettings\",\n      \"type\": \"object\"\n    },\n    \"MinimalFieldInfo\": {\n      \"description\": \"Represents the most basic information about a data field (column).\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"MinimalFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that reads from a database.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"database_settings\": {\n      \"$ref\": \"#/$defs/DatabaseSettings\"\n    },\n    \"fields\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/MinimalFieldInfo\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Fields\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"database_settings\"\n  ],\n  \"title\": \"NodeDatabaseReader\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>database_settings</code>                 (<code>DatabaseSettings</code>)             </li> <li> <code>fields</code>                 (<code>list[MinimalFieldInfo] | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeDatabaseReader(NodeBase):\n    \"\"\"Settings for a node that reads from a database.\"\"\"\n\n    database_settings: DatabaseSettings\n    fields: list[MinimalFieldInfo] | None = None\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the database source.\"\"\"\n        ds = self.database_settings\n        if ds.query_mode == \"table\" and ds.table_name:\n            table = f\"{ds.schema_name}.{ds.table_name}\" if ds.schema_name else ds.table_name\n            return f\"Read from {table}\"\n        if ds.query_mode == \"query\" and ds.query:\n            q = ds.query\n            if len(q) &gt; 60:\n                q = q[:57] + \"...\"\n            return f\"Query: {q}\"\n        return \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeDatabaseReader.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the database source.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the database source.\"\"\"\n    ds = self.database_settings\n    if ds.query_mode == \"table\" and ds.table_name:\n        table = f\"{ds.schema_name}.{ds.table_name}\" if ds.schema_name else ds.table_name\n        return f\"Read from {table}\"\n    if ds.query_mode == \"query\" and ds.query:\n        q = ds.query\n        if len(q) &gt; 60:\n            q = q[:57] + \"...\"\n        return f\"Query: {q}\"\n    return \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeDatabaseWriter","title":"<code>NodeDatabaseWriter</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that writes data to a database.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"DatabaseConnection\": {\n      \"description\": \"Defines the connection parameters for a database.\",\n      \"properties\": {\n        \"database_type\": {\n          \"default\": \"postgresql\",\n          \"title\": \"Database Type\",\n          \"type\": \"string\"\n        },\n        \"username\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Username\"\n        },\n        \"password_ref\": {\n          \"anyOf\": [\n            {\n              \"description\": \"An ID referencing an encrypted secret.\",\n              \"maxLength\": 100,\n              \"minLength\": 1,\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Password Ref\"\n        },\n        \"host\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Host\"\n        },\n        \"port\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Port\"\n        },\n        \"database\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Database\"\n        },\n        \"url\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Url\"\n        }\n      },\n      \"title\": \"DatabaseConnection\",\n      \"type\": \"object\"\n    },\n    \"DatabaseWriteSettings\": {\n      \"description\": \"Defines settings for writing data to a database table.\",\n      \"properties\": {\n        \"connection_mode\": {\n          \"anyOf\": [\n            {\n              \"enum\": [\n                \"inline\",\n                \"reference\"\n              ],\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": \"inline\",\n          \"title\": \"Connection Mode\"\n        },\n        \"database_connection\": {\n          \"anyOf\": [\n            {\n              \"$ref\": \"#/$defs/DatabaseConnection\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null\n        },\n        \"database_connection_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Database Connection Name\"\n        },\n        \"table_name\": {\n          \"title\": \"Table Name\",\n          \"type\": \"string\"\n        },\n        \"schema_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Schema Name\"\n        },\n        \"if_exists\": {\n          \"anyOf\": [\n            {\n              \"enum\": [\n                \"append\",\n                \"replace\",\n                \"fail\"\n              ],\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": \"append\",\n          \"title\": \"If Exists\"\n        }\n      },\n      \"required\": [\n        \"table_name\"\n      ],\n      \"title\": \"DatabaseWriteSettings\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that writes data to a database.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"database_write_settings\": {\n      \"$ref\": \"#/$defs/DatabaseWriteSettings\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"database_write_settings\"\n  ],\n  \"title\": \"NodeDatabaseWriter\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>database_write_settings</code>                 (<code>DatabaseWriteSettings</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeDatabaseWriter(NodeSingleInput):\n    \"\"\"Settings for a node that writes data to a database.\"\"\"\n\n    database_write_settings: DatabaseWriteSettings\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the database write target.\"\"\"\n        dw = self.database_write_settings\n        table = f\"{dw.schema_name}.{dw.table_name}\" if dw.schema_name else dw.table_name\n        return f\"Write to {table} ({dw.if_exists})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeDatabaseWriter.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the database write target.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the database write target.\"\"\"\n    dw = self.database_write_settings\n    table = f\"{dw.schema_name}.{dw.table_name}\" if dw.schema_name else dw.table_name\n    return f\"Write to {table} ({dw.if_exists})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeDatasource","title":"<code>NodeDatasource</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>Base settings for a node that acts as a data source.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Base settings for a node that acts as a data source.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"file_ref\": {\n      \"default\": null,\n      \"title\": \"File Ref\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodeDatasource\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>file_ref</code>                 (<code>str</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeDatasource(NodeBase):\n    \"\"\"Base settings for a node that acts as a data source.\"\"\"\n\n    file_ref: str = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeDescription","title":"<code>NodeDescription</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A simple model for updating a node's description text.</p> Show JSON schema: <pre><code>{\n  \"description\": \"A simple model for updating a node's description text.\",\n  \"properties\": {\n    \"description\": {\n      \"default\": \"\",\n      \"title\": \"Description\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"NodeDescription\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>description</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeDescription(BaseModel):\n    \"\"\"A simple model for updating a node's description text.\"\"\"\n\n    description: str = \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeExploreData","title":"<code>NodeExploreData</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>Settings for a node that provides an interactive data exploration interface.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"DataModel\": {\n      \"properties\": {\n        \"data\": {\n          \"items\": {\n            \"type\": \"object\"\n          },\n          \"title\": \"Data\",\n          \"type\": \"array\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/MutField\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"data\",\n        \"fields\"\n      ],\n      \"title\": \"DataModel\",\n      \"type\": \"object\"\n    },\n    \"GraphicWalkerInput\": {\n      \"properties\": {\n        \"dataModel\": {\n          \"$ref\": \"#/$defs/DataModel\"\n        },\n        \"is_initial\": {\n          \"default\": true,\n          \"title\": \"Is Initial\",\n          \"type\": \"boolean\"\n        },\n        \"specList\": {\n          \"anyOf\": [\n            {\n              \"items\": {},\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Speclist\"\n        }\n      },\n      \"title\": \"GraphicWalkerInput\",\n      \"type\": \"object\"\n    },\n    \"MutField\": {\n      \"properties\": {\n        \"fid\": {\n          \"title\": \"Fid\",\n          \"type\": \"string\"\n        },\n        \"key\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Key\"\n        },\n        \"name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Name\"\n        },\n        \"basename\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Basename\"\n        },\n        \"disable\": {\n          \"anyOf\": [\n            {\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": false,\n          \"title\": \"Disable\"\n        },\n        \"semanticType\": {\n          \"title\": \"Semantictype\",\n          \"type\": \"string\"\n        },\n        \"analyticType\": {\n          \"enum\": [\n            \"measure\",\n            \"dimension\"\n          ],\n          \"title\": \"Analytictype\",\n          \"type\": \"string\"\n        },\n        \"path\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Path\"\n        },\n        \"offset\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Offset\"\n        }\n      },\n      \"required\": [\n        \"fid\",\n        \"semanticType\",\n        \"analyticType\"\n      ],\n      \"title\": \"MutField\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that provides an interactive data exploration interface.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"graphic_walker_input\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/GraphicWalkerInput\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodeExploreData\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>graphic_walker_input</code>                 (<code>GraphicWalkerInput | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeExploreData(NodeBase):\n    \"\"\"Settings for a node that provides an interactive data exploration interface.\"\"\"\n\n    graphic_walker_input: gs_schemas.GraphicWalkerInput | None = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeExternalSource","title":"<code>NodeExternalSource</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>Settings for a node that connects to a registered external data source.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"MinimalFieldInfo\": {\n      \"description\": \"Represents the most basic information about a data field (column).\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"MinimalFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"SampleUsers\": {\n      \"description\": \"Settings for generating a sample dataset of users.\",\n      \"properties\": {\n        \"orientation\": {\n          \"default\": \"row\",\n          \"title\": \"Orientation\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/MinimalFieldInfo\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Fields\"\n        },\n        \"SAMPLE_USERS\": {\n          \"title\": \"Sample Users\",\n          \"type\": \"boolean\"\n        },\n        \"class_name\": {\n          \"default\": \"sample_users\",\n          \"title\": \"Class Name\",\n          \"type\": \"string\"\n        },\n        \"size\": {\n          \"default\": 100,\n          \"title\": \"Size\",\n          \"type\": \"integer\"\n        }\n      },\n      \"required\": [\n        \"SAMPLE_USERS\"\n      ],\n      \"title\": \"SampleUsers\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that connects to a registered external data source.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"identifier\": {\n      \"title\": \"Identifier\",\n      \"type\": \"string\"\n    },\n    \"source_settings\": {\n      \"$ref\": \"#/$defs/SampleUsers\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"identifier\",\n    \"source_settings\"\n  ],\n  \"title\": \"NodeExternalSource\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>identifier</code>                 (<code>str</code>)             </li> <li> <code>source_settings</code>                 (<code>SampleUsers</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeExternalSource(NodeBase):\n    \"\"\"Settings for a node that connects to a registered external data source.\"\"\"\n\n    identifier: str\n    source_settings: SampleUsers\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the external source.\"\"\"\n        return self.identifier\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeExternalSource.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the external source.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the external source.\"\"\"\n    return self.identifier\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeFilter","title":"<code>NodeFilter</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that filters rows based on a condition.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"BasicFilter\": {\n      \"description\": \"Defines a simple, single-condition filter (e.g., 'column' 'equals' 'value').\\n\\nAttributes:\\n    field: The column name to filter on.\\n    operator: The comparison operator (FilterOperator enum value or symbol).\\n    value: The value to compare against.\\n    value2: Second value for BETWEEN operator (optional).\",\n      \"properties\": {\n        \"field\": {\n          \"default\": \"\",\n          \"title\": \"Field\",\n          \"type\": \"string\"\n        },\n        \"operator\": {\n          \"anyOf\": [\n            {\n              \"$ref\": \"#/$defs/FilterOperator\"\n            },\n            {\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"equals\",\n          \"title\": \"Operator\"\n        },\n        \"value\": {\n          \"default\": \"\",\n          \"title\": \"Value\",\n          \"type\": \"string\"\n        },\n        \"value2\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Value2\"\n        },\n        \"filter_type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Filter Type\"\n        },\n        \"filter_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Filter Value\"\n        }\n      },\n      \"title\": \"BasicFilter\",\n      \"type\": \"object\"\n    },\n    \"FilterInput\": {\n      \"description\": \"Defines the settings for a filter operation, supporting basic or advanced (expression-based) modes.\\n\\nAttributes:\\n    mode: The filter mode - \\\"basic\\\" or \\\"advanced\\\".\\n    basic_filter: The basic filter configuration (used when mode=\\\"basic\\\").\\n    advanced_filter: The advanced filter expression string (used when mode=\\\"advanced\\\").\",\n      \"properties\": {\n        \"mode\": {\n          \"default\": \"basic\",\n          \"enum\": [\n            \"basic\",\n            \"advanced\"\n          ],\n          \"title\": \"Mode\",\n          \"type\": \"string\"\n        },\n        \"basic_filter\": {\n          \"anyOf\": [\n            {\n              \"$ref\": \"#/$defs/BasicFilter\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null\n        },\n        \"advanced_filter\": {\n          \"default\": \"\",\n          \"title\": \"Advanced Filter\",\n          \"type\": \"string\"\n        },\n        \"filter_type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Filter Type\"\n        }\n      },\n      \"title\": \"FilterInput\",\n      \"type\": \"object\"\n    },\n    \"FilterOperator\": {\n      \"description\": \"Supported filter comparison operators.\",\n      \"enum\": [\n        \"equals\",\n        \"not_equals\",\n        \"greater_than\",\n        \"greater_than_or_equals\",\n        \"less_than\",\n        \"less_than_or_equals\",\n        \"contains\",\n        \"not_contains\",\n        \"starts_with\",\n        \"ends_with\",\n        \"is_null\",\n        \"is_not_null\",\n        \"in\",\n        \"not_in\",\n        \"between\"\n      ],\n      \"title\": \"FilterOperator\",\n      \"type\": \"string\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that filters rows based on a condition.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"filter_input\": {\n      \"$ref\": \"#/$defs/FilterInput\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"filter_input\"\n  ],\n  \"title\": \"NodeFilter\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>filter_input</code>                 (<code>FilterInput</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeFilter(NodeSingleInput):\n    \"\"\"Settings for a node that filters rows based on a condition.\"\"\"\n\n    filter_input: transform_schema.FilterInput\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the filter condition.\"\"\"\n        fi = self.filter_input\n        if fi.mode == \"advanced\" and fi.advanced_filter:\n            expr = fi.advanced_filter\n            if len(expr) &gt; 80:\n                expr = expr[:77] + \"...\"\n            return expr\n        if fi.mode == \"basic\" and fi.basic_filter:\n            bf = fi.basic_filter\n            if not bf.field:\n                return \"\"\n            op = bf.operator\n            op_str = op.to_symbol() if hasattr(op, \"to_symbol\") else str(op)\n            if op_str in (\"is_null\", \"is_not_null\"):\n                return f\"{bf.field} {op_str}\"\n            if op_str == \"between\" and bf.value2:\n                return f\"{bf.field} between {bf.value} and {bf.value2}\"\n            return f\"{bf.field} {op_str} {bf.value}\"\n        return \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeFilter.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the filter condition.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the filter condition.\"\"\"\n    fi = self.filter_input\n    if fi.mode == \"advanced\" and fi.advanced_filter:\n        expr = fi.advanced_filter\n        if len(expr) &gt; 80:\n            expr = expr[:77] + \"...\"\n        return expr\n    if fi.mode == \"basic\" and fi.basic_filter:\n        bf = fi.basic_filter\n        if not bf.field:\n            return \"\"\n        op = bf.operator\n        op_str = op.to_symbol() if hasattr(op, \"to_symbol\") else str(op)\n        if op_str in (\"is_null\", \"is_not_null\"):\n            return f\"{bf.field} {op_str}\"\n        if op_str == \"between\" and bf.value2:\n            return f\"{bf.field} between {bf.value} and {bf.value2}\"\n        return f\"{bf.field} {op_str} {bf.value}\"\n    return \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeFormula","title":"<code>NodeFormula</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that applies a formula to create/modify a column.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"DataType\": {\n      \"description\": \"Specific data types for fine-grained control.\",\n      \"enum\": [\n        \"Int8\",\n        \"Int16\",\n        \"Int32\",\n        \"Int64\",\n        \"UInt8\",\n        \"UInt16\",\n        \"UInt32\",\n        \"UInt64\",\n        \"Float32\",\n        \"Float64\",\n        \"Decimal\",\n        \"String\",\n        \"Categorical\",\n        \"Date\",\n        \"Datetime\",\n        \"Time\",\n        \"Duration\",\n        \"Boolean\",\n        \"Binary\",\n        \"List\",\n        \"Struct\",\n        \"Array\"\n      ],\n      \"title\": \"DataType\",\n      \"type\": \"string\"\n    },\n    \"FieldInput\": {\n      \"description\": \"Represents a single field with its name and data type, typically for defining an output column.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"anyOf\": [\n            {\n              \"$ref\": \"#/$defs/DataType\"\n            },\n            {\n              \"const\": \"Auto\",\n              \"enum\": [\n                \"Auto\"\n              ],\n              \"type\": \"string\"\n            },\n            {\n              \"enum\": [\n                \"Int8\",\n                \"Int16\",\n                \"Int32\",\n                \"Int64\",\n                \"UInt8\",\n                \"UInt16\",\n                \"UInt32\",\n                \"UInt64\",\n                \"Float32\",\n                \"Float64\",\n                \"Decimal\",\n                \"String\",\n                \"Date\",\n                \"Datetime\",\n                \"Time\",\n                \"Duration\",\n                \"Boolean\",\n                \"Binary\",\n                \"List\",\n                \"Struct\",\n                \"Array\",\n                \"Integer\",\n                \"Double\",\n                \"Utf8\"\n              ],\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": \"Auto\",\n          \"title\": \"Data Type\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"FieldInput\",\n      \"type\": \"object\"\n    },\n    \"FunctionInput\": {\n      \"description\": \"Defines a formula to be applied, including the output field information.\",\n      \"properties\": {\n        \"field\": {\n          \"$ref\": \"#/$defs/FieldInput\"\n        },\n        \"function\": {\n          \"title\": \"Function\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"field\",\n        \"function\"\n      ],\n      \"title\": \"FunctionInput\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that applies a formula to create/modify a column.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"function\": {\n      \"$ref\": \"#/$defs/FunctionInput\",\n      \"default\": null\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodeFormula\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>function</code>                 (<code>FunctionInput</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeFormula(NodeSingleInput):\n    \"\"\"Settings for a node that applies a formula to create/modify a column.\"\"\"\n\n    function: transform_schema.FunctionInput = None\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the formula being applied.\"\"\"\n        if self.function is None:\n            return \"\"\n        name = self.function.field.name if self.function.field else \"\"\n        expr = self.function.function or \"\"\n        if len(expr) &gt; 60:\n            expr = expr[:57] + \"...\"\n        return f\"{name} = {expr}\" if name else expr\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeFormula.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the formula being applied.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the formula being applied.\"\"\"\n    if self.function is None:\n        return \"\"\n    name = self.function.field.name if self.function.field else \"\"\n    expr = self.function.function or \"\"\n    if len(expr) &gt; 60:\n        expr = expr[:57] + \"...\"\n    return f\"{name} = {expr}\" if name else expr\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeFuzzyMatch","title":"<code>NodeFuzzyMatch</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeJoin</code></p> <p>Settings for a node that performs a fuzzy join based on string similarity.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FuzzyMapping\": {\n      \"properties\": {\n        \"left_col\": {\n          \"title\": \"Left Col\",\n          \"type\": \"string\"\n        },\n        \"right_col\": {\n          \"title\": \"Right Col\",\n          \"type\": \"string\"\n        },\n        \"threshold_score\": {\n          \"default\": 80.0,\n          \"title\": \"Threshold Score\",\n          \"type\": \"number\"\n        },\n        \"fuzzy_type\": {\n          \"default\": \"levenshtein\",\n          \"enum\": [\n            \"levenshtein\",\n            \"jaro\",\n            \"jaro_winkler\",\n            \"hamming\",\n            \"damerau_levenshtein\",\n            \"indel\"\n          ],\n          \"title\": \"Fuzzy Type\",\n          \"type\": \"string\"\n        },\n        \"perc_unique\": {\n          \"default\": 0.0,\n          \"title\": \"Perc Unique\",\n          \"type\": \"number\"\n        },\n        \"output_column_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Output Column Name\"\n        },\n        \"valid\": {\n          \"default\": true,\n          \"title\": \"Valid\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"left_col\",\n        \"right_col\"\n      ],\n      \"title\": \"FuzzyMapping\",\n      \"type\": \"object\"\n    },\n    \"FuzzyMatchInput\": {\n      \"description\": \"Data model for fuzzy matching join operations.\",\n      \"properties\": {\n        \"join_mapping\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FuzzyMapping\"\n          },\n          \"title\": \"Join Mapping\",\n          \"type\": \"array\"\n        },\n        \"left_select\": {\n          \"$ref\": \"#/$defs/JoinInputs\"\n        },\n        \"right_select\": {\n          \"$ref\": \"#/$defs/JoinInputs\"\n        },\n        \"how\": {\n          \"default\": \"inner\",\n          \"enum\": [\n            \"inner\",\n            \"left\",\n            \"right\",\n            \"full\",\n            \"semi\",\n            \"anti\",\n            \"cross\",\n            \"outer\"\n          ],\n          \"title\": \"How\",\n          \"type\": \"string\"\n        },\n        \"aggregate_output\": {\n          \"default\": false,\n          \"title\": \"Aggregate Output\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"join_mapping\",\n        \"left_select\",\n        \"right_select\"\n      ],\n      \"title\": \"FuzzyMatchInput\",\n      \"type\": \"object\"\n    },\n    \"JoinInputs\": {\n      \"description\": \"Data model for join-specific select inputs (extends SelectInputs).\",\n      \"properties\": {\n        \"renames\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/SelectInput\"\n          },\n          \"title\": \"Renames\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"JoinInputs\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"SelectInput\": {\n      \"description\": \"Defines how a single column should be selected, renamed, or type-cast.\\n\\nThis is a core building block for any operation that involves column manipulation.\\nIt holds all the configuration for a single field in a selection operation.\",\n      \"properties\": {\n        \"old_name\": {\n          \"title\": \"Old Name\",\n          \"type\": \"string\"\n        },\n        \"original_position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Original Position\"\n        },\n        \"new_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"New Name\"\n        },\n        \"data_type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Data Type\"\n        },\n        \"data_type_change\": {\n          \"default\": false,\n          \"title\": \"Data Type Change\",\n          \"type\": \"boolean\"\n        },\n        \"join_key\": {\n          \"default\": false,\n          \"title\": \"Join Key\",\n          \"type\": \"boolean\"\n        },\n        \"is_altered\": {\n          \"default\": false,\n          \"title\": \"Is Altered\",\n          \"type\": \"boolean\"\n        },\n        \"position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Position\"\n        },\n        \"is_available\": {\n          \"default\": true,\n          \"title\": \"Is Available\",\n          \"type\": \"boolean\"\n        },\n        \"keep\": {\n          \"default\": true,\n          \"title\": \"Keep\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"old_name\"\n      ],\n      \"title\": \"SelectInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that performs a fuzzy join based on string similarity.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_ids\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"integer\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Depending On Ids\"\n    },\n    \"auto_generate_selection\": {\n      \"default\": true,\n      \"title\": \"Auto Generate Selection\",\n      \"type\": \"boolean\"\n    },\n    \"verify_integrity\": {\n      \"default\": true,\n      \"title\": \"Verify Integrity\",\n      \"type\": \"boolean\"\n    },\n    \"join_input\": {\n      \"$ref\": \"#/$defs/FuzzyMatchInput\"\n    },\n    \"auto_keep_all\": {\n      \"default\": true,\n      \"title\": \"Auto Keep All\",\n      \"type\": \"boolean\"\n    },\n    \"auto_keep_right\": {\n      \"default\": true,\n      \"title\": \"Auto Keep Right\",\n      \"type\": \"boolean\"\n    },\n    \"auto_keep_left\": {\n      \"default\": true,\n      \"title\": \"Auto Keep Left\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"join_input\"\n  ],\n  \"title\": \"NodeFuzzyMatch\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_ids</code>                 (<code>list[int] | None</code>)             </li> <li> <code>auto_generate_selection</code>                 (<code>bool</code>)             </li> <li> <code>verify_integrity</code>                 (<code>bool</code>)             </li> <li> <code>auto_keep_all</code>                 (<code>bool</code>)             </li> <li> <code>auto_keep_right</code>                 (<code>bool</code>)             </li> <li> <code>auto_keep_left</code>                 (<code>bool</code>)             </li> <li> <code>join_input</code>                 (<code>FuzzyMatchInput</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeFuzzyMatch(NodeJoin):\n    \"\"\"Settings for a node that performs a fuzzy join based on string similarity.\"\"\"\n\n    join_input: transform_schema.FuzzyMatchInput\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the fuzzy match join.\"\"\"\n        ji = self.join_input\n        how = ji.how\n        if ji.join_mapping:\n            keys = [\n                f\"{fm.left_col} ~ {fm.right_col}\" if fm.left_col != fm.right_col else fm.left_col\n                for fm in ji.join_mapping[:3]\n            ]\n            key_str = \", \".join(keys)\n            if len(ji.join_mapping) &gt; 3:\n                key_str += f\" (+{len(ji.join_mapping) - 3} more)\"\n            return f\"Fuzzy {how} join on {key_str}\"\n        return f\"Fuzzy {how} join\"\n\n    def to_yaml_dict(self) -&gt; NodeFuzzyMatchYaml:\n        \"\"\"Converts the fuzzy match node settings to a dictionary for YAML serialization.\"\"\"\n        result: NodeFuzzyMatchYaml = {\n            \"cache_results\": self.cache_results,\n            \"auto_generate_selection\": self.auto_generate_selection,\n            \"verify_integrity\": self.verify_integrity,\n            \"join_input\": self.join_input.to_yaml_dict(),\n            \"auto_keep_all\": self.auto_keep_all,\n            \"auto_keep_right\": self.auto_keep_right,\n            \"auto_keep_left\": self.auto_keep_left,\n        }\n        if self.output_field_config:\n            result[\"output_field_config\"] = {\n                \"enabled\": self.output_field_config.enabled,\n                \"validation_mode_behavior\": self.output_field_config.validation_mode_behavior,\n                \"validate_data_types\": self.output_field_config.validate_data_types,\n                \"fields\": [\n                    {\n                        \"name\": f.name,\n                        \"data_type\": f.data_type,\n                        \"default_value\": f.default_value,\n                    }\n                    for f in self.output_field_config.fields\n                ],\n            }\n        return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeFuzzyMatch.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the fuzzy match join.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the fuzzy match join.\"\"\"\n    ji = self.join_input\n    how = ji.how\n    if ji.join_mapping:\n        keys = [\n            f\"{fm.left_col} ~ {fm.right_col}\" if fm.left_col != fm.right_col else fm.left_col\n            for fm in ji.join_mapping[:3]\n        ]\n        key_str = \", \".join(keys)\n        if len(ji.join_mapping) &gt; 3:\n            key_str += f\" (+{len(ji.join_mapping) - 3} more)\"\n        return f\"Fuzzy {how} join on {key_str}\"\n    return f\"Fuzzy {how} join\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeFuzzyMatch.to_yaml_dict","title":"<code>to_yaml_dict()</code>","text":"<p>Converts the fuzzy match node settings to a dictionary for YAML serialization.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def to_yaml_dict(self) -&gt; NodeFuzzyMatchYaml:\n    \"\"\"Converts the fuzzy match node settings to a dictionary for YAML serialization.\"\"\"\n    result: NodeFuzzyMatchYaml = {\n        \"cache_results\": self.cache_results,\n        \"auto_generate_selection\": self.auto_generate_selection,\n        \"verify_integrity\": self.verify_integrity,\n        \"join_input\": self.join_input.to_yaml_dict(),\n        \"auto_keep_all\": self.auto_keep_all,\n        \"auto_keep_right\": self.auto_keep_right,\n        \"auto_keep_left\": self.auto_keep_left,\n    }\n    if self.output_field_config:\n        result[\"output_field_config\"] = {\n            \"enabled\": self.output_field_config.enabled,\n            \"validation_mode_behavior\": self.output_field_config.validation_mode_behavior,\n            \"validate_data_types\": self.output_field_config.validate_data_types,\n            \"fields\": [\n                {\n                    \"name\": f.name,\n                    \"data_type\": f.data_type,\n                    \"default_value\": f.default_value,\n                }\n                for f in self.output_field_config.fields\n            ],\n        }\n    return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeGraphSolver","title":"<code>NodeGraphSolver</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that solves graph-based problems (e.g., connected components).</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"GraphSolverInput\": {\n      \"description\": \"Defines settings for a graph-solving operation (e.g., finding connected components).\",\n      \"properties\": {\n        \"col_from\": {\n          \"title\": \"Col From\",\n          \"type\": \"string\"\n        },\n        \"col_to\": {\n          \"title\": \"Col To\",\n          \"type\": \"string\"\n        },\n        \"output_column_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": \"graph_group\",\n          \"title\": \"Output Column Name\"\n        }\n      },\n      \"required\": [\n        \"col_from\",\n        \"col_to\"\n      ],\n      \"title\": \"GraphSolverInput\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that solves graph-based problems (e.g., connected components).\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"graph_solver_input\": {\n      \"$ref\": \"#/$defs/GraphSolverInput\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"graph_solver_input\"\n  ],\n  \"title\": \"NodeGraphSolver\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>graph_solver_input</code>                 (<code>GraphSolverInput</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeGraphSolver(NodeSingleInput):\n    \"\"\"Settings for a node that solves graph-based problems (e.g., connected components).\"\"\"\n\n    graph_solver_input: transform_schema.GraphSolverInput\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the graph solver operation.\"\"\"\n        g = self.graph_solver_input\n        return f\"{g.col_from} -&gt; {g.col_to} as '{g.output_column_name}'\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeGraphSolver.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the graph solver operation.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the graph solver operation.\"\"\"\n    g = self.graph_solver_input\n    return f\"{g.col_from} -&gt; {g.col_to} as '{g.output_column_name}'\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeGroupBy","title":"<code>NodeGroupBy</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that performs a group-by and aggregation operation.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AggColl\": {\n      \"description\": \"A data class that represents a single aggregation operation for a group by operation.\\n\\nAttributes\\n----------\\nold_name : str\\n    The name of the column in the original DataFrame to be aggregated.\\n\\nagg : str\\n    The aggregation function to use. This can be a string representing a built-in function or a custom function.\\n\\nnew_name : Optional[str]\\n    The name of the resulting aggregated column in the output DataFrame. If not provided, it will default to the\\n    old_name appended with the aggregation function.\\n\\noutput_type : Optional[str]\\n    The type of the output values of the aggregation. If not provided, it is inferred from the aggregation function\\n    using the `get_func_type_mapping` function.\\n\\nExample\\n--------\\nagg_col = AggColl(\\n    old_name='col1',\\n    agg='sum',\\n    new_name='sum_col1',\\n    output_type='float'\\n)\",\n      \"properties\": {\n        \"old_name\": {\n          \"title\": \"Old Name\",\n          \"type\": \"string\"\n        },\n        \"agg\": {\n          \"title\": \"Agg\",\n          \"type\": \"string\"\n        },\n        \"new_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"New Name\"\n        },\n        \"output_type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Output Type\"\n        }\n      },\n      \"required\": [\n        \"old_name\",\n        \"agg\"\n      ],\n      \"title\": \"AggColl\",\n      \"type\": \"object\"\n    },\n    \"GroupByInput\": {\n      \"description\": \"A data class that represents the input for a group by operation.\\n\\nAttributes\\n----------\\nagg_cols : List[AggColl]\\n    A list of `AggColl` objects that specify the aggregation operations to perform on the DataFrame columns\\n    after grouping. Each `AggColl` object should specify the column to be aggregated and the aggregation\\n    function to use.\\n\\nExample\\n--------\\ngroup_by_input = GroupByInput(\\n    agg_cols=[AggColl(old_name='ix', agg='groupby'), AggColl(old_name='groups', agg='groupby'),\\n              AggColl(old_name='col1', agg='sum'), AggColl(old_name='col2', agg='mean')]\\n)\",\n      \"properties\": {\n        \"agg_cols\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/AggColl\"\n          },\n          \"title\": \"Agg Cols\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"agg_cols\"\n      ],\n      \"title\": \"GroupByInput\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that performs a group-by and aggregation operation.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"groupby_input\": {\n      \"$ref\": \"#/$defs/GroupByInput\",\n      \"default\": null\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodeGroupBy\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>groupby_input</code>                 (<code>GroupByInput</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeGroupBy(NodeSingleInput):\n    \"\"\"Settings for a node that performs a group-by and aggregation operation.\"\"\"\n\n    groupby_input: transform_schema.GroupByInput = None\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the group-by columns and aggregations.\"\"\"\n        if self.groupby_input is None or not self.groupby_input.agg_cols:\n            return \"\"\n        group_cols = [a.old_name for a in self.groupby_input.agg_cols if a.agg == \"groupby\"]\n        agg_cols = [a for a in self.groupby_input.agg_cols if a.agg != \"groupby\"]\n        parts = []\n        if group_cols:\n            cols_str = \", \".join(group_cols[:3])\n            if len(group_cols) &gt; 3:\n                cols_str += f\" (+{len(group_cols) - 3} more)\"\n            parts.append(f\"By {cols_str}\")\n        if agg_cols:\n            agg_strs = [f\"{a.agg}({a.old_name})\" for a in agg_cols[:3]]\n            if len(agg_cols) &gt; 3:\n                agg_strs.append(f\"+{len(agg_cols) - 3} more\")\n            parts.append(\", \".join(agg_strs))\n        return \": \".join(parts) if parts else \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeGroupBy.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the group-by columns and aggregations.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the group-by columns and aggregations.\"\"\"\n    if self.groupby_input is None or not self.groupby_input.agg_cols:\n        return \"\"\n    group_cols = [a.old_name for a in self.groupby_input.agg_cols if a.agg == \"groupby\"]\n    agg_cols = [a for a in self.groupby_input.agg_cols if a.agg != \"groupby\"]\n    parts = []\n    if group_cols:\n        cols_str = \", \".join(group_cols[:3])\n        if len(group_cols) &gt; 3:\n            cols_str += f\" (+{len(group_cols) - 3} more)\"\n        parts.append(f\"By {cols_str}\")\n    if agg_cols:\n        agg_strs = [f\"{a.agg}({a.old_name})\" for a in agg_cols[:3]]\n        if len(agg_cols) &gt; 3:\n            agg_strs.append(f\"+{len(agg_cols) - 3} more\")\n        parts.append(\", \".join(agg_strs))\n    return \": \".join(parts) if parts else \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeInputConnection","title":"<code>NodeInputConnection</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the input side of a connection between two nodes.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Represents the input side of a connection between two nodes.\",\n  \"properties\": {\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"connection_class\": {\n      \"enum\": [\n        \"input-0\",\n        \"input-1\",\n        \"input-2\",\n        \"input-3\",\n        \"input-4\",\n        \"input-5\",\n        \"input-6\",\n        \"input-7\",\n        \"input-8\",\n        \"input-9\"\n      ],\n      \"title\": \"Connection Class\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"node_id\",\n    \"connection_class\"\n  ],\n  \"title\": \"NodeInputConnection\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>connection_class</code>                 (<code>InputConnectionClass</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeInputConnection(BaseModel):\n    \"\"\"Represents the input side of a connection between two nodes.\"\"\"\n\n    node_id: int\n    connection_class: InputConnectionClass\n\n    def get_node_input_connection_type(self) -&gt; Literal[\"main\", \"right\", \"left\"]:\n        \"\"\"Determines the semantic type of the input (e.g., for a join).\"\"\"\n        match self.connection_class:\n            case \"input-0\":\n                return \"main\"\n            case \"input-1\":\n                return \"right\"\n            case \"input-2\":\n                return \"left\"\n            case _:\n                raise ValueError(f\"Unexpected connection_class: {self.connection_class}\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeInputConnection.get_node_input_connection_type","title":"<code>get_node_input_connection_type()</code>","text":"<p>Determines the semantic type of the input (e.g., for a join).</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_node_input_connection_type(self) -&gt; Literal[\"main\", \"right\", \"left\"]:\n    \"\"\"Determines the semantic type of the input (e.g., for a join).\"\"\"\n    match self.connection_class:\n        case \"input-0\":\n            return \"main\"\n        case \"input-1\":\n            return \"right\"\n        case \"input-2\":\n            return \"left\"\n        case _:\n            raise ValueError(f\"Unexpected connection_class: {self.connection_class}\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeJoin","title":"<code>NodeJoin</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeMultiInput</code></p> <p>Settings for a node that performs a standard SQL-style join.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"JoinInput\": {\n      \"description\": \"Data model for standard SQL-style join operations.\",\n      \"properties\": {\n        \"join_mapping\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/JoinMap\"\n          },\n          \"title\": \"Join Mapping\",\n          \"type\": \"array\"\n        },\n        \"left_select\": {\n          \"$ref\": \"#/$defs/JoinInputs\"\n        },\n        \"right_select\": {\n          \"$ref\": \"#/$defs/JoinInputs\"\n        },\n        \"how\": {\n          \"default\": \"inner\",\n          \"enum\": [\n            \"inner\",\n            \"left\",\n            \"right\",\n            \"full\",\n            \"semi\",\n            \"anti\",\n            \"cross\",\n            \"outer\"\n          ],\n          \"title\": \"How\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"join_mapping\",\n        \"left_select\",\n        \"right_select\"\n      ],\n      \"title\": \"JoinInput\",\n      \"type\": \"object\"\n    },\n    \"JoinInputs\": {\n      \"description\": \"Data model for join-specific select inputs (extends SelectInputs).\",\n      \"properties\": {\n        \"renames\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/SelectInput\"\n          },\n          \"title\": \"Renames\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"JoinInputs\",\n      \"type\": \"object\"\n    },\n    \"JoinMap\": {\n      \"description\": \"Defines a single mapping between a left and right column for a join key.\",\n      \"properties\": {\n        \"left_col\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Left Col\"\n        },\n        \"right_col\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Right Col\"\n        }\n      },\n      \"title\": \"JoinMap\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"SelectInput\": {\n      \"description\": \"Defines how a single column should be selected, renamed, or type-cast.\\n\\nThis is a core building block for any operation that involves column manipulation.\\nIt holds all the configuration for a single field in a selection operation.\",\n      \"properties\": {\n        \"old_name\": {\n          \"title\": \"Old Name\",\n          \"type\": \"string\"\n        },\n        \"original_position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Original Position\"\n        },\n        \"new_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"New Name\"\n        },\n        \"data_type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Data Type\"\n        },\n        \"data_type_change\": {\n          \"default\": false,\n          \"title\": \"Data Type Change\",\n          \"type\": \"boolean\"\n        },\n        \"join_key\": {\n          \"default\": false,\n          \"title\": \"Join Key\",\n          \"type\": \"boolean\"\n        },\n        \"is_altered\": {\n          \"default\": false,\n          \"title\": \"Is Altered\",\n          \"type\": \"boolean\"\n        },\n        \"position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Position\"\n        },\n        \"is_available\": {\n          \"default\": true,\n          \"title\": \"Is Available\",\n          \"type\": \"boolean\"\n        },\n        \"keep\": {\n          \"default\": true,\n          \"title\": \"Keep\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"old_name\"\n      ],\n      \"title\": \"SelectInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that performs a standard SQL-style join.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_ids\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"integer\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Depending On Ids\"\n    },\n    \"auto_generate_selection\": {\n      \"default\": true,\n      \"title\": \"Auto Generate Selection\",\n      \"type\": \"boolean\"\n    },\n    \"verify_integrity\": {\n      \"default\": true,\n      \"title\": \"Verify Integrity\",\n      \"type\": \"boolean\"\n    },\n    \"join_input\": {\n      \"$ref\": \"#/$defs/JoinInput\"\n    },\n    \"auto_keep_all\": {\n      \"default\": true,\n      \"title\": \"Auto Keep All\",\n      \"type\": \"boolean\"\n    },\n    \"auto_keep_right\": {\n      \"default\": true,\n      \"title\": \"Auto Keep Right\",\n      \"type\": \"boolean\"\n    },\n    \"auto_keep_left\": {\n      \"default\": true,\n      \"title\": \"Auto Keep Left\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"join_input\"\n  ],\n  \"title\": \"NodeJoin\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_ids</code>                 (<code>list[int] | None</code>)             </li> <li> <code>auto_generate_selection</code>                 (<code>bool</code>)             </li> <li> <code>verify_integrity</code>                 (<code>bool</code>)             </li> <li> <code>join_input</code>                 (<code>JoinInput</code>)             </li> <li> <code>auto_keep_all</code>                 (<code>bool</code>)             </li> <li> <code>auto_keep_right</code>                 (<code>bool</code>)             </li> <li> <code>auto_keep_left</code>                 (<code>bool</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeJoin(NodeMultiInput):\n    \"\"\"Settings for a node that performs a standard SQL-style join.\"\"\"\n\n    auto_generate_selection: bool = True\n    verify_integrity: bool = True\n    join_input: transform_schema.JoinInput\n    auto_keep_all: bool = True\n    auto_keep_right: bool = True\n    auto_keep_left: bool = True\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the join type and key columns.\"\"\"\n        ji = self.join_input\n        how = ji.how\n        if ji.join_mapping:\n            keys = [\n                f\"{jm.left_col} = {jm.right_col}\" if jm.left_col != jm.right_col else jm.left_col\n                for jm in ji.join_mapping[:3]\n            ]\n            key_str = \", \".join(keys)\n            if len(ji.join_mapping) &gt; 3:\n                key_str += f\" (+{len(ji.join_mapping) - 3} more)\"\n            return f\"{how} join on {key_str}\"\n        return f\"{how} join\"\n\n    def to_yaml_dict(self) -&gt; NodeJoinYaml:\n        \"\"\"Converts the join node settings to a dictionary for YAML serialization.\"\"\"\n        result: NodeJoinYaml = {\n            \"cache_results\": self.cache_results,\n            \"auto_generate_selection\": self.auto_generate_selection,\n            \"verify_integrity\": self.verify_integrity,\n            \"join_input\": self.join_input.to_yaml_dict(),\n            \"auto_keep_all\": self.auto_keep_all,\n            \"auto_keep_right\": self.auto_keep_right,\n            \"auto_keep_left\": self.auto_keep_left,\n        }\n        if self.output_field_config:\n            result[\"output_field_config\"] = {\n                \"enabled\": self.output_field_config.enabled,\n                \"validation_mode_behavior\": self.output_field_config.validation_mode_behavior,\n                \"validate_data_types\": self.output_field_config.validate_data_types,\n                \"fields\": [\n                    {\n                        \"name\": f.name,\n                        \"data_type\": f.data_type,\n                        \"default_value\": f.default_value,\n                    }\n                    for f in self.output_field_config.fields\n                ],\n            }\n        return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeJoin.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the join type and key columns.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the join type and key columns.\"\"\"\n    ji = self.join_input\n    how = ji.how\n    if ji.join_mapping:\n        keys = [\n            f\"{jm.left_col} = {jm.right_col}\" if jm.left_col != jm.right_col else jm.left_col\n            for jm in ji.join_mapping[:3]\n        ]\n        key_str = \", \".join(keys)\n        if len(ji.join_mapping) &gt; 3:\n            key_str += f\" (+{len(ji.join_mapping) - 3} more)\"\n        return f\"{how} join on {key_str}\"\n    return f\"{how} join\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeJoin.to_yaml_dict","title":"<code>to_yaml_dict()</code>","text":"<p>Converts the join node settings to a dictionary for YAML serialization.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def to_yaml_dict(self) -&gt; NodeJoinYaml:\n    \"\"\"Converts the join node settings to a dictionary for YAML serialization.\"\"\"\n    result: NodeJoinYaml = {\n        \"cache_results\": self.cache_results,\n        \"auto_generate_selection\": self.auto_generate_selection,\n        \"verify_integrity\": self.verify_integrity,\n        \"join_input\": self.join_input.to_yaml_dict(),\n        \"auto_keep_all\": self.auto_keep_all,\n        \"auto_keep_right\": self.auto_keep_right,\n        \"auto_keep_left\": self.auto_keep_left,\n    }\n    if self.output_field_config:\n        result[\"output_field_config\"] = {\n            \"enabled\": self.output_field_config.enabled,\n            \"validation_mode_behavior\": self.output_field_config.validation_mode_behavior,\n            \"validate_data_types\": self.output_field_config.validate_data_types,\n            \"fields\": [\n                {\n                    \"name\": f.name,\n                    \"data_type\": f.data_type,\n                    \"default_value\": f.default_value,\n                }\n                for f in self.output_field_config.fields\n            ],\n        }\n    return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeManualInput","title":"<code>NodeManualInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>Settings for a node that allows direct data entry in the UI.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"MinimalFieldInfo\": {\n      \"description\": \"Represents the most basic information about a data field (column).\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"MinimalFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"RawData\": {\n      \"description\": \"Represents data in a raw, columnar format for manual input.\",\n      \"properties\": {\n        \"columns\": {\n          \"default\": null,\n          \"items\": {\n            \"$ref\": \"#/$defs/MinimalFieldInfo\"\n          },\n          \"title\": \"Columns\",\n          \"type\": \"array\"\n        },\n        \"data\": {\n          \"items\": {\n            \"items\": {},\n            \"type\": \"array\"\n          },\n          \"title\": \"Data\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"data\"\n      ],\n      \"title\": \"RawData\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that allows direct data entry in the UI.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"raw_data_format\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/RawData\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodeManualInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>raw_data_format</code>                 (<code>RawData | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeManualInput(NodeBase):\n    \"\"\"Settings for a node that allows direct data entry in the UI.\"\"\"\n\n    raw_data_format: RawData | None = None\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the manual input columns.\"\"\"\n        if self.raw_data_format and self.raw_data_format.columns:\n            cols = [c.name for c in self.raw_data_format.columns[:5]]\n            desc = \", \".join(cols)\n            if len(self.raw_data_format.columns) &gt; 5:\n                desc += f\" (+{len(self.raw_data_format.columns) - 5} more)\"\n            num_rows = (\n                len(self.raw_data_format.data[0]) if self.raw_data_format.data and self.raw_data_format.data[0] else 0\n            )\n            return f\"{len(self.raw_data_format.columns)} cols, {num_rows} rows: {desc}\"\n        return \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeManualInput.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the manual input columns.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the manual input columns.\"\"\"\n    if self.raw_data_format and self.raw_data_format.columns:\n        cols = [c.name for c in self.raw_data_format.columns[:5]]\n        desc = \", \".join(cols)\n        if len(self.raw_data_format.columns) &gt; 5:\n            desc += f\" (+{len(self.raw_data_format.columns) - 5} more)\"\n        num_rows = (\n            len(self.raw_data_format.data[0]) if self.raw_data_format.data and self.raw_data_format.data[0] else 0\n        )\n        return f\"{len(self.raw_data_format.columns)} cols, {num_rows} rows: {desc}\"\n    return \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeMultiInput","title":"<code>NodeMultiInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>A base model for any node that takes multiple data inputs.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"A base model for any node that takes multiple data inputs.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_ids\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"integer\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Depending On Ids\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodeMultiInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_ids</code>                 (<code>list[int] | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeMultiInput(NodeBase):\n    \"\"\"A base model for any node that takes multiple data inputs.\"\"\"\n\n    depending_on_ids: list[int] | None = Field(default_factory=list)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeOutput","title":"<code>NodeOutput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that writes its input to a file.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputCsvTable\": {\n      \"description\": \"Defines settings for writing a CSV file.\",\n      \"properties\": {\n        \"file_type\": {\n          \"const\": \"csv\",\n          \"default\": \"csv\",\n          \"enum\": [\n            \"csv\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        },\n        \"delimiter\": {\n          \"default\": \",\",\n          \"title\": \"Delimiter\",\n          \"type\": \"string\"\n        },\n        \"encoding\": {\n          \"default\": \"utf-8\",\n          \"title\": \"Encoding\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"OutputCsvTable\",\n      \"type\": \"object\"\n    },\n    \"OutputExcelTable\": {\n      \"description\": \"Defines settings for writing an Excel file.\",\n      \"properties\": {\n        \"file_type\": {\n          \"const\": \"excel\",\n          \"default\": \"excel\",\n          \"enum\": [\n            \"excel\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        },\n        \"sheet_name\": {\n          \"default\": \"Sheet1\",\n          \"title\": \"Sheet Name\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"OutputExcelTable\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"OutputParquetTable\": {\n      \"description\": \"Defines settings for writing a Parquet file.\",\n      \"properties\": {\n        \"file_type\": {\n          \"const\": \"parquet\",\n          \"default\": \"parquet\",\n          \"enum\": [\n            \"parquet\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"OutputParquetTable\",\n      \"type\": \"object\"\n    },\n    \"OutputSettings\": {\n      \"description\": \"Defines the complete settings for an output node.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"directory\": {\n          \"title\": \"Directory\",\n          \"type\": \"string\"\n        },\n        \"file_type\": {\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"title\": \"Fields\"\n        },\n        \"write_mode\": {\n          \"default\": \"overwrite\",\n          \"title\": \"Write Mode\",\n          \"type\": \"string\"\n        },\n        \"table_settings\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"csv\": \"#/$defs/OutputCsvTable\",\n              \"excel\": \"#/$defs/OutputExcelTable\",\n              \"parquet\": \"#/$defs/OutputParquetTable\"\n            },\n            \"propertyName\": \"file_type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/OutputCsvTable\"\n            },\n            {\n              \"$ref\": \"#/$defs/OutputParquetTable\"\n            },\n            {\n              \"$ref\": \"#/$defs/OutputExcelTable\"\n            }\n          ],\n          \"title\": \"Table Settings\"\n        },\n        \"abs_file_path\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Abs File Path\"\n        }\n      },\n      \"required\": [\n        \"name\",\n        \"directory\",\n        \"file_type\",\n        \"table_settings\"\n      ],\n      \"title\": \"OutputSettings\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that writes its input to a file.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"output_settings\": {\n      \"$ref\": \"#/$defs/OutputSettings\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"output_settings\"\n  ],\n  \"title\": \"NodeOutput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>output_settings</code>                 (<code>OutputSettings</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeOutput(NodeSingleInput):\n    \"\"\"Settings for a node that writes its input to a file.\"\"\"\n\n    output_settings: OutputSettings\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the output file target.\"\"\"\n        o = self.output_settings\n        return f\"{o.name} ({o.file_type})\"\n\n    def to_yaml_dict(self) -&gt; NodeOutputYaml:\n        \"\"\"Converts the output node settings to a dictionary for YAML serialization.\"\"\"\n        result: NodeOutputYaml = {\n            \"cache_results\": self.cache_results,\n            \"output_settings\": self.output_settings.to_yaml_dict(),\n        }\n        if self.output_field_config:\n            result[\"output_field_config\"] = {\n                \"enabled\": self.output_field_config.enabled,\n                \"validation_mode_behavior\": self.output_field_config.validation_mode_behavior,\n                \"validate_data_types\": self.output_field_config.validate_data_types,\n                \"fields\": [\n                    {\n                        \"name\": f.name,\n                        \"data_type\": f.data_type,\n                        \"default_value\": f.default_value,\n                    }\n                    for f in self.output_field_config.fields\n                ],\n            }\n        return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeOutput.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the output file target.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the output file target.\"\"\"\n    o = self.output_settings\n    return f\"{o.name} ({o.file_type})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeOutput.to_yaml_dict","title":"<code>to_yaml_dict()</code>","text":"<p>Converts the output node settings to a dictionary for YAML serialization.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def to_yaml_dict(self) -&gt; NodeOutputYaml:\n    \"\"\"Converts the output node settings to a dictionary for YAML serialization.\"\"\"\n    result: NodeOutputYaml = {\n        \"cache_results\": self.cache_results,\n        \"output_settings\": self.output_settings.to_yaml_dict(),\n    }\n    if self.output_field_config:\n        result[\"output_field_config\"] = {\n            \"enabled\": self.output_field_config.enabled,\n            \"validation_mode_behavior\": self.output_field_config.validation_mode_behavior,\n            \"validate_data_types\": self.output_field_config.validate_data_types,\n            \"fields\": [\n                {\n                    \"name\": f.name,\n                    \"data_type\": f.data_type,\n                    \"default_value\": f.default_value,\n                }\n                for f in self.output_field_config.fields\n            ],\n        }\n    return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeOutputConnection","title":"<code>NodeOutputConnection</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the output side of a connection between two nodes.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Represents the output side of a connection between two nodes.\",\n  \"properties\": {\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"connection_class\": {\n      \"enum\": [\n        \"output-0\",\n        \"output-1\",\n        \"output-2\",\n        \"output-3\",\n        \"output-4\",\n        \"output-5\",\n        \"output-6\",\n        \"output-7\",\n        \"output-8\",\n        \"output-9\"\n      ],\n      \"title\": \"Connection Class\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"node_id\",\n    \"connection_class\"\n  ],\n  \"title\": \"NodeOutputConnection\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>connection_class</code>                 (<code>OutputConnectionClass</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeOutputConnection(BaseModel):\n    \"\"\"Represents the output side of a connection between two nodes.\"\"\"\n\n    node_id: int\n    connection_class: OutputConnectionClass\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodePivot","title":"<code>NodePivot</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that pivots data from a long to a wide format.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"MinimalFieldInfo\": {\n      \"description\": \"Represents the most basic information about a data field (column).\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"MinimalFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"PivotInput\": {\n      \"description\": \"Defines the settings for a pivot (long-to-wide) operation.\",\n      \"properties\": {\n        \"index_columns\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Index Columns\",\n          \"type\": \"array\"\n        },\n        \"pivot_column\": {\n          \"title\": \"Pivot Column\",\n          \"type\": \"string\"\n        },\n        \"value_col\": {\n          \"title\": \"Value Col\",\n          \"type\": \"string\"\n        },\n        \"aggregations\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Aggregations\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"index_columns\",\n        \"pivot_column\",\n        \"value_col\",\n        \"aggregations\"\n      ],\n      \"title\": \"PivotInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that pivots data from a long to a wide format.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"pivot_input\": {\n      \"$ref\": \"#/$defs/PivotInput\",\n      \"default\": null\n    },\n    \"output_fields\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/MinimalFieldInfo\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Output Fields\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodePivot\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>pivot_input</code>                 (<code>PivotInput</code>)             </li> <li> <code>output_fields</code>                 (<code>list[MinimalFieldInfo] | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodePivot(NodeSingleInput):\n    \"\"\"Settings for a node that pivots data from a long to a wide format.\"\"\"\n\n    pivot_input: transform_schema.PivotInput = None\n    output_fields: list[MinimalFieldInfo] | None = None\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the pivot operation.\"\"\"\n        if self.pivot_input is None:\n            return \"\"\n        p = self.pivot_input\n        aggs = \", \".join(p.aggregations[:2]) if p.aggregations else \"\"\n        if len(p.aggregations) &gt; 2:\n            aggs += f\" (+{len(p.aggregations) - 2} more)\"\n        return f\"Pivot {p.value_col} by {p.pivot_column} ({aggs})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodePivot.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the pivot operation.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the pivot operation.\"\"\"\n    if self.pivot_input is None:\n        return \"\"\n    p = self.pivot_input\n    aggs = \", \".join(p.aggregations[:2]) if p.aggregations else \"\"\n    if len(p.aggregations) &gt; 2:\n        aggs += f\" (+{len(p.aggregations) - 2} more)\"\n    return f\"Pivot {p.value_col} by {p.pivot_column} ({aggs})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodePolarsCode","title":"<code>NodePolarsCode</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeMultiInput</code></p> <p>Settings for a node that executes arbitrary user-provided Polars code.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"PolarsCodeInput\": {\n      \"description\": \"A simple container for a string of user-provided Polars code to be executed.\",\n      \"properties\": {\n        \"polars_code\": {\n          \"title\": \"Polars Code\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"polars_code\"\n      ],\n      \"title\": \"PolarsCodeInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that executes arbitrary user-provided Polars code.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_ids\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"integer\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Depending On Ids\"\n    },\n    \"polars_code_input\": {\n      \"$ref\": \"#/$defs/PolarsCodeInput\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"polars_code_input\"\n  ],\n  \"title\": \"NodePolarsCode\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_ids</code>                 (<code>list[int] | None</code>)             </li> <li> <code>polars_code_input</code>                 (<code>PolarsCodeInput</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodePolarsCode(NodeMultiInput):\n    \"\"\"Settings for a node that executes arbitrary user-provided Polars code.\"\"\"\n\n    polars_code_input: transform_schema.PolarsCodeInput\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the Polars code snippet.\"\"\"\n        code = self.polars_code_input.polars_code\n        first_line = code.strip().split(\"\\n\")[0] if code else \"\"\n        if len(first_line) &gt; 80:\n            first_line = first_line[:77] + \"...\"\n        return first_line\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodePolarsCode.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the Polars code snippet.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the Polars code snippet.\"\"\"\n    code = self.polars_code_input.polars_code\n    first_line = code.strip().split(\"\\n\")[0] if code else \"\"\n    if len(first_line) &gt; 80:\n        first_line = first_line[:77] + \"...\"\n    return first_line\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodePromise","title":"<code>NodePromise</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>A placeholder node for an operation that has not yet been configured.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"A placeholder node for an operation that has not yet been configured.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"default\": false,\n      \"title\": \"Is Setup\",\n      \"type\": \"boolean\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"node_type\": {\n      \"title\": \"Node Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"node_type\"\n  ],\n  \"title\": \"NodePromise\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool</code>)             </li> <li> <code>node_type</code>                 (<code>str</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodePromise(NodeBase):\n    \"\"\"A placeholder node for an operation that has not yet been configured.\"\"\"\n\n    is_setup: bool = False\n    node_type: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodePythonScript","title":"<code>NodePythonScript</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeMultiInput</code></p> <p>Node that executes Python code on a kernel container.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"NotebookCell\": {\n      \"description\": \"A single cell in the notebook editor.\\n\\nNote: Cell output (stdout, display_outputs, errors) is handled entirely\\non the frontend and is not persisted. Only id and code are stored.\",\n      \"properties\": {\n        \"id\": {\n          \"title\": \"Id\",\n          \"type\": \"string\"\n        },\n        \"code\": {\n          \"default\": \"\",\n          \"title\": \"Code\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"id\"\n      ],\n      \"title\": \"NotebookCell\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"PythonScriptInput\": {\n      \"description\": \"Settings for Python code execution on a kernel.\",\n      \"properties\": {\n        \"code\": {\n          \"default\": \"\",\n          \"title\": \"Code\",\n          \"type\": \"string\"\n        },\n        \"kernel_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Kernel Id\"\n        },\n        \"cells\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/NotebookCell\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Cells\"\n        }\n      },\n      \"title\": \"PythonScriptInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Node that executes Python code on a kernel container.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_ids\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"integer\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Depending On Ids\"\n    },\n    \"python_script_input\": {\n      \"$ref\": \"#/$defs/PythonScriptInput\",\n      \"default\": {\n        \"code\": \"\",\n        \"kernel_id\": null,\n        \"cells\": null\n      }\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodePythonScript\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_ids</code>                 (<code>list[int] | None</code>)             </li> <li> <code>python_script_input</code>                 (<code>PythonScriptInput</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodePythonScript(NodeMultiInput):\n    \"\"\"Node that executes Python code on a kernel container.\"\"\"\n\n    python_script_input: PythonScriptInput = PythonScriptInput()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeRead","title":"<code>NodeRead</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>Settings for a node that reads data from a file.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"InputCsvTable\": {\n      \"description\": \"Defines settings for reading a CSV file.\",\n      \"properties\": {\n        \"file_type\": {\n          \"const\": \"csv\",\n          \"default\": \"csv\",\n          \"enum\": [\n            \"csv\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        },\n        \"reference\": {\n          \"default\": \"\",\n          \"title\": \"Reference\",\n          \"type\": \"string\"\n        },\n        \"starting_from_line\": {\n          \"default\": 0,\n          \"title\": \"Starting From Line\",\n          \"type\": \"integer\"\n        },\n        \"delimiter\": {\n          \"default\": \",\",\n          \"title\": \"Delimiter\",\n          \"type\": \"string\"\n        },\n        \"has_headers\": {\n          \"default\": true,\n          \"title\": \"Has Headers\",\n          \"type\": \"boolean\"\n        },\n        \"encoding\": {\n          \"default\": \"utf-8\",\n          \"title\": \"Encoding\",\n          \"type\": \"string\"\n        },\n        \"parquet_ref\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Parquet Ref\"\n        },\n        \"row_delimiter\": {\n          \"default\": \"\\n\",\n          \"title\": \"Row Delimiter\",\n          \"type\": \"string\"\n        },\n        \"quote_char\": {\n          \"default\": \"\\\"\",\n          \"title\": \"Quote Char\",\n          \"type\": \"string\"\n        },\n        \"infer_schema_length\": {\n          \"default\": 10000,\n          \"title\": \"Infer Schema Length\",\n          \"type\": \"integer\"\n        },\n        \"truncate_ragged_lines\": {\n          \"default\": false,\n          \"title\": \"Truncate Ragged Lines\",\n          \"type\": \"boolean\"\n        },\n        \"ignore_errors\": {\n          \"default\": false,\n          \"title\": \"Ignore Errors\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"InputCsvTable\",\n      \"type\": \"object\"\n    },\n    \"InputExcelTable\": {\n      \"description\": \"Defines settings for reading an Excel file.\",\n      \"properties\": {\n        \"file_type\": {\n          \"const\": \"excel\",\n          \"default\": \"excel\",\n          \"enum\": [\n            \"excel\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        },\n        \"sheet_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Sheet Name\"\n        },\n        \"start_row\": {\n          \"default\": 0,\n          \"title\": \"Start Row\",\n          \"type\": \"integer\"\n        },\n        \"start_column\": {\n          \"default\": 0,\n          \"title\": \"Start Column\",\n          \"type\": \"integer\"\n        },\n        \"end_row\": {\n          \"default\": 0,\n          \"title\": \"End Row\",\n          \"type\": \"integer\"\n        },\n        \"end_column\": {\n          \"default\": 0,\n          \"title\": \"End Column\",\n          \"type\": \"integer\"\n        },\n        \"has_headers\": {\n          \"default\": true,\n          \"title\": \"Has Headers\",\n          \"type\": \"boolean\"\n        },\n        \"type_inference\": {\n          \"default\": false,\n          \"title\": \"Type Inference\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"InputExcelTable\",\n      \"type\": \"object\"\n    },\n    \"InputJsonTable\": {\n      \"description\": \"Defines settings for reading a JSON file.\",\n      \"properties\": {\n        \"file_type\": {\n          \"const\": \"json\",\n          \"default\": \"json\",\n          \"enum\": [\n            \"json\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        },\n        \"reference\": {\n          \"default\": \"\",\n          \"title\": \"Reference\",\n          \"type\": \"string\"\n        },\n        \"starting_from_line\": {\n          \"default\": 0,\n          \"title\": \"Starting From Line\",\n          \"type\": \"integer\"\n        },\n        \"delimiter\": {\n          \"default\": \",\",\n          \"title\": \"Delimiter\",\n          \"type\": \"string\"\n        },\n        \"has_headers\": {\n          \"default\": true,\n          \"title\": \"Has Headers\",\n          \"type\": \"boolean\"\n        },\n        \"encoding\": {\n          \"default\": \"utf-8\",\n          \"title\": \"Encoding\",\n          \"type\": \"string\"\n        },\n        \"parquet_ref\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Parquet Ref\"\n        },\n        \"row_delimiter\": {\n          \"default\": \"\\n\",\n          \"title\": \"Row Delimiter\",\n          \"type\": \"string\"\n        },\n        \"quote_char\": {\n          \"default\": \"\\\"\",\n          \"title\": \"Quote Char\",\n          \"type\": \"string\"\n        },\n        \"infer_schema_length\": {\n          \"default\": 10000,\n          \"title\": \"Infer Schema Length\",\n          \"type\": \"integer\"\n        },\n        \"truncate_ragged_lines\": {\n          \"default\": false,\n          \"title\": \"Truncate Ragged Lines\",\n          \"type\": \"boolean\"\n        },\n        \"ignore_errors\": {\n          \"default\": false,\n          \"title\": \"Ignore Errors\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"InputJsonTable\",\n      \"type\": \"object\"\n    },\n    \"InputParquetTable\": {\n      \"description\": \"Defines settings for reading a Parquet file.\",\n      \"properties\": {\n        \"file_type\": {\n          \"const\": \"parquet\",\n          \"default\": \"parquet\",\n          \"enum\": [\n            \"parquet\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"InputParquetTable\",\n      \"type\": \"object\"\n    },\n    \"MinimalFieldInfo\": {\n      \"description\": \"Represents the most basic information about a data field (column).\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"MinimalFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"ReceivedTable\": {\n      \"description\": \"Model for defining a table received from an external source.\",\n      \"properties\": {\n        \"id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Id\"\n        },\n        \"name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Name\"\n        },\n        \"path\": {\n          \"title\": \"Path\",\n          \"type\": \"string\"\n        },\n        \"directory\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Directory\"\n        },\n        \"analysis_file_available\": {\n          \"default\": false,\n          \"title\": \"Analysis File Available\",\n          \"type\": \"boolean\"\n        },\n        \"status\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Status\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/MinimalFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"abs_file_path\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Abs File Path\"\n        },\n        \"file_type\": {\n          \"enum\": [\n            \"csv\",\n            \"json\",\n            \"parquet\",\n            \"excel\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        },\n        \"table_settings\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"csv\": \"#/$defs/InputCsvTable\",\n              \"excel\": \"#/$defs/InputExcelTable\",\n              \"json\": \"#/$defs/InputJsonTable\",\n              \"parquet\": \"#/$defs/InputParquetTable\"\n            },\n            \"propertyName\": \"file_type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/InputCsvTable\"\n            },\n            {\n              \"$ref\": \"#/$defs/InputJsonTable\"\n            },\n            {\n              \"$ref\": \"#/$defs/InputParquetTable\"\n            },\n            {\n              \"$ref\": \"#/$defs/InputExcelTable\"\n            }\n          ],\n          \"title\": \"Table Settings\"\n        }\n      },\n      \"required\": [\n        \"path\",\n        \"file_type\",\n        \"table_settings\"\n      ],\n      \"title\": \"ReceivedTable\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that reads data from a file.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"received_file\": {\n      \"$ref\": \"#/$defs/ReceivedTable\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"received_file\"\n  ],\n  \"title\": \"NodeRead\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>received_file</code>                 (<code>ReceivedTable</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeRead(NodeBase):\n    \"\"\"Settings for a node that reads data from a file.\"\"\"\n\n    received_file: ReceivedTable\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the file being read.\"\"\"\n        rf = self.received_file\n        name = rf.name or Path(rf.path).name\n        return f\"{name} ({rf.file_type})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeRead.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the file being read.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the file being read.\"\"\"\n    rf = self.received_file\n    name = rf.name or Path(rf.path).name\n    return f\"{name} ({rf.file_type})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeRecordCount","title":"<code>NodeRecordCount</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that counts the number of records.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that counts the number of records.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodeRecordCount\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeRecordCount(NodeSingleInput):\n    \"\"\"Settings for a node that counts the number of records.\"\"\"\n\n    pass\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeRecordId","title":"<code>NodeRecordId</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that adds a unique record ID column.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"RecordIdInput\": {\n      \"description\": \"Defines settings for adding a record ID (row number) column to the data.\",\n      \"properties\": {\n        \"output_column_name\": {\n          \"default\": \"record_id\",\n          \"title\": \"Output Column Name\",\n          \"type\": \"string\"\n        },\n        \"offset\": {\n          \"default\": 1,\n          \"title\": \"Offset\",\n          \"type\": \"integer\"\n        },\n        \"group_by\": {\n          \"anyOf\": [\n            {\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": false,\n          \"title\": \"Group By\"\n        },\n        \"group_by_columns\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"title\": \"Group By Columns\"\n        }\n      },\n      \"title\": \"RecordIdInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that adds a unique record ID column.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"record_id_input\": {\n      \"$ref\": \"#/$defs/RecordIdInput\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"record_id_input\"\n  ],\n  \"title\": \"NodeRecordId\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>record_id_input</code>                 (<code>RecordIdInput</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeRecordId(NodeSingleInput):\n    \"\"\"Settings for a node that adds a unique record ID column.\"\"\"\n\n    record_id_input: transform_schema.RecordIdInput\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the record ID column being added.\"\"\"\n        r = self.record_id_input\n        desc = f\"Add column '{r.output_column_name}'\"\n        if r.group_by and r.group_by_columns:\n            cols = \", \".join(r.group_by_columns[:3])\n            desc += f\" per group ({cols})\"\n        return desc\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeRecordId.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the record ID column being added.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the record ID column being added.\"\"\"\n    r = self.record_id_input\n    desc = f\"Add column '{r.output_column_name}'\"\n    if r.group_by and r.group_by_columns:\n        cols = \", \".join(r.group_by_columns[:3])\n        desc += f\" per group ({cols})\"\n    return desc\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeSample","title":"<code>NodeSample</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that samples a subset of the data.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that samples a subset of the data.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"sample_size\": {\n      \"default\": 1000,\n      \"title\": \"Sample Size\",\n      \"type\": \"integer\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodeSample\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>sample_size</code>                 (<code>int</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeSample(NodeSingleInput):\n    \"\"\"Settings for a node that samples a subset of the data.\"\"\"\n\n    sample_size: int = 1000\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the sample size.\"\"\"\n        return f\"Sample {self.sample_size} rows\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeSample.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the sample size.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the sample size.\"\"\"\n    return f\"Sample {self.sample_size} rows\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeSelect","title":"<code>NodeSelect</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that selects, renames, and reorders columns.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"SelectInput\": {\n      \"description\": \"Defines how a single column should be selected, renamed, or type-cast.\\n\\nThis is a core building block for any operation that involves column manipulation.\\nIt holds all the configuration for a single field in a selection operation.\",\n      \"properties\": {\n        \"old_name\": {\n          \"title\": \"Old Name\",\n          \"type\": \"string\"\n        },\n        \"original_position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Original Position\"\n        },\n        \"new_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"New Name\"\n        },\n        \"data_type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Data Type\"\n        },\n        \"data_type_change\": {\n          \"default\": false,\n          \"title\": \"Data Type Change\",\n          \"type\": \"boolean\"\n        },\n        \"join_key\": {\n          \"default\": false,\n          \"title\": \"Join Key\",\n          \"type\": \"boolean\"\n        },\n        \"is_altered\": {\n          \"default\": false,\n          \"title\": \"Is Altered\",\n          \"type\": \"boolean\"\n        },\n        \"position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Position\"\n        },\n        \"is_available\": {\n          \"default\": true,\n          \"title\": \"Is Available\",\n          \"type\": \"boolean\"\n        },\n        \"keep\": {\n          \"default\": true,\n          \"title\": \"Keep\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"old_name\"\n      ],\n      \"title\": \"SelectInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that selects, renames, and reorders columns.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"keep_missing\": {\n      \"default\": true,\n      \"title\": \"Keep Missing\",\n      \"type\": \"boolean\"\n    },\n    \"select_input\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/SelectInput\"\n      },\n      \"title\": \"Select Input\",\n      \"type\": \"array\"\n    },\n    \"sorted_by\": {\n      \"anyOf\": [\n        {\n          \"enum\": [\n            \"none\",\n            \"asc\",\n            \"desc\"\n          ],\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"none\",\n      \"title\": \"Sorted By\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodeSelect\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>keep_missing</code>                 (<code>bool</code>)             </li> <li> <code>select_input</code>                 (<code>list[SelectInput]</code>)             </li> <li> <code>sorted_by</code>                 (<code>Literal['none', 'asc', 'desc'] | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeSelect(NodeSingleInput):\n    \"\"\"Settings for a node that selects, renames, and reorders columns.\"\"\"\n\n    keep_missing: bool = True\n    select_input: list[transform_schema.SelectInput] = Field(default_factory=list)\n    sorted_by: Literal[\"none\", \"asc\", \"desc\"] | None = \"none\"\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes column selections, renames, and drops.\"\"\"\n        if not self.select_input:\n            return \"\"\n        parts = []\n        renames = [s for s in self.select_input if s.old_name != s.new_name and s.keep]\n        drops = [s for s in self.select_input if not s.keep]\n        type_changes = [s for s in self.select_input if s.data_type_change and s.keep]\n        if renames:\n            rename_strs = [f\"{r.old_name} -&gt; {r.new_name}\" for r in renames[:3]]\n            parts.append(\"Rename: \" + \", \".join(rename_strs))\n            if len(renames) &gt; 3:\n                parts[-1] += f\" (+{len(renames) - 3} more)\"\n        if drops:\n            drop_names = [d.old_name for d in drops[:3]]\n            parts.append(\"Drop: \" + \", \".join(drop_names))\n            if len(drops) &gt; 3:\n                parts[-1] += f\" (+{len(drops) - 3} more)\"\n        if type_changes and not renames and not drops:\n            cast_strs = [f\"{t.old_name} to {t.data_type}\" for t in type_changes[:3]]\n            parts.append(\"Cast: \" + \", \".join(cast_strs))\n            if len(type_changes) &gt; 3:\n                parts[-1] += f\" (+{len(type_changes) - 3} more)\"\n        return \"; \".join(parts) if parts else \"\"\n\n    def to_yaml_dict(self) -&gt; NodeSelectYaml:\n        \"\"\"Converts the select node settings to a dictionary for YAML serialization.\"\"\"\n        result: NodeSelectYaml = {\n            \"cache_results\": bool(self.cache_results),\n            \"keep_missing\": self.keep_missing,\n            \"select_input\": [s.to_yaml_dict() for s in self.select_input],\n            \"sorted_by\": self.sorted_by,\n        }\n        if self.output_field_config:\n            result[\"output_field_config\"] = {\n                \"enabled\": self.output_field_config.enabled,\n                \"validation_mode_behavior\": self.output_field_config.validation_mode_behavior,\n                \"validate_data_types\": self.output_field_config.validate_data_types,\n                \"fields\": [\n                    {\n                        \"name\": f.name,\n                        \"data_type\": f.data_type,\n                        \"default_value\": f.default_value,\n                    }\n                    for f in self.output_field_config.fields\n                ],\n            }\n        return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeSelect.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes column selections, renames, and drops.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes column selections, renames, and drops.\"\"\"\n    if not self.select_input:\n        return \"\"\n    parts = []\n    renames = [s for s in self.select_input if s.old_name != s.new_name and s.keep]\n    drops = [s for s in self.select_input if not s.keep]\n    type_changes = [s for s in self.select_input if s.data_type_change and s.keep]\n    if renames:\n        rename_strs = [f\"{r.old_name} -&gt; {r.new_name}\" for r in renames[:3]]\n        parts.append(\"Rename: \" + \", \".join(rename_strs))\n        if len(renames) &gt; 3:\n            parts[-1] += f\" (+{len(renames) - 3} more)\"\n    if drops:\n        drop_names = [d.old_name for d in drops[:3]]\n        parts.append(\"Drop: \" + \", \".join(drop_names))\n        if len(drops) &gt; 3:\n            parts[-1] += f\" (+{len(drops) - 3} more)\"\n    if type_changes and not renames and not drops:\n        cast_strs = [f\"{t.old_name} to {t.data_type}\" for t in type_changes[:3]]\n        parts.append(\"Cast: \" + \", \".join(cast_strs))\n        if len(type_changes) &gt; 3:\n            parts[-1] += f\" (+{len(type_changes) - 3} more)\"\n    return \"; \".join(parts) if parts else \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeSelect.to_yaml_dict","title":"<code>to_yaml_dict()</code>","text":"<p>Converts the select node settings to a dictionary for YAML serialization.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def to_yaml_dict(self) -&gt; NodeSelectYaml:\n    \"\"\"Converts the select node settings to a dictionary for YAML serialization.\"\"\"\n    result: NodeSelectYaml = {\n        \"cache_results\": bool(self.cache_results),\n        \"keep_missing\": self.keep_missing,\n        \"select_input\": [s.to_yaml_dict() for s in self.select_input],\n        \"sorted_by\": self.sorted_by,\n    }\n    if self.output_field_config:\n        result[\"output_field_config\"] = {\n            \"enabled\": self.output_field_config.enabled,\n            \"validation_mode_behavior\": self.output_field_config.validation_mode_behavior,\n            \"validate_data_types\": self.output_field_config.validate_data_types,\n            \"fields\": [\n                {\n                    \"name\": f.name,\n                    \"data_type\": f.data_type,\n                    \"default_value\": f.default_value,\n                }\n                for f in self.output_field_config.fields\n            ],\n        }\n    return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeSingleInput","title":"<code>NodeSingleInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>A base model for any node that takes a single data input.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"A base model for any node that takes a single data input.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodeSingleInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeSingleInput(NodeBase):\n    \"\"\"A base model for any node that takes a single data input.\"\"\"\n\n    depending_on_id: int | None = -1\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeSort","title":"<code>NodeSort</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that sorts the data by one or more columns.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"SortByInput\": {\n      \"description\": \"Defines a single sort condition on a column, including the direction.\",\n      \"properties\": {\n        \"column\": {\n          \"title\": \"Column\",\n          \"type\": \"string\"\n        },\n        \"how\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": \"asc\",\n          \"title\": \"How\"\n        }\n      },\n      \"required\": [\n        \"column\"\n      ],\n      \"title\": \"SortByInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that sorts the data by one or more columns.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"sort_input\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/SortByInput\"\n      },\n      \"title\": \"Sort Input\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodeSort\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>sort_input</code>                 (<code>list[SortByInput]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeSort(NodeSingleInput):\n    \"\"\"Settings for a node that sorts the data by one or more columns.\"\"\"\n\n    sort_input: list[transform_schema.SortByInput] = Field(default_factory=list)\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the sort columns and directions.\"\"\"\n        if not self.sort_input:\n            return \"\"\n        parts = [f\"{s.column} {s.how or 'asc'}\" for s in self.sort_input[:3]]\n        desc = \"Sort by \" + \", \".join(parts)\n        if len(self.sort_input) &gt; 3:\n            desc += f\" (+{len(self.sort_input) - 3} more)\"\n        return desc\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeSort.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the sort columns and directions.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the sort columns and directions.\"\"\"\n    if not self.sort_input:\n        return \"\"\n    parts = [f\"{s.column} {s.how or 'asc'}\" for s in self.sort_input[:3]]\n    desc = \"Sort by \" + \", \".join(parts)\n    if len(self.sort_input) &gt; 3:\n        desc += f\" (+{len(self.sort_input) - 3} more)\"\n    return desc\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeTextToRows","title":"<code>NodeTextToRows</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that splits a text column into multiple rows.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"TextToRowsInput\": {\n      \"description\": \"Defines settings for splitting a text column into multiple rows based on a delimiter.\",\n      \"properties\": {\n        \"column_to_split\": {\n          \"title\": \"Column To Split\",\n          \"type\": \"string\"\n        },\n        \"output_column_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Output Column Name\"\n        },\n        \"split_by_fixed_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": true,\n          \"title\": \"Split By Fixed Value\"\n        },\n        \"split_fixed_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": \",\",\n          \"title\": \"Split Fixed Value\"\n        },\n        \"split_by_column\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Split By Column\"\n        }\n      },\n      \"required\": [\n        \"column_to_split\"\n      ],\n      \"title\": \"TextToRowsInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that splits a text column into multiple rows.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"text_to_rows_input\": {\n      \"$ref\": \"#/$defs/TextToRowsInput\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"text_to_rows_input\"\n  ],\n  \"title\": \"NodeTextToRows\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>text_to_rows_input</code>                 (<code>TextToRowsInput</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeTextToRows(NodeSingleInput):\n    \"\"\"Settings for a node that splits a text column into multiple rows.\"\"\"\n\n    text_to_rows_input: transform_schema.TextToRowsInput\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the text-to-rows split operation.\"\"\"\n        t = self.text_to_rows_input\n        delim = t.split_fixed_value if t.split_by_fixed_value else t.split_by_column\n        return f\"Split {t.column_to_split} by '{delim}'\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeTextToRows.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the text-to-rows split operation.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the text-to-rows split operation.\"\"\"\n    t = self.text_to_rows_input\n    delim = t.split_fixed_value if t.split_by_fixed_value else t.split_by_column\n    return f\"Split {t.column_to_split} by '{delim}'\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeUnion","title":"<code>NodeUnion</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeMultiInput</code></p> <p>Settings for a node that concatenates multiple data inputs.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"UnionInput\": {\n      \"description\": \"Defines settings for a union (concatenation) operation.\",\n      \"properties\": {\n        \"mode\": {\n          \"default\": \"relaxed\",\n          \"enum\": [\n            \"selective\",\n            \"relaxed\"\n          ],\n          \"title\": \"Mode\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"UnionInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that concatenates multiple data inputs.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_ids\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"integer\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Depending On Ids\"\n    },\n    \"union_input\": {\n      \"$ref\": \"#/$defs/UnionInput\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodeUnion\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_ids</code>                 (<code>list[int] | None</code>)             </li> <li> <code>union_input</code>                 (<code>UnionInput</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeUnion(NodeMultiInput):\n    \"\"\"Settings for a node that concatenates multiple data inputs.\"\"\"\n\n    union_input: transform_schema.UnionInput = Field(default_factory=transform_schema.UnionInput)\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the union mode.\"\"\"\n        return f\"Union ({self.union_input.mode})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeUnion.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the union mode.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the union mode.\"\"\"\n    return f\"Union ({self.union_input.mode})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeUnique","title":"<code>NodeUnique</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that returns the unique rows from the data.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"UniqueInput\": {\n      \"description\": \"Defines settings for a uniqueness operation, specifying columns and which row to keep.\",\n      \"properties\": {\n        \"columns\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Columns\"\n        },\n        \"strategy\": {\n          \"default\": \"any\",\n          \"enum\": [\n            \"first\",\n            \"last\",\n            \"any\",\n            \"none\"\n          ],\n          \"title\": \"Strategy\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"UniqueInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that returns the unique rows from the data.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"unique_input\": {\n      \"$ref\": \"#/$defs/UniqueInput\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"unique_input\"\n  ],\n  \"title\": \"NodeUnique\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>unique_input</code>                 (<code>UniqueInput</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeUnique(NodeSingleInput):\n    \"\"\"Settings for a node that returns the unique rows from the data.\"\"\"\n\n    unique_input: transform_schema.UniqueInput\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the uniqueness operation.\"\"\"\n        u = self.unique_input\n        if u.columns:\n            cols = \", \".join(u.columns[:3])\n            if len(u.columns) &gt; 3:\n                cols += f\" (+{len(u.columns) - 3} more)\"\n            return f\"Unique by {cols} (keep {u.strategy})\"\n        return f\"Unique rows (keep {u.strategy})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeUnique.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the uniqueness operation.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the uniqueness operation.\"\"\"\n    u = self.unique_input\n    if u.columns:\n        cols = \", \".join(u.columns[:3])\n        if len(u.columns) &gt; 3:\n            cols += f\" (+{len(u.columns) - 3} more)\"\n        return f\"Unique by {cols} (keep {u.strategy})\"\n    return f\"Unique rows (keep {u.strategy})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeUnpivot","title":"<code>NodeUnpivot</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that unpivots data from a wide to a long format.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    },\n    \"UnpivotInput\": {\n      \"description\": \"Defines settings for an unpivot (wide-to-long) operation.\",\n      \"properties\": {\n        \"index_columns\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Index Columns\",\n          \"type\": \"array\"\n        },\n        \"value_columns\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Value Columns\",\n          \"type\": \"array\"\n        },\n        \"data_type_selector\": {\n          \"anyOf\": [\n            {\n              \"enum\": [\n                \"float\",\n                \"all\",\n                \"date\",\n                \"numeric\",\n                \"string\"\n              ],\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Data Type Selector\"\n        },\n        \"data_type_selector_mode\": {\n          \"default\": \"column\",\n          \"enum\": [\n            \"data_type\",\n            \"column\"\n          ],\n          \"title\": \"Data Type Selector Mode\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"UnpivotInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that unpivots data from a wide to a long format.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Depending On Id\"\n    },\n    \"unpivot_input\": {\n      \"$ref\": \"#/$defs/UnpivotInput\",\n      \"default\": null\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\"\n  ],\n  \"title\": \"NodeUnpivot\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_id</code>                 (<code>int | None</code>)             </li> <li> <code>unpivot_input</code>                 (<code>UnpivotInput</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeUnpivot(NodeSingleInput):\n    \"\"\"Settings for a node that unpivots data from a wide to a long format.\"\"\"\n\n    unpivot_input: transform_schema.UnpivotInput = None\n\n    def get_default_description(self) -&gt; str:\n        \"\"\"Describes the unpivot operation.\"\"\"\n        if self.unpivot_input is None:\n            return \"\"\n        u = self.unpivot_input\n        if u.value_columns:\n            cols = \", \".join(u.value_columns[:3])\n            if len(u.value_columns) &gt; 3:\n                cols += f\" (+{len(u.value_columns) - 3} more)\"\n            return f\"Unpivot {cols}\"\n        if u.data_type_selector:\n            return f\"Unpivot {u.data_type_selector} columns\"\n        return \"Unpivot\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeUnpivot.get_default_description","title":"<code>get_default_description()</code>","text":"<p>Describes the unpivot operation.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_default_description(self) -&gt; str:\n    \"\"\"Describes the unpivot operation.\"\"\"\n    if self.unpivot_input is None:\n        return \"\"\n    u = self.unpivot_input\n    if u.value_columns:\n        cols = \", \".join(u.value_columns[:3])\n        if len(u.value_columns) &gt; 3:\n            cols += f\" (+{len(u.value_columns) - 3} more)\"\n        return f\"Unpivot {cols}\"\n    if u.data_type_selector:\n        return f\"Unpivot {u.data_type_selector} columns\"\n    return \"Unpivot\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NotebookCell","title":"<code>NotebookCell</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single cell in the notebook editor.</p> <p>Note: Cell output (stdout, display_outputs, errors) is handled entirely on the frontend and is not persisted. Only id and code are stored.</p> Show JSON schema: <pre><code>{\n  \"description\": \"A single cell in the notebook editor.\\n\\nNote: Cell output (stdout, display_outputs, errors) is handled entirely\\non the frontend and is not persisted. Only id and code are stored.\",\n  \"properties\": {\n    \"id\": {\n      \"title\": \"Id\",\n      \"type\": \"string\"\n    },\n    \"code\": {\n      \"default\": \"\",\n      \"title\": \"Code\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"id\"\n  ],\n  \"title\": \"NotebookCell\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>str</code>)             </li> <li> <code>code</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NotebookCell(BaseModel):\n    \"\"\"A single cell in the notebook editor.\n\n    Note: Cell output (stdout, display_outputs, errors) is handled entirely\n    on the frontend and is not persisted. Only id and code are stored.\n    \"\"\"\n\n    id: str\n    code: str = \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputCsvTable","title":"<code>OutputCsvTable</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for writing a CSV file.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines settings for writing a CSV file.\",\n  \"properties\": {\n    \"file_type\": {\n      \"const\": \"csv\",\n      \"default\": \"csv\",\n      \"enum\": [\n        \"csv\"\n      ],\n      \"title\": \"File Type\",\n      \"type\": \"string\"\n    },\n    \"delimiter\": {\n      \"default\": \",\",\n      \"title\": \"Delimiter\",\n      \"type\": \"string\"\n    },\n    \"encoding\": {\n      \"default\": \"utf-8\",\n      \"title\": \"Encoding\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"OutputCsvTable\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>file_type</code>                 (<code>Literal['csv']</code>)             </li> <li> <code>delimiter</code>                 (<code>str</code>)             </li> <li> <code>encoding</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class OutputCsvTable(BaseModel):\n    \"\"\"Defines settings for writing a CSV file.\"\"\"\n\n    file_type: Literal[\"csv\"] = \"csv\"\n    delimiter: str = \",\"\n    encoding: str = \"utf-8\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputExcelTable","title":"<code>OutputExcelTable</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for writing an Excel file.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines settings for writing an Excel file.\",\n  \"properties\": {\n    \"file_type\": {\n      \"const\": \"excel\",\n      \"default\": \"excel\",\n      \"enum\": [\n        \"excel\"\n      ],\n      \"title\": \"File Type\",\n      \"type\": \"string\"\n    },\n    \"sheet_name\": {\n      \"default\": \"Sheet1\",\n      \"title\": \"Sheet Name\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"OutputExcelTable\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>file_type</code>                 (<code>Literal['excel']</code>)             </li> <li> <code>sheet_name</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class OutputExcelTable(BaseModel):\n    \"\"\"Defines settings for writing an Excel file.\"\"\"\n\n    file_type: Literal[\"excel\"] = \"excel\"\n    sheet_name: str = \"Sheet1\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputFieldConfig","title":"<code>OutputFieldConfig</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for output field validation and transformation behavior.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Configuration for output field validation and transformation behavior.\",\n  \"properties\": {\n    \"enabled\": {\n      \"default\": false,\n      \"title\": \"Enabled\",\n      \"type\": \"boolean\"\n    },\n    \"validation_mode_behavior\": {\n      \"default\": \"select_only\",\n      \"enum\": [\n        \"add_missing\",\n        \"add_missing_keep_extra\",\n        \"raise_on_missing\",\n        \"select_only\"\n      ],\n      \"title\": \"Validation Mode Behavior\",\n      \"type\": \"string\"\n    },\n    \"fields\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/OutputFieldInfo\"\n      },\n      \"title\": \"Fields\",\n      \"type\": \"array\"\n    },\n    \"validate_data_types\": {\n      \"default\": false,\n      \"title\": \"Validate Data Types\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"title\": \"OutputFieldConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>enabled</code>                 (<code>bool</code>)             </li> <li> <code>validation_mode_behavior</code>                 (<code>Literal['add_missing', 'add_missing_keep_extra', 'raise_on_missing', 'select_only']</code>)             </li> <li> <code>fields</code>                 (<code>list[OutputFieldInfo]</code>)             </li> <li> <code>validate_data_types</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class OutputFieldConfig(BaseModel):\n    \"\"\"Configuration for output field validation and transformation behavior.\"\"\"\n\n    enabled: bool = False\n    validation_mode_behavior: Literal[\n        \"add_missing\",  # Add missing fields with defaults, remove extra columns\n        \"add_missing_keep_extra\",  # Add missing fields with defaults, keep all incoming columns\n        \"raise_on_missing\",  # Raise error if any fields are missing\n        \"select_only\",  # Select only specified fields, skip missing silently\n    ] = \"select_only\"\n    fields: list[OutputFieldInfo] = Field(default_factory=list)\n    validate_data_types: bool = False  # Enable data type validation without casting\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputFieldInfo","title":"<code>OutputFieldInfo</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Field information with optional default value for output field configuration.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Field information with optional default value for output field configuration.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"data_type\": {\n      \"default\": \"String\",\n      \"enum\": [\n        \"Int8\",\n        \"Int16\",\n        \"Int32\",\n        \"Int64\",\n        \"UInt8\",\n        \"UInt16\",\n        \"UInt32\",\n        \"UInt64\",\n        \"Float32\",\n        \"Float64\",\n        \"Decimal\",\n        \"String\",\n        \"Date\",\n        \"Datetime\",\n        \"Time\",\n        \"Duration\",\n        \"Boolean\",\n        \"Binary\",\n        \"List\",\n        \"Struct\",\n        \"Array\",\n        \"Integer\",\n        \"Double\",\n        \"Utf8\"\n      ],\n      \"title\": \"Data Type\",\n      \"type\": \"string\"\n    },\n    \"default_value\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Default Value\"\n    }\n  },\n  \"required\": [\n    \"name\"\n  ],\n  \"title\": \"OutputFieldInfo\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>data_type</code>                 (<code>DataTypeStr</code>)             </li> <li> <code>default_value</code>                 (<code>str | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class OutputFieldInfo(BaseModel):\n    \"\"\"Field information with optional default value for output field configuration.\"\"\"\n\n    name: str\n    data_type: DataTypeStr = \"String\"\n    default_value: str | None = None  # Can be a literal value or expression\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputParquetTable","title":"<code>OutputParquetTable</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for writing a Parquet file.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines settings for writing a Parquet file.\",\n  \"properties\": {\n    \"file_type\": {\n      \"const\": \"parquet\",\n      \"default\": \"parquet\",\n      \"enum\": [\n        \"parquet\"\n      ],\n      \"title\": \"File Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"OutputParquetTable\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>file_type</code>                 (<code>Literal['parquet']</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class OutputParquetTable(BaseModel):\n    \"\"\"Defines settings for writing a Parquet file.\"\"\"\n\n    file_type: Literal[\"parquet\"] = \"parquet\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputSettings","title":"<code>OutputSettings</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the complete settings for an output node.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputCsvTable\": {\n      \"description\": \"Defines settings for writing a CSV file.\",\n      \"properties\": {\n        \"file_type\": {\n          \"const\": \"csv\",\n          \"default\": \"csv\",\n          \"enum\": [\n            \"csv\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        },\n        \"delimiter\": {\n          \"default\": \",\",\n          \"title\": \"Delimiter\",\n          \"type\": \"string\"\n        },\n        \"encoding\": {\n          \"default\": \"utf-8\",\n          \"title\": \"Encoding\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"OutputCsvTable\",\n      \"type\": \"object\"\n    },\n    \"OutputExcelTable\": {\n      \"description\": \"Defines settings for writing an Excel file.\",\n      \"properties\": {\n        \"file_type\": {\n          \"const\": \"excel\",\n          \"default\": \"excel\",\n          \"enum\": [\n            \"excel\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        },\n        \"sheet_name\": {\n          \"default\": \"Sheet1\",\n          \"title\": \"Sheet Name\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"OutputExcelTable\",\n      \"type\": \"object\"\n    },\n    \"OutputParquetTable\": {\n      \"description\": \"Defines settings for writing a Parquet file.\",\n      \"properties\": {\n        \"file_type\": {\n          \"const\": \"parquet\",\n          \"default\": \"parquet\",\n          \"enum\": [\n            \"parquet\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"OutputParquetTable\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Defines the complete settings for an output node.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"directory\": {\n      \"title\": \"Directory\",\n      \"type\": \"string\"\n    },\n    \"file_type\": {\n      \"title\": \"File Type\",\n      \"type\": \"string\"\n    },\n    \"fields\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Fields\"\n    },\n    \"write_mode\": {\n      \"default\": \"overwrite\",\n      \"title\": \"Write Mode\",\n      \"type\": \"string\"\n    },\n    \"table_settings\": {\n      \"discriminator\": {\n        \"mapping\": {\n          \"csv\": \"#/$defs/OutputCsvTable\",\n          \"excel\": \"#/$defs/OutputExcelTable\",\n          \"parquet\": \"#/$defs/OutputParquetTable\"\n        },\n        \"propertyName\": \"file_type\"\n      },\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputCsvTable\"\n        },\n        {\n          \"$ref\": \"#/$defs/OutputParquetTable\"\n        },\n        {\n          \"$ref\": \"#/$defs/OutputExcelTable\"\n        }\n      ],\n      \"title\": \"Table Settings\"\n    },\n    \"abs_file_path\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Abs File Path\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"directory\",\n    \"file_type\",\n    \"table_settings\"\n  ],\n  \"title\": \"OutputSettings\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>directory</code>                 (<code>str</code>)             </li> <li> <code>file_type</code>                 (<code>str</code>)             </li> <li> <code>fields</code>                 (<code>list[str] | None</code>)             </li> <li> <code>write_mode</code>                 (<code>str</code>)             </li> <li> <code>table_settings</code>                 (<code>OutputTableSettings</code>)             </li> <li> <code>abs_file_path</code>                 (<code>str | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_table_settings</code>                 \u2192                   <code>table_settings</code> </li> <li> <code>populate_abs_file_path</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class OutputSettings(BaseModel):\n    \"\"\"Defines the complete settings for an output node.\"\"\"\n\n    name: str\n    directory: str\n    file_type: str  # This drives which table_settings to use\n    fields: list[str] | None = Field(default_factory=list)\n    write_mode: str = \"overwrite\"\n    table_settings: OutputTableSettings\n    abs_file_path: str | None = None\n\n    def to_yaml_dict(self) -&gt; OutputSettingsYaml:\n        \"\"\"Converts the output settings to a dictionary suitable for YAML serialization.\"\"\"\n        result: OutputSettingsYaml = {\n            \"name\": self.name,\n            \"directory\": self.directory,\n            \"file_type\": self.file_type,\n            \"write_mode\": self.write_mode,\n        }\n        if self.abs_file_path:\n            result[\"abs_file_path\"] = self.abs_file_path\n        if self.fields:\n            result[\"fields\"] = self.fields\n        # Only include table_settings if it has non-default values beyond file_type\n        ts_dict = self.table_settings.model_dump(exclude={\"file_type\"})\n        if any(v for v in ts_dict.values()):  # Has meaningful settings\n            result[\"table_settings\"] = ts_dict\n        return result\n\n    @property\n    def sheet_name(self) -&gt; str | None:\n        if self.file_type == \"excel\":\n            return self.table_settings.sheet_name\n\n    @property\n    def delimiter(self) -&gt; str | None:\n        if self.file_type == \"csv\":\n            return self.table_settings.delimiter\n\n    @field_validator(\"table_settings\", mode=\"before\")\n    @classmethod\n    def validate_table_settings(cls, v, info: ValidationInfo):\n        \"\"\"Ensures table_settings matches the file_type.\"\"\"\n        if v is None:\n            file_type = info.data.get(\"file_type\", \"csv\")\n            # Create default based on file_type\n            match file_type:\n                case \"csv\":\n                    return OutputCsvTable()\n                case \"parquet\":\n                    return OutputParquetTable()\n                case \"excel\":\n                    return OutputExcelTable()\n                case _:\n                    return OutputCsvTable()\n\n        # If it's a dict, add file_type if missing\n        if isinstance(v, dict) and \"file_type\" not in v:\n            v[\"file_type\"] = info.data.get(\"file_type\", \"csv\")\n\n        return v\n\n    def set_absolute_filepath(self):\n        \"\"\"Resolves the output directory and name into an absolute path.\"\"\"\n        base_path = Path(self.directory)\n        if not base_path.is_absolute():\n            base_path = Path.cwd() / base_path\n        if self.name and self.name not in base_path.name:\n            base_path = base_path / self.name\n        self.abs_file_path = str(base_path.resolve())\n\n    @model_validator(mode=\"after\")\n    def populate_abs_file_path(self):\n        \"\"\"Ensures the absolute file path is populated after validation.\"\"\"\n        self.set_absolute_filepath()\n        return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputSettings.populate_abs_file_path","title":"<code>populate_abs_file_path()</code>  <code>pydantic-validator</code>","text":"<p>Ensures the absolute file path is populated after validation.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@model_validator(mode=\"after\")\ndef populate_abs_file_path(self):\n    \"\"\"Ensures the absolute file path is populated after validation.\"\"\"\n    self.set_absolute_filepath()\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputSettings.set_absolute_filepath","title":"<code>set_absolute_filepath()</code>","text":"<p>Resolves the output directory and name into an absolute path.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def set_absolute_filepath(self):\n    \"\"\"Resolves the output directory and name into an absolute path.\"\"\"\n    base_path = Path(self.directory)\n    if not base_path.is_absolute():\n        base_path = Path.cwd() / base_path\n    if self.name and self.name not in base_path.name:\n        base_path = base_path / self.name\n    self.abs_file_path = str(base_path.resolve())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputSettings.to_yaml_dict","title":"<code>to_yaml_dict()</code>","text":"<p>Converts the output settings to a dictionary suitable for YAML serialization.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def to_yaml_dict(self) -&gt; OutputSettingsYaml:\n    \"\"\"Converts the output settings to a dictionary suitable for YAML serialization.\"\"\"\n    result: OutputSettingsYaml = {\n        \"name\": self.name,\n        \"directory\": self.directory,\n        \"file_type\": self.file_type,\n        \"write_mode\": self.write_mode,\n    }\n    if self.abs_file_path:\n        result[\"abs_file_path\"] = self.abs_file_path\n    if self.fields:\n        result[\"fields\"] = self.fields\n    # Only include table_settings if it has non-default values beyond file_type\n    ts_dict = self.table_settings.model_dump(exclude={\"file_type\"})\n    if any(v for v in ts_dict.values()):  # Has meaningful settings\n        result[\"table_settings\"] = ts_dict\n    return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputSettings.validate_table_settings","title":"<code>validate_table_settings(v, info)</code>  <code>pydantic-validator</code>","text":"<p>Ensures table_settings matches the file_type.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@field_validator(\"table_settings\", mode=\"before\")\n@classmethod\ndef validate_table_settings(cls, v, info: ValidationInfo):\n    \"\"\"Ensures table_settings matches the file_type.\"\"\"\n    if v is None:\n        file_type = info.data.get(\"file_type\", \"csv\")\n        # Create default based on file_type\n        match file_type:\n            case \"csv\":\n                return OutputCsvTable()\n            case \"parquet\":\n                return OutputParquetTable()\n            case \"excel\":\n                return OutputExcelTable()\n            case _:\n                return OutputCsvTable()\n\n    # If it's a dict, add file_type if missing\n    if isinstance(v, dict) and \"file_type\" not in v:\n        v[\"file_type\"] = info.data.get(\"file_type\", \"csv\")\n\n    return v\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.PythonScriptInput","title":"<code>PythonScriptInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Settings for Python code execution on a kernel.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"NotebookCell\": {\n      \"description\": \"A single cell in the notebook editor.\\n\\nNote: Cell output (stdout, display_outputs, errors) is handled entirely\\non the frontend and is not persisted. Only id and code are stored.\",\n      \"properties\": {\n        \"id\": {\n          \"title\": \"Id\",\n          \"type\": \"string\"\n        },\n        \"code\": {\n          \"default\": \"\",\n          \"title\": \"Code\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"id\"\n      ],\n      \"title\": \"NotebookCell\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for Python code execution on a kernel.\",\n  \"properties\": {\n    \"code\": {\n      \"default\": \"\",\n      \"title\": \"Code\",\n      \"type\": \"string\"\n    },\n    \"kernel_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Kernel Id\"\n    },\n    \"cells\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/NotebookCell\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Cells\"\n    }\n  },\n  \"title\": \"PythonScriptInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>code</code>                 (<code>str</code>)             </li> <li> <code>kernel_id</code>                 (<code>str | None</code>)             </li> <li> <code>cells</code>                 (<code>list[NotebookCell] | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class PythonScriptInput(BaseModel):\n    \"\"\"Settings for Python code execution on a kernel.\"\"\"\n\n    code: str = \"\"\n    kernel_id: str | None = None\n    cells: list[NotebookCell] | None = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.RawData","title":"<code>RawData</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents data in a raw, columnar format for manual input.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"MinimalFieldInfo\": {\n      \"description\": \"Represents the most basic information about a data field (column).\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"MinimalFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Represents data in a raw, columnar format for manual input.\",\n  \"properties\": {\n    \"columns\": {\n      \"default\": null,\n      \"items\": {\n        \"$ref\": \"#/$defs/MinimalFieldInfo\"\n      },\n      \"title\": \"Columns\",\n      \"type\": \"array\"\n    },\n    \"data\": {\n      \"items\": {\n        \"items\": {},\n        \"type\": \"array\"\n      },\n      \"title\": \"Data\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"data\"\n  ],\n  \"title\": \"RawData\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>columns</code>                 (<code>list[MinimalFieldInfo]</code>)             </li> <li> <code>data</code>                 (<code>list[list]</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class RawData(BaseModel):\n    \"\"\"Represents data in a raw, columnar format for manual input.\"\"\"\n\n    columns: list[MinimalFieldInfo] = None\n    data: list[list]\n\n    @classmethod\n    def from_pylist(cls, pylist: list[dict]):\n        \"\"\"Creates a RawData object from a list of Python dictionaries.\"\"\"\n        if len(pylist) == 0:\n            return cls(columns=[], data=[])\n        pylist = ensure_similarity_dicts(pylist)\n        values = [standardize_col_dtype([vv for vv in c]) for c in zip(*(r.values() for r in pylist), strict=False)]\n        data_types = (pl.DataType.from_python(type(next((v for v in column_values), None))) for column_values in values)\n        columns = [MinimalFieldInfo(name=c, data_type=str(next(data_types))) for c in pylist[0].keys()]\n        return cls(columns=columns, data=values)\n\n    @classmethod\n    def from_pydict(cls, pydict: dict[str, list]):\n        \"\"\"Creates a RawData object from a dictionary of lists.\"\"\"\n        if len(pydict) == 0:\n            return cls(columns=[], data=[])\n        values = [standardize_col_dtype(column_values) for column_values in pydict.values()]\n        data_types = (pl.DataType.from_python(type(next((v for v in column_values), None))) for column_values in values)\n        columns = [MinimalFieldInfo(name=c, data_type=str(next(data_types))) for c in pydict.keys()]\n        return cls(columns=columns, data=values)\n\n    def to_pylist(self) -&gt; list[dict]:\n        \"\"\"Converts the RawData object back into a list of Python dictionaries.\"\"\"\n        return [{c.name: self.data[ci][ri] for ci, c in enumerate(self.columns)} for ri in range(len(self.data[0]))]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.RawData.from_pydict","title":"<code>from_pydict(pydict)</code>  <code>classmethod</code>","text":"<p>Creates a RawData object from a dictionary of lists.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@classmethod\ndef from_pydict(cls, pydict: dict[str, list]):\n    \"\"\"Creates a RawData object from a dictionary of lists.\"\"\"\n    if len(pydict) == 0:\n        return cls(columns=[], data=[])\n    values = [standardize_col_dtype(column_values) for column_values in pydict.values()]\n    data_types = (pl.DataType.from_python(type(next((v for v in column_values), None))) for column_values in values)\n    columns = [MinimalFieldInfo(name=c, data_type=str(next(data_types))) for c in pydict.keys()]\n    return cls(columns=columns, data=values)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.RawData.from_pylist","title":"<code>from_pylist(pylist)</code>  <code>classmethod</code>","text":"<p>Creates a RawData object from a list of Python dictionaries.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@classmethod\ndef from_pylist(cls, pylist: list[dict]):\n    \"\"\"Creates a RawData object from a list of Python dictionaries.\"\"\"\n    if len(pylist) == 0:\n        return cls(columns=[], data=[])\n    pylist = ensure_similarity_dicts(pylist)\n    values = [standardize_col_dtype([vv for vv in c]) for c in zip(*(r.values() for r in pylist), strict=False)]\n    data_types = (pl.DataType.from_python(type(next((v for v in column_values), None))) for column_values in values)\n    columns = [MinimalFieldInfo(name=c, data_type=str(next(data_types))) for c in pylist[0].keys()]\n    return cls(columns=columns, data=values)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.RawData.to_pylist","title":"<code>to_pylist()</code>","text":"<p>Converts the RawData object back into a list of Python dictionaries.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def to_pylist(self) -&gt; list[dict]:\n    \"\"\"Converts the RawData object back into a list of Python dictionaries.\"\"\"\n    return [{c.name: self.data[ci][ri] for ci, c in enumerate(self.columns)} for ri in range(len(self.data[0]))]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedTable","title":"<code>ReceivedTable</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for defining a table received from an external source.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"InputCsvTable\": {\n      \"description\": \"Defines settings for reading a CSV file.\",\n      \"properties\": {\n        \"file_type\": {\n          \"const\": \"csv\",\n          \"default\": \"csv\",\n          \"enum\": [\n            \"csv\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        },\n        \"reference\": {\n          \"default\": \"\",\n          \"title\": \"Reference\",\n          \"type\": \"string\"\n        },\n        \"starting_from_line\": {\n          \"default\": 0,\n          \"title\": \"Starting From Line\",\n          \"type\": \"integer\"\n        },\n        \"delimiter\": {\n          \"default\": \",\",\n          \"title\": \"Delimiter\",\n          \"type\": \"string\"\n        },\n        \"has_headers\": {\n          \"default\": true,\n          \"title\": \"Has Headers\",\n          \"type\": \"boolean\"\n        },\n        \"encoding\": {\n          \"default\": \"utf-8\",\n          \"title\": \"Encoding\",\n          \"type\": \"string\"\n        },\n        \"parquet_ref\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Parquet Ref\"\n        },\n        \"row_delimiter\": {\n          \"default\": \"\\n\",\n          \"title\": \"Row Delimiter\",\n          \"type\": \"string\"\n        },\n        \"quote_char\": {\n          \"default\": \"\\\"\",\n          \"title\": \"Quote Char\",\n          \"type\": \"string\"\n        },\n        \"infer_schema_length\": {\n          \"default\": 10000,\n          \"title\": \"Infer Schema Length\",\n          \"type\": \"integer\"\n        },\n        \"truncate_ragged_lines\": {\n          \"default\": false,\n          \"title\": \"Truncate Ragged Lines\",\n          \"type\": \"boolean\"\n        },\n        \"ignore_errors\": {\n          \"default\": false,\n          \"title\": \"Ignore Errors\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"InputCsvTable\",\n      \"type\": \"object\"\n    },\n    \"InputExcelTable\": {\n      \"description\": \"Defines settings for reading an Excel file.\",\n      \"properties\": {\n        \"file_type\": {\n          \"const\": \"excel\",\n          \"default\": \"excel\",\n          \"enum\": [\n            \"excel\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        },\n        \"sheet_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Sheet Name\"\n        },\n        \"start_row\": {\n          \"default\": 0,\n          \"title\": \"Start Row\",\n          \"type\": \"integer\"\n        },\n        \"start_column\": {\n          \"default\": 0,\n          \"title\": \"Start Column\",\n          \"type\": \"integer\"\n        },\n        \"end_row\": {\n          \"default\": 0,\n          \"title\": \"End Row\",\n          \"type\": \"integer\"\n        },\n        \"end_column\": {\n          \"default\": 0,\n          \"title\": \"End Column\",\n          \"type\": \"integer\"\n        },\n        \"has_headers\": {\n          \"default\": true,\n          \"title\": \"Has Headers\",\n          \"type\": \"boolean\"\n        },\n        \"type_inference\": {\n          \"default\": false,\n          \"title\": \"Type Inference\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"InputExcelTable\",\n      \"type\": \"object\"\n    },\n    \"InputJsonTable\": {\n      \"description\": \"Defines settings for reading a JSON file.\",\n      \"properties\": {\n        \"file_type\": {\n          \"const\": \"json\",\n          \"default\": \"json\",\n          \"enum\": [\n            \"json\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        },\n        \"reference\": {\n          \"default\": \"\",\n          \"title\": \"Reference\",\n          \"type\": \"string\"\n        },\n        \"starting_from_line\": {\n          \"default\": 0,\n          \"title\": \"Starting From Line\",\n          \"type\": \"integer\"\n        },\n        \"delimiter\": {\n          \"default\": \",\",\n          \"title\": \"Delimiter\",\n          \"type\": \"string\"\n        },\n        \"has_headers\": {\n          \"default\": true,\n          \"title\": \"Has Headers\",\n          \"type\": \"boolean\"\n        },\n        \"encoding\": {\n          \"default\": \"utf-8\",\n          \"title\": \"Encoding\",\n          \"type\": \"string\"\n        },\n        \"parquet_ref\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Parquet Ref\"\n        },\n        \"row_delimiter\": {\n          \"default\": \"\\n\",\n          \"title\": \"Row Delimiter\",\n          \"type\": \"string\"\n        },\n        \"quote_char\": {\n          \"default\": \"\\\"\",\n          \"title\": \"Quote Char\",\n          \"type\": \"string\"\n        },\n        \"infer_schema_length\": {\n          \"default\": 10000,\n          \"title\": \"Infer Schema Length\",\n          \"type\": \"integer\"\n        },\n        \"truncate_ragged_lines\": {\n          \"default\": false,\n          \"title\": \"Truncate Ragged Lines\",\n          \"type\": \"boolean\"\n        },\n        \"ignore_errors\": {\n          \"default\": false,\n          \"title\": \"Ignore Errors\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"InputJsonTable\",\n      \"type\": \"object\"\n    },\n    \"InputParquetTable\": {\n      \"description\": \"Defines settings for reading a Parquet file.\",\n      \"properties\": {\n        \"file_type\": {\n          \"const\": \"parquet\",\n          \"default\": \"parquet\",\n          \"enum\": [\n            \"parquet\"\n          ],\n          \"title\": \"File Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"InputParquetTable\",\n      \"type\": \"object\"\n    },\n    \"MinimalFieldInfo\": {\n      \"description\": \"Represents the most basic information about a data field (column).\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"MinimalFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Model for defining a table received from an external source.\",\n  \"properties\": {\n    \"id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Id\"\n    },\n    \"name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Name\"\n    },\n    \"path\": {\n      \"title\": \"Path\",\n      \"type\": \"string\"\n    },\n    \"directory\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Directory\"\n    },\n    \"analysis_file_available\": {\n      \"default\": false,\n      \"title\": \"Analysis File Available\",\n      \"type\": \"boolean\"\n    },\n    \"status\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Status\"\n    },\n    \"fields\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/MinimalFieldInfo\"\n      },\n      \"title\": \"Fields\",\n      \"type\": \"array\"\n    },\n    \"abs_file_path\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Abs File Path\"\n    },\n    \"file_type\": {\n      \"enum\": [\n        \"csv\",\n        \"json\",\n        \"parquet\",\n        \"excel\"\n      ],\n      \"title\": \"File Type\",\n      \"type\": \"string\"\n    },\n    \"table_settings\": {\n      \"discriminator\": {\n        \"mapping\": {\n          \"csv\": \"#/$defs/InputCsvTable\",\n          \"excel\": \"#/$defs/InputExcelTable\",\n          \"json\": \"#/$defs/InputJsonTable\",\n          \"parquet\": \"#/$defs/InputParquetTable\"\n        },\n        \"propertyName\": \"file_type\"\n      },\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/$defs/InputCsvTable\"\n        },\n        {\n          \"$ref\": \"#/$defs/InputJsonTable\"\n        },\n        {\n          \"$ref\": \"#/$defs/InputParquetTable\"\n        },\n        {\n          \"$ref\": \"#/$defs/InputExcelTable\"\n        }\n      ],\n      \"title\": \"Table Settings\"\n    }\n  },\n  \"required\": [\n    \"path\",\n    \"file_type\",\n    \"table_settings\"\n  ],\n  \"title\": \"ReceivedTable\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>int | None</code>)             </li> <li> <code>name</code>                 (<code>str | None</code>)             </li> <li> <code>path</code>                 (<code>str</code>)             </li> <li> <code>directory</code>                 (<code>str | None</code>)             </li> <li> <code>analysis_file_available</code>                 (<code>bool</code>)             </li> <li> <code>status</code>                 (<code>str | None</code>)             </li> <li> <code>fields</code>                 (<code>list[MinimalFieldInfo]</code>)             </li> <li> <code>abs_file_path</code>                 (<code>str | None</code>)             </li> <li> <code>file_type</code>                 (<code>Literal['csv', 'json', 'parquet', 'excel']</code>)             </li> <li> <code>table_settings</code>                 (<code>InputTableSettings</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>set_default_table_settings</code> </li> <li> <code>populate_abs_file_path</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class ReceivedTable(BaseModel):\n    \"\"\"Model for defining a table received from an external source.\"\"\"\n\n    # Metadata fields\n    id: int | None = None\n    name: str | None = None\n    path: str  # This can be an absolute or relative path\n    directory: str | None = None\n    analysis_file_available: bool = False\n    status: str | None = None\n    fields: list[MinimalFieldInfo] = Field(default_factory=list)\n    abs_file_path: str | None = None\n\n    file_type: Literal[\"csv\", \"json\", \"parquet\", \"excel\"]\n\n    table_settings: InputTableSettings\n\n    @classmethod\n    def create_from_path(cls, path: str, file_type: Literal[\"csv\", \"json\", \"parquet\", \"excel\"] = \"csv\"):\n        \"\"\"Creates an instance from a file path string.\"\"\"\n        filename = Path(path).name\n\n        # Create appropriate table_settings based on file_type\n        settings_map = {\n            \"csv\": InputCsvTable(),\n            \"json\": InputJsonTable(),\n            \"parquet\": InputParquetTable(),\n            \"excel\": InputExcelTable(),\n        }\n\n        return cls(\n            name=filename, path=path, file_type=file_type, table_settings=settings_map.get(file_type, InputCsvTable())\n        )\n\n    @property\n    def file_path(self) -&gt; str:\n        \"\"\"Constructs the full file path from the directory and name.\"\"\"\n        if self.name and self.name not in self.path:\n            return os.path.join(self.path, self.name)\n        else:\n            return self.path\n\n    def set_absolute_filepath(self):\n        \"\"\"Resolves the path to an absolute file path.\"\"\"\n        base_path = Path(self.path).expanduser()\n        if not base_path.is_absolute():\n            base_path = Path.cwd() / base_path\n        if self.name and self.name not in base_path.name:\n            base_path = base_path / self.name\n        self.abs_file_path = str(base_path.resolve())\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def set_default_table_settings(cls, data):\n        \"\"\"Create default table_settings based on file_type if not provided.\"\"\"\n        if isinstance(data, dict):\n            if \"table_settings\" not in data or data[\"table_settings\"] is None:\n                data[\"table_settings\"] = {}\n\n            if isinstance(data[\"table_settings\"], dict) and \"file_type\" not in data[\"table_settings\"]:\n                data[\"table_settings\"][\"file_type\"] = data.get(\"file_type\", \"csv\")\n        return data\n\n    @model_validator(mode=\"after\")\n    def populate_abs_file_path(self):\n        \"\"\"Ensures the absolute file path is populated after validation.\"\"\"\n        if not self.abs_file_path:\n            self.set_absolute_filepath()\n        return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedTable.file_path","title":"<code>file_path</code>  <code>property</code>","text":"<p>Constructs the full file path from the directory and name.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedTable.create_from_path","title":"<code>create_from_path(path, file_type='csv')</code>  <code>classmethod</code>","text":"<p>Creates an instance from a file path string.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@classmethod\ndef create_from_path(cls, path: str, file_type: Literal[\"csv\", \"json\", \"parquet\", \"excel\"] = \"csv\"):\n    \"\"\"Creates an instance from a file path string.\"\"\"\n    filename = Path(path).name\n\n    # Create appropriate table_settings based on file_type\n    settings_map = {\n        \"csv\": InputCsvTable(),\n        \"json\": InputJsonTable(),\n        \"parquet\": InputParquetTable(),\n        \"excel\": InputExcelTable(),\n    }\n\n    return cls(\n        name=filename, path=path, file_type=file_type, table_settings=settings_map.get(file_type, InputCsvTable())\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedTable.populate_abs_file_path","title":"<code>populate_abs_file_path()</code>  <code>pydantic-validator</code>","text":"<p>Ensures the absolute file path is populated after validation.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@model_validator(mode=\"after\")\ndef populate_abs_file_path(self):\n    \"\"\"Ensures the absolute file path is populated after validation.\"\"\"\n    if not self.abs_file_path:\n        self.set_absolute_filepath()\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedTable.set_absolute_filepath","title":"<code>set_absolute_filepath()</code>","text":"<p>Resolves the path to an absolute file path.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def set_absolute_filepath(self):\n    \"\"\"Resolves the path to an absolute file path.\"\"\"\n    base_path = Path(self.path).expanduser()\n    if not base_path.is_absolute():\n        base_path = Path.cwd() / base_path\n    if self.name and self.name not in base_path.name:\n        base_path = base_path / self.name\n    self.abs_file_path = str(base_path.resolve())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedTable.set_default_table_settings","title":"<code>set_default_table_settings(data)</code>  <code>pydantic-validator</code>","text":"<p>Create default table_settings based on file_type if not provided.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef set_default_table_settings(cls, data):\n    \"\"\"Create default table_settings based on file_type if not provided.\"\"\"\n    if isinstance(data, dict):\n        if \"table_settings\" not in data or data[\"table_settings\"] is None:\n            data[\"table_settings\"] = {}\n\n        if isinstance(data[\"table_settings\"], dict) and \"file_type\" not in data[\"table_settings\"]:\n            data[\"table_settings\"][\"file_type\"] = data.get(\"file_type\", \"csv\")\n    return data\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.RemoveItem","title":"<code>RemoveItem</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single item to be removed from a directory or list.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Represents a single item to be removed from a directory or list.\",\n  \"properties\": {\n    \"path\": {\n      \"title\": \"Path\",\n      \"type\": \"string\"\n    },\n    \"id\": {\n      \"default\": -1,\n      \"title\": \"Id\",\n      \"type\": \"integer\"\n    }\n  },\n  \"required\": [\n    \"path\"\n  ],\n  \"title\": \"RemoveItem\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>path</code>                 (<code>str</code>)             </li> <li> <code>id</code>                 (<code>int</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class RemoveItem(BaseModel):\n    \"\"\"Represents a single item to be removed from a directory or list.\"\"\"\n\n    path: str\n    id: int = -1\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.RemoveItemsInput","title":"<code>RemoveItemsInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines a list of items to be removed.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"RemoveItem\": {\n      \"description\": \"Represents a single item to be removed from a directory or list.\",\n      \"properties\": {\n        \"path\": {\n          \"title\": \"Path\",\n          \"type\": \"string\"\n        },\n        \"id\": {\n          \"default\": -1,\n          \"title\": \"Id\",\n          \"type\": \"integer\"\n        }\n      },\n      \"required\": [\n        \"path\"\n      ],\n      \"title\": \"RemoveItem\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Defines a list of items to be removed.\",\n  \"properties\": {\n    \"paths\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/RemoveItem\"\n      },\n      \"title\": \"Paths\",\n      \"type\": \"array\"\n    },\n    \"source_path\": {\n      \"title\": \"Source Path\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"paths\",\n    \"source_path\"\n  ],\n  \"title\": \"RemoveItemsInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>paths</code>                 (<code>list[RemoveItem]</code>)             </li> <li> <code>source_path</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class RemoveItemsInput(BaseModel):\n    \"\"\"Defines a list of items to be removed.\"\"\"\n\n    paths: list[RemoveItem]\n    source_path: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.SampleUsers","title":"<code>SampleUsers</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>ExternalSource</code></p> <p>Settings for generating a sample dataset of users.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"MinimalFieldInfo\": {\n      \"description\": \"Represents the most basic information about a data field (column).\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"MinimalFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for generating a sample dataset of users.\",\n  \"properties\": {\n    \"orientation\": {\n      \"default\": \"row\",\n      \"title\": \"Orientation\",\n      \"type\": \"string\"\n    },\n    \"fields\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/MinimalFieldInfo\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Fields\"\n    },\n    \"SAMPLE_USERS\": {\n      \"title\": \"Sample Users\",\n      \"type\": \"boolean\"\n    },\n    \"class_name\": {\n      \"default\": \"sample_users\",\n      \"title\": \"Class Name\",\n      \"type\": \"string\"\n    },\n    \"size\": {\n      \"default\": 100,\n      \"title\": \"Size\",\n      \"type\": \"integer\"\n    }\n  },\n  \"required\": [\n    \"SAMPLE_USERS\"\n  ],\n  \"title\": \"SampleUsers\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>orientation</code>                 (<code>str</code>)             </li> <li> <code>fields</code>                 (<code>list[MinimalFieldInfo] | None</code>)             </li> <li> <code>SAMPLE_USERS</code>                 (<code>bool</code>)             </li> <li> <code>class_name</code>                 (<code>str</code>)             </li> <li> <code>size</code>                 (<code>int</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class SampleUsers(ExternalSource):\n    \"\"\"Settings for generating a sample dataset of users.\"\"\"\n\n    SAMPLE_USERS: bool\n    class_name: str = \"sample_users\"\n    size: int = 100\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.UserDefinedNode","title":"<code>UserDefinedNode</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>NodeMultiInput</code></p> <p>Settings for a node that contains the user defined node information</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFieldConfig\": {\n      \"description\": \"Configuration for output field validation and transformation behavior.\",\n      \"properties\": {\n        \"enabled\": {\n          \"default\": false,\n          \"title\": \"Enabled\",\n          \"type\": \"boolean\"\n        },\n        \"validation_mode_behavior\": {\n          \"default\": \"select_only\",\n          \"enum\": [\n            \"add_missing\",\n            \"add_missing_keep_extra\",\n            \"raise_on_missing\",\n            \"select_only\"\n          ],\n          \"title\": \"Validation Mode Behavior\",\n          \"type\": \"string\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFieldInfo\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"validate_data_types\": {\n          \"default\": false,\n          \"title\": \"Validate Data Types\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"OutputFieldConfig\",\n      \"type\": \"object\"\n    },\n    \"OutputFieldInfo\": {\n      \"description\": \"Field information with optional default value for output field configuration.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"default\": \"String\",\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"default_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Default Value\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"OutputFieldInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for a node that contains the user defined node information\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"cache_results\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Cache Results\"\n    },\n    \"pos_x\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos X\"\n    },\n    \"pos_y\": {\n      \"anyOf\": [\n        {\n          \"type\": \"number\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": 0,\n      \"title\": \"Pos Y\"\n    },\n    \"is_setup\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Is Setup\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"\",\n      \"title\": \"Description\"\n    },\n    \"node_reference\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Reference\"\n    },\n    \"user_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User Id\"\n    },\n    \"is_flow_output\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is Flow Output\"\n    },\n    \"is_user_defined\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Is User Defined\"\n    },\n    \"output_field_config\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/OutputFieldConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"depending_on_ids\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"integer\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Depending On Ids\"\n    },\n    \"settings\": {\n      \"title\": \"Settings\"\n    },\n    \"kernel_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Kernel Id\"\n    },\n    \"output_names\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Output Names\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"settings\"\n  ],\n  \"title\": \"UserDefinedNode\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>cache_results</code>                 (<code>bool | None</code>)             </li> <li> <code>pos_x</code>                 (<code>float | None</code>)             </li> <li> <code>pos_y</code>                 (<code>float | None</code>)             </li> <li> <code>is_setup</code>                 (<code>bool | None</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> <li> <code>node_reference</code>                 (<code>str | None</code>)             </li> <li> <code>user_id</code>                 (<code>int | None</code>)             </li> <li> <code>is_flow_output</code>                 (<code>bool | None</code>)             </li> <li> <code>is_user_defined</code>                 (<code>bool | None</code>)             </li> <li> <code>output_field_config</code>                 (<code>OutputFieldConfig | None</code>)             </li> <li> <code>depending_on_ids</code>                 (<code>list[int] | None</code>)             </li> <li> <code>settings</code>                 (<code>Any</code>)             </li> <li> <code>kernel_id</code>                 (<code>str | None</code>)             </li> <li> <code>output_names</code>                 (<code>list[str]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_node_reference</code>                 \u2192                   <code>node_reference</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class UserDefinedNode(NodeMultiInput):\n    \"\"\"Settings for a node that contains the user defined node information\"\"\"\n\n    settings: Any\n    kernel_id: str | None = None\n    output_names: list[str] = Field(default_factory=lambda: [\"main\"])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#transform_schema","title":"<code>transform_schema</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema","title":"<code>flowfile_core.schemas.transform_schema</code>","text":"<p>Classes:</p> Name Description <code>AggColl</code> <p>A data class that represents a single aggregation operation for a group by operation.</p> <code>BasicFilter</code> <p>Defines a simple, single-condition filter (e.g., 'column' 'equals' 'value').</p> <code>CrossJoinInput</code> <p>Data model for cross join operations.</p> <code>CrossJoinInputManager</code> <p>Manager for cross join operations.</p> <code>FieldInput</code> <p>Represents a single field with its name and data type, typically for defining an output column.</p> <code>FilterInput</code> <p>Defines the settings for a filter operation, supporting basic or advanced (expression-based) modes.</p> <code>FilterOperator</code> <p>Supported filter comparison operators.</p> <code>FullJoinKeyResponse</code> <p>Holds the join key rename responses for both sides of a join.</p> <code>FunctionInput</code> <p>Defines a formula to be applied, including the output field information.</p> <code>FuzzyMatchInput</code> <p>Data model for fuzzy matching join operations.</p> <code>FuzzyMatchInputManager</code> <p>Manager for fuzzy matching join operations.</p> <code>GraphSolverInput</code> <p>Defines settings for a graph-solving operation (e.g., finding connected components).</p> <code>GroupByInput</code> <p>A data class that represents the input for a group by operation.</p> <code>JoinInput</code> <p>Data model for standard SQL-style join operations.</p> <code>JoinInputManager</code> <p>Manager for standard SQL-style join operations.</p> <code>JoinInputs</code> <p>Data model for join-specific select inputs (extends SelectInputs).</p> <code>JoinInputsManager</code> <p>Manager for join-specific operations, extends SelectInputsManager.</p> <code>JoinKeyRename</code> <p>Represents the renaming of a join key from its original to a temporary name.</p> <code>JoinKeyRenameResponse</code> <p>Contains a list of join key renames for one side of a join.</p> <code>JoinMap</code> <p>Defines a single mapping between a left and right column for a join key.</p> <code>JoinSelectManagerMixin</code> <p>Mixin providing common methods for join-like operations.</p> <code>PivotInput</code> <p>Defines the settings for a pivot (long-to-wide) operation.</p> <code>PolarsCodeInput</code> <p>A simple container for a string of user-provided Polars code to be executed.</p> <code>RecordIdInput</code> <p>Defines settings for adding a record ID (row number) column to the data.</p> <code>SelectInput</code> <p>Defines how a single column should be selected, renamed, or type-cast.</p> <code>SelectInputs</code> <p>A container for a list of <code>SelectInput</code> objects (pure data, no logic).</p> <code>SelectInputsManager</code> <p>Manager class that provides all query and mutation operations.</p> <code>SortByInput</code> <p>Defines a single sort condition on a column, including the direction.</p> <code>TextToRowsInput</code> <p>Defines settings for splitting a text column into multiple rows based on a delimiter.</p> <code>UnionInput</code> <p>Defines settings for a union (concatenation) operation.</p> <code>UniqueInput</code> <p>Defines settings for a uniqueness operation, specifying columns and which row to keep.</p> <code>UnpivotInput</code> <p>Defines settings for an unpivot (wide-to-long) operation.</p> <p>Functions:</p> Name Description <code>construct_join_key_name</code> <p>Creates a temporary, unique name for a join key column.</p> <code>get_func_type_mapping</code> <p>Infers the output data type of common aggregation functions.</p> <code>string_concat</code> <p>A simple wrapper to concatenate string columns in Polars.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.AggColl","title":"<code>AggColl</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A data class that represents a single aggregation operation for a group by operation.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.AggColl--attributes","title":"Attributes","text":"<p>old_name : str     The name of the column in the original DataFrame to be aggregated.</p> str <p>The aggregation function to use. This can be a string representing a built-in function or a custom function.</p> Optional[str] <p>The name of the resulting aggregated column in the output DataFrame. If not provided, it will default to the old_name appended with the aggregation function.</p> Optional[str] <p>The type of the output values of the aggregation. If not provided, it is inferred from the aggregation function using the <code>get_func_type_mapping</code> function.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.AggColl--example","title":"Example","text":"<p>agg_col = AggColl(     old_name='col1',     agg='sum',     new_name='sum_col1',     output_type='float' )</p> Show JSON schema: <pre><code>{\n  \"description\": \"A data class that represents a single aggregation operation for a group by operation.\\n\\nAttributes\\n----------\\nold_name : str\\n    The name of the column in the original DataFrame to be aggregated.\\n\\nagg : str\\n    The aggregation function to use. This can be a string representing a built-in function or a custom function.\\n\\nnew_name : Optional[str]\\n    The name of the resulting aggregated column in the output DataFrame. If not provided, it will default to the\\n    old_name appended with the aggregation function.\\n\\noutput_type : Optional[str]\\n    The type of the output values of the aggregation. If not provided, it is inferred from the aggregation function\\n    using the `get_func_type_mapping` function.\\n\\nExample\\n--------\\nagg_col = AggColl(\\n    old_name='col1',\\n    agg='sum',\\n    new_name='sum_col1',\\n    output_type='float'\\n)\",\n  \"properties\": {\n    \"old_name\": {\n      \"title\": \"Old Name\",\n      \"type\": \"string\"\n    },\n    \"agg\": {\n      \"title\": \"Agg\",\n      \"type\": \"string\"\n    },\n    \"new_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"New Name\"\n    },\n    \"output_type\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Output Type\"\n    }\n  },\n  \"required\": [\n    \"old_name\",\n    \"agg\"\n  ],\n  \"title\": \"AggColl\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>old_name</code>                 (<code>str</code>)             </li> <li> <code>agg</code>                 (<code>str</code>)             </li> <li> <code>new_name</code>                 (<code>str | None</code>)             </li> <li> <code>output_type</code>                 (<code>str | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>set_defaults</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class AggColl(BaseModel):\n    \"\"\"\n    A data class that represents a single aggregation operation for a group by operation.\n\n    Attributes\n    ----------\n    old_name : str\n        The name of the column in the original DataFrame to be aggregated.\n\n    agg : str\n        The aggregation function to use. This can be a string representing a built-in function or a custom function.\n\n    new_name : Optional[str]\n        The name of the resulting aggregated column in the output DataFrame. If not provided, it will default to the\n        old_name appended with the aggregation function.\n\n    output_type : Optional[str]\n        The type of the output values of the aggregation. If not provided, it is inferred from the aggregation function\n        using the `get_func_type_mapping` function.\n\n    Example\n    --------\n    agg_col = AggColl(\n        old_name='col1',\n        agg='sum',\n        new_name='sum_col1',\n        output_type='float'\n    )\n    \"\"\"\n\n    old_name: str\n    agg: str\n    new_name: str | None = None\n    output_type: str | None = None\n\n    def __init__(self, old_name: str, agg: str, new_name: str | None = None, output_type: str | None = None):\n        data = {\"old_name\": old_name, \"agg\": agg}\n        if new_name is not None:\n            data[\"new_name\"] = new_name\n        if output_type is not None:\n            data[\"output_type\"] = output_type\n\n        super().__init__(**data)\n\n    @model_validator(mode=\"after\")\n    def set_defaults(self):\n        \"\"\"Set default new_name and output_type based on agg function.\"\"\"\n        # Set new_name\n        if self.new_name is None:\n            if self.agg != \"groupby\":\n                self.new_name = self.old_name + \"_\" + self.agg\n            else:\n                self.new_name = self.old_name\n\n        # Set output_type\n        if self.output_type is None:\n            self.output_type = get_func_type_mapping(self.agg)\n\n        # Ensure old_name is a string\n        self.old_name = str(self.old_name)\n\n        return self\n\n    @property\n    def agg_func(self):\n        \"\"\"Returns the corresponding Polars aggregation function from the `agg` string.\"\"\"\n        if self.agg == \"groupby\":\n            return self.agg\n        elif self.agg == \"concat\":\n            return string_concat\n        else:\n            return getattr(pl, self.agg) if isinstance(self.agg, str) else self.agg\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.AggColl.agg_func","title":"<code>agg_func</code>  <code>property</code>","text":"<p>Returns the corresponding Polars aggregation function from the <code>agg</code> string.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.AggColl.set_defaults","title":"<code>set_defaults()</code>  <code>pydantic-validator</code>","text":"<p>Set default new_name and output_type based on agg function.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@model_validator(mode=\"after\")\ndef set_defaults(self):\n    \"\"\"Set default new_name and output_type based on agg function.\"\"\"\n    # Set new_name\n    if self.new_name is None:\n        if self.agg != \"groupby\":\n            self.new_name = self.old_name + \"_\" + self.agg\n        else:\n            self.new_name = self.old_name\n\n    # Set output_type\n    if self.output_type is None:\n        self.output_type = get_func_type_mapping(self.agg)\n\n    # Ensure old_name is a string\n    self.old_name = str(self.old_name)\n\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.BasicFilter","title":"<code>BasicFilter</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines a simple, single-condition filter (e.g., 'column' 'equals' 'value').</p> <p>Attributes:</p> Name Type Description <code>field</code> <code>str</code> <p>The column name to filter on.</p> <code>operator</code> <code>FilterOperator | str</code> <p>The comparison operator (FilterOperator enum value or symbol).</p> <code>value</code> <code>str</code> <p>The value to compare against.</p> <code>value2</code> <code>str | None</code> <p>Second value for BETWEEN operator (optional).</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FilterOperator\": {\n      \"description\": \"Supported filter comparison operators.\",\n      \"enum\": [\n        \"equals\",\n        \"not_equals\",\n        \"greater_than\",\n        \"greater_than_or_equals\",\n        \"less_than\",\n        \"less_than_or_equals\",\n        \"contains\",\n        \"not_contains\",\n        \"starts_with\",\n        \"ends_with\",\n        \"is_null\",\n        \"is_not_null\",\n        \"in\",\n        \"not_in\",\n        \"between\"\n      ],\n      \"title\": \"FilterOperator\",\n      \"type\": \"string\"\n    }\n  },\n  \"description\": \"Defines a simple, single-condition filter (e.g., 'column' 'equals' 'value').\\n\\nAttributes:\\n    field: The column name to filter on.\\n    operator: The comparison operator (FilterOperator enum value or symbol).\\n    value: The value to compare against.\\n    value2: Second value for BETWEEN operator (optional).\",\n  \"properties\": {\n    \"field\": {\n      \"default\": \"\",\n      \"title\": \"Field\",\n      \"type\": \"string\"\n    },\n    \"operator\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/FilterOperator\"\n        },\n        {\n          \"type\": \"string\"\n        }\n      ],\n      \"default\": \"equals\",\n      \"title\": \"Operator\"\n    },\n    \"value\": {\n      \"default\": \"\",\n      \"title\": \"Value\",\n      \"type\": \"string\"\n    },\n    \"value2\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Value2\"\n    },\n    \"filter_type\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Filter Type\"\n    },\n    \"filter_value\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Filter Value\"\n    }\n  },\n  \"title\": \"BasicFilter\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>field</code>                 (<code>str</code>)             </li> <li> <code>operator</code>                 (<code>FilterOperator | str</code>)             </li> <li> <code>value</code>                 (<code>str</code>)             </li> <li> <code>value2</code>                 (<code>str | None</code>)             </li> <li> <code>filter_type</code>                 (<code>str | None</code>)             </li> <li> <code>filter_value</code>                 (<code>str | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>normalize_operator</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class BasicFilter(BaseModel):\n    \"\"\"Defines a simple, single-condition filter (e.g., 'column' 'equals' 'value').\n\n    Attributes:\n        field: The column name to filter on.\n        operator: The comparison operator (FilterOperator enum value or symbol).\n        value: The value to compare against.\n        value2: Second value for BETWEEN operator (optional).\n    \"\"\"\n\n    field: str = \"\"\n    operator: FilterOperator | str = FilterOperator.EQUALS\n    value: str = \"\"\n    value2: str | None = None  # For BETWEEN operator\n\n    # Keep old field names for backward compatibility\n    filter_type: str | None = None\n    filter_value: str | None = None\n\n    def __init__(\n        self,\n        field: str = None,\n        operator: FilterOperator | str = None,\n        value: str = None,\n        value2: str = None,\n        # Backward compatibility parameters\n        filter_type: str = None,\n        filter_value: str = None,\n        **data,\n    ):\n        # Handle backward compatibility\n        if filter_type is not None and operator is None:\n            data[\"operator\"] = filter_type\n        elif operator is not None:\n            data[\"operator\"] = operator\n\n        if filter_value is not None and value is None:\n            data[\"value\"] = filter_value\n        elif value is not None:\n            data[\"value\"] = value\n\n        if field is not None:\n            data[\"field\"] = field\n        if value2 is not None:\n            data[\"value2\"] = value2\n\n        super().__init__(**data)\n\n    @model_validator(mode=\"after\")\n    def normalize_operator(self):\n        \"\"\"Normalize the operator to FilterOperator enum.\"\"\"\n        if isinstance(self.operator, str):\n            try:\n                self.operator = FilterOperator.from_symbol(self.operator)\n            except ValueError:\n                # Keep as string if conversion fails (for backward compat)\n                pass\n        return self\n\n    def get_operator(self) -&gt; FilterOperator:\n        \"\"\"Get the operator as FilterOperator enum.\"\"\"\n        if isinstance(self.operator, FilterOperator):\n            return self.operator\n        return FilterOperator.from_symbol(self.operator)\n\n    def to_yaml_dict(self) -&gt; BasicFilterYaml:\n        \"\"\"Serialize for YAML output.\"\"\"\n        result: BasicFilterYaml = {\n            \"field\": self.field,\n            \"operator\": self.operator.value if isinstance(self.operator, FilterOperator) else self.operator,\n            \"value\": self.value,\n        }\n        if self.value2:\n            result[\"value2\"] = self.value2\n        return result\n\n    @classmethod\n    def from_yaml_dict(cls, data: dict) -&gt; \"BasicFilter\":\n        \"\"\"Load from YAML format.\"\"\"\n        return cls(\n            field=data.get(\"field\", \"\"),\n            operator=data.get(\"operator\", FilterOperator.EQUALS),\n            value=data.get(\"value\", \"\"),\n            value2=data.get(\"value2\"),\n        )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.BasicFilter.from_yaml_dict","title":"<code>from_yaml_dict(data)</code>  <code>classmethod</code>","text":"<p>Load from YAML format.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@classmethod\ndef from_yaml_dict(cls, data: dict) -&gt; \"BasicFilter\":\n    \"\"\"Load from YAML format.\"\"\"\n    return cls(\n        field=data.get(\"field\", \"\"),\n        operator=data.get(\"operator\", FilterOperator.EQUALS),\n        value=data.get(\"value\", \"\"),\n        value2=data.get(\"value2\"),\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.BasicFilter.get_operator","title":"<code>get_operator()</code>","text":"<p>Get the operator as FilterOperator enum.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_operator(self) -&gt; FilterOperator:\n    \"\"\"Get the operator as FilterOperator enum.\"\"\"\n    if isinstance(self.operator, FilterOperator):\n        return self.operator\n    return FilterOperator.from_symbol(self.operator)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.BasicFilter.normalize_operator","title":"<code>normalize_operator()</code>  <code>pydantic-validator</code>","text":"<p>Normalize the operator to FilterOperator enum.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@model_validator(mode=\"after\")\ndef normalize_operator(self):\n    \"\"\"Normalize the operator to FilterOperator enum.\"\"\"\n    if isinstance(self.operator, str):\n        try:\n            self.operator = FilterOperator.from_symbol(self.operator)\n        except ValueError:\n            # Keep as string if conversion fails (for backward compat)\n            pass\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.BasicFilter.to_yaml_dict","title":"<code>to_yaml_dict()</code>","text":"<p>Serialize for YAML output.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def to_yaml_dict(self) -&gt; BasicFilterYaml:\n    \"\"\"Serialize for YAML output.\"\"\"\n    result: BasicFilterYaml = {\n        \"field\": self.field,\n        \"operator\": self.operator.value if isinstance(self.operator, FilterOperator) else self.operator,\n        \"value\": self.value,\n    }\n    if self.value2:\n        result[\"value2\"] = self.value2\n    return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInput","title":"<code>CrossJoinInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data model for cross join operations.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"JoinInputs\": {\n      \"description\": \"Data model for join-specific select inputs (extends SelectInputs).\",\n      \"properties\": {\n        \"renames\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/SelectInput\"\n          },\n          \"title\": \"Renames\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"JoinInputs\",\n      \"type\": \"object\"\n    },\n    \"SelectInput\": {\n      \"description\": \"Defines how a single column should be selected, renamed, or type-cast.\\n\\nThis is a core building block for any operation that involves column manipulation.\\nIt holds all the configuration for a single field in a selection operation.\",\n      \"properties\": {\n        \"old_name\": {\n          \"title\": \"Old Name\",\n          \"type\": \"string\"\n        },\n        \"original_position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Original Position\"\n        },\n        \"new_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"New Name\"\n        },\n        \"data_type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Data Type\"\n        },\n        \"data_type_change\": {\n          \"default\": false,\n          \"title\": \"Data Type Change\",\n          \"type\": \"boolean\"\n        },\n        \"join_key\": {\n          \"default\": false,\n          \"title\": \"Join Key\",\n          \"type\": \"boolean\"\n        },\n        \"is_altered\": {\n          \"default\": false,\n          \"title\": \"Is Altered\",\n          \"type\": \"boolean\"\n        },\n        \"position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Position\"\n        },\n        \"is_available\": {\n          \"default\": true,\n          \"title\": \"Is Available\",\n          \"type\": \"boolean\"\n        },\n        \"keep\": {\n          \"default\": true,\n          \"title\": \"Keep\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"old_name\"\n      ],\n      \"title\": \"SelectInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Data model for cross join operations.\",\n  \"properties\": {\n    \"left_select\": {\n      \"$ref\": \"#/$defs/JoinInputs\"\n    },\n    \"right_select\": {\n      \"$ref\": \"#/$defs/JoinInputs\"\n    }\n  },\n  \"required\": [\n    \"left_select\",\n    \"right_select\"\n  ],\n  \"title\": \"CrossJoinInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>left_select</code>                 (<code>JoinInputs</code>)             </li> <li> <code>right_select</code>                 (<code>JoinInputs</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>parse_inputs</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class CrossJoinInput(BaseModel):\n    \"\"\"Data model for cross join operations.\"\"\"\n\n    left_select: JoinInputs\n    right_select: JoinInputs\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def parse_inputs(cls, data: Any) -&gt; Any:\n        \"\"\"Parse flexible input formats before validation.\"\"\"\n        if isinstance(data, dict):\n            # Parse join_mapping\n            if \"join_mapping\" in data:\n                data[\"join_mapping\"] = cls._parse_join_mapping(data[\"join_mapping\"])\n\n            # Parse left_select\n            if \"left_select\" in data:\n                data[\"left_select\"] = cls._parse_select(data[\"left_select\"])\n\n            # Parse right_select\n            if \"right_select\" in data:\n                data[\"right_select\"] = cls._parse_select(data[\"right_select\"])\n\n        return data\n\n    @staticmethod\n    def _parse_join_mapping(join_mapping: Any) -&gt; list[JoinMap]:\n        \"\"\"Parse various join_mapping formats.\"\"\"\n        # Already a list of JoinMaps\n        if isinstance(join_mapping, list):\n            result = []\n            for jm in join_mapping:\n                if isinstance(jm, JoinMap):\n                    result.append(jm)\n                elif isinstance(jm, dict):\n                    result.append(JoinMap(**jm))\n                elif isinstance(jm, (tuple, list)) and len(jm) == 2:\n                    result.append(JoinMap(left_col=jm[0], right_col=jm[1]))\n                elif isinstance(jm, str):\n                    result.append(JoinMap(left_col=jm, right_col=jm))\n                else:\n                    raise ValueError(f\"Invalid join mapping item: {jm}\")\n            return result\n\n        # Single JoinMap\n        if isinstance(join_mapping, JoinMap):\n            return [join_mapping]\n\n        # String: same column on both sides\n        if isinstance(join_mapping, str):\n            return [JoinMap(left_col=join_mapping, right_col=join_mapping)]\n\n        # Tuple: (left, right)\n        if isinstance(join_mapping, tuple) and len(join_mapping) == 2:\n            return [JoinMap(left_col=join_mapping[0], right_col=join_mapping[1])]\n\n        raise ValueError(f\"Invalid join_mapping format: {type(join_mapping)}\")\n\n    @staticmethod\n    def _parse_select(select: Any) -&gt; JoinInputs:\n        \"\"\"Parse various select input formats.\"\"\"\n        # Already JoinInputs\n        if isinstance(select, JoinInputs):\n            return select\n\n        # List of SelectInput objects\n        if isinstance(select, list):\n            if all(isinstance(s, SelectInput) for s in select):\n                return JoinInputs(renames=select)\n            elif all(isinstance(s, str) for s in select):\n                return JoinInputs(renames=[SelectInput(old_name=s) for s in select])\n            elif all(isinstance(s, dict) for s in select):\n                return JoinInputs(renames=[SelectInput(**s) for s in select])\n\n        # Dict with 'select' (new YAML) or 'renames' (internal) key\n        if isinstance(select, dict):\n            if \"select\" in select:\n                return JoinInputs(renames=[SelectInput.from_yaml_dict(s) for s in select[\"select\"]])\n            if \"renames\" in select:\n                return JoinInputs(**select)\n\n        raise ValueError(f\"Invalid select format: {type(select)}\")\n\n    def __init__(\n        self,\n        left_select: JoinInputs | list[SelectInput] | list[str] = None,\n        right_select: JoinInputs | list[SelectInput] | list[str] = None,\n        **data,\n    ):\n        \"\"\"Custom init for backward compatibility with positional arguments.\"\"\"\n        if left_select is not None:\n            data[\"left_select\"] = left_select\n        if right_select is not None:\n            data[\"right_select\"] = right_select\n        super().__init__(**data)\n\n    def to_yaml_dict(self) -&gt; CrossJoinInputYaml:\n        \"\"\"Serialize for YAML output.\"\"\"\n        return {\n            \"left_select\": self.left_select.to_yaml_dict(),\n            \"right_select\": self.right_select.to_yaml_dict(),\n        }\n\n    def add_new_select_column(self, select_input: SelectInput, side: str) -&gt; None:\n        \"\"\"Adds a new column to the selection for either the left or right side.\"\"\"\n        target_input = self.right_select if side == \"right\" else self.left_select\n        if select_input.new_name is None:\n            select_input.new_name = select_input.old_name\n        target_input.renames.append(select_input)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInput.__init__","title":"<code>__init__(left_select=None, right_select=None, **data)</code>","text":"<p>Custom init for backward compatibility with positional arguments.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def __init__(\n    self,\n    left_select: JoinInputs | list[SelectInput] | list[str] = None,\n    right_select: JoinInputs | list[SelectInput] | list[str] = None,\n    **data,\n):\n    \"\"\"Custom init for backward compatibility with positional arguments.\"\"\"\n    if left_select is not None:\n        data[\"left_select\"] = left_select\n    if right_select is not None:\n        data[\"right_select\"] = right_select\n    super().__init__(**data)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInput.add_new_select_column","title":"<code>add_new_select_column(select_input, side)</code>","text":"<p>Adds a new column to the selection for either the left or right side.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def add_new_select_column(self, select_input: SelectInput, side: str) -&gt; None:\n    \"\"\"Adds a new column to the selection for either the left or right side.\"\"\"\n    target_input = self.right_select if side == \"right\" else self.left_select\n    if select_input.new_name is None:\n        select_input.new_name = select_input.old_name\n    target_input.renames.append(select_input)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInput.parse_inputs","title":"<code>parse_inputs(data)</code>  <code>pydantic-validator</code>","text":"<p>Parse flexible input formats before validation.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef parse_inputs(cls, data: Any) -&gt; Any:\n    \"\"\"Parse flexible input formats before validation.\"\"\"\n    if isinstance(data, dict):\n        # Parse join_mapping\n        if \"join_mapping\" in data:\n            data[\"join_mapping\"] = cls._parse_join_mapping(data[\"join_mapping\"])\n\n        # Parse left_select\n        if \"left_select\" in data:\n            data[\"left_select\"] = cls._parse_select(data[\"left_select\"])\n\n        # Parse right_select\n        if \"right_select\" in data:\n            data[\"right_select\"] = cls._parse_select(data[\"right_select\"])\n\n    return data\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInput.to_yaml_dict","title":"<code>to_yaml_dict()</code>","text":"<p>Serialize for YAML output.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def to_yaml_dict(self) -&gt; CrossJoinInputYaml:\n    \"\"\"Serialize for YAML output.\"\"\"\n    return {\n        \"left_select\": self.left_select.to_yaml_dict(),\n        \"right_select\": self.right_select.to_yaml_dict(),\n    }\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInputManager","title":"<code>CrossJoinInputManager</code>","text":"<p>               Bases: <code>JoinSelectManagerMixin</code></p> <p>Manager for cross join operations.</p> <p>Methods:</p> Name Description <code>auto_rename</code> <p>Automatically renames columns on the right side to prevent naming conflicts.</p> <code>create</code> <p>Factory method to create CrossJoinInput from various input formats.</p> <code>get_overlapping_records</code> <p>Finds column names that would conflict after the join.</p> <code>to_cross_join_input</code> <p>Creates a new CrossJoinInput instance based on the current manager settings.</p> <p>Attributes:</p> Name Type Description <code>left_select</code> <code>JoinInputsManager</code> <p>Backward compatibility: Access left_manager as left_select.</p> <code>overlapping_records</code> <code>set[str]</code> <p>Backward compatibility: Returns overlapping column names.</p> <code>right_select</code> <code>JoinInputsManager</code> <p>Backward compatibility: Access right_manager as right_select.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class CrossJoinInputManager(JoinSelectManagerMixin):\n    \"\"\"Manager for cross join operations.\"\"\"\n\n    def __init__(self, cross_join_input: CrossJoinInput):\n        self.input = deepcopy(cross_join_input)\n        self.left_manager = JoinInputsManager(self.input.left_select)\n        self.right_manager = JoinInputsManager(self.input.right_select)\n\n    @classmethod\n    def create(\n        cls, left_select: list[SelectInput] | list[str], right_select: list[SelectInput] | list[str]\n    ) -&gt; \"CrossJoinInputManager\":\n        \"\"\"Factory method to create CrossJoinInput from various input formats.\"\"\"\n        left_inputs = cls.parse_select(left_select)\n        right_inputs = cls.parse_select(right_select)\n\n        cross_join = CrossJoinInput(left_select=left_inputs, right_select=right_inputs)\n        return cls(cross_join)\n\n    def get_overlapping_records(self) -&gt; set[str]:\n        \"\"\"Finds column names that would conflict after the join.\"\"\"\n        return self.get_overlapping_columns()\n\n    def auto_rename(self, rename_mode: Literal[\"suffix\", \"prefix\"] = \"prefix\") -&gt; None:\n        \"\"\"Automatically renames columns on the right side to prevent naming conflicts.\"\"\"\n        overlapping_records = self.get_overlapping_records()\n\n        while len(overlapping_records) &gt; 0:\n            for right_col in self.input.right_select.renames:\n                if right_col.new_name in overlapping_records:\n                    if rename_mode == \"prefix\":\n                        right_col.new_name = \"right_\" + right_col.new_name\n                    elif rename_mode == \"suffix\":\n                        right_col.new_name = right_col.new_name + \"_right\"\n                    else:\n                        raise ValueError(f\"Unknown rename_mode: {rename_mode}\")\n            overlapping_records = self.get_overlapping_records()\n\n    # === Backward Compatibility Properties ===\n\n    @property\n    def left_select(self) -&gt; JoinInputsManager:\n        \"\"\"Backward compatibility: Access left_manager as left_select.\"\"\"\n        return self.left_manager\n\n    @property\n    def right_select(self) -&gt; JoinInputsManager:\n        \"\"\"Backward compatibility: Access right_manager as right_select.\"\"\"\n        return self.right_manager\n\n    @property\n    def overlapping_records(self) -&gt; set[str]:\n        \"\"\"Backward compatibility: Returns overlapping column names.\"\"\"\n        return self.get_overlapping_records()\n\n    def to_cross_join_input(self) -&gt; CrossJoinInput:\n        \"\"\"Creates a new CrossJoinInput instance based on the current manager settings.\n\n        This is useful when you've modified the manager (e.g., via auto_rename) and\n        want to get a fresh CrossJoinInput with all the current settings applied.\n\n        Returns:\n            A new CrossJoinInput instance with current settings\n        \"\"\"\n        return CrossJoinInput(\n            left_select=JoinInputs(renames=self.input.left_select.renames.copy()),\n            right_select=JoinInputs(renames=self.input.right_select.renames.copy()),\n        )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInputManager.left_select","title":"<code>left_select</code>  <code>property</code>","text":"<p>Backward compatibility: Access left_manager as left_select.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInputManager.overlapping_records","title":"<code>overlapping_records</code>  <code>property</code>","text":"<p>Backward compatibility: Returns overlapping column names.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInputManager.right_select","title":"<code>right_select</code>  <code>property</code>","text":"<p>Backward compatibility: Access right_manager as right_select.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInputManager.auto_rename","title":"<code>auto_rename(rename_mode='prefix')</code>","text":"<p>Automatically renames columns on the right side to prevent naming conflicts.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def auto_rename(self, rename_mode: Literal[\"suffix\", \"prefix\"] = \"prefix\") -&gt; None:\n    \"\"\"Automatically renames columns on the right side to prevent naming conflicts.\"\"\"\n    overlapping_records = self.get_overlapping_records()\n\n    while len(overlapping_records) &gt; 0:\n        for right_col in self.input.right_select.renames:\n            if right_col.new_name in overlapping_records:\n                if rename_mode == \"prefix\":\n                    right_col.new_name = \"right_\" + right_col.new_name\n                elif rename_mode == \"suffix\":\n                    right_col.new_name = right_col.new_name + \"_right\"\n                else:\n                    raise ValueError(f\"Unknown rename_mode: {rename_mode}\")\n        overlapping_records = self.get_overlapping_records()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInputManager.create","title":"<code>create(left_select, right_select)</code>  <code>classmethod</code>","text":"<p>Factory method to create CrossJoinInput from various input formats.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@classmethod\ndef create(\n    cls, left_select: list[SelectInput] | list[str], right_select: list[SelectInput] | list[str]\n) -&gt; \"CrossJoinInputManager\":\n    \"\"\"Factory method to create CrossJoinInput from various input formats.\"\"\"\n    left_inputs = cls.parse_select(left_select)\n    right_inputs = cls.parse_select(right_select)\n\n    cross_join = CrossJoinInput(left_select=left_inputs, right_select=right_inputs)\n    return cls(cross_join)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInputManager.get_overlapping_records","title":"<code>get_overlapping_records()</code>","text":"<p>Finds column names that would conflict after the join.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_overlapping_records(self) -&gt; set[str]:\n    \"\"\"Finds column names that would conflict after the join.\"\"\"\n    return self.get_overlapping_columns()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInputManager.to_cross_join_input","title":"<code>to_cross_join_input()</code>","text":"<p>Creates a new CrossJoinInput instance based on the current manager settings.</p> <p>This is useful when you've modified the manager (e.g., via auto_rename) and want to get a fresh CrossJoinInput with all the current settings applied.</p> <p>Returns:</p> Type Description <code>CrossJoinInput</code> <p>A new CrossJoinInput instance with current settings</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def to_cross_join_input(self) -&gt; CrossJoinInput:\n    \"\"\"Creates a new CrossJoinInput instance based on the current manager settings.\n\n    This is useful when you've modified the manager (e.g., via auto_rename) and\n    want to get a fresh CrossJoinInput with all the current settings applied.\n\n    Returns:\n        A new CrossJoinInput instance with current settings\n    \"\"\"\n    return CrossJoinInput(\n        left_select=JoinInputs(renames=self.input.left_select.renames.copy()),\n        right_select=JoinInputs(renames=self.input.right_select.renames.copy()),\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FieldInput","title":"<code>FieldInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single field with its name and data type, typically for defining an output column.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"DataType\": {\n      \"description\": \"Specific data types for fine-grained control.\",\n      \"enum\": [\n        \"Int8\",\n        \"Int16\",\n        \"Int32\",\n        \"Int64\",\n        \"UInt8\",\n        \"UInt16\",\n        \"UInt32\",\n        \"UInt64\",\n        \"Float32\",\n        \"Float64\",\n        \"Decimal\",\n        \"String\",\n        \"Categorical\",\n        \"Date\",\n        \"Datetime\",\n        \"Time\",\n        \"Duration\",\n        \"Boolean\",\n        \"Binary\",\n        \"List\",\n        \"Struct\",\n        \"Array\"\n      ],\n      \"title\": \"DataType\",\n      \"type\": \"string\"\n    }\n  },\n  \"description\": \"Represents a single field with its name and data type, typically for defining an output column.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"data_type\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/DataType\"\n        },\n        {\n          \"const\": \"Auto\",\n          \"enum\": [\n            \"Auto\"\n          ],\n          \"type\": \"string\"\n        },\n        {\n          \"enum\": [\n            \"Int8\",\n            \"Int16\",\n            \"Int32\",\n            \"Int64\",\n            \"UInt8\",\n            \"UInt16\",\n            \"UInt32\",\n            \"UInt64\",\n            \"Float32\",\n            \"Float64\",\n            \"Decimal\",\n            \"String\",\n            \"Date\",\n            \"Datetime\",\n            \"Time\",\n            \"Duration\",\n            \"Boolean\",\n            \"Binary\",\n            \"List\",\n            \"Struct\",\n            \"Array\",\n            \"Integer\",\n            \"Double\",\n            \"Utf8\"\n          ],\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"Auto\",\n      \"title\": \"Data Type\"\n    }\n  },\n  \"required\": [\n    \"name\"\n  ],\n  \"title\": \"FieldInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>data_type</code>                 (<code>DataType | Literal['Auto'] | DataTypeStr | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class FieldInput(BaseModel):\n    \"\"\"Represents a single field with its name and data type, typically for defining an output column.\"\"\"\n\n    name: str\n    data_type: DataType | Literal[\"Auto\"] | DataTypeStr | None = AUTO_DATA_TYPE\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FilterInput","title":"<code>FilterInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the settings for a filter operation, supporting basic or advanced (expression-based) modes.</p> <p>Attributes:</p> Name Type Description <code>mode</code> <code>FilterModeLiteral</code> <p>The filter mode - \"basic\" or \"advanced\".</p> <code>basic_filter</code> <code>BasicFilter | None</code> <p>The basic filter configuration (used when mode=\"basic\").</p> <code>advanced_filter</code> <code>str</code> <p>The advanced filter expression string (used when mode=\"advanced\").</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"BasicFilter\": {\n      \"description\": \"Defines a simple, single-condition filter (e.g., 'column' 'equals' 'value').\\n\\nAttributes:\\n    field: The column name to filter on.\\n    operator: The comparison operator (FilterOperator enum value or symbol).\\n    value: The value to compare against.\\n    value2: Second value for BETWEEN operator (optional).\",\n      \"properties\": {\n        \"field\": {\n          \"default\": \"\",\n          \"title\": \"Field\",\n          \"type\": \"string\"\n        },\n        \"operator\": {\n          \"anyOf\": [\n            {\n              \"$ref\": \"#/$defs/FilterOperator\"\n            },\n            {\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"equals\",\n          \"title\": \"Operator\"\n        },\n        \"value\": {\n          \"default\": \"\",\n          \"title\": \"Value\",\n          \"type\": \"string\"\n        },\n        \"value2\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Value2\"\n        },\n        \"filter_type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Filter Type\"\n        },\n        \"filter_value\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Filter Value\"\n        }\n      },\n      \"title\": \"BasicFilter\",\n      \"type\": \"object\"\n    },\n    \"FilterOperator\": {\n      \"description\": \"Supported filter comparison operators.\",\n      \"enum\": [\n        \"equals\",\n        \"not_equals\",\n        \"greater_than\",\n        \"greater_than_or_equals\",\n        \"less_than\",\n        \"less_than_or_equals\",\n        \"contains\",\n        \"not_contains\",\n        \"starts_with\",\n        \"ends_with\",\n        \"is_null\",\n        \"is_not_null\",\n        \"in\",\n        \"not_in\",\n        \"between\"\n      ],\n      \"title\": \"FilterOperator\",\n      \"type\": \"string\"\n    }\n  },\n  \"description\": \"Defines the settings for a filter operation, supporting basic or advanced (expression-based) modes.\\n\\nAttributes:\\n    mode: The filter mode - \\\"basic\\\" or \\\"advanced\\\".\\n    basic_filter: The basic filter configuration (used when mode=\\\"basic\\\").\\n    advanced_filter: The advanced filter expression string (used when mode=\\\"advanced\\\").\",\n  \"properties\": {\n    \"mode\": {\n      \"default\": \"basic\",\n      \"enum\": [\n        \"basic\",\n        \"advanced\"\n      ],\n      \"title\": \"Mode\",\n      \"type\": \"string\"\n    },\n    \"basic_filter\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/BasicFilter\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"advanced_filter\": {\n      \"default\": \"\",\n      \"title\": \"Advanced Filter\",\n      \"type\": \"string\"\n    },\n    \"filter_type\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Filter Type\"\n    }\n  },\n  \"title\": \"FilterInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>mode</code>                 (<code>FilterModeLiteral</code>)             </li> <li> <code>basic_filter</code>                 (<code>BasicFilter | None</code>)             </li> <li> <code>advanced_filter</code>                 (<code>str</code>)             </li> <li> <code>filter_type</code>                 (<code>str | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>ensure_basic_filter</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class FilterInput(BaseModel):\n    \"\"\"Defines the settings for a filter operation, supporting basic or advanced (expression-based) modes.\n\n    Attributes:\n        mode: The filter mode - \"basic\" or \"advanced\".\n        basic_filter: The basic filter configuration (used when mode=\"basic\").\n        advanced_filter: The advanced filter expression string (used when mode=\"advanced\").\n    \"\"\"\n\n    mode: FilterModeLiteral = \"basic\"\n    basic_filter: BasicFilter | None = None\n    advanced_filter: str = \"\"\n\n    # Keep old field name for backward compatibility\n    filter_type: str | None = None\n\n    def __init__(\n        self,\n        mode: FilterModeLiteral = None,\n        basic_filter: BasicFilter = None,\n        advanced_filter: str = None,\n        # Backward compatibility\n        filter_type: str = None,\n        **data,\n    ):\n        # Handle backward compatibility: filter_type -&gt; mode\n        if filter_type is not None and mode is None:\n            data[\"mode\"] = filter_type\n        elif mode is not None:\n            data[\"mode\"] = mode\n\n        if advanced_filter is not None:\n            data[\"advanced_filter\"] = advanced_filter\n        if basic_filter is not None:\n            data[\"basic_filter\"] = basic_filter\n\n        super().__init__(**data)\n\n    @model_validator(mode=\"after\")\n    def ensure_basic_filter(self):\n        \"\"\"Ensure basic_filter exists when mode is basic.\"\"\"\n        if self.mode == \"basic\" and self.basic_filter is None:\n            self.basic_filter = BasicFilter()\n        return self\n\n    def is_advanced(self) -&gt; bool:\n        \"\"\"Check if filter is in advanced mode.\"\"\"\n        return self.mode == \"advanced\"\n\n    def to_yaml_dict(self) -&gt; FilterInputYaml:\n        \"\"\"Serialize for YAML output.\"\"\"\n        result: FilterInputYaml = {\"mode\": self.mode}\n        if self.mode == \"basic\" and self.basic_filter:\n            result[\"basic_filter\"] = self.basic_filter.to_yaml_dict()\n        elif self.mode == \"advanced\" and self.advanced_filter:\n            result[\"advanced_filter\"] = self.advanced_filter\n        return result\n\n    @classmethod\n    def from_yaml_dict(cls, data: dict) -&gt; \"FilterInput\":\n        \"\"\"Load from YAML format.\"\"\"\n        mode = data.get(\"mode\", \"basic\")\n        basic_filter = None\n        if \"basic_filter\" in data:\n            basic_filter = BasicFilter.from_yaml_dict(data[\"basic_filter\"])\n        return cls(\n            mode=mode,\n            basic_filter=basic_filter,\n            advanced_filter=data.get(\"advanced_filter\", \"\"),\n        )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FilterInput.ensure_basic_filter","title":"<code>ensure_basic_filter()</code>  <code>pydantic-validator</code>","text":"<p>Ensure basic_filter exists when mode is basic.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@model_validator(mode=\"after\")\ndef ensure_basic_filter(self):\n    \"\"\"Ensure basic_filter exists when mode is basic.\"\"\"\n    if self.mode == \"basic\" and self.basic_filter is None:\n        self.basic_filter = BasicFilter()\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FilterInput.from_yaml_dict","title":"<code>from_yaml_dict(data)</code>  <code>classmethod</code>","text":"<p>Load from YAML format.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@classmethod\ndef from_yaml_dict(cls, data: dict) -&gt; \"FilterInput\":\n    \"\"\"Load from YAML format.\"\"\"\n    mode = data.get(\"mode\", \"basic\")\n    basic_filter = None\n    if \"basic_filter\" in data:\n        basic_filter = BasicFilter.from_yaml_dict(data[\"basic_filter\"])\n    return cls(\n        mode=mode,\n        basic_filter=basic_filter,\n        advanced_filter=data.get(\"advanced_filter\", \"\"),\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FilterInput.is_advanced","title":"<code>is_advanced()</code>","text":"<p>Check if filter is in advanced mode.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def is_advanced(self) -&gt; bool:\n    \"\"\"Check if filter is in advanced mode.\"\"\"\n    return self.mode == \"advanced\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FilterInput.to_yaml_dict","title":"<code>to_yaml_dict()</code>","text":"<p>Serialize for YAML output.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def to_yaml_dict(self) -&gt; FilterInputYaml:\n    \"\"\"Serialize for YAML output.\"\"\"\n    result: FilterInputYaml = {\"mode\": self.mode}\n    if self.mode == \"basic\" and self.basic_filter:\n        result[\"basic_filter\"] = self.basic_filter.to_yaml_dict()\n    elif self.mode == \"advanced\" and self.advanced_filter:\n        result[\"advanced_filter\"] = self.advanced_filter\n    return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FilterOperator","title":"<code>FilterOperator</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported filter comparison operators.</p> <p>Methods:</p> Name Description <code>from_symbol</code> <p>Convert UI symbol to FilterOperator enum.</p> <code>to_symbol</code> <p>Convert FilterOperator to UI-friendly symbol.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class FilterOperator(str, Enum):\n    \"\"\"Supported filter comparison operators.\"\"\"\n\n    EQUALS = \"equals\"\n    NOT_EQUALS = \"not_equals\"\n    GREATER_THAN = \"greater_than\"\n    GREATER_THAN_OR_EQUALS = \"greater_than_or_equals\"\n    LESS_THAN = \"less_than\"\n    LESS_THAN_OR_EQUALS = \"less_than_or_equals\"\n    CONTAINS = \"contains\"\n    NOT_CONTAINS = \"not_contains\"\n    STARTS_WITH = \"starts_with\"\n    ENDS_WITH = \"ends_with\"\n    IS_NULL = \"is_null\"\n    IS_NOT_NULL = \"is_not_null\"\n    IN = \"in\"\n    NOT_IN = \"not_in\"\n    BETWEEN = \"between\"\n\n    def __str__(self) -&gt; str:\n        return self.value\n\n    @classmethod\n    def from_symbol(cls, symbol: str) -&gt; \"FilterOperator\":\n        \"\"\"Convert UI symbol to FilterOperator enum.\"\"\"\n        symbol_mapping = {\n            \"=\": cls.EQUALS,\n            \"==\": cls.EQUALS,\n            \"!=\": cls.NOT_EQUALS,\n            \"&lt;&gt;\": cls.NOT_EQUALS,\n            \"&gt;\": cls.GREATER_THAN,\n            \"&gt;=\": cls.GREATER_THAN_OR_EQUALS,\n            \"&lt;\": cls.LESS_THAN,\n            \"&lt;=\": cls.LESS_THAN_OR_EQUALS,\n            \"contains\": cls.CONTAINS,\n            \"not_contains\": cls.NOT_CONTAINS,\n            \"starts_with\": cls.STARTS_WITH,\n            \"ends_with\": cls.ENDS_WITH,\n            \"is_null\": cls.IS_NULL,\n            \"is_not_null\": cls.IS_NOT_NULL,\n            \"in\": cls.IN,\n            \"not_in\": cls.NOT_IN,\n            \"between\": cls.BETWEEN,\n        }\n        if symbol in symbol_mapping:\n            return symbol_mapping[symbol]\n        # Try to match by value directly\n        try:\n            return cls(symbol)\n        except ValueError:\n            raise ValueError(f\"Unknown filter operator symbol: {symbol}\")\n\n    def to_symbol(self) -&gt; str:\n        \"\"\"Convert FilterOperator to UI-friendly symbol.\"\"\"\n        symbol_mapping = {\n            FilterOperator.EQUALS: \"=\",\n            FilterOperator.NOT_EQUALS: \"!=\",\n            FilterOperator.GREATER_THAN: \"&gt;\",\n            FilterOperator.GREATER_THAN_OR_EQUALS: \"&gt;=\",\n            FilterOperator.LESS_THAN: \"&lt;\",\n            FilterOperator.LESS_THAN_OR_EQUALS: \"&lt;=\",\n            FilterOperator.CONTAINS: \"contains\",\n            FilterOperator.NOT_CONTAINS: \"not_contains\",\n            FilterOperator.STARTS_WITH: \"starts_with\",\n            FilterOperator.ENDS_WITH: \"ends_with\",\n            FilterOperator.IS_NULL: \"is_null\",\n            FilterOperator.IS_NOT_NULL: \"is_not_null\",\n            FilterOperator.IN: \"in\",\n            FilterOperator.NOT_IN: \"not_in\",\n            FilterOperator.BETWEEN: \"between\",\n        }\n        return symbol_mapping.get(self, self.value)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FilterOperator.from_symbol","title":"<code>from_symbol(symbol)</code>  <code>classmethod</code>","text":"<p>Convert UI symbol to FilterOperator enum.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@classmethod\ndef from_symbol(cls, symbol: str) -&gt; \"FilterOperator\":\n    \"\"\"Convert UI symbol to FilterOperator enum.\"\"\"\n    symbol_mapping = {\n        \"=\": cls.EQUALS,\n        \"==\": cls.EQUALS,\n        \"!=\": cls.NOT_EQUALS,\n        \"&lt;&gt;\": cls.NOT_EQUALS,\n        \"&gt;\": cls.GREATER_THAN,\n        \"&gt;=\": cls.GREATER_THAN_OR_EQUALS,\n        \"&lt;\": cls.LESS_THAN,\n        \"&lt;=\": cls.LESS_THAN_OR_EQUALS,\n        \"contains\": cls.CONTAINS,\n        \"not_contains\": cls.NOT_CONTAINS,\n        \"starts_with\": cls.STARTS_WITH,\n        \"ends_with\": cls.ENDS_WITH,\n        \"is_null\": cls.IS_NULL,\n        \"is_not_null\": cls.IS_NOT_NULL,\n        \"in\": cls.IN,\n        \"not_in\": cls.NOT_IN,\n        \"between\": cls.BETWEEN,\n    }\n    if symbol in symbol_mapping:\n        return symbol_mapping[symbol]\n    # Try to match by value directly\n    try:\n        return cls(symbol)\n    except ValueError:\n        raise ValueError(f\"Unknown filter operator symbol: {symbol}\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FilterOperator.to_symbol","title":"<code>to_symbol()</code>","text":"<p>Convert FilterOperator to UI-friendly symbol.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def to_symbol(self) -&gt; str:\n    \"\"\"Convert FilterOperator to UI-friendly symbol.\"\"\"\n    symbol_mapping = {\n        FilterOperator.EQUALS: \"=\",\n        FilterOperator.NOT_EQUALS: \"!=\",\n        FilterOperator.GREATER_THAN: \"&gt;\",\n        FilterOperator.GREATER_THAN_OR_EQUALS: \"&gt;=\",\n        FilterOperator.LESS_THAN: \"&lt;\",\n        FilterOperator.LESS_THAN_OR_EQUALS: \"&lt;=\",\n        FilterOperator.CONTAINS: \"contains\",\n        FilterOperator.NOT_CONTAINS: \"not_contains\",\n        FilterOperator.STARTS_WITH: \"starts_with\",\n        FilterOperator.ENDS_WITH: \"ends_with\",\n        FilterOperator.IS_NULL: \"is_null\",\n        FilterOperator.IS_NOT_NULL: \"is_not_null\",\n        FilterOperator.IN: \"in\",\n        FilterOperator.NOT_IN: \"not_in\",\n        FilterOperator.BETWEEN: \"between\",\n    }\n    return symbol_mapping.get(self, self.value)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FullJoinKeyResponse","title":"<code>FullJoinKeyResponse</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Holds the join key rename responses for both sides of a join.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class FullJoinKeyResponse(NamedTuple):\n    \"\"\"Holds the join key rename responses for both sides of a join.\"\"\"\n\n    left: JoinKeyRenameResponse\n    right: JoinKeyRenameResponse\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FunctionInput","title":"<code>FunctionInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines a formula to be applied, including the output field information.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"DataType\": {\n      \"description\": \"Specific data types for fine-grained control.\",\n      \"enum\": [\n        \"Int8\",\n        \"Int16\",\n        \"Int32\",\n        \"Int64\",\n        \"UInt8\",\n        \"UInt16\",\n        \"UInt32\",\n        \"UInt64\",\n        \"Float32\",\n        \"Float64\",\n        \"Decimal\",\n        \"String\",\n        \"Categorical\",\n        \"Date\",\n        \"Datetime\",\n        \"Time\",\n        \"Duration\",\n        \"Boolean\",\n        \"Binary\",\n        \"List\",\n        \"Struct\",\n        \"Array\"\n      ],\n      \"title\": \"DataType\",\n      \"type\": \"string\"\n    },\n    \"FieldInput\": {\n      \"description\": \"Represents a single field with its name and data type, typically for defining an output column.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"anyOf\": [\n            {\n              \"$ref\": \"#/$defs/DataType\"\n            },\n            {\n              \"const\": \"Auto\",\n              \"enum\": [\n                \"Auto\"\n              ],\n              \"type\": \"string\"\n            },\n            {\n              \"enum\": [\n                \"Int8\",\n                \"Int16\",\n                \"Int32\",\n                \"Int64\",\n                \"UInt8\",\n                \"UInt16\",\n                \"UInt32\",\n                \"UInt64\",\n                \"Float32\",\n                \"Float64\",\n                \"Decimal\",\n                \"String\",\n                \"Date\",\n                \"Datetime\",\n                \"Time\",\n                \"Duration\",\n                \"Boolean\",\n                \"Binary\",\n                \"List\",\n                \"Struct\",\n                \"Array\",\n                \"Integer\",\n                \"Double\",\n                \"Utf8\"\n              ],\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": \"Auto\",\n          \"title\": \"Data Type\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"FieldInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Defines a formula to be applied, including the output field information.\",\n  \"properties\": {\n    \"field\": {\n      \"$ref\": \"#/$defs/FieldInput\"\n    },\n    \"function\": {\n      \"title\": \"Function\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"field\",\n    \"function\"\n  ],\n  \"title\": \"FunctionInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>field</code>                 (<code>FieldInput</code>)             </li> <li> <code>function</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class FunctionInput(BaseModel):\n    \"\"\"Defines a formula to be applied, including the output field information.\"\"\"\n\n    field: FieldInput\n    function: str\n\n    def __init__(self, field: FieldInput = None, function: str = None, **data):\n        if field is not None:\n            data[\"field\"] = field\n        if function is not None:\n            data[\"function\"] = function\n        super().__init__(**data)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInput","title":"<code>FuzzyMatchInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data model for fuzzy matching join operations.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FuzzyMapping\": {\n      \"properties\": {\n        \"left_col\": {\n          \"title\": \"Left Col\",\n          \"type\": \"string\"\n        },\n        \"right_col\": {\n          \"title\": \"Right Col\",\n          \"type\": \"string\"\n        },\n        \"threshold_score\": {\n          \"default\": 80.0,\n          \"title\": \"Threshold Score\",\n          \"type\": \"number\"\n        },\n        \"fuzzy_type\": {\n          \"default\": \"levenshtein\",\n          \"enum\": [\n            \"levenshtein\",\n            \"jaro\",\n            \"jaro_winkler\",\n            \"hamming\",\n            \"damerau_levenshtein\",\n            \"indel\"\n          ],\n          \"title\": \"Fuzzy Type\",\n          \"type\": \"string\"\n        },\n        \"perc_unique\": {\n          \"default\": 0.0,\n          \"title\": \"Perc Unique\",\n          \"type\": \"number\"\n        },\n        \"output_column_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Output Column Name\"\n        },\n        \"valid\": {\n          \"default\": true,\n          \"title\": \"Valid\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"left_col\",\n        \"right_col\"\n      ],\n      \"title\": \"FuzzyMapping\",\n      \"type\": \"object\"\n    },\n    \"JoinInputs\": {\n      \"description\": \"Data model for join-specific select inputs (extends SelectInputs).\",\n      \"properties\": {\n        \"renames\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/SelectInput\"\n          },\n          \"title\": \"Renames\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"JoinInputs\",\n      \"type\": \"object\"\n    },\n    \"SelectInput\": {\n      \"description\": \"Defines how a single column should be selected, renamed, or type-cast.\\n\\nThis is a core building block for any operation that involves column manipulation.\\nIt holds all the configuration for a single field in a selection operation.\",\n      \"properties\": {\n        \"old_name\": {\n          \"title\": \"Old Name\",\n          \"type\": \"string\"\n        },\n        \"original_position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Original Position\"\n        },\n        \"new_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"New Name\"\n        },\n        \"data_type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Data Type\"\n        },\n        \"data_type_change\": {\n          \"default\": false,\n          \"title\": \"Data Type Change\",\n          \"type\": \"boolean\"\n        },\n        \"join_key\": {\n          \"default\": false,\n          \"title\": \"Join Key\",\n          \"type\": \"boolean\"\n        },\n        \"is_altered\": {\n          \"default\": false,\n          \"title\": \"Is Altered\",\n          \"type\": \"boolean\"\n        },\n        \"position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Position\"\n        },\n        \"is_available\": {\n          \"default\": true,\n          \"title\": \"Is Available\",\n          \"type\": \"boolean\"\n        },\n        \"keep\": {\n          \"default\": true,\n          \"title\": \"Keep\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"old_name\"\n      ],\n      \"title\": \"SelectInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Data model for fuzzy matching join operations.\",\n  \"properties\": {\n    \"join_mapping\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/FuzzyMapping\"\n      },\n      \"title\": \"Join Mapping\",\n      \"type\": \"array\"\n    },\n    \"left_select\": {\n      \"$ref\": \"#/$defs/JoinInputs\"\n    },\n    \"right_select\": {\n      \"$ref\": \"#/$defs/JoinInputs\"\n    },\n    \"how\": {\n      \"default\": \"inner\",\n      \"enum\": [\n        \"inner\",\n        \"left\",\n        \"right\",\n        \"full\",\n        \"semi\",\n        \"anti\",\n        \"cross\",\n        \"outer\"\n      ],\n      \"title\": \"How\",\n      \"type\": \"string\"\n    },\n    \"aggregate_output\": {\n      \"default\": false,\n      \"title\": \"Aggregate Output\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"required\": [\n    \"join_mapping\",\n    \"left_select\",\n    \"right_select\"\n  ],\n  \"title\": \"FuzzyMatchInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>join_mapping</code>                 (<code>list[FuzzyMapping]</code>)             </li> <li> <code>left_select</code>                 (<code>JoinInputs</code>)             </li> <li> <code>right_select</code>                 (<code>JoinInputs</code>)             </li> <li> <code>how</code>                 (<code>JoinStrategy</code>)             </li> <li> <code>aggregate_output</code>                 (<code>bool</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>parse_inputs</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class FuzzyMatchInput(BaseModel):\n    \"\"\"Data model for fuzzy matching join operations.\"\"\"\n\n    join_mapping: list[FuzzyMapping]\n    left_select: JoinInputs\n    right_select: JoinInputs\n    how: JoinStrategy = \"inner\"\n    aggregate_output: bool = False\n\n    def __init__(\n        self,\n        left_select: JoinInputs | list[SelectInput] | list[str] = None,\n        right_select: JoinInputs | list[SelectInput] | list[str] = None,\n        **data,\n    ):\n        \"\"\"Custom init for backward compatibility with positional arguments.\"\"\"\n        if left_select is not None:\n            data[\"left_select\"] = left_select\n        if right_select is not None:\n            data[\"right_select\"] = right_select\n\n        super().__init__(**data)\n\n    def to_yaml_dict(self) -&gt; FuzzyMatchInputYaml:\n        \"\"\"Serialize for YAML output.\"\"\"\n        return {\n            \"join_mapping\": [asdict(jm) for jm in self.join_mapping],\n            \"left_select\": self.left_select.to_yaml_dict(),\n            \"right_select\": self.right_select.to_yaml_dict(),\n            \"how\": self.how,\n            \"aggregate_output\": self.aggregate_output,\n        }\n\n    def add_new_select_column(self, select_input: SelectInput, side: str) -&gt; None:\n        \"\"\"Adds a new column to the selection for either the left or right side.\"\"\"\n        target_input = self.right_select if side == \"right\" else self.left_select\n        if select_input.new_name is None:\n            select_input.new_name = select_input.old_name\n        target_input.renames.append(select_input)\n\n    @staticmethod\n    def _parse_select(select: Any) -&gt; JoinInputs:\n        \"\"\"Parse various select input formats.\"\"\"\n        # Already JoinInputs\n        if isinstance(select, JoinInputs):\n            return select\n\n        # List of SelectInput objects\n        if isinstance(select, list):\n            if all(isinstance(s, SelectInput) for s in select):\n                return JoinInputs(renames=select)\n            elif all(isinstance(s, str) for s in select):\n                return JoinInputs(renames=[SelectInput(old_name=s) for s in select])\n            elif all(isinstance(s, dict) for s in select):\n                return JoinInputs(renames=[SelectInput(**s) for s in select])\n\n        # Dict with 'select' (new YAML) or 'renames' (internal) key\n        if isinstance(select, dict):\n            if \"select\" in select:\n                return JoinInputs(renames=[SelectInput.from_yaml_dict(s) for s in select[\"select\"]])\n            if \"renames\" in select:\n                return JoinInputs(**select)\n\n        raise ValueError(f\"Invalid select format: {type(select)}\")\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def parse_inputs(cls, data: Any) -&gt; Any:\n        \"\"\"Parse flexible input formats before validation.\"\"\"\n        if isinstance(data, dict):\n            # Parse left_select\n            if \"left_select\" in data:\n                data[\"left_select\"] = cls._parse_select(data[\"left_select\"])\n\n            # Parse right_select\n            if \"right_select\" in data:\n                data[\"right_select\"] = cls._parse_select(data[\"right_select\"])\n\n        return data\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInput.__init__","title":"<code>__init__(left_select=None, right_select=None, **data)</code>","text":"<p>Custom init for backward compatibility with positional arguments.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def __init__(\n    self,\n    left_select: JoinInputs | list[SelectInput] | list[str] = None,\n    right_select: JoinInputs | list[SelectInput] | list[str] = None,\n    **data,\n):\n    \"\"\"Custom init for backward compatibility with positional arguments.\"\"\"\n    if left_select is not None:\n        data[\"left_select\"] = left_select\n    if right_select is not None:\n        data[\"right_select\"] = right_select\n\n    super().__init__(**data)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInput.add_new_select_column","title":"<code>add_new_select_column(select_input, side)</code>","text":"<p>Adds a new column to the selection for either the left or right side.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def add_new_select_column(self, select_input: SelectInput, side: str) -&gt; None:\n    \"\"\"Adds a new column to the selection for either the left or right side.\"\"\"\n    target_input = self.right_select if side == \"right\" else self.left_select\n    if select_input.new_name is None:\n        select_input.new_name = select_input.old_name\n    target_input.renames.append(select_input)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInput.parse_inputs","title":"<code>parse_inputs(data)</code>  <code>pydantic-validator</code>","text":"<p>Parse flexible input formats before validation.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef parse_inputs(cls, data: Any) -&gt; Any:\n    \"\"\"Parse flexible input formats before validation.\"\"\"\n    if isinstance(data, dict):\n        # Parse left_select\n        if \"left_select\" in data:\n            data[\"left_select\"] = cls._parse_select(data[\"left_select\"])\n\n        # Parse right_select\n        if \"right_select\" in data:\n            data[\"right_select\"] = cls._parse_select(data[\"right_select\"])\n\n    return data\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInput.to_yaml_dict","title":"<code>to_yaml_dict()</code>","text":"<p>Serialize for YAML output.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def to_yaml_dict(self) -&gt; FuzzyMatchInputYaml:\n    \"\"\"Serialize for YAML output.\"\"\"\n    return {\n        \"join_mapping\": [asdict(jm) for jm in self.join_mapping],\n        \"left_select\": self.left_select.to_yaml_dict(),\n        \"right_select\": self.right_select.to_yaml_dict(),\n        \"how\": self.how,\n        \"aggregate_output\": self.aggregate_output,\n    }\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInputManager","title":"<code>FuzzyMatchInputManager</code>","text":"<p>               Bases: <code>JoinInputManager</code></p> <p>Manager for fuzzy matching join operations.</p> <p>Methods:</p> Name Description <code>create</code> <p>Factory method to create FuzzyMatchInput from various input formats.</p> <code>get_fuzzy_maps</code> <p>Returns the final fuzzy mappings after applying all column renames.</p> <code>parse_fuzz_mapping</code> <p>Parses various input formats into a list of FuzzyMapping objects.</p> <code>to_fuzzy_match_input</code> <p>Creates a new FuzzyMatchInput instance based on the current manager settings.</p> <p>Attributes:</p> Name Type Description <code>aggregate_output</code> <code>bool</code> <p>Backward compatibility: Access aggregate_output setting.</p> <code>fuzzy_maps</code> <code>list[FuzzyMapping]</code> <p>Backward compatibility: Returns fuzzy mappings.</p> <code>join_mapping</code> <code>list[FuzzyMapping]</code> <p>Backward compatibility: Access fuzzy join mapping.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class FuzzyMatchInputManager(JoinInputManager):\n    \"\"\"Manager for fuzzy matching join operations.\"\"\"\n\n    def __init__(self, fuzzy_input: FuzzyMatchInput):\n        self.fuzzy_input = deepcopy(fuzzy_input)\n        super().__init__(\n            JoinInput(\n                join_mapping=[\n                    JoinMap(left_col=fm.left_col, right_col=fm.right_col) for fm in self.fuzzy_input.join_mapping\n                ],\n                left_select=self.fuzzy_input.left_select,\n                right_select=self.fuzzy_input.right_select,\n                how=self.fuzzy_input.how,\n            )\n        )\n\n    @classmethod\n    def create(\n        cls,\n        join_mapping: list[FuzzyMapping] | tuple[str, str] | str,\n        left_select: list[SelectInput] | list[str],\n        right_select: list[SelectInput] | list[str],\n        aggregate_output: bool = False,\n        how: JoinStrategy = \"inner\",\n    ) -&gt; \"FuzzyMatchInputManager\":\n        \"\"\"Factory method to create FuzzyMatchInput from various input formats.\"\"\"\n        parsed_mapping = cls.parse_fuzz_mapping(join_mapping)\n        left_inputs = cls.parse_select(left_select)\n        right_inputs = cls.parse_select(right_select)\n\n        fuzzy_input = FuzzyMatchInput(\n            join_mapping=parsed_mapping,\n            left_select=left_inputs,\n            right_select=right_inputs,\n            how=how,\n            aggregate_output=aggregate_output,\n        )\n\n        manager = cls(fuzzy_input)\n\n        right_old_names = {v.old_name for v in fuzzy_input.right_select.renames}\n        left_old_names = {v.old_name for v in fuzzy_input.left_select.renames}\n\n        for jm in parsed_mapping:\n            if jm.right_col not in right_old_names:\n                manager.right_manager.append(SelectInput(old_name=jm.right_col, keep=False, join_key=True))\n            if jm.left_col not in left_old_names:\n                manager.left_manager.append(SelectInput(old_name=jm.left_col, keep=False, join_key=True))\n\n        manager.set_join_keys()\n        return manager\n\n    @staticmethod\n    def parse_fuzz_mapping(\n        fuzz_mapping: list[FuzzyMapping] | tuple[str, str] | str | FuzzyMapping | list[dict],\n    ) -&gt; list[FuzzyMapping]:\n        \"\"\"Parses various input formats into a list of FuzzyMapping objects.\"\"\"\n        if isinstance(fuzz_mapping, (tuple, list)):\n            if len(fuzz_mapping) == 0:\n                raise ValueError(\"Fuzzy mapping cannot be empty\")\n\n            if all(isinstance(fm, dict) for fm in fuzz_mapping):\n                return [FuzzyMapping(**fm) for fm in fuzz_mapping]\n\n            if all(isinstance(fm, FuzzyMapping) for fm in fuzz_mapping):\n                return fuzz_mapping\n\n            if len(fuzz_mapping) &lt;= 2:\n                if len(fuzz_mapping) == 2:\n                    if isinstance(fuzz_mapping[0], str) and isinstance(fuzz_mapping[1], str):\n                        return [FuzzyMapping(left_col=fuzz_mapping[0], right_col=fuzz_mapping[1])]\n                elif len(fuzz_mapping) == 1 and isinstance(fuzz_mapping[0], str):\n                    return [FuzzyMapping(left_col=fuzz_mapping[0], right_col=fuzz_mapping[0])]\n\n        elif isinstance(fuzz_mapping, str):\n            return [FuzzyMapping(left_col=fuzz_mapping, right_col=fuzz_mapping)]\n\n        elif isinstance(fuzz_mapping, FuzzyMapping):\n            return [fuzz_mapping]\n\n        raise ValueError(f\"No valid fuzzy mapping as input: {type(fuzz_mapping)}\")\n\n    def get_fuzzy_maps(self) -&gt; list[FuzzyMapping]:\n        \"\"\"Returns the final fuzzy mappings after applying all column renames.\"\"\"\n        new_mappings = []\n        left_rename_table = self.left_manager.get_rename_table()\n        right_rename_table = self.right_manager.get_rename_table()\n\n        for org_fuzzy_map in self.fuzzy_input.join_mapping:\n            right_col = right_rename_table.get(org_fuzzy_map.right_col, org_fuzzy_map.right_col)\n            left_col = left_rename_table.get(org_fuzzy_map.left_col, org_fuzzy_map.left_col)\n\n            if right_col != org_fuzzy_map.right_col or left_col != org_fuzzy_map.left_col:\n                new_mapping = deepcopy(org_fuzzy_map)\n                new_mapping.left_col = left_col\n                new_mapping.right_col = right_col\n                new_mappings.append(new_mapping)\n            else:\n                new_mappings.append(org_fuzzy_map)\n\n        return new_mappings\n\n    # === Backward Compatibility Properties ===\n\n    @property\n    def fuzzy_maps(self) -&gt; list[FuzzyMapping]:\n        \"\"\"Backward compatibility: Returns fuzzy mappings.\"\"\"\n        return self.get_fuzzy_maps()\n\n    @property\n    def join_mapping(self) -&gt; list[FuzzyMapping]:\n        \"\"\"Backward compatibility: Access fuzzy join mapping.\"\"\"\n        return self.get_fuzzy_maps()\n\n    @property\n    def aggregate_output(self) -&gt; bool:\n        \"\"\"Backward compatibility: Access aggregate_output setting.\"\"\"\n        return self.fuzzy_input.aggregate_output\n\n    def to_fuzzy_match_input(self) -&gt; FuzzyMatchInput:\n        \"\"\"Creates a new FuzzyMatchInput instance based on the current manager settings.\n\n        This is useful when you've modified the manager (e.g., via auto_rename) and\n        want to get a fresh FuzzyMatchInput with all the current settings applied.\n\n        Returns:\n            A new FuzzyMatchInput instance with current settings\n        \"\"\"\n        return FuzzyMatchInput(\n            join_mapping=self.fuzzy_input.join_mapping,\n            left_select=JoinInputs(renames=self.input.left_select.renames.copy()),\n            right_select=JoinInputs(renames=self.input.right_select.renames.copy()),\n            how=self.fuzzy_input.how,\n            aggregate_output=self.fuzzy_input.aggregate_output,\n        )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInputManager.aggregate_output","title":"<code>aggregate_output</code>  <code>property</code>","text":"<p>Backward compatibility: Access aggregate_output setting.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInputManager.fuzzy_maps","title":"<code>fuzzy_maps</code>  <code>property</code>","text":"<p>Backward compatibility: Returns fuzzy mappings.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInputManager.join_mapping","title":"<code>join_mapping</code>  <code>property</code>","text":"<p>Backward compatibility: Access fuzzy join mapping.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInputManager.create","title":"<code>create(join_mapping, left_select, right_select, aggregate_output=False, how='inner')</code>  <code>classmethod</code>","text":"<p>Factory method to create FuzzyMatchInput from various input formats.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    join_mapping: list[FuzzyMapping] | tuple[str, str] | str,\n    left_select: list[SelectInput] | list[str],\n    right_select: list[SelectInput] | list[str],\n    aggregate_output: bool = False,\n    how: JoinStrategy = \"inner\",\n) -&gt; \"FuzzyMatchInputManager\":\n    \"\"\"Factory method to create FuzzyMatchInput from various input formats.\"\"\"\n    parsed_mapping = cls.parse_fuzz_mapping(join_mapping)\n    left_inputs = cls.parse_select(left_select)\n    right_inputs = cls.parse_select(right_select)\n\n    fuzzy_input = FuzzyMatchInput(\n        join_mapping=parsed_mapping,\n        left_select=left_inputs,\n        right_select=right_inputs,\n        how=how,\n        aggregate_output=aggregate_output,\n    )\n\n    manager = cls(fuzzy_input)\n\n    right_old_names = {v.old_name for v in fuzzy_input.right_select.renames}\n    left_old_names = {v.old_name for v in fuzzy_input.left_select.renames}\n\n    for jm in parsed_mapping:\n        if jm.right_col not in right_old_names:\n            manager.right_manager.append(SelectInput(old_name=jm.right_col, keep=False, join_key=True))\n        if jm.left_col not in left_old_names:\n            manager.left_manager.append(SelectInput(old_name=jm.left_col, keep=False, join_key=True))\n\n    manager.set_join_keys()\n    return manager\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInputManager.get_fuzzy_maps","title":"<code>get_fuzzy_maps()</code>","text":"<p>Returns the final fuzzy mappings after applying all column renames.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_fuzzy_maps(self) -&gt; list[FuzzyMapping]:\n    \"\"\"Returns the final fuzzy mappings after applying all column renames.\"\"\"\n    new_mappings = []\n    left_rename_table = self.left_manager.get_rename_table()\n    right_rename_table = self.right_manager.get_rename_table()\n\n    for org_fuzzy_map in self.fuzzy_input.join_mapping:\n        right_col = right_rename_table.get(org_fuzzy_map.right_col, org_fuzzy_map.right_col)\n        left_col = left_rename_table.get(org_fuzzy_map.left_col, org_fuzzy_map.left_col)\n\n        if right_col != org_fuzzy_map.right_col or left_col != org_fuzzy_map.left_col:\n            new_mapping = deepcopy(org_fuzzy_map)\n            new_mapping.left_col = left_col\n            new_mapping.right_col = right_col\n            new_mappings.append(new_mapping)\n        else:\n            new_mappings.append(org_fuzzy_map)\n\n    return new_mappings\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInputManager.parse_fuzz_mapping","title":"<code>parse_fuzz_mapping(fuzz_mapping)</code>  <code>staticmethod</code>","text":"<p>Parses various input formats into a list of FuzzyMapping objects.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@staticmethod\ndef parse_fuzz_mapping(\n    fuzz_mapping: list[FuzzyMapping] | tuple[str, str] | str | FuzzyMapping | list[dict],\n) -&gt; list[FuzzyMapping]:\n    \"\"\"Parses various input formats into a list of FuzzyMapping objects.\"\"\"\n    if isinstance(fuzz_mapping, (tuple, list)):\n        if len(fuzz_mapping) == 0:\n            raise ValueError(\"Fuzzy mapping cannot be empty\")\n\n        if all(isinstance(fm, dict) for fm in fuzz_mapping):\n            return [FuzzyMapping(**fm) for fm in fuzz_mapping]\n\n        if all(isinstance(fm, FuzzyMapping) for fm in fuzz_mapping):\n            return fuzz_mapping\n\n        if len(fuzz_mapping) &lt;= 2:\n            if len(fuzz_mapping) == 2:\n                if isinstance(fuzz_mapping[0], str) and isinstance(fuzz_mapping[1], str):\n                    return [FuzzyMapping(left_col=fuzz_mapping[0], right_col=fuzz_mapping[1])]\n            elif len(fuzz_mapping) == 1 and isinstance(fuzz_mapping[0], str):\n                return [FuzzyMapping(left_col=fuzz_mapping[0], right_col=fuzz_mapping[0])]\n\n    elif isinstance(fuzz_mapping, str):\n        return [FuzzyMapping(left_col=fuzz_mapping, right_col=fuzz_mapping)]\n\n    elif isinstance(fuzz_mapping, FuzzyMapping):\n        return [fuzz_mapping]\n\n    raise ValueError(f\"No valid fuzzy mapping as input: {type(fuzz_mapping)}\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInputManager.to_fuzzy_match_input","title":"<code>to_fuzzy_match_input()</code>","text":"<p>Creates a new FuzzyMatchInput instance based on the current manager settings.</p> <p>This is useful when you've modified the manager (e.g., via auto_rename) and want to get a fresh FuzzyMatchInput with all the current settings applied.</p> <p>Returns:</p> Type Description <code>FuzzyMatchInput</code> <p>A new FuzzyMatchInput instance with current settings</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def to_fuzzy_match_input(self) -&gt; FuzzyMatchInput:\n    \"\"\"Creates a new FuzzyMatchInput instance based on the current manager settings.\n\n    This is useful when you've modified the manager (e.g., via auto_rename) and\n    want to get a fresh FuzzyMatchInput with all the current settings applied.\n\n    Returns:\n        A new FuzzyMatchInput instance with current settings\n    \"\"\"\n    return FuzzyMatchInput(\n        join_mapping=self.fuzzy_input.join_mapping,\n        left_select=JoinInputs(renames=self.input.left_select.renames.copy()),\n        right_select=JoinInputs(renames=self.input.right_select.renames.copy()),\n        how=self.fuzzy_input.how,\n        aggregate_output=self.fuzzy_input.aggregate_output,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.GraphSolverInput","title":"<code>GraphSolverInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for a graph-solving operation (e.g., finding connected components).</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines settings for a graph-solving operation (e.g., finding connected components).\",\n  \"properties\": {\n    \"col_from\": {\n      \"title\": \"Col From\",\n      \"type\": \"string\"\n    },\n    \"col_to\": {\n      \"title\": \"Col To\",\n      \"type\": \"string\"\n    },\n    \"output_column_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"graph_group\",\n      \"title\": \"Output Column Name\"\n    }\n  },\n  \"required\": [\n    \"col_from\",\n    \"col_to\"\n  ],\n  \"title\": \"GraphSolverInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>col_from</code>                 (<code>str</code>)             </li> <li> <code>col_to</code>                 (<code>str</code>)             </li> <li> <code>output_column_name</code>                 (<code>str | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class GraphSolverInput(BaseModel):\n    \"\"\"Defines settings for a graph-solving operation (e.g., finding connected components).\"\"\"\n\n    col_from: str\n    col_to: str\n    output_column_name: str | None = \"graph_group\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.GroupByInput","title":"<code>GroupByInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A data class that represents the input for a group by operation.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.GroupByInput--attributes","title":"Attributes","text":"<p>agg_cols : List[AggColl]     A list of <code>AggColl</code> objects that specify the aggregation operations to perform on the DataFrame columns     after grouping. Each <code>AggColl</code> object should specify the column to be aggregated and the aggregation     function to use.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.GroupByInput--example","title":"Example","text":"<p>group_by_input = GroupByInput(     agg_cols=[AggColl(old_name='ix', agg='groupby'), AggColl(old_name='groups', agg='groupby'),               AggColl(old_name='col1', agg='sum'), AggColl(old_name='col2', agg='mean')] )</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AggColl\": {\n      \"description\": \"A data class that represents a single aggregation operation for a group by operation.\\n\\nAttributes\\n----------\\nold_name : str\\n    The name of the column in the original DataFrame to be aggregated.\\n\\nagg : str\\n    The aggregation function to use. This can be a string representing a built-in function or a custom function.\\n\\nnew_name : Optional[str]\\n    The name of the resulting aggregated column in the output DataFrame. If not provided, it will default to the\\n    old_name appended with the aggregation function.\\n\\noutput_type : Optional[str]\\n    The type of the output values of the aggregation. If not provided, it is inferred from the aggregation function\\n    using the `get_func_type_mapping` function.\\n\\nExample\\n--------\\nagg_col = AggColl(\\n    old_name='col1',\\n    agg='sum',\\n    new_name='sum_col1',\\n    output_type='float'\\n)\",\n      \"properties\": {\n        \"old_name\": {\n          \"title\": \"Old Name\",\n          \"type\": \"string\"\n        },\n        \"agg\": {\n          \"title\": \"Agg\",\n          \"type\": \"string\"\n        },\n        \"new_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"New Name\"\n        },\n        \"output_type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Output Type\"\n        }\n      },\n      \"required\": [\n        \"old_name\",\n        \"agg\"\n      ],\n      \"title\": \"AggColl\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"A data class that represents the input for a group by operation.\\n\\nAttributes\\n----------\\nagg_cols : List[AggColl]\\n    A list of `AggColl` objects that specify the aggregation operations to perform on the DataFrame columns\\n    after grouping. Each `AggColl` object should specify the column to be aggregated and the aggregation\\n    function to use.\\n\\nExample\\n--------\\ngroup_by_input = GroupByInput(\\n    agg_cols=[AggColl(old_name='ix', agg='groupby'), AggColl(old_name='groups', agg='groupby'),\\n              AggColl(old_name='col1', agg='sum'), AggColl(old_name='col2', agg='mean')]\\n)\",\n  \"properties\": {\n    \"agg_cols\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/AggColl\"\n      },\n      \"title\": \"Agg Cols\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"agg_cols\"\n  ],\n  \"title\": \"GroupByInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>agg_cols</code>                 (<code>list[AggColl]</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class GroupByInput(BaseModel):\n    \"\"\"\n    A data class that represents the input for a group by operation.\n\n    Attributes\n    ----------\n    agg_cols : List[AggColl]\n        A list of `AggColl` objects that specify the aggregation operations to perform on the DataFrame columns\n        after grouping. Each `AggColl` object should specify the column to be aggregated and the aggregation\n        function to use.\n\n    Example\n    --------\n    group_by_input = GroupByInput(\n        agg_cols=[AggColl(old_name='ix', agg='groupby'), AggColl(old_name='groups', agg='groupby'),\n                  AggColl(old_name='col1', agg='sum'), AggColl(old_name='col2', agg='mean')]\n    )\n    \"\"\"\n\n    agg_cols: list[AggColl]\n\n    def __init__(self, agg_cols: list[AggColl]):\n        \"\"\"Backwards compatibility implementation\"\"\"\n        super().__init__(agg_cols=agg_cols)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.GroupByInput.__init__","title":"<code>__init__(agg_cols)</code>","text":"<p>Backwards compatibility implementation</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def __init__(self, agg_cols: list[AggColl]):\n    \"\"\"Backwards compatibility implementation\"\"\"\n    super().__init__(agg_cols=agg_cols)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInput","title":"<code>JoinInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data model for standard SQL-style join operations.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"JoinInputs\": {\n      \"description\": \"Data model for join-specific select inputs (extends SelectInputs).\",\n      \"properties\": {\n        \"renames\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/SelectInput\"\n          },\n          \"title\": \"Renames\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"JoinInputs\",\n      \"type\": \"object\"\n    },\n    \"JoinMap\": {\n      \"description\": \"Defines a single mapping between a left and right column for a join key.\",\n      \"properties\": {\n        \"left_col\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Left Col\"\n        },\n        \"right_col\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Right Col\"\n        }\n      },\n      \"title\": \"JoinMap\",\n      \"type\": \"object\"\n    },\n    \"SelectInput\": {\n      \"description\": \"Defines how a single column should be selected, renamed, or type-cast.\\n\\nThis is a core building block for any operation that involves column manipulation.\\nIt holds all the configuration for a single field in a selection operation.\",\n      \"properties\": {\n        \"old_name\": {\n          \"title\": \"Old Name\",\n          \"type\": \"string\"\n        },\n        \"original_position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Original Position\"\n        },\n        \"new_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"New Name\"\n        },\n        \"data_type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Data Type\"\n        },\n        \"data_type_change\": {\n          \"default\": false,\n          \"title\": \"Data Type Change\",\n          \"type\": \"boolean\"\n        },\n        \"join_key\": {\n          \"default\": false,\n          \"title\": \"Join Key\",\n          \"type\": \"boolean\"\n        },\n        \"is_altered\": {\n          \"default\": false,\n          \"title\": \"Is Altered\",\n          \"type\": \"boolean\"\n        },\n        \"position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Position\"\n        },\n        \"is_available\": {\n          \"default\": true,\n          \"title\": \"Is Available\",\n          \"type\": \"boolean\"\n        },\n        \"keep\": {\n          \"default\": true,\n          \"title\": \"Keep\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"old_name\"\n      ],\n      \"title\": \"SelectInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Data model for standard SQL-style join operations.\",\n  \"properties\": {\n    \"join_mapping\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/JoinMap\"\n      },\n      \"title\": \"Join Mapping\",\n      \"type\": \"array\"\n    },\n    \"left_select\": {\n      \"$ref\": \"#/$defs/JoinInputs\"\n    },\n    \"right_select\": {\n      \"$ref\": \"#/$defs/JoinInputs\"\n    },\n    \"how\": {\n      \"default\": \"inner\",\n      \"enum\": [\n        \"inner\",\n        \"left\",\n        \"right\",\n        \"full\",\n        \"semi\",\n        \"anti\",\n        \"cross\",\n        \"outer\"\n      ],\n      \"title\": \"How\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"join_mapping\",\n    \"left_select\",\n    \"right_select\"\n  ],\n  \"title\": \"JoinInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>join_mapping</code>                 (<code>list[JoinMap]</code>)             </li> <li> <code>left_select</code>                 (<code>JoinInputs</code>)             </li> <li> <code>right_select</code>                 (<code>JoinInputs</code>)             </li> <li> <code>how</code>                 (<code>JoinStrategy</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>parse_inputs</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class JoinInput(BaseModel):\n    \"\"\"Data model for standard SQL-style join operations.\"\"\"\n\n    join_mapping: list[JoinMap]\n    left_select: JoinInputs\n    right_select: JoinInputs\n    how: JoinStrategy = \"inner\"\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def parse_inputs(cls, data: Any) -&gt; Any:\n        \"\"\"Parse flexible input formats before validation.\"\"\"\n        if isinstance(data, dict):\n            # Parse join_mapping\n            if \"join_mapping\" in data:\n                data[\"join_mapping\"] = cls._parse_join_mapping(data[\"join_mapping\"])\n\n            # Parse left_select\n            if \"left_select\" in data:\n                data[\"left_select\"] = cls._parse_select(data[\"left_select\"])\n\n            # Parse right_select\n            if \"right_select\" in data:\n                data[\"right_select\"] = cls._parse_select(data[\"right_select\"])\n\n        return data\n\n    @staticmethod\n    def _parse_join_mapping(join_mapping: Any) -&gt; list[JoinMap]:\n        \"\"\"Parse various join_mapping formats.\"\"\"\n        # Already a list of JoinMaps\n        if isinstance(join_mapping, list):\n            result = []\n            for jm in join_mapping:\n                if isinstance(jm, JoinMap):\n                    result.append(jm)\n                elif isinstance(jm, dict):\n                    result.append(JoinMap(**jm))\n                elif isinstance(jm, (tuple, list)) and len(jm) == 2:\n                    result.append(JoinMap(left_col=jm[0], right_col=jm[1]))\n                elif isinstance(jm, str):\n                    result.append(JoinMap(left_col=jm, right_col=jm))\n                else:\n                    raise ValueError(f\"Invalid join mapping item: {jm}\")\n            return result\n\n        # Single JoinMap\n        if isinstance(join_mapping, JoinMap):\n            return [join_mapping]\n\n        # String: same column on both sides\n        if isinstance(join_mapping, str):\n            return [JoinMap(left_col=join_mapping, right_col=join_mapping)]\n\n        # Tuple: (left, right)\n        if isinstance(join_mapping, tuple) and len(join_mapping) == 2:\n            return [JoinMap(left_col=join_mapping[0], right_col=join_mapping[1])]\n\n        raise ValueError(f\"Invalid join_mapping format: {type(join_mapping)}\")\n\n    @staticmethod\n    def _parse_select(select: Any) -&gt; JoinInputs:\n        \"\"\"Parse various select input formats.\"\"\"\n        # Already JoinInputs\n        if isinstance(select, JoinInputs):\n            return select\n\n        # List of SelectInput objects\n        if isinstance(select, list):\n            if all(isinstance(s, SelectInput) for s in select):\n                return JoinInputs(renames=select)\n            elif all(isinstance(s, str) for s in select):\n                return JoinInputs(renames=[SelectInput(old_name=s) for s in select])\n            elif all(isinstance(s, dict) for s in select):\n                return JoinInputs(renames=[SelectInput(**s) for s in select])\n\n        # Dict with 'select' (new YAML) or 'renames' (internal) key\n        if isinstance(select, dict):\n            if \"select\" in select:\n                return JoinInputs(renames=[SelectInput.from_yaml_dict(s) for s in select[\"select\"]])\n            if \"renames\" in select:\n                return JoinInputs(**select)\n\n        raise ValueError(f\"Invalid select format: {type(select)}\")\n\n    def __init__(\n        self,\n        join_mapping: list[JoinMap] | JoinMap | tuple[str, str] | str | list[tuple] | list[str] = None,\n        left_select: JoinInputs | list[SelectInput] | list[str] = None,\n        right_select: JoinInputs | list[SelectInput] | list[str] = None,\n        how: JoinStrategy = \"inner\",\n        **data,\n    ):\n        \"\"\"Custom init for backward compatibility with positional arguments.\"\"\"\n        if join_mapping is not None:\n            data[\"join_mapping\"] = join_mapping\n        if left_select is not None:\n            data[\"left_select\"] = left_select\n        if right_select is not None:\n            data[\"right_select\"] = right_select\n        if how is not None:\n            data[\"how\"] = how\n\n        super().__init__(**data)\n\n    def to_yaml_dict(self) -&gt; JoinInputYaml:\n        \"\"\"Serialize for YAML output.\"\"\"\n        return {\n            \"join_mapping\": [{\"left_col\": jm.left_col, \"right_col\": jm.right_col} for jm in self.join_mapping],\n            \"left_select\": self.left_select.to_yaml_dict(),\n            \"right_select\": self.right_select.to_yaml_dict(),\n            \"how\": self.how,\n        }\n\n    def add_new_select_column(self, select_input: SelectInput, side: str) -&gt; None:\n        \"\"\"Adds a new column to the selection for either the left or right side.\"\"\"\n        target_input = self.right_select if side == \"right\" else self.left_select\n        if select_input.new_name is None:\n            select_input.new_name = select_input.old_name\n        target_input.renames.append(select_input)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInput.__init__","title":"<code>__init__(join_mapping=None, left_select=None, right_select=None, how='inner', **data)</code>","text":"<p>Custom init for backward compatibility with positional arguments.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def __init__(\n    self,\n    join_mapping: list[JoinMap] | JoinMap | tuple[str, str] | str | list[tuple] | list[str] = None,\n    left_select: JoinInputs | list[SelectInput] | list[str] = None,\n    right_select: JoinInputs | list[SelectInput] | list[str] = None,\n    how: JoinStrategy = \"inner\",\n    **data,\n):\n    \"\"\"Custom init for backward compatibility with positional arguments.\"\"\"\n    if join_mapping is not None:\n        data[\"join_mapping\"] = join_mapping\n    if left_select is not None:\n        data[\"left_select\"] = left_select\n    if right_select is not None:\n        data[\"right_select\"] = right_select\n    if how is not None:\n        data[\"how\"] = how\n\n    super().__init__(**data)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInput.add_new_select_column","title":"<code>add_new_select_column(select_input, side)</code>","text":"<p>Adds a new column to the selection for either the left or right side.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def add_new_select_column(self, select_input: SelectInput, side: str) -&gt; None:\n    \"\"\"Adds a new column to the selection for either the left or right side.\"\"\"\n    target_input = self.right_select if side == \"right\" else self.left_select\n    if select_input.new_name is None:\n        select_input.new_name = select_input.old_name\n    target_input.renames.append(select_input)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInput.parse_inputs","title":"<code>parse_inputs(data)</code>  <code>pydantic-validator</code>","text":"<p>Parse flexible input formats before validation.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef parse_inputs(cls, data: Any) -&gt; Any:\n    \"\"\"Parse flexible input formats before validation.\"\"\"\n    if isinstance(data, dict):\n        # Parse join_mapping\n        if \"join_mapping\" in data:\n            data[\"join_mapping\"] = cls._parse_join_mapping(data[\"join_mapping\"])\n\n        # Parse left_select\n        if \"left_select\" in data:\n            data[\"left_select\"] = cls._parse_select(data[\"left_select\"])\n\n        # Parse right_select\n        if \"right_select\" in data:\n            data[\"right_select\"] = cls._parse_select(data[\"right_select\"])\n\n    return data\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInput.to_yaml_dict","title":"<code>to_yaml_dict()</code>","text":"<p>Serialize for YAML output.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def to_yaml_dict(self) -&gt; JoinInputYaml:\n    \"\"\"Serialize for YAML output.\"\"\"\n    return {\n        \"join_mapping\": [{\"left_col\": jm.left_col, \"right_col\": jm.right_col} for jm in self.join_mapping],\n        \"left_select\": self.left_select.to_yaml_dict(),\n        \"right_select\": self.right_select.to_yaml_dict(),\n        \"how\": self.how,\n    }\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager","title":"<code>JoinInputManager</code>","text":"<p>               Bases: <code>JoinSelectManagerMixin</code></p> <p>Manager for standard SQL-style join operations.</p> <p>Methods:</p> Name Description <code>auto_rename</code> <p>Automatically renames columns on the right side to prevent naming conflicts.</p> <code>create</code> <p>Factory method to create JoinInput from various input formats.</p> <code>get_join_key_renames</code> <p>Gets the temporary rename mappings for the join keys on both sides.</p> <code>get_left_join_keys</code> <p>Returns a set of the left-side join key column names.</p> <code>get_left_join_keys_list</code> <p>Returns an ordered list of the left-side join key column names.</p> <code>get_names_for_table_rename</code> <p>Gets join mapping with renamed columns applied.</p> <code>get_overlapping_records</code> <p>Finds column names that would conflict after the join.</p> <code>get_right_join_keys</code> <p>Returns a set of the right-side join key column names.</p> <code>get_right_join_keys_list</code> <p>Returns an ordered list of the right-side join key column names.</p> <code>get_used_join_mapping</code> <p>Returns the final join mapping after applying all renames and transformations.</p> <code>set_join_keys</code> <p>Marks the <code>SelectInput</code> objects corresponding to join keys.</p> <code>to_join_input</code> <p>Creates a new JoinInput instance based on the current manager settings.</p> <p>Attributes:</p> Name Type Description <code>how</code> <code>JoinStrategy</code> <p>Backward compatibility: Access join strategy.</p> <code>join_mapping</code> <code>list[JoinMap]</code> <p>Backward compatibility: Access join mapping.</p> <code>left_join_keys</code> <code>list[str]</code> <p>Backward compatibility: Returns left join keys list.</p> <code>left_select</code> <code>JoinInputsManager</code> <p>Backward compatibility: Access left_manager as left_select.</p> <code>overlapping_records</code> <code>set[str]</code> <p>Backward compatibility: Returns overlapping column names.</p> <code>right_join_keys</code> <code>list[str]</code> <p>Backward compatibility: Returns right join keys list.</p> <code>right_select</code> <code>JoinInputsManager</code> <p>Backward compatibility: Access right_manager as right_select.</p> <code>used_join_mapping</code> <code>list[JoinMap]</code> <p>Backward compatibility: Returns used join mapping.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class JoinInputManager(JoinSelectManagerMixin):\n    \"\"\"Manager for standard SQL-style join operations.\"\"\"\n\n    def __init__(self, join_input: JoinInput):\n        self.input = deepcopy(join_input)\n        self.left_manager = JoinInputsManager(self.input.left_select)\n        self.right_manager = JoinInputsManager(self.input.right_select)\n        self.set_join_keys()\n\n    @classmethod\n    def create(\n        cls,\n        join_mapping: list[JoinMap] | tuple[str, str] | str,\n        left_select: list[SelectInput] | list[str],\n        right_select: list[SelectInput] | list[str],\n        how: JoinStrategy = \"inner\",\n    ) -&gt; \"JoinInputManager\":\n        \"\"\"Factory method to create JoinInput from various input formats.\"\"\"\n        # Use JoinInput's own create method for parsing\n        join_input = JoinInput(join_mapping=join_mapping, left_select=left_select, right_select=right_select, how=how)\n\n        manager = cls(join_input)\n        manager.set_join_keys()\n        return manager\n\n    def set_join_keys(self) -&gt; None:\n        \"\"\"Marks the `SelectInput` objects corresponding to join keys.\"\"\"\n        left_join_keys = self._get_left_join_keys_set()\n        right_join_keys = self._get_right_join_keys_set()\n\n        for select_input in self.input.left_select.renames:\n            select_input.join_key = select_input.old_name in left_join_keys\n\n        for select_input in self.input.right_select.renames:\n            select_input.join_key = select_input.old_name in right_join_keys\n\n    def _get_left_join_keys_set(self) -&gt; set[str]:\n        \"\"\"Internal: Returns a set of the left-side join key column names.\"\"\"\n        return {jm.left_col for jm in self.input.join_mapping}\n\n    def _get_right_join_keys_set(self) -&gt; set[str]:\n        \"\"\"Internal: Returns a set of the right-side join key column names.\"\"\"\n        return {jm.right_col for jm in self.input.join_mapping}\n\n    def get_left_join_keys(self) -&gt; set[str]:\n        \"\"\"Returns a set of the left-side join key column names.\"\"\"\n        return self._get_left_join_keys_set()\n\n    def get_right_join_keys(self) -&gt; set[str]:\n        \"\"\"Returns a set of the right-side join key column names.\"\"\"\n        return self._get_right_join_keys_set()\n\n    def get_left_join_keys_list(self) -&gt; list[str]:\n        \"\"\"Returns an ordered list of the left-side join key column names.\"\"\"\n        return [jm.left_col for jm in self.used_join_mapping]\n\n    def get_right_join_keys_list(self) -&gt; list[str]:\n        \"\"\"Returns an ordered list of the right-side join key column names.\"\"\"\n        return [jm.right_col for jm in self.used_join_mapping]\n\n    def get_overlapping_records(self) -&gt; set[str]:\n        \"\"\"Finds column names that would conflict after the join.\"\"\"\n        return self.get_overlapping_columns()\n\n    def auto_rename(self) -&gt; None:\n        \"\"\"Automatically renames columns on the right side to prevent naming conflicts.\"\"\"\n        self.set_join_keys()\n        overlapping_records = self.get_overlapping_records()\n\n        while len(overlapping_records) &gt; 0:\n            for right_col in self.input.right_select.renames:\n                if right_col.new_name in overlapping_records:\n                    right_col.new_name = right_col.new_name + \"_right\"\n            overlapping_records = self.get_overlapping_records()\n\n    def get_join_key_renames(self, filter_drop: bool = False) -&gt; FullJoinKeyResponse:\n        \"\"\"Gets the temporary rename mappings for the join keys on both sides.\"\"\"\n        left_renames = self.left_manager.get_join_key_renames(side=\"left\", filter_drop=filter_drop)\n        right_renames = self.right_manager.get_join_key_renames(side=\"right\", filter_drop=filter_drop)\n        return FullJoinKeyResponse(left_renames, right_renames)\n\n    def get_names_for_table_rename(self) -&gt; list[JoinMap]:\n        \"\"\"Gets join mapping with renamed columns applied.\"\"\"\n        new_mappings: list[JoinMap] = []\n        left_rename_table = self.left_manager.get_rename_table()\n        right_rename_table = self.right_manager.get_rename_table()\n\n        for join_map in self.input.join_mapping:\n            new_left = left_rename_table.get(join_map.left_col, join_map.left_col)\n            new_right = right_rename_table.get(join_map.right_col, join_map.right_col)\n            new_mappings.append(JoinMap(left_col=new_left, right_col=new_right))\n\n        return new_mappings\n\n    def get_used_join_mapping(self) -&gt; list[JoinMap]:\n        \"\"\"Returns the final join mapping after applying all renames and transformations.\"\"\"\n        new_mappings: list[JoinMap] = []\n        left_rename_table = self.left_manager.get_rename_table()\n        right_rename_table = self.right_manager.get_rename_table()\n        left_join_rename_mapping = self.left_manager.get_join_key_rename_mapping(\"left\")\n        right_join_rename_mapping = self.right_manager.get_join_key_rename_mapping(\"right\")\n        for join_map in self.input.join_mapping:\n            left_col = left_rename_table.get(join_map.left_col, join_map.left_col)\n            right_col = right_rename_table.get(join_map.right_col, join_map.left_col)\n\n            final_left = left_join_rename_mapping.get(left_col, None)\n            final_right = right_join_rename_mapping.get(right_col, None)\n\n            new_mappings.append(JoinMap(left_col=final_left, right_col=final_right))\n\n        return new_mappings\n\n    def to_join_input(self) -&gt; JoinInput:\n        \"\"\"Creates a new JoinInput instance based on the current manager settings.\n\n        This is useful when you've modified the manager (e.g., via auto_rename) and\n        want to get a fresh JoinInput with all the current settings applied.\n\n        Returns:\n            A new JoinInput instance with current settings\n        \"\"\"\n        return JoinInput(\n            join_mapping=self.input.join_mapping,\n            left_select=JoinInputs(renames=self.input.left_select.renames.copy()),\n            right_select=JoinInputs(renames=self.input.right_select.renames.copy()),\n            how=self.input.how,\n        )\n\n    @property\n    def left_select(self) -&gt; JoinInputsManager:\n        \"\"\"Backward compatibility: Access left_manager as left_select.\n\n        This returns the MANAGER, not the data model.\n        Usage: manager.left_select.join_key_selects\n        \"\"\"\n        return self.left_manager\n\n    @property\n    def right_select(self) -&gt; JoinInputsManager:\n        \"\"\"Backward compatibility: Access right_manager as right_select.\n\n        This returns the MANAGER, not the data model.\n        Usage: manager.right_select.join_key_selects\n        \"\"\"\n        return self.right_manager\n\n    @property\n    def how(self) -&gt; JoinStrategy:\n        \"\"\"Backward compatibility: Access join strategy.\"\"\"\n        return self.input.how\n\n    @property\n    def join_mapping(self) -&gt; list[JoinMap]:\n        \"\"\"Backward compatibility: Access join mapping.\"\"\"\n        return self.input.join_mapping\n\n    @property\n    def overlapping_records(self) -&gt; set[str]:\n        \"\"\"Backward compatibility: Returns overlapping column names.\"\"\"\n        return self.get_overlapping_records()\n\n    @property\n    def used_join_mapping(self) -&gt; list[JoinMap]:\n        \"\"\"Backward compatibility: Returns used join mapping.\n\n        This property is critical - it's used by left_join_keys and right_join_keys.\n        \"\"\"\n        return self.get_used_join_mapping()\n\n    @property\n    def left_join_keys(self) -&gt; list[str]:\n        \"\"\"Backward compatibility: Returns left join keys list.\n\n        IMPORTANT: Uses the used_join_mapping PROPERTY (not method).\n        \"\"\"\n        return [jm.left_col for jm in self.used_join_mapping]\n\n    @property\n    def right_join_keys(self) -&gt; list[str]:\n        \"\"\"Backward compatibility: Returns right join keys list.\n\n        IMPORTANT: Uses the used_join_mapping PROPERTY (not method).\n        \"\"\"\n        return [jm.right_col for jm in self.used_join_mapping]\n\n    @property\n    def _left_join_keys(self) -&gt; set[str]:\n        \"\"\"Backward compatibility: Private property for left join key set.\"\"\"\n        return self._get_left_join_keys_set()\n\n    @property\n    def _right_join_keys(self) -&gt; set[str]:\n        \"\"\"Backward compatibility: Private property for right join key set.\"\"\"\n        return self._get_right_join_keys_set()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.how","title":"<code>how</code>  <code>property</code>","text":"<p>Backward compatibility: Access join strategy.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.join_mapping","title":"<code>join_mapping</code>  <code>property</code>","text":"<p>Backward compatibility: Access join mapping.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.left_join_keys","title":"<code>left_join_keys</code>  <code>property</code>","text":"<p>Backward compatibility: Returns left join keys list.</p> <p>IMPORTANT: Uses the used_join_mapping PROPERTY (not method).</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.left_select","title":"<code>left_select</code>  <code>property</code>","text":"<p>Backward compatibility: Access left_manager as left_select.</p> <p>This returns the MANAGER, not the data model. Usage: manager.left_select.join_key_selects</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.overlapping_records","title":"<code>overlapping_records</code>  <code>property</code>","text":"<p>Backward compatibility: Returns overlapping column names.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.right_join_keys","title":"<code>right_join_keys</code>  <code>property</code>","text":"<p>Backward compatibility: Returns right join keys list.</p> <p>IMPORTANT: Uses the used_join_mapping PROPERTY (not method).</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.right_select","title":"<code>right_select</code>  <code>property</code>","text":"<p>Backward compatibility: Access right_manager as right_select.</p> <p>This returns the MANAGER, not the data model. Usage: manager.right_select.join_key_selects</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.used_join_mapping","title":"<code>used_join_mapping</code>  <code>property</code>","text":"<p>Backward compatibility: Returns used join mapping.</p> <p>This property is critical - it's used by left_join_keys and right_join_keys.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.auto_rename","title":"<code>auto_rename()</code>","text":"<p>Automatically renames columns on the right side to prevent naming conflicts.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def auto_rename(self) -&gt; None:\n    \"\"\"Automatically renames columns on the right side to prevent naming conflicts.\"\"\"\n    self.set_join_keys()\n    overlapping_records = self.get_overlapping_records()\n\n    while len(overlapping_records) &gt; 0:\n        for right_col in self.input.right_select.renames:\n            if right_col.new_name in overlapping_records:\n                right_col.new_name = right_col.new_name + \"_right\"\n        overlapping_records = self.get_overlapping_records()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.create","title":"<code>create(join_mapping, left_select, right_select, how='inner')</code>  <code>classmethod</code>","text":"<p>Factory method to create JoinInput from various input formats.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    join_mapping: list[JoinMap] | tuple[str, str] | str,\n    left_select: list[SelectInput] | list[str],\n    right_select: list[SelectInput] | list[str],\n    how: JoinStrategy = \"inner\",\n) -&gt; \"JoinInputManager\":\n    \"\"\"Factory method to create JoinInput from various input formats.\"\"\"\n    # Use JoinInput's own create method for parsing\n    join_input = JoinInput(join_mapping=join_mapping, left_select=left_select, right_select=right_select, how=how)\n\n    manager = cls(join_input)\n    manager.set_join_keys()\n    return manager\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.get_join_key_renames","title":"<code>get_join_key_renames(filter_drop=False)</code>","text":"<p>Gets the temporary rename mappings for the join keys on both sides.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_join_key_renames(self, filter_drop: bool = False) -&gt; FullJoinKeyResponse:\n    \"\"\"Gets the temporary rename mappings for the join keys on both sides.\"\"\"\n    left_renames = self.left_manager.get_join_key_renames(side=\"left\", filter_drop=filter_drop)\n    right_renames = self.right_manager.get_join_key_renames(side=\"right\", filter_drop=filter_drop)\n    return FullJoinKeyResponse(left_renames, right_renames)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.get_left_join_keys","title":"<code>get_left_join_keys()</code>","text":"<p>Returns a set of the left-side join key column names.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_left_join_keys(self) -&gt; set[str]:\n    \"\"\"Returns a set of the left-side join key column names.\"\"\"\n    return self._get_left_join_keys_set()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.get_left_join_keys_list","title":"<code>get_left_join_keys_list()</code>","text":"<p>Returns an ordered list of the left-side join key column names.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_left_join_keys_list(self) -&gt; list[str]:\n    \"\"\"Returns an ordered list of the left-side join key column names.\"\"\"\n    return [jm.left_col for jm in self.used_join_mapping]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.get_names_for_table_rename","title":"<code>get_names_for_table_rename()</code>","text":"<p>Gets join mapping with renamed columns applied.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_names_for_table_rename(self) -&gt; list[JoinMap]:\n    \"\"\"Gets join mapping with renamed columns applied.\"\"\"\n    new_mappings: list[JoinMap] = []\n    left_rename_table = self.left_manager.get_rename_table()\n    right_rename_table = self.right_manager.get_rename_table()\n\n    for join_map in self.input.join_mapping:\n        new_left = left_rename_table.get(join_map.left_col, join_map.left_col)\n        new_right = right_rename_table.get(join_map.right_col, join_map.right_col)\n        new_mappings.append(JoinMap(left_col=new_left, right_col=new_right))\n\n    return new_mappings\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.get_overlapping_records","title":"<code>get_overlapping_records()</code>","text":"<p>Finds column names that would conflict after the join.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_overlapping_records(self) -&gt; set[str]:\n    \"\"\"Finds column names that would conflict after the join.\"\"\"\n    return self.get_overlapping_columns()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.get_right_join_keys","title":"<code>get_right_join_keys()</code>","text":"<p>Returns a set of the right-side join key column names.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_right_join_keys(self) -&gt; set[str]:\n    \"\"\"Returns a set of the right-side join key column names.\"\"\"\n    return self._get_right_join_keys_set()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.get_right_join_keys_list","title":"<code>get_right_join_keys_list()</code>","text":"<p>Returns an ordered list of the right-side join key column names.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_right_join_keys_list(self) -&gt; list[str]:\n    \"\"\"Returns an ordered list of the right-side join key column names.\"\"\"\n    return [jm.right_col for jm in self.used_join_mapping]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.get_used_join_mapping","title":"<code>get_used_join_mapping()</code>","text":"<p>Returns the final join mapping after applying all renames and transformations.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_used_join_mapping(self) -&gt; list[JoinMap]:\n    \"\"\"Returns the final join mapping after applying all renames and transformations.\"\"\"\n    new_mappings: list[JoinMap] = []\n    left_rename_table = self.left_manager.get_rename_table()\n    right_rename_table = self.right_manager.get_rename_table()\n    left_join_rename_mapping = self.left_manager.get_join_key_rename_mapping(\"left\")\n    right_join_rename_mapping = self.right_manager.get_join_key_rename_mapping(\"right\")\n    for join_map in self.input.join_mapping:\n        left_col = left_rename_table.get(join_map.left_col, join_map.left_col)\n        right_col = right_rename_table.get(join_map.right_col, join_map.left_col)\n\n        final_left = left_join_rename_mapping.get(left_col, None)\n        final_right = right_join_rename_mapping.get(right_col, None)\n\n        new_mappings.append(JoinMap(left_col=final_left, right_col=final_right))\n\n    return new_mappings\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.set_join_keys","title":"<code>set_join_keys()</code>","text":"<p>Marks the <code>SelectInput</code> objects corresponding to join keys.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def set_join_keys(self) -&gt; None:\n    \"\"\"Marks the `SelectInput` objects corresponding to join keys.\"\"\"\n    left_join_keys = self._get_left_join_keys_set()\n    right_join_keys = self._get_right_join_keys_set()\n\n    for select_input in self.input.left_select.renames:\n        select_input.join_key = select_input.old_name in left_join_keys\n\n    for select_input in self.input.right_select.renames:\n        select_input.join_key = select_input.old_name in right_join_keys\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputManager.to_join_input","title":"<code>to_join_input()</code>","text":"<p>Creates a new JoinInput instance based on the current manager settings.</p> <p>This is useful when you've modified the manager (e.g., via auto_rename) and want to get a fresh JoinInput with all the current settings applied.</p> <p>Returns:</p> Type Description <code>JoinInput</code> <p>A new JoinInput instance with current settings</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def to_join_input(self) -&gt; JoinInput:\n    \"\"\"Creates a new JoinInput instance based on the current manager settings.\n\n    This is useful when you've modified the manager (e.g., via auto_rename) and\n    want to get a fresh JoinInput with all the current settings applied.\n\n    Returns:\n        A new JoinInput instance with current settings\n    \"\"\"\n    return JoinInput(\n        join_mapping=self.input.join_mapping,\n        left_select=JoinInputs(renames=self.input.left_select.renames.copy()),\n        right_select=JoinInputs(renames=self.input.right_select.renames.copy()),\n        how=self.input.how,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputs","title":"<code>JoinInputs</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>SelectInputs</code></p> <p>Data model for join-specific select inputs (extends SelectInputs).</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"SelectInput\": {\n      \"description\": \"Defines how a single column should be selected, renamed, or type-cast.\\n\\nThis is a core building block for any operation that involves column manipulation.\\nIt holds all the configuration for a single field in a selection operation.\",\n      \"properties\": {\n        \"old_name\": {\n          \"title\": \"Old Name\",\n          \"type\": \"string\"\n        },\n        \"original_position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Original Position\"\n        },\n        \"new_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"New Name\"\n        },\n        \"data_type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Data Type\"\n        },\n        \"data_type_change\": {\n          \"default\": false,\n          \"title\": \"Data Type Change\",\n          \"type\": \"boolean\"\n        },\n        \"join_key\": {\n          \"default\": false,\n          \"title\": \"Join Key\",\n          \"type\": \"boolean\"\n        },\n        \"is_altered\": {\n          \"default\": false,\n          \"title\": \"Is Altered\",\n          \"type\": \"boolean\"\n        },\n        \"position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Position\"\n        },\n        \"is_available\": {\n          \"default\": true,\n          \"title\": \"Is Available\",\n          \"type\": \"boolean\"\n        },\n        \"keep\": {\n          \"default\": true,\n          \"title\": \"Keep\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"old_name\"\n      ],\n      \"title\": \"SelectInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Data model for join-specific select inputs (extends SelectInputs).\",\n  \"properties\": {\n    \"renames\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/SelectInput\"\n      },\n      \"title\": \"Renames\",\n      \"type\": \"array\"\n    }\n  },\n  \"title\": \"JoinInputs\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>renames</code>                 (<code>list[SelectInput]</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class JoinInputs(SelectInputs):\n    \"\"\"Data model for join-specific select inputs (extends SelectInputs).\"\"\"\n\n    def __init__(self, renames: list[SelectInput] = None, **kwargs):\n        if renames is not None:\n            kwargs[\"renames\"] = renames\n        else:\n            kwargs[\"renames\"] = []\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputsManager","title":"<code>JoinInputsManager</code>","text":"<p>               Bases: <code>SelectInputsManager</code></p> <p>Manager for join-specific operations, extends SelectInputsManager.</p> <p>Methods:</p> Name Description <code>get_join_key_rename_mapping</code> <p>Returns a dictionary mapping original join key names to their temporary names.</p> <code>get_join_key_renames</code> <p>Gets the temporary rename mapping for all join keys on one side of a join.</p> <code>get_join_key_selects</code> <p>Returns only the <code>SelectInput</code> objects that are marked as join keys.</p> <p>Attributes:</p> Name Type Description <code>join_key_selects</code> <code>list[SelectInput]</code> <p>Backward compatibility: Returns join key SelectInputs.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class JoinInputsManager(SelectInputsManager):\n    \"\"\"Manager for join-specific operations, extends SelectInputsManager.\"\"\"\n\n    def __init__(self, join_inputs: JoinInputs):\n        super().__init__(join_inputs)\n        self.join_inputs = join_inputs\n\n    # === Query Methods ===\n\n    def get_join_key_selects(self) -&gt; list[SelectInput]:\n        \"\"\"Returns only the `SelectInput` objects that are marked as join keys.\"\"\"\n        return [v for v in self.join_inputs.renames if v.join_key]\n\n    def get_join_key_renames(self, side: SideLit, filter_drop: bool = False) -&gt; JoinKeyRenameResponse:\n        \"\"\"Gets the temporary rename mapping for all join keys on one side of a join.\"\"\"\n        join_key_selects = self.get_join_key_selects()\n        join_key_list = [\n            JoinKeyRename(jk.new_name, construct_join_key_name(side, jk.new_name))\n            for jk in join_key_selects\n            if jk.keep or not filter_drop\n        ]\n        return JoinKeyRenameResponse(side, join_key_list)\n\n    def get_join_key_rename_mapping(self, side: SideLit) -&gt; dict[str, str]:\n        \"\"\"Returns a dictionary mapping original join key names to their temporary names.\"\"\"\n        join_key_response = self.get_join_key_renames(side)\n        return {jkr.original_name: jkr.temp_name for jkr in join_key_response.join_key_renames}\n\n    @property\n    def join_key_selects(self) -&gt; list[SelectInput]:\n        \"\"\"Backward compatibility: Returns join key SelectInputs.\"\"\"\n        return self.get_join_key_selects()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputsManager.join_key_selects","title":"<code>join_key_selects</code>  <code>property</code>","text":"<p>Backward compatibility: Returns join key SelectInputs.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputsManager.get_join_key_rename_mapping","title":"<code>get_join_key_rename_mapping(side)</code>","text":"<p>Returns a dictionary mapping original join key names to their temporary names.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_join_key_rename_mapping(self, side: SideLit) -&gt; dict[str, str]:\n    \"\"\"Returns a dictionary mapping original join key names to their temporary names.\"\"\"\n    join_key_response = self.get_join_key_renames(side)\n    return {jkr.original_name: jkr.temp_name for jkr in join_key_response.join_key_renames}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputsManager.get_join_key_renames","title":"<code>get_join_key_renames(side, filter_drop=False)</code>","text":"<p>Gets the temporary rename mapping for all join keys on one side of a join.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_join_key_renames(self, side: SideLit, filter_drop: bool = False) -&gt; JoinKeyRenameResponse:\n    \"\"\"Gets the temporary rename mapping for all join keys on one side of a join.\"\"\"\n    join_key_selects = self.get_join_key_selects()\n    join_key_list = [\n        JoinKeyRename(jk.new_name, construct_join_key_name(side, jk.new_name))\n        for jk in join_key_selects\n        if jk.keep or not filter_drop\n    ]\n    return JoinKeyRenameResponse(side, join_key_list)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputsManager.get_join_key_selects","title":"<code>get_join_key_selects()</code>","text":"<p>Returns only the <code>SelectInput</code> objects that are marked as join keys.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_join_key_selects(self) -&gt; list[SelectInput]:\n    \"\"\"Returns only the `SelectInput` objects that are marked as join keys.\"\"\"\n    return [v for v in self.join_inputs.renames if v.join_key]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinKeyRename","title":"<code>JoinKeyRename</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Represents the renaming of a join key from its original to a temporary name.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class JoinKeyRename(NamedTuple):\n    \"\"\"Represents the renaming of a join key from its original to a temporary name.\"\"\"\n\n    original_name: str\n    temp_name: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinKeyRenameResponse","title":"<code>JoinKeyRenameResponse</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Contains a list of join key renames for one side of a join.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class JoinKeyRenameResponse(NamedTuple):\n    \"\"\"Contains a list of join key renames for one side of a join.\"\"\"\n\n    side: SideLit\n    join_key_renames: list[JoinKeyRename]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinMap","title":"<code>JoinMap</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines a single mapping between a left and right column for a join key.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines a single mapping between a left and right column for a join key.\",\n  \"properties\": {\n    \"left_col\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Left Col\"\n    },\n    \"right_col\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Right Col\"\n    }\n  },\n  \"title\": \"JoinMap\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>left_col</code>                 (<code>str | None</code>)             </li> <li> <code>right_col</code>                 (<code>str | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>set_default_right_col</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class JoinMap(BaseModel):\n    \"\"\"Defines a single mapping between a left and right column for a join key.\"\"\"\n\n    left_col: str | None = None\n    right_col: str | None = None\n\n    def __init__(self, left_col: str = None, right_col: str = None, **data):\n        if left_col is not None:\n            data[\"left_col\"] = left_col\n        if right_col is not None:\n            data[\"right_col\"] = right_col\n        super().__init__(**data)\n\n    @model_validator(mode=\"after\")\n    def set_default_right_col(self):\n        \"\"\"If right_col is None, default it to left_col.\"\"\"\n        if self.right_col is None:\n            self.right_col = self.left_col\n        return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinMap.set_default_right_col","title":"<code>set_default_right_col()</code>  <code>pydantic-validator</code>","text":"<p>If right_col is None, default it to left_col.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@model_validator(mode=\"after\")\ndef set_default_right_col(self):\n    \"\"\"If right_col is None, default it to left_col.\"\"\"\n    if self.right_col is None:\n        self.right_col = self.left_col\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinSelectManagerMixin","title":"<code>JoinSelectManagerMixin</code>","text":"<p>Mixin providing common methods for join-like operations.</p> <p>Methods:</p> Name Description <code>add_new_select_column</code> <p>Adds a new column to the selection for either the left or right side.</p> <code>auto_generate_new_col_name</code> <p>Generates a new, non-conflicting column name by adding a suffix if necessary.</p> <code>get_overlapping_columns</code> <p>Finds column names that would conflict after the join.</p> <code>parse_select</code> <p>Parses various input formats into a standardized <code>JoinInputs</code> object.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class JoinSelectManagerMixin:\n    \"\"\"Mixin providing common methods for join-like operations.\"\"\"\n\n    left_manager: JoinInputsManager\n    right_manager: JoinInputsManager\n    input: CrossJoinInput | JoinInput | FuzzyMatchInput\n\n    @staticmethod\n    def parse_select(select: list[SelectInput] | list[str] | list[dict] | dict) -&gt; JoinInputs:\n        \"\"\"Parses various input formats into a standardized `JoinInputs` object.\"\"\"\n        if not select:\n            return JoinInputs(renames=[])\n\n        if all(isinstance(c, SelectInput) for c in select):\n            return JoinInputs(renames=select)\n        elif all(isinstance(c, dict) for c in select):\n            return JoinInputs(renames=[SelectInput(**c) for c in select])\n        elif isinstance(select, dict):\n            renames = select.get(\"renames\")\n            if renames:\n                return JoinInputs(renames=[SelectInput(**c) for c in renames])\n            return JoinInputs(renames=[])\n        elif all(isinstance(c, str) for c in select):\n            return JoinInputs(renames=[SelectInput(old_name=s, new_name=s) for s in select])\n\n        raise ValueError(f\"Unable to parse select input: {type(select)}\")\n\n    def get_overlapping_columns(self) -&gt; set[str]:\n        \"\"\"Finds column names that would conflict after the join.\"\"\"\n        return self.left_manager.get_new_cols() &amp; self.right_manager.get_new_cols()\n\n    def auto_generate_new_col_name(self, old_col_name: str, side: str) -&gt; str:\n        \"\"\"Generates a new, non-conflicting column name by adding a suffix if necessary.\"\"\"\n        current_names = self.get_overlapping_columns()\n        if old_col_name not in current_names:\n            return old_col_name\n\n        new_name = old_col_name\n        while new_name in current_names:\n            new_name = f\"{side}_{new_name}\"\n        return new_name\n\n    def add_new_select_column(self, select_input: SelectInput, side: str) -&gt; None:\n        \"\"\"Adds a new column to the selection for either the left or right side.\"\"\"\n        target_input = self.input.right_select if side == \"right\" else self.input.left_select\n\n        select_input.new_name = self.auto_generate_new_col_name(select_input.old_name, side=side)\n\n        target_input.renames.append(select_input)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinSelectManagerMixin.add_new_select_column","title":"<code>add_new_select_column(select_input, side)</code>","text":"<p>Adds a new column to the selection for either the left or right side.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def add_new_select_column(self, select_input: SelectInput, side: str) -&gt; None:\n    \"\"\"Adds a new column to the selection for either the left or right side.\"\"\"\n    target_input = self.input.right_select if side == \"right\" else self.input.left_select\n\n    select_input.new_name = self.auto_generate_new_col_name(select_input.old_name, side=side)\n\n    target_input.renames.append(select_input)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinSelectManagerMixin.auto_generate_new_col_name","title":"<code>auto_generate_new_col_name(old_col_name, side)</code>","text":"<p>Generates a new, non-conflicting column name by adding a suffix if necessary.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def auto_generate_new_col_name(self, old_col_name: str, side: str) -&gt; str:\n    \"\"\"Generates a new, non-conflicting column name by adding a suffix if necessary.\"\"\"\n    current_names = self.get_overlapping_columns()\n    if old_col_name not in current_names:\n        return old_col_name\n\n    new_name = old_col_name\n    while new_name in current_names:\n        new_name = f\"{side}_{new_name}\"\n    return new_name\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinSelectManagerMixin.get_overlapping_columns","title":"<code>get_overlapping_columns()</code>","text":"<p>Finds column names that would conflict after the join.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_overlapping_columns(self) -&gt; set[str]:\n    \"\"\"Finds column names that would conflict after the join.\"\"\"\n    return self.left_manager.get_new_cols() &amp; self.right_manager.get_new_cols()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinSelectManagerMixin.parse_select","title":"<code>parse_select(select)</code>  <code>staticmethod</code>","text":"<p>Parses various input formats into a standardized <code>JoinInputs</code> object.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@staticmethod\ndef parse_select(select: list[SelectInput] | list[str] | list[dict] | dict) -&gt; JoinInputs:\n    \"\"\"Parses various input formats into a standardized `JoinInputs` object.\"\"\"\n    if not select:\n        return JoinInputs(renames=[])\n\n    if all(isinstance(c, SelectInput) for c in select):\n        return JoinInputs(renames=select)\n    elif all(isinstance(c, dict) for c in select):\n        return JoinInputs(renames=[SelectInput(**c) for c in select])\n    elif isinstance(select, dict):\n        renames = select.get(\"renames\")\n        if renames:\n            return JoinInputs(renames=[SelectInput(**c) for c in renames])\n        return JoinInputs(renames=[])\n    elif all(isinstance(c, str) for c in select):\n        return JoinInputs(renames=[SelectInput(old_name=s, new_name=s) for s in select])\n\n    raise ValueError(f\"Unable to parse select input: {type(select)}\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.PivotInput","title":"<code>PivotInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the settings for a pivot (long-to-wide) operation.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines the settings for a pivot (long-to-wide) operation.\",\n  \"properties\": {\n    \"index_columns\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Index Columns\",\n      \"type\": \"array\"\n    },\n    \"pivot_column\": {\n      \"title\": \"Pivot Column\",\n      \"type\": \"string\"\n    },\n    \"value_col\": {\n      \"title\": \"Value Col\",\n      \"type\": \"string\"\n    },\n    \"aggregations\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Aggregations\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"index_columns\",\n    \"pivot_column\",\n    \"value_col\",\n    \"aggregations\"\n  ],\n  \"title\": \"PivotInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>index_columns</code>                 (<code>list[str]</code>)             </li> <li> <code>pivot_column</code>                 (<code>str</code>)             </li> <li> <code>value_col</code>                 (<code>str</code>)             </li> <li> <code>aggregations</code>                 (<code>list[str]</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class PivotInput(BaseModel):\n    \"\"\"Defines the settings for a pivot (long-to-wide) operation.\"\"\"\n\n    index_columns: list[str]\n    pivot_column: str\n    value_col: str\n    aggregations: list[str]\n\n    @property\n    def grouped_columns(self) -&gt; list[str]:\n        \"\"\"Returns the list of columns to be used for the initial grouping stage of the pivot.\"\"\"\n        return self.index_columns + [self.pivot_column]\n\n    def get_group_by_input(self) -&gt; GroupByInput:\n        \"\"\"Constructs the `GroupByInput` needed for the pre-aggregation step of the pivot.\"\"\"\n        group_by_cols = [AggColl(old_name=c, agg=\"groupby\") for c in self.grouped_columns]\n        agg_cols = [\n            AggColl(old_name=self.value_col, agg=aggregation, new_name=aggregation) for aggregation in self.aggregations\n        ]\n        return GroupByInput(agg_cols=group_by_cols + agg_cols)\n\n    def get_index_columns(self) -&gt; list[pl.col]:\n        \"\"\"Returns the index columns as Polars column expressions.\"\"\"\n        return [pl.col(c) for c in self.index_columns]\n\n    def get_pivot_column(self) -&gt; pl.Expr:\n        \"\"\"Returns the pivot column as a Polars column expression.\"\"\"\n        return pl.col(self.pivot_column)\n\n    def get_values_expr(self) -&gt; pl.Expr:\n        \"\"\"Creates the struct expression used to gather the values for pivoting.\"\"\"\n        return pl.struct([pl.col(c) for c in self.aggregations]).alias(\"vals\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.PivotInput.grouped_columns","title":"<code>grouped_columns</code>  <code>property</code>","text":"<p>Returns the list of columns to be used for the initial grouping stage of the pivot.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.PivotInput.get_group_by_input","title":"<code>get_group_by_input()</code>","text":"<p>Constructs the <code>GroupByInput</code> needed for the pre-aggregation step of the pivot.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_group_by_input(self) -&gt; GroupByInput:\n    \"\"\"Constructs the `GroupByInput` needed for the pre-aggregation step of the pivot.\"\"\"\n    group_by_cols = [AggColl(old_name=c, agg=\"groupby\") for c in self.grouped_columns]\n    agg_cols = [\n        AggColl(old_name=self.value_col, agg=aggregation, new_name=aggregation) for aggregation in self.aggregations\n    ]\n    return GroupByInput(agg_cols=group_by_cols + agg_cols)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.PivotInput.get_index_columns","title":"<code>get_index_columns()</code>","text":"<p>Returns the index columns as Polars column expressions.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_index_columns(self) -&gt; list[pl.col]:\n    \"\"\"Returns the index columns as Polars column expressions.\"\"\"\n    return [pl.col(c) for c in self.index_columns]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.PivotInput.get_pivot_column","title":"<code>get_pivot_column()</code>","text":"<p>Returns the pivot column as a Polars column expression.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_pivot_column(self) -&gt; pl.Expr:\n    \"\"\"Returns the pivot column as a Polars column expression.\"\"\"\n    return pl.col(self.pivot_column)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.PivotInput.get_values_expr","title":"<code>get_values_expr()</code>","text":"<p>Creates the struct expression used to gather the values for pivoting.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_values_expr(self) -&gt; pl.Expr:\n    \"\"\"Creates the struct expression used to gather the values for pivoting.\"\"\"\n    return pl.struct([pl.col(c) for c in self.aggregations]).alias(\"vals\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.PolarsCodeInput","title":"<code>PolarsCodeInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A simple container for a string of user-provided Polars code to be executed.</p> Show JSON schema: <pre><code>{\n  \"description\": \"A simple container for a string of user-provided Polars code to be executed.\",\n  \"properties\": {\n    \"polars_code\": {\n      \"title\": \"Polars Code\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"polars_code\"\n  ],\n  \"title\": \"PolarsCodeInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>polars_code</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class PolarsCodeInput(BaseModel):\n    \"\"\"A simple container for a string of user-provided Polars code to be executed.\"\"\"\n\n    polars_code: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.RecordIdInput","title":"<code>RecordIdInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for adding a record ID (row number) column to the data.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines settings for adding a record ID (row number) column to the data.\",\n  \"properties\": {\n    \"output_column_name\": {\n      \"default\": \"record_id\",\n      \"title\": \"Output Column Name\",\n      \"type\": \"string\"\n    },\n    \"offset\": {\n      \"default\": 1,\n      \"title\": \"Offset\",\n      \"type\": \"integer\"\n    },\n    \"group_by\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": false,\n      \"title\": \"Group By\"\n    },\n    \"group_by_columns\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Group By Columns\"\n    }\n  },\n  \"title\": \"RecordIdInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>output_column_name</code>                 (<code>str</code>)             </li> <li> <code>offset</code>                 (<code>int</code>)             </li> <li> <code>group_by</code>                 (<code>bool | None</code>)             </li> <li> <code>group_by_columns</code>                 (<code>list[str] | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class RecordIdInput(BaseModel):\n    \"\"\"Defines settings for adding a record ID (row number) column to the data.\"\"\"\n\n    output_column_name: str = \"record_id\"\n    offset: int = 1\n    group_by: bool | None = False\n    group_by_columns: list[str] | None = Field(default_factory=list)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInput","title":"<code>SelectInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines how a single column should be selected, renamed, or type-cast.</p> <p>This is a core building block for any operation that involves column manipulation. It holds all the configuration for a single field in a selection operation.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines how a single column should be selected, renamed, or type-cast.\\n\\nThis is a core building block for any operation that involves column manipulation.\\nIt holds all the configuration for a single field in a selection operation.\",\n  \"properties\": {\n    \"old_name\": {\n      \"title\": \"Old Name\",\n      \"type\": \"string\"\n    },\n    \"original_position\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Original Position\"\n    },\n    \"new_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"New Name\"\n    },\n    \"data_type\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Data Type\"\n    },\n    \"data_type_change\": {\n      \"default\": false,\n      \"title\": \"Data Type Change\",\n      \"type\": \"boolean\"\n    },\n    \"join_key\": {\n      \"default\": false,\n      \"title\": \"Join Key\",\n      \"type\": \"boolean\"\n    },\n    \"is_altered\": {\n      \"default\": false,\n      \"title\": \"Is Altered\",\n      \"type\": \"boolean\"\n    },\n    \"position\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Position\"\n    },\n    \"is_available\": {\n      \"default\": true,\n      \"title\": \"Is Available\",\n      \"type\": \"boolean\"\n    },\n    \"keep\": {\n      \"default\": true,\n      \"title\": \"Keep\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"required\": [\n    \"old_name\"\n  ],\n  \"title\": \"SelectInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>frozen</code>: <code>False</code></li> </ul> <p>Fields:</p> <ul> <li> <code>old_name</code>                 (<code>str</code>)             </li> <li> <code>original_position</code>                 (<code>int | None</code>)             </li> <li> <code>new_name</code>                 (<code>str | None</code>)             </li> <li> <code>data_type</code>                 (<code>str | None</code>)             </li> <li> <code>data_type_change</code>                 (<code>bool</code>)             </li> <li> <code>join_key</code>                 (<code>bool</code>)             </li> <li> <code>is_altered</code>                 (<code>bool</code>)             </li> <li> <code>position</code>                 (<code>int | None</code>)             </li> <li> <code>is_available</code>                 (<code>bool</code>)             </li> <li> <code>keep</code>                 (<code>bool</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>infer_data_type_change</code> </li> <li> <code>set_default_new_name</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class SelectInput(BaseModel):\n    \"\"\"Defines how a single column should be selected, renamed, or type-cast.\n\n    This is a core building block for any operation that involves column manipulation.\n    It holds all the configuration for a single field in a selection operation.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=False)\n\n    old_name: str\n    original_position: int | None = None\n    new_name: str | None = None\n    data_type: str | None = None\n    data_type_change: bool = False\n    join_key: bool = False\n    is_altered: bool = False\n    position: int | None = None\n    is_available: bool = True\n    keep: bool = True\n\n    def __init__(self, old_name: str = None, new_name: str = None, **data):\n        if old_name is not None:\n            data[\"old_name\"] = old_name\n        if new_name is not None:\n            data[\"new_name\"] = new_name\n        super().__init__(**data)\n\n    def to_yaml_dict(self) -&gt; SelectInputYaml:\n        \"\"\"Serialize for YAML output - only user-relevant fields.\"\"\"\n        result: SelectInputYaml = {\"old_name\": self.old_name}\n        if self.new_name != self.old_name:\n            result[\"new_name\"] = self.new_name\n        if not self.keep:\n            result[\"keep\"] = self.keep\n        # Always include data_type if it's set, not just when data_type_change is True\n        # This ensures undo/redo snapshots preserve the data_type field\n        if self.data_type:\n            result[\"data_type\"] = self.data_type\n        return result\n\n    @classmethod\n    def from_yaml_dict(cls, data: dict) -&gt; \"SelectInput\":\n        \"\"\"Load from slim YAML format.\"\"\"\n        old_name = data[\"old_name\"]\n        new_name = data.get(\"new_name\", old_name)\n        data_type = data.get(\"data_type\")\n        # is_altered should be True if either name was changed OR data_type was explicitly set\n        # This ensures updateNodeSelect in the frontend won't overwrite user-specified data_type\n        is_altered = (old_name != new_name) or (data_type is not None)\n        return cls(\n            old_name=old_name,\n            new_name=new_name,\n            keep=data.get(\"keep\", True),\n            data_type=data_type,\n            data_type_change=data_type is not None,\n            is_altered=is_altered,\n        )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def infer_data_type_change(cls, data):\n        \"\"\"Infer data_type_change when loading from YAML.\n\n        When data_type is present but data_type_change is not explicitly set,\n        infer that the user explicitly set the data_type (e.g., when loading from YAML).\n        This ensures is_altered will be set correctly in the after validator.\n        \"\"\"\n        if isinstance(data, dict):\n            if data.get(\"data_type\") is not None and \"data_type_change\" not in data:\n                data[\"data_type_change\"] = True\n        return data\n\n    @model_validator(mode=\"after\")\n    def set_default_new_name(self):\n        \"\"\"If new_name is None, default it to old_name. Also set is_altered if needed.\"\"\"\n        if self.new_name is None:\n            self.new_name = self.old_name\n        if self.old_name != self.new_name:\n            self.is_altered = True\n        if self.data_type_change:\n            self.is_altered = True\n        return self\n\n    def __hash__(self):\n        \"\"\"Allow SelectInput to be used in sets and as dict keys.\"\"\"\n        return hash(self.old_name)\n\n    def __eq__(self, other):\n        \"\"\"Required when implementing __hash__.\"\"\"\n        if not isinstance(other, SelectInput):\n            return False\n        return self.old_name == other.old_name\n\n    @property\n    def polars_type(self) -&gt; str:\n        \"\"\"Translates a user-friendly type name to a Polars data type string.\"\"\"\n        data_type_lower = self.data_type.lower()\n        if data_type_lower == \"string\":\n            return \"Utf8\"\n        elif data_type_lower == \"integer\":\n            return \"Int64\"\n        elif data_type_lower == \"double\":\n            return \"Float64\"\n        return self.data_type\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInput.polars_type","title":"<code>polars_type</code>  <code>property</code>","text":"<p>Translates a user-friendly type name to a Polars data type string.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInput.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Required when implementing hash.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def __eq__(self, other):\n    \"\"\"Required when implementing __hash__.\"\"\"\n    if not isinstance(other, SelectInput):\n        return False\n    return self.old_name == other.old_name\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInput.__hash__","title":"<code>__hash__()</code>","text":"<p>Allow SelectInput to be used in sets and as dict keys.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def __hash__(self):\n    \"\"\"Allow SelectInput to be used in sets and as dict keys.\"\"\"\n    return hash(self.old_name)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInput.from_yaml_dict","title":"<code>from_yaml_dict(data)</code>  <code>classmethod</code>","text":"<p>Load from slim YAML format.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@classmethod\ndef from_yaml_dict(cls, data: dict) -&gt; \"SelectInput\":\n    \"\"\"Load from slim YAML format.\"\"\"\n    old_name = data[\"old_name\"]\n    new_name = data.get(\"new_name\", old_name)\n    data_type = data.get(\"data_type\")\n    # is_altered should be True if either name was changed OR data_type was explicitly set\n    # This ensures updateNodeSelect in the frontend won't overwrite user-specified data_type\n    is_altered = (old_name != new_name) or (data_type is not None)\n    return cls(\n        old_name=old_name,\n        new_name=new_name,\n        keep=data.get(\"keep\", True),\n        data_type=data_type,\n        data_type_change=data_type is not None,\n        is_altered=is_altered,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInput.infer_data_type_change","title":"<code>infer_data_type_change(data)</code>  <code>pydantic-validator</code>","text":"<p>Infer data_type_change when loading from YAML.</p> <p>When data_type is present but data_type_change is not explicitly set, infer that the user explicitly set the data_type (e.g., when loading from YAML). This ensures is_altered will be set correctly in the after validator.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef infer_data_type_change(cls, data):\n    \"\"\"Infer data_type_change when loading from YAML.\n\n    When data_type is present but data_type_change is not explicitly set,\n    infer that the user explicitly set the data_type (e.g., when loading from YAML).\n    This ensures is_altered will be set correctly in the after validator.\n    \"\"\"\n    if isinstance(data, dict):\n        if data.get(\"data_type\") is not None and \"data_type_change\" not in data:\n            data[\"data_type_change\"] = True\n    return data\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInput.set_default_new_name","title":"<code>set_default_new_name()</code>  <code>pydantic-validator</code>","text":"<p>If new_name is None, default it to old_name. Also set is_altered if needed.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@model_validator(mode=\"after\")\ndef set_default_new_name(self):\n    \"\"\"If new_name is None, default it to old_name. Also set is_altered if needed.\"\"\"\n    if self.new_name is None:\n        self.new_name = self.old_name\n    if self.old_name != self.new_name:\n        self.is_altered = True\n    if self.data_type_change:\n        self.is_altered = True\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInput.to_yaml_dict","title":"<code>to_yaml_dict()</code>","text":"<p>Serialize for YAML output - only user-relevant fields.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def to_yaml_dict(self) -&gt; SelectInputYaml:\n    \"\"\"Serialize for YAML output - only user-relevant fields.\"\"\"\n    result: SelectInputYaml = {\"old_name\": self.old_name}\n    if self.new_name != self.old_name:\n        result[\"new_name\"] = self.new_name\n    if not self.keep:\n        result[\"keep\"] = self.keep\n    # Always include data_type if it's set, not just when data_type_change is True\n    # This ensures undo/redo snapshots preserve the data_type field\n    if self.data_type:\n        result[\"data_type\"] = self.data_type\n    return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs","title":"<code>SelectInputs</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A container for a list of <code>SelectInput</code> objects (pure data, no logic).</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"SelectInput\": {\n      \"description\": \"Defines how a single column should be selected, renamed, or type-cast.\\n\\nThis is a core building block for any operation that involves column manipulation.\\nIt holds all the configuration for a single field in a selection operation.\",\n      \"properties\": {\n        \"old_name\": {\n          \"title\": \"Old Name\",\n          \"type\": \"string\"\n        },\n        \"original_position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Original Position\"\n        },\n        \"new_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"New Name\"\n        },\n        \"data_type\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Data Type\"\n        },\n        \"data_type_change\": {\n          \"default\": false,\n          \"title\": \"Data Type Change\",\n          \"type\": \"boolean\"\n        },\n        \"join_key\": {\n          \"default\": false,\n          \"title\": \"Join Key\",\n          \"type\": \"boolean\"\n        },\n        \"is_altered\": {\n          \"default\": false,\n          \"title\": \"Is Altered\",\n          \"type\": \"boolean\"\n        },\n        \"position\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Position\"\n        },\n        \"is_available\": {\n          \"default\": true,\n          \"title\": \"Is Available\",\n          \"type\": \"boolean\"\n        },\n        \"keep\": {\n          \"default\": true,\n          \"title\": \"Keep\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"old_name\"\n      ],\n      \"title\": \"SelectInput\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"A container for a list of `SelectInput` objects (pure data, no logic).\",\n  \"properties\": {\n    \"renames\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/SelectInput\"\n      },\n      \"title\": \"Renames\",\n      \"type\": \"array\"\n    }\n  },\n  \"title\": \"SelectInputs\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>renames</code>                 (<code>list[SelectInput]</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class SelectInputs(BaseModel):\n    \"\"\"A container for a list of `SelectInput` objects (pure data, no logic).\"\"\"\n\n    renames: list[SelectInput] = Field(default_factory=list)\n\n    def __init__(self, renames: list[SelectInput] = None, **kwargs):\n        if renames is not None:\n            kwargs[\"renames\"] = renames\n        else:\n            kwargs[\"renames\"] = []\n        super().__init__(**kwargs)\n\n    def to_yaml_dict(self) -&gt; JoinInputsYaml:\n        \"\"\"Serialize for YAML output.\"\"\"\n        return {\"select\": [r.to_yaml_dict() for r in self.renames]}\n\n    @classmethod\n    def from_yaml_dict(cls, data: dict) -&gt; \"SelectInputs\":\n        \"\"\"Load from slim YAML format. Supports both 'select' (new) and 'renames' (internal).\"\"\"\n        items = data.get(\"select\", data.get(\"renames\", []))\n        return cls(renames=[SelectInput.from_yaml_dict(item) for item in items])\n\n    @classmethod\n    def create_from_list(cls, col_list: list[str]) -&gt; \"SelectInputs\":\n        \"\"\"Creates a SelectInputs object from a simple list of column names.\"\"\"\n        return cls(renames=[SelectInput(old_name=c) for c in col_list])\n\n    @classmethod\n    def create_from_pl_df(cls, df: pl.DataFrame | pl.LazyFrame) -&gt; \"SelectInputs\":\n        \"\"\"Creates a SelectInputs object from a Polars DataFrame's columns.\"\"\"\n        return cls(renames=[SelectInput(old_name=c) for c in df.columns])\n\n    def remove_select_input(self, old_key: str) -&gt; None:\n        \"\"\"Removes a SelectInput from the list based on its original name.\"\"\"\n        self.renames = [rename for rename in self.renames if rename.old_name != old_key]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.create_from_list","title":"<code>create_from_list(col_list)</code>  <code>classmethod</code>","text":"<p>Creates a SelectInputs object from a simple list of column names.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@classmethod\ndef create_from_list(cls, col_list: list[str]) -&gt; \"SelectInputs\":\n    \"\"\"Creates a SelectInputs object from a simple list of column names.\"\"\"\n    return cls(renames=[SelectInput(old_name=c) for c in col_list])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.create_from_pl_df","title":"<code>create_from_pl_df(df)</code>  <code>classmethod</code>","text":"<p>Creates a SelectInputs object from a Polars DataFrame's columns.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@classmethod\ndef create_from_pl_df(cls, df: pl.DataFrame | pl.LazyFrame) -&gt; \"SelectInputs\":\n    \"\"\"Creates a SelectInputs object from a Polars DataFrame's columns.\"\"\"\n    return cls(renames=[SelectInput(old_name=c) for c in df.columns])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.from_yaml_dict","title":"<code>from_yaml_dict(data)</code>  <code>classmethod</code>","text":"<p>Load from slim YAML format. Supports both 'select' (new) and 'renames' (internal).</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@classmethod\ndef from_yaml_dict(cls, data: dict) -&gt; \"SelectInputs\":\n    \"\"\"Load from slim YAML format. Supports both 'select' (new) and 'renames' (internal).\"\"\"\n    items = data.get(\"select\", data.get(\"renames\", []))\n    return cls(renames=[SelectInput.from_yaml_dict(item) for item in items])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.remove_select_input","title":"<code>remove_select_input(old_key)</code>","text":"<p>Removes a SelectInput from the list based on its original name.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def remove_select_input(self, old_key: str) -&gt; None:\n    \"\"\"Removes a SelectInput from the list based on its original name.\"\"\"\n    self.renames = [rename for rename in self.renames if rename.old_name != old_key]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.to_yaml_dict","title":"<code>to_yaml_dict()</code>","text":"<p>Serialize for YAML output.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def to_yaml_dict(self) -&gt; JoinInputsYaml:\n    \"\"\"Serialize for YAML output.\"\"\"\n    return {\"select\": [r.to_yaml_dict() for r in self.renames]}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager","title":"<code>SelectInputsManager</code>","text":"<p>Manager class that provides all query and mutation operations.</p> <p>Methods:</p> Name Description <code>__add__</code> <p>Backward compatibility: Support += operator for appending.</p> <code>append</code> <p>Appends a new SelectInput to the list of renames.</p> <code>find_by_new_name</code> <p>Find SelectInput by new column name.</p> <code>find_by_old_name</code> <p>Find SelectInput by original column name.</p> <code>get_drop_columns</code> <p>Returns a list of SelectInput objects that are marked to be dropped.</p> <code>get_new_cols</code> <p>Returns a set of new (renamed) column names to be kept in the selection.</p> <code>get_non_jk_drop_columns</code> <p>Returns drop columns that are not join keys.</p> <code>get_old_cols</code> <p>Returns a set of original column names to be kept in the selection.</p> <code>get_rename_table</code> <p>Generates a dictionary for use in Polars' <code>.rename()</code> method.</p> <code>get_select_cols</code> <p>Gets a list of original column names to select from the source DataFrame.</p> <code>get_select_input_on_new_name</code> <p>Backward compatibility alias: Find SelectInput by new column name.</p> <code>get_select_input_on_old_name</code> <p>Backward compatibility alias: Find SelectInput by original column name.</p> <code>has_drop_cols</code> <p>Checks if any column is marked to be dropped from the selection.</p> <code>remove_select_input</code> <p>Removes a SelectInput from the list based on its original name.</p> <code>unselect_field</code> <p>Marks a field to be dropped from the final selection by setting <code>keep</code> to False.</p> <p>Attributes:</p> Name Type Description <code>drop_columns</code> <code>list[SelectInput]</code> <p>Backward compatibility: Returns list of columns to drop.</p> <code>new_cols</code> <code>set[str]</code> <p>Backward compatibility: Returns set of new column names.</p> <code>non_jk_drop_columns</code> <code>list[SelectInput]</code> <p>Backward compatibility: Returns non-join-key columns to drop.</p> <code>old_cols</code> <code>set[str]</code> <p>Backward compatibility: Returns set of old column names.</p> <code>rename_table</code> <code>dict[str, str]</code> <p>Backward compatibility: Returns rename table dictionary.</p> <code>renames</code> <code>list[SelectInput]</code> <p>Backward compatibility: Direct access to renames list.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class SelectInputsManager:\n    \"\"\"Manager class that provides all query and mutation operations.\"\"\"\n\n    def __init__(self, select_inputs: SelectInputs):\n        self.select_inputs = select_inputs\n\n    # === Query Methods (read-only) ===\n\n    def get_old_cols(self) -&gt; set[str]:\n        \"\"\"Returns a set of original column names to be kept in the selection.\"\"\"\n        return set(v.old_name for v in self.select_inputs.renames if v.keep)\n\n    def get_new_cols(self) -&gt; set[str]:\n        \"\"\"Returns a set of new (renamed) column names to be kept in the selection.\"\"\"\n        return set(v.new_name for v in self.select_inputs.renames if v.keep)\n\n    def get_rename_table(self) -&gt; dict[str, str]:\n        \"\"\"Generates a dictionary for use in Polars' `.rename()` method.\"\"\"\n        return {v.old_name: v.new_name for v in self.select_inputs.renames if v.is_available and (v.keep or v.join_key)}\n\n    def get_select_cols(self, include_join_key: bool = True) -&gt; list[str]:\n        \"\"\"Gets a list of original column names to select from the source DataFrame.\"\"\"\n        return [v.old_name for v in self.select_inputs.renames if v.keep or (v.join_key and include_join_key)]\n\n    def has_drop_cols(self) -&gt; bool:\n        \"\"\"Checks if any column is marked to be dropped from the selection.\"\"\"\n        return any(not v.keep for v in self.select_inputs.renames)\n\n    def get_drop_columns(self) -&gt; list[SelectInput]:\n        \"\"\"Returns a list of SelectInput objects that are marked to be dropped.\"\"\"\n        return [v for v in self.select_inputs.renames if not v.keep and v.is_available]\n\n    def get_non_jk_drop_columns(self) -&gt; list[SelectInput]:\n        \"\"\"Returns drop columns that are not join keys.\"\"\"\n        return [v for v in self.select_inputs.renames if not v.keep and v.is_available and not v.join_key]\n\n    def find_by_old_name(self, old_name: str) -&gt; SelectInput | None:\n        \"\"\"Find SelectInput by original column name.\"\"\"\n        return next((v for v in self.select_inputs.renames if v.old_name == old_name), None)\n\n    def find_by_new_name(self, new_name: str) -&gt; SelectInput | None:\n        \"\"\"Find SelectInput by new column name.\"\"\"\n        return next((v for v in self.select_inputs.renames if v.new_name == new_name), None)\n\n    # === Mutation Methods ===\n\n    def append(self, other: SelectInput) -&gt; None:\n        \"\"\"Appends a new SelectInput to the list of renames.\"\"\"\n        self.select_inputs.renames.append(other)\n\n    def remove_select_input(self, old_key: str) -&gt; None:\n        \"\"\"Removes a SelectInput from the list based on its original name.\"\"\"\n        self.select_inputs.renames = [rename for rename in self.select_inputs.renames if rename.old_name != old_key]\n\n    def unselect_field(self, old_key: str) -&gt; None:\n        \"\"\"Marks a field to be dropped from the final selection by setting `keep` to False.\"\"\"\n        for rename in self.select_inputs.renames:\n            if old_key == rename.old_name:\n                rename.keep = False\n\n    # === Backward Compatibility Properties ===\n\n    @property\n    def old_cols(self) -&gt; set[str]:\n        \"\"\"Backward compatibility: Returns set of old column names.\"\"\"\n        return self.get_old_cols()\n\n    @property\n    def new_cols(self) -&gt; set[str]:\n        \"\"\"Backward compatibility: Returns set of new column names.\"\"\"\n        return self.get_new_cols()\n\n    @property\n    def rename_table(self) -&gt; dict[str, str]:\n        \"\"\"Backward compatibility: Returns rename table dictionary.\"\"\"\n        return self.get_rename_table()\n\n    @property\n    def drop_columns(self) -&gt; list[SelectInput]:\n        \"\"\"Backward compatibility: Returns list of columns to drop.\"\"\"\n        return self.get_drop_columns()\n\n    @property\n    def non_jk_drop_columns(self) -&gt; list[SelectInput]:\n        \"\"\"Backward compatibility: Returns non-join-key columns to drop.\"\"\"\n        return self.get_non_jk_drop_columns()\n\n    @property\n    def renames(self) -&gt; list[SelectInput]:\n        \"\"\"Backward compatibility: Direct access to renames list.\"\"\"\n        return self.select_inputs.renames\n\n    def get_select_input_on_old_name(self, old_name: str) -&gt; SelectInput | None:\n        \"\"\"Backward compatibility alias: Find SelectInput by original column name.\"\"\"\n        return self.find_by_old_name(old_name)\n\n    def get_select_input_on_new_name(self, new_name: str) -&gt; SelectInput | None:\n        \"\"\"Backward compatibility alias: Find SelectInput by new column name.\"\"\"\n        return self.find_by_new_name(new_name)\n\n    def __add__(self, other: SelectInput) -&gt; \"SelectInputsManager\":\n        \"\"\"Backward compatibility: Support += operator for appending.\"\"\"\n        self.append(other)\n        return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.drop_columns","title":"<code>drop_columns</code>  <code>property</code>","text":"<p>Backward compatibility: Returns list of columns to drop.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.new_cols","title":"<code>new_cols</code>  <code>property</code>","text":"<p>Backward compatibility: Returns set of new column names.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.non_jk_drop_columns","title":"<code>non_jk_drop_columns</code>  <code>property</code>","text":"<p>Backward compatibility: Returns non-join-key columns to drop.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.old_cols","title":"<code>old_cols</code>  <code>property</code>","text":"<p>Backward compatibility: Returns set of old column names.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.rename_table","title":"<code>rename_table</code>  <code>property</code>","text":"<p>Backward compatibility: Returns rename table dictionary.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.renames","title":"<code>renames</code>  <code>property</code>","text":"<p>Backward compatibility: Direct access to renames list.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.__add__","title":"<code>__add__(other)</code>","text":"<p>Backward compatibility: Support += operator for appending.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def __add__(self, other: SelectInput) -&gt; \"SelectInputsManager\":\n    \"\"\"Backward compatibility: Support += operator for appending.\"\"\"\n    self.append(other)\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.append","title":"<code>append(other)</code>","text":"<p>Appends a new SelectInput to the list of renames.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def append(self, other: SelectInput) -&gt; None:\n    \"\"\"Appends a new SelectInput to the list of renames.\"\"\"\n    self.select_inputs.renames.append(other)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.find_by_new_name","title":"<code>find_by_new_name(new_name)</code>","text":"<p>Find SelectInput by new column name.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def find_by_new_name(self, new_name: str) -&gt; SelectInput | None:\n    \"\"\"Find SelectInput by new column name.\"\"\"\n    return next((v for v in self.select_inputs.renames if v.new_name == new_name), None)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.find_by_old_name","title":"<code>find_by_old_name(old_name)</code>","text":"<p>Find SelectInput by original column name.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def find_by_old_name(self, old_name: str) -&gt; SelectInput | None:\n    \"\"\"Find SelectInput by original column name.\"\"\"\n    return next((v for v in self.select_inputs.renames if v.old_name == old_name), None)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.get_drop_columns","title":"<code>get_drop_columns()</code>","text":"<p>Returns a list of SelectInput objects that are marked to be dropped.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_drop_columns(self) -&gt; list[SelectInput]:\n    \"\"\"Returns a list of SelectInput objects that are marked to be dropped.\"\"\"\n    return [v for v in self.select_inputs.renames if not v.keep and v.is_available]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.get_new_cols","title":"<code>get_new_cols()</code>","text":"<p>Returns a set of new (renamed) column names to be kept in the selection.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_new_cols(self) -&gt; set[str]:\n    \"\"\"Returns a set of new (renamed) column names to be kept in the selection.\"\"\"\n    return set(v.new_name for v in self.select_inputs.renames if v.keep)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.get_non_jk_drop_columns","title":"<code>get_non_jk_drop_columns()</code>","text":"<p>Returns drop columns that are not join keys.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_non_jk_drop_columns(self) -&gt; list[SelectInput]:\n    \"\"\"Returns drop columns that are not join keys.\"\"\"\n    return [v for v in self.select_inputs.renames if not v.keep and v.is_available and not v.join_key]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.get_old_cols","title":"<code>get_old_cols()</code>","text":"<p>Returns a set of original column names to be kept in the selection.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_old_cols(self) -&gt; set[str]:\n    \"\"\"Returns a set of original column names to be kept in the selection.\"\"\"\n    return set(v.old_name for v in self.select_inputs.renames if v.keep)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.get_rename_table","title":"<code>get_rename_table()</code>","text":"<p>Generates a dictionary for use in Polars' <code>.rename()</code> method.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_rename_table(self) -&gt; dict[str, str]:\n    \"\"\"Generates a dictionary for use in Polars' `.rename()` method.\"\"\"\n    return {v.old_name: v.new_name for v in self.select_inputs.renames if v.is_available and (v.keep or v.join_key)}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.get_select_cols","title":"<code>get_select_cols(include_join_key=True)</code>","text":"<p>Gets a list of original column names to select from the source DataFrame.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_select_cols(self, include_join_key: bool = True) -&gt; list[str]:\n    \"\"\"Gets a list of original column names to select from the source DataFrame.\"\"\"\n    return [v.old_name for v in self.select_inputs.renames if v.keep or (v.join_key and include_join_key)]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.get_select_input_on_new_name","title":"<code>get_select_input_on_new_name(new_name)</code>","text":"<p>Backward compatibility alias: Find SelectInput by new column name.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_select_input_on_new_name(self, new_name: str) -&gt; SelectInput | None:\n    \"\"\"Backward compatibility alias: Find SelectInput by new column name.\"\"\"\n    return self.find_by_new_name(new_name)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.get_select_input_on_old_name","title":"<code>get_select_input_on_old_name(old_name)</code>","text":"<p>Backward compatibility alias: Find SelectInput by original column name.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_select_input_on_old_name(self, old_name: str) -&gt; SelectInput | None:\n    \"\"\"Backward compatibility alias: Find SelectInput by original column name.\"\"\"\n    return self.find_by_old_name(old_name)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.has_drop_cols","title":"<code>has_drop_cols()</code>","text":"<p>Checks if any column is marked to be dropped from the selection.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def has_drop_cols(self) -&gt; bool:\n    \"\"\"Checks if any column is marked to be dropped from the selection.\"\"\"\n    return any(not v.keep for v in self.select_inputs.renames)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.remove_select_input","title":"<code>remove_select_input(old_key)</code>","text":"<p>Removes a SelectInput from the list based on its original name.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def remove_select_input(self, old_key: str) -&gt; None:\n    \"\"\"Removes a SelectInput from the list based on its original name.\"\"\"\n    self.select_inputs.renames = [rename for rename in self.select_inputs.renames if rename.old_name != old_key]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputsManager.unselect_field","title":"<code>unselect_field(old_key)</code>","text":"<p>Marks a field to be dropped from the final selection by setting <code>keep</code> to False.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def unselect_field(self, old_key: str) -&gt; None:\n    \"\"\"Marks a field to be dropped from the final selection by setting `keep` to False.\"\"\"\n    for rename in self.select_inputs.renames:\n        if old_key == rename.old_name:\n            rename.keep = False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SortByInput","title":"<code>SortByInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines a single sort condition on a column, including the direction.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines a single sort condition on a column, including the direction.\",\n  \"properties\": {\n    \"column\": {\n      \"title\": \"Column\",\n      \"type\": \"string\"\n    },\n    \"how\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"asc\",\n      \"title\": \"How\"\n    }\n  },\n  \"required\": [\n    \"column\"\n  ],\n  \"title\": \"SortByInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>column</code>                 (<code>str</code>)             </li> <li> <code>how</code>                 (<code>str | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class SortByInput(BaseModel):\n    \"\"\"Defines a single sort condition on a column, including the direction.\"\"\"\n\n    column: str\n    how: str | None = \"asc\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.TextToRowsInput","title":"<code>TextToRowsInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for splitting a text column into multiple rows based on a delimiter.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines settings for splitting a text column into multiple rows based on a delimiter.\",\n  \"properties\": {\n    \"column_to_split\": {\n      \"title\": \"Column To Split\",\n      \"type\": \"string\"\n    },\n    \"output_column_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Output Column Name\"\n    },\n    \"split_by_fixed_value\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Split By Fixed Value\"\n    },\n    \"split_fixed_value\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \",\",\n      \"title\": \"Split Fixed Value\"\n    },\n    \"split_by_column\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Split By Column\"\n    }\n  },\n  \"required\": [\n    \"column_to_split\"\n  ],\n  \"title\": \"TextToRowsInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>column_to_split</code>                 (<code>str</code>)             </li> <li> <code>output_column_name</code>                 (<code>str | None</code>)             </li> <li> <code>split_by_fixed_value</code>                 (<code>bool | None</code>)             </li> <li> <code>split_fixed_value</code>                 (<code>str | None</code>)             </li> <li> <code>split_by_column</code>                 (<code>str | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class TextToRowsInput(BaseModel):\n    \"\"\"Defines settings for splitting a text column into multiple rows based on a delimiter.\"\"\"\n\n    column_to_split: str\n    output_column_name: str | None = None\n    split_by_fixed_value: bool | None = True\n    split_fixed_value: str | None = \",\"\n    split_by_column: str | None = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.UnionInput","title":"<code>UnionInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for a union (concatenation) operation.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines settings for a union (concatenation) operation.\",\n  \"properties\": {\n    \"mode\": {\n      \"default\": \"relaxed\",\n      \"enum\": [\n        \"selective\",\n        \"relaxed\"\n      ],\n      \"title\": \"Mode\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"UnionInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>mode</code>                 (<code>Literal['selective', 'relaxed']</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class UnionInput(BaseModel):\n    \"\"\"Defines settings for a union (concatenation) operation.\"\"\"\n\n    mode: Literal[\"selective\", \"relaxed\"] = \"relaxed\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.UniqueInput","title":"<code>UniqueInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for a uniqueness operation, specifying columns and which row to keep.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines settings for a uniqueness operation, specifying columns and which row to keep.\",\n  \"properties\": {\n    \"columns\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Columns\"\n    },\n    \"strategy\": {\n      \"default\": \"any\",\n      \"enum\": [\n        \"first\",\n        \"last\",\n        \"any\",\n        \"none\"\n      ],\n      \"title\": \"Strategy\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"UniqueInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>columns</code>                 (<code>list[str] | None</code>)             </li> <li> <code>strategy</code>                 (<code>Literal['first', 'last', 'any', 'none']</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class UniqueInput(BaseModel):\n    \"\"\"Defines settings for a uniqueness operation, specifying columns and which row to keep.\"\"\"\n\n    columns: list[str] | None = None\n    strategy: Literal[\"first\", \"last\", \"any\", \"none\"] = \"any\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.UnpivotInput","title":"<code>UnpivotInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for an unpivot (wide-to-long) operation.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Defines settings for an unpivot (wide-to-long) operation.\",\n  \"properties\": {\n    \"index_columns\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Index Columns\",\n      \"type\": \"array\"\n    },\n    \"value_columns\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Value Columns\",\n      \"type\": \"array\"\n    },\n    \"data_type_selector\": {\n      \"anyOf\": [\n        {\n          \"enum\": [\n            \"float\",\n            \"all\",\n            \"date\",\n            \"numeric\",\n            \"string\"\n          ],\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Data Type Selector\"\n    },\n    \"data_type_selector_mode\": {\n      \"default\": \"column\",\n      \"enum\": [\n        \"data_type\",\n        \"column\"\n      ],\n      \"title\": \"Data Type Selector Mode\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"UnpivotInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>index_columns</code>                 (<code>list[str]</code>)             </li> <li> <code>value_columns</code>                 (<code>list[str]</code>)             </li> <li> <code>data_type_selector</code>                 (<code>Literal['float', 'all', 'date', 'numeric', 'string'] | None</code>)             </li> <li> <code>data_type_selector_mode</code>                 (<code>Literal['data_type', 'column']</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class UnpivotInput(BaseModel):\n    \"\"\"Defines settings for an unpivot (wide-to-long) operation.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    index_columns: list[str] = Field(default_factory=list)\n    value_columns: list[str] = Field(default_factory=list)\n    data_type_selector: Literal[\"float\", \"all\", \"date\", \"numeric\", \"string\"] | None = None\n    data_type_selector_mode: Literal[\"data_type\", \"column\"] = \"column\"\n\n    @property\n    def data_type_selector_expr(self) -&gt; Callable | None:\n        \"\"\"Returns a Polars selector function based on the `data_type_selector` string.\"\"\"\n        if self.data_type_selector_mode == \"data_type\":\n            if self.data_type_selector is not None:\n                try:\n                    return getattr(selectors, self.data_type_selector)\n                except Exception:\n                    print(f\"Could not find the selector: {self.data_type_selector}\")\n                    return selectors.all\n            return selectors.all\n        return None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.UnpivotInput.data_type_selector_expr","title":"<code>data_type_selector_expr</code>  <code>property</code>","text":"<p>Returns a Polars selector function based on the <code>data_type_selector</code> string.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.construct_join_key_name","title":"<code>construct_join_key_name(side, column_name)</code>","text":"<p>Creates a temporary, unique name for a join key column.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def construct_join_key_name(side: SideLit, column_name: str) -&gt; str:\n    \"\"\"Creates a temporary, unique name for a join key column.\"\"\"\n    return \"_FLOWFILE_JOIN_KEY_\" + side.upper() + \"_\" + column_name\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.get_func_type_mapping","title":"<code>get_func_type_mapping(func)</code>","text":"<p>Infers the output data type of common aggregation functions.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_func_type_mapping(func: str):\n    \"\"\"Infers the output data type of common aggregation functions.\"\"\"\n    if func in [\"mean\", \"avg\", \"median\", \"std\", \"var\"]:\n        return \"Float64\"\n    elif func in [\"min\", \"max\", \"first\", \"last\", \"cumsum\", \"sum\"]:\n        return None\n    elif func in [\"count\", \"n_unique\"]:\n        return \"Int64\"\n    elif func in [\"concat\"]:\n        return \"Utf8\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.string_concat","title":"<code>string_concat(*column)</code>","text":"<p>A simple wrapper to concatenate string columns in Polars.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def string_concat(*column: str):\n    \"\"\"A simple wrapper to concatenate string columns in Polars.\"\"\"\n    return pl.col(column).cast(pl.Utf8).str.concat(delimiter=\",\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#cloud_storage_schemas","title":"<code>cloud_storage_schemas</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas","title":"<code>flowfile_core.schemas.cloud_storage_schemas</code>","text":"<p>Cloud storage connection schemas for S3, ADLS, and other cloud providers.</p> <p>Classes:</p> Name Description <code>AuthSettingsInput</code> <p>The information needed for the user to provide the details that are needed to provide how to connect to the</p> <code>CloudStorageReadSettings</code> <p>Settings for reading from cloud storage</p> <code>CloudStorageSettings</code> <p>Settings for cloud storage nodes in the visual designer</p> <code>CloudStorageWriteSettings</code> <p>Settings for writing to cloud storage</p> <code>CloudStorageWriteSettingsWorkerInterface</code> <p>Settings for writing to cloud storage in worker context</p> <code>FullCloudStorageConnection</code> <p>Internal model with decrypted secrets</p> <code>FullCloudStorageConnectionInterface</code> <p>API response model - no secrets exposed</p> <code>FullCloudStorageConnectionWorkerInterface</code> <p>Internal model with decrypted secrets</p> <code>WriteSettingsWorkerInterface</code> <p>Settings for writing to cloud storage</p> <p>Functions:</p> Name Description <code>encrypt_for_worker</code> <p>Encrypts a secret value for use in worker contexts using per-user key derivation.</p> <code>get_cloud_storage_write_settings_worker_interface</code> <p>Convert to a worker interface model with encrypted secrets.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.AuthSettingsInput","title":"<code>AuthSettingsInput</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The information needed for the user to provide the details that are needed to provide how to connect to the  Cloud provider</p> Show JSON schema: <pre><code>{\n  \"description\": \"The information needed for the user to provide the details that are needed to provide how to connect to the\\n Cloud provider\",\n  \"properties\": {\n    \"storage_type\": {\n      \"enum\": [\n        \"s3\",\n        \"adls\",\n        \"gcs\"\n      ],\n      \"title\": \"Storage Type\",\n      \"type\": \"string\"\n    },\n    \"auth_method\": {\n      \"enum\": [\n        \"access_key\",\n        \"iam_role\",\n        \"service_principal\",\n        \"managed_identity\",\n        \"sas_token\",\n        \"aws-cli\",\n        \"env_vars\"\n      ],\n      \"title\": \"Auth Method\",\n      \"type\": \"string\"\n    },\n    \"connection_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"None\",\n      \"title\": \"Connection Name\"\n    }\n  },\n  \"required\": [\n    \"storage_type\",\n    \"auth_method\"\n  ],\n  \"title\": \"AuthSettingsInput\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>storage_type</code>                 (<code>CloudStorageType</code>)             </li> <li> <code>auth_method</code>                 (<code>AuthMethod</code>)             </li> <li> <code>connection_name</code>                 (<code>str | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class AuthSettingsInput(BaseModel):\n    \"\"\"\n    The information needed for the user to provide the details that are needed to provide how to connect to the\n     Cloud provider\n    \"\"\"\n\n    storage_type: CloudStorageType\n    auth_method: AuthMethod\n    connection_name: str | None = \"None\"  # This is the reference to the item we will fetch that contains the data\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.CloudStorageReadSettings","title":"<code>CloudStorageReadSettings</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>CloudStorageSettings</code></p> <p>Settings for reading from cloud storage</p> Show JSON schema: <pre><code>{\n  \"description\": \"Settings for reading from cloud storage\",\n  \"properties\": {\n    \"auth_mode\": {\n      \"default\": \"auto\",\n      \"enum\": [\n        \"access_key\",\n        \"iam_role\",\n        \"service_principal\",\n        \"managed_identity\",\n        \"sas_token\",\n        \"aws-cli\",\n        \"env_vars\"\n      ],\n      \"title\": \"Auth Mode\",\n      \"type\": \"string\"\n    },\n    \"connection_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Connection Name\"\n    },\n    \"resource_path\": {\n      \"title\": \"Resource Path\",\n      \"type\": \"string\"\n    },\n    \"scan_mode\": {\n      \"default\": \"single_file\",\n      \"enum\": [\n        \"single_file\",\n        \"directory\"\n      ],\n      \"title\": \"Scan Mode\",\n      \"type\": \"string\"\n    },\n    \"file_format\": {\n      \"default\": \"parquet\",\n      \"enum\": [\n        \"csv\",\n        \"parquet\",\n        \"json\",\n        \"delta\",\n        \"iceberg\"\n      ],\n      \"title\": \"File Format\",\n      \"type\": \"string\"\n    },\n    \"csv_has_header\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": true,\n      \"title\": \"Csv Has Header\"\n    },\n    \"csv_delimiter\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \",\",\n      \"title\": \"Csv Delimiter\"\n    },\n    \"csv_encoding\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"utf8\",\n      \"title\": \"Csv Encoding\"\n    },\n    \"delta_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Delta Version\"\n    }\n  },\n  \"required\": [\n    \"resource_path\"\n  ],\n  \"title\": \"CloudStorageReadSettings\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>auth_mode</code>                 (<code>AuthMethod</code>)             </li> <li> <code>connection_name</code>                 (<code>str | None</code>)             </li> <li> <code>resource_path</code>                 (<code>str</code>)             </li> <li> <code>scan_mode</code>                 (<code>Literal['single_file', 'directory']</code>)             </li> <li> <code>file_format</code>                 (<code>Literal['csv', 'parquet', 'json', 'delta', 'iceberg']</code>)             </li> <li> <code>csv_has_header</code>                 (<code>bool | None</code>)             </li> <li> <code>csv_delimiter</code>                 (<code>str | None</code>)             </li> <li> <code>csv_encoding</code>                 (<code>str | None</code>)             </li> <li> <code>delta_version</code>                 (<code>int | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_auth_requirements</code>                 \u2192                   <code>auth_mode</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class CloudStorageReadSettings(CloudStorageSettings):\n    \"\"\"Settings for reading from cloud storage\"\"\"\n\n    scan_mode: Literal[\"single_file\", \"directory\"] = \"single_file\"\n    file_format: Literal[\"csv\", \"parquet\", \"json\", \"delta\", \"iceberg\"] = \"parquet\"\n    csv_has_header: bool | None = True\n    csv_delimiter: str | None = \",\"\n    csv_encoding: str | None = \"utf8\"\n    delta_version: int | None = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.CloudStorageSettings","title":"<code>CloudStorageSettings</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Settings for cloud storage nodes in the visual designer</p> Show JSON schema: <pre><code>{\n  \"description\": \"Settings for cloud storage nodes in the visual designer\",\n  \"properties\": {\n    \"auth_mode\": {\n      \"default\": \"auto\",\n      \"enum\": [\n        \"access_key\",\n        \"iam_role\",\n        \"service_principal\",\n        \"managed_identity\",\n        \"sas_token\",\n        \"aws-cli\",\n        \"env_vars\"\n      ],\n      \"title\": \"Auth Mode\",\n      \"type\": \"string\"\n    },\n    \"connection_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Connection Name\"\n    },\n    \"resource_path\": {\n      \"title\": \"Resource Path\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"resource_path\"\n  ],\n  \"title\": \"CloudStorageSettings\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>auth_mode</code>                 (<code>AuthMethod</code>)             </li> <li> <code>connection_name</code>                 (<code>str | None</code>)             </li> <li> <code>resource_path</code>                 (<code>str</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_auth_requirements</code>                 \u2192                   <code>auth_mode</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class CloudStorageSettings(BaseModel):\n    \"\"\"Settings for cloud storage nodes in the visual designer\"\"\"\n\n    auth_mode: AuthMethod = \"auto\"\n    connection_name: str | None = None  # Required only for 'reference' mode\n    resource_path: str  # s3://bucket/path/to/file.csv\n\n    @field_validator(\"auth_mode\", mode=\"after\")\n    def validate_auth_requirements(cls, v, values):\n        data = values.data\n        if v == \"reference\" and not data.get(\"connection_name\"):\n            raise ValueError(\"connection_name required when using reference mode\")\n        return v\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.CloudStorageWriteSettings","title":"<code>CloudStorageWriteSettings</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>CloudStorageSettings</code>, <code>WriteSettingsWorkerInterface</code></p> <p>Settings for writing to cloud storage</p> Show JSON schema: <pre><code>{\n  \"description\": \"Settings for writing to cloud storage\",\n  \"properties\": {\n    \"resource_path\": {\n      \"title\": \"Resource Path\",\n      \"type\": \"string\"\n    },\n    \"write_mode\": {\n      \"default\": \"overwrite\",\n      \"enum\": [\n        \"overwrite\",\n        \"append\"\n      ],\n      \"title\": \"Write Mode\",\n      \"type\": \"string\"\n    },\n    \"file_format\": {\n      \"default\": \"parquet\",\n      \"enum\": [\n        \"csv\",\n        \"parquet\",\n        \"json\",\n        \"delta\"\n      ],\n      \"title\": \"File Format\",\n      \"type\": \"string\"\n    },\n    \"parquet_compression\": {\n      \"default\": \"snappy\",\n      \"enum\": [\n        \"snappy\",\n        \"gzip\",\n        \"brotli\",\n        \"lz4\",\n        \"zstd\"\n      ],\n      \"title\": \"Parquet Compression\",\n      \"type\": \"string\"\n    },\n    \"csv_delimiter\": {\n      \"default\": \",\",\n      \"title\": \"Csv Delimiter\",\n      \"type\": \"string\"\n    },\n    \"csv_encoding\": {\n      \"default\": \"utf8\",\n      \"title\": \"Csv Encoding\",\n      \"type\": \"string\"\n    },\n    \"auth_mode\": {\n      \"default\": \"auto\",\n      \"enum\": [\n        \"access_key\",\n        \"iam_role\",\n        \"service_principal\",\n        \"managed_identity\",\n        \"sas_token\",\n        \"aws-cli\",\n        \"env_vars\"\n      ],\n      \"title\": \"Auth Mode\",\n      \"type\": \"string\"\n    },\n    \"connection_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Connection Name\"\n    }\n  },\n  \"required\": [\n    \"resource_path\"\n  ],\n  \"title\": \"CloudStorageWriteSettings\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>resource_path</code>                 (<code>str</code>)             </li> <li> <code>write_mode</code>                 (<code>Literal['overwrite', 'append']</code>)             </li> <li> <code>file_format</code>                 (<code>Literal['csv', 'parquet', 'json', 'delta']</code>)             </li> <li> <code>parquet_compression</code>                 (<code>Literal['snappy', 'gzip', 'brotli', 'lz4', 'zstd']</code>)             </li> <li> <code>csv_delimiter</code>                 (<code>str</code>)             </li> <li> <code>csv_encoding</code>                 (<code>str</code>)             </li> <li> <code>auth_mode</code>                 (<code>AuthMethod</code>)             </li> <li> <code>connection_name</code>                 (<code>str | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_auth_requirements</code>                 \u2192                   <code>auth_mode</code> </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class CloudStorageWriteSettings(CloudStorageSettings, WriteSettingsWorkerInterface):\n    \"\"\"Settings for writing to cloud storage\"\"\"\n\n    pass\n\n    def get_write_setting_worker_interface(self) -&gt; WriteSettingsWorkerInterface:\n        \"\"\"\n        Convert to a worker interface model without secrets.\n        \"\"\"\n        return WriteSettingsWorkerInterface(\n            resource_path=self.resource_path,\n            write_mode=self.write_mode,\n            file_format=self.file_format,\n            parquet_compression=self.parquet_compression,\n            csv_delimiter=self.csv_delimiter,\n            csv_encoding=self.csv_encoding,\n        )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.CloudStorageWriteSettings.get_write_setting_worker_interface","title":"<code>get_write_setting_worker_interface()</code>","text":"<p>Convert to a worker interface model without secrets.</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>def get_write_setting_worker_interface(self) -&gt; WriteSettingsWorkerInterface:\n    \"\"\"\n    Convert to a worker interface model without secrets.\n    \"\"\"\n    return WriteSettingsWorkerInterface(\n        resource_path=self.resource_path,\n        write_mode=self.write_mode,\n        file_format=self.file_format,\n        parquet_compression=self.parquet_compression,\n        csv_delimiter=self.csv_delimiter,\n        csv_encoding=self.csv_encoding,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.CloudStorageWriteSettingsWorkerInterface","title":"<code>CloudStorageWriteSettingsWorkerInterface</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Settings for writing to cloud storage in worker context</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FullCloudStorageConnectionWorkerInterface\": {\n      \"description\": \"Internal model with decrypted secrets\",\n      \"properties\": {\n        \"storage_type\": {\n          \"enum\": [\n            \"s3\",\n            \"adls\",\n            \"gcs\"\n          ],\n          \"title\": \"Storage Type\",\n          \"type\": \"string\"\n        },\n        \"auth_method\": {\n          \"enum\": [\n            \"access_key\",\n            \"iam_role\",\n            \"service_principal\",\n            \"managed_identity\",\n            \"sas_token\",\n            \"aws-cli\",\n            \"env_vars\"\n          ],\n          \"title\": \"Auth Method\",\n          \"type\": \"string\"\n        },\n        \"connection_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": \"None\",\n          \"title\": \"Connection Name\"\n        },\n        \"aws_region\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Aws Region\"\n        },\n        \"aws_access_key_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Aws Access Key Id\"\n        },\n        \"aws_secret_access_key\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Aws Secret Access Key\"\n        },\n        \"aws_role_arn\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Aws Role Arn\"\n        },\n        \"aws_allow_unsafe_html\": {\n          \"anyOf\": [\n            {\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Aws Allow Unsafe Html\"\n        },\n        \"aws_session_token\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Aws Session Token\"\n        },\n        \"azure_account_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Azure Account Name\"\n        },\n        \"azure_account_key\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Azure Account Key\"\n        },\n        \"azure_tenant_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Azure Tenant Id\"\n        },\n        \"azure_client_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Azure Client Id\"\n        },\n        \"azure_client_secret\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Azure Client Secret\"\n        },\n        \"endpoint_url\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Endpoint Url\"\n        },\n        \"verify_ssl\": {\n          \"default\": true,\n          \"title\": \"Verify Ssl\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"storage_type\",\n        \"auth_method\"\n      ],\n      \"title\": \"FullCloudStorageConnectionWorkerInterface\",\n      \"type\": \"object\"\n    },\n    \"WriteSettingsWorkerInterface\": {\n      \"description\": \"Settings for writing to cloud storage\",\n      \"properties\": {\n        \"resource_path\": {\n          \"title\": \"Resource Path\",\n          \"type\": \"string\"\n        },\n        \"write_mode\": {\n          \"default\": \"overwrite\",\n          \"enum\": [\n            \"overwrite\",\n            \"append\"\n          ],\n          \"title\": \"Write Mode\",\n          \"type\": \"string\"\n        },\n        \"file_format\": {\n          \"default\": \"parquet\",\n          \"enum\": [\n            \"csv\",\n            \"parquet\",\n            \"json\",\n            \"delta\"\n          ],\n          \"title\": \"File Format\",\n          \"type\": \"string\"\n        },\n        \"parquet_compression\": {\n          \"default\": \"snappy\",\n          \"enum\": [\n            \"snappy\",\n            \"gzip\",\n            \"brotli\",\n            \"lz4\",\n            \"zstd\"\n          ],\n          \"title\": \"Parquet Compression\",\n          \"type\": \"string\"\n        },\n        \"csv_delimiter\": {\n          \"default\": \",\",\n          \"title\": \"Csv Delimiter\",\n          \"type\": \"string\"\n        },\n        \"csv_encoding\": {\n          \"default\": \"utf8\",\n          \"title\": \"Csv Encoding\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"resource_path\"\n      ],\n      \"title\": \"WriteSettingsWorkerInterface\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Settings for writing to cloud storage in worker context\",\n  \"properties\": {\n    \"operation\": {\n      \"title\": \"Operation\",\n      \"type\": \"string\"\n    },\n    \"write_settings\": {\n      \"$ref\": \"#/$defs/WriteSettingsWorkerInterface\"\n    },\n    \"connection\": {\n      \"$ref\": \"#/$defs/FullCloudStorageConnectionWorkerInterface\"\n    },\n    \"flowfile_flow_id\": {\n      \"default\": 1,\n      \"title\": \"Flowfile Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"flowfile_node_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"string\"\n        }\n      ],\n      \"default\": -1,\n      \"title\": \"Flowfile Node Id\"\n    }\n  },\n  \"required\": [\n    \"operation\",\n    \"write_settings\",\n    \"connection\"\n  ],\n  \"title\": \"CloudStorageWriteSettingsWorkerInterface\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>operation</code>                 (<code>str</code>)             </li> <li> <code>write_settings</code>                 (<code>WriteSettingsWorkerInterface</code>)             </li> <li> <code>connection</code>                 (<code>FullCloudStorageConnectionWorkerInterface</code>)             </li> <li> <code>flowfile_flow_id</code>                 (<code>int</code>)             </li> <li> <code>flowfile_node_id</code>                 (<code>int | str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class CloudStorageWriteSettingsWorkerInterface(BaseModel):\n    \"\"\"Settings for writing to cloud storage in worker context\"\"\"\n\n    operation: str\n    write_settings: WriteSettingsWorkerInterface\n    connection: FullCloudStorageConnectionWorkerInterface\n    flowfile_flow_id: int = 1\n    flowfile_node_id: int | str = -1\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.FullCloudStorageConnection","title":"<code>FullCloudStorageConnection</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>AuthSettingsInput</code></p> <p>Internal model with decrypted secrets</p> Show JSON schema: <pre><code>{\n  \"description\": \"Internal model with decrypted secrets\",\n  \"properties\": {\n    \"storage_type\": {\n      \"enum\": [\n        \"s3\",\n        \"adls\",\n        \"gcs\"\n      ],\n      \"title\": \"Storage Type\",\n      \"type\": \"string\"\n    },\n    \"auth_method\": {\n      \"enum\": [\n        \"access_key\",\n        \"iam_role\",\n        \"service_principal\",\n        \"managed_identity\",\n        \"sas_token\",\n        \"aws-cli\",\n        \"env_vars\"\n      ],\n      \"title\": \"Auth Method\",\n      \"type\": \"string\"\n    },\n    \"connection_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"None\",\n      \"title\": \"Connection Name\"\n    },\n    \"aws_region\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Region\"\n    },\n    \"aws_access_key_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Access Key Id\"\n    },\n    \"aws_secret_access_key\": {\n      \"anyOf\": [\n        {\n          \"format\": \"password\",\n          \"type\": \"string\",\n          \"writeOnly\": true\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Secret Access Key\"\n    },\n    \"aws_role_arn\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Role Arn\"\n    },\n    \"aws_allow_unsafe_html\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Allow Unsafe Html\"\n    },\n    \"aws_session_token\": {\n      \"anyOf\": [\n        {\n          \"format\": \"password\",\n          \"type\": \"string\",\n          \"writeOnly\": true\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Session Token\"\n    },\n    \"azure_account_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Azure Account Name\"\n    },\n    \"azure_account_key\": {\n      \"anyOf\": [\n        {\n          \"format\": \"password\",\n          \"type\": \"string\",\n          \"writeOnly\": true\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Azure Account Key\"\n    },\n    \"azure_tenant_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Azure Tenant Id\"\n    },\n    \"azure_client_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Azure Client Id\"\n    },\n    \"azure_client_secret\": {\n      \"anyOf\": [\n        {\n          \"format\": \"password\",\n          \"type\": \"string\",\n          \"writeOnly\": true\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Azure Client Secret\"\n    },\n    \"endpoint_url\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Endpoint Url\"\n    },\n    \"verify_ssl\": {\n      \"default\": true,\n      \"title\": \"Verify Ssl\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"required\": [\n    \"storage_type\",\n    \"auth_method\"\n  ],\n  \"title\": \"FullCloudStorageConnection\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>storage_type</code>                 (<code>CloudStorageType</code>)             </li> <li> <code>auth_method</code>                 (<code>AuthMethod</code>)             </li> <li> <code>connection_name</code>                 (<code>str | None</code>)             </li> <li> <code>aws_region</code>                 (<code>str | None</code>)             </li> <li> <code>aws_access_key_id</code>                 (<code>str | None</code>)             </li> <li> <code>aws_secret_access_key</code>                 (<code>SecretStr | None</code>)             </li> <li> <code>aws_role_arn</code>                 (<code>str | None</code>)             </li> <li> <code>aws_allow_unsafe_html</code>                 (<code>bool | None</code>)             </li> <li> <code>aws_session_token</code>                 (<code>SecretStr | None</code>)             </li> <li> <code>azure_account_name</code>                 (<code>str | None</code>)             </li> <li> <code>azure_account_key</code>                 (<code>SecretStr | None</code>)             </li> <li> <code>azure_tenant_id</code>                 (<code>str | None</code>)             </li> <li> <code>azure_client_id</code>                 (<code>str | None</code>)             </li> <li> <code>azure_client_secret</code>                 (<code>SecretStr | None</code>)             </li> <li> <code>endpoint_url</code>                 (<code>str | None</code>)             </li> <li> <code>verify_ssl</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class FullCloudStorageConnection(AuthSettingsInput):\n    \"\"\"Internal model with decrypted secrets\"\"\"\n\n    # AWS S3\n    aws_region: str | None = None\n    aws_access_key_id: str | None = None\n    aws_secret_access_key: SecretStr | None = None\n    aws_role_arn: str | None = None\n    aws_allow_unsafe_html: bool | None = None\n    aws_session_token: SecretStr | None = None\n\n    # Azure ADLS\n    azure_account_name: str | None = None\n    azure_account_key: SecretStr | None = None\n    azure_tenant_id: str | None = None\n    azure_client_id: str | None = None\n    azure_client_secret: SecretStr | None = None\n\n    # Common\n    endpoint_url: str | None = None\n    verify_ssl: bool = True\n\n    def get_worker_interface(self, user_id: int) -&gt; \"FullCloudStorageConnectionWorkerInterface\":\n        \"\"\"\n        Convert to a worker interface model with encrypted secrets.\n\n        Args:\n            user_id: The user ID for per-user key derivation\n\n        Returns:\n            FullCloudStorageConnectionWorkerInterface with encrypted secrets\n        \"\"\"\n        return FullCloudStorageConnectionWorkerInterface(\n            storage_type=self.storage_type,\n            auth_method=self.auth_method,\n            connection_name=self.connection_name,\n            aws_allow_unsafe_html=self.aws_allow_unsafe_html,\n            aws_secret_access_key=encrypt_for_worker(self.aws_secret_access_key, user_id),\n            aws_region=self.aws_region,\n            aws_access_key_id=self.aws_access_key_id,\n            aws_role_arn=self.aws_role_arn,\n            aws_session_token=encrypt_for_worker(self.aws_session_token, user_id),\n            azure_account_name=self.azure_account_name,\n            azure_tenant_id=self.azure_tenant_id,\n            azure_account_key=encrypt_for_worker(self.azure_account_key, user_id),\n            azure_client_id=self.azure_client_id,\n            azure_client_secret=encrypt_for_worker(self.azure_client_secret, user_id),\n            endpoint_url=self.endpoint_url,\n            verify_ssl=self.verify_ssl,\n        )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.FullCloudStorageConnection.get_worker_interface","title":"<code>get_worker_interface(user_id)</code>","text":"<p>Convert to a worker interface model with encrypted secrets.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>int</code> <p>The user ID for per-user key derivation</p> required <p>Returns:</p> Type Description <code>FullCloudStorageConnectionWorkerInterface</code> <p>FullCloudStorageConnectionWorkerInterface with encrypted secrets</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>def get_worker_interface(self, user_id: int) -&gt; \"FullCloudStorageConnectionWorkerInterface\":\n    \"\"\"\n    Convert to a worker interface model with encrypted secrets.\n\n    Args:\n        user_id: The user ID for per-user key derivation\n\n    Returns:\n        FullCloudStorageConnectionWorkerInterface with encrypted secrets\n    \"\"\"\n    return FullCloudStorageConnectionWorkerInterface(\n        storage_type=self.storage_type,\n        auth_method=self.auth_method,\n        connection_name=self.connection_name,\n        aws_allow_unsafe_html=self.aws_allow_unsafe_html,\n        aws_secret_access_key=encrypt_for_worker(self.aws_secret_access_key, user_id),\n        aws_region=self.aws_region,\n        aws_access_key_id=self.aws_access_key_id,\n        aws_role_arn=self.aws_role_arn,\n        aws_session_token=encrypt_for_worker(self.aws_session_token, user_id),\n        azure_account_name=self.azure_account_name,\n        azure_tenant_id=self.azure_tenant_id,\n        azure_account_key=encrypt_for_worker(self.azure_account_key, user_id),\n        azure_client_id=self.azure_client_id,\n        azure_client_secret=encrypt_for_worker(self.azure_client_secret, user_id),\n        endpoint_url=self.endpoint_url,\n        verify_ssl=self.verify_ssl,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.FullCloudStorageConnectionInterface","title":"<code>FullCloudStorageConnectionInterface</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>AuthSettingsInput</code></p> <p>API response model - no secrets exposed</p> Show JSON schema: <pre><code>{\n  \"description\": \"API response model - no secrets exposed\",\n  \"properties\": {\n    \"storage_type\": {\n      \"enum\": [\n        \"s3\",\n        \"adls\",\n        \"gcs\"\n      ],\n      \"title\": \"Storage Type\",\n      \"type\": \"string\"\n    },\n    \"auth_method\": {\n      \"enum\": [\n        \"access_key\",\n        \"iam_role\",\n        \"service_principal\",\n        \"managed_identity\",\n        \"sas_token\",\n        \"aws-cli\",\n        \"env_vars\"\n      ],\n      \"title\": \"Auth Method\",\n      \"type\": \"string\"\n    },\n    \"connection_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"None\",\n      \"title\": \"Connection Name\"\n    },\n    \"aws_allow_unsafe_html\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Allow Unsafe Html\"\n    },\n    \"aws_region\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Region\"\n    },\n    \"aws_access_key_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Access Key Id\"\n    },\n    \"aws_role_arn\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Role Arn\"\n    },\n    \"azure_account_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Azure Account Name\"\n    },\n    \"azure_tenant_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Azure Tenant Id\"\n    },\n    \"azure_client_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Azure Client Id\"\n    },\n    \"endpoint_url\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Endpoint Url\"\n    },\n    \"verify_ssl\": {\n      \"default\": true,\n      \"title\": \"Verify Ssl\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"required\": [\n    \"storage_type\",\n    \"auth_method\"\n  ],\n  \"title\": \"FullCloudStorageConnectionInterface\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>storage_type</code>                 (<code>CloudStorageType</code>)             </li> <li> <code>auth_method</code>                 (<code>AuthMethod</code>)             </li> <li> <code>connection_name</code>                 (<code>str | None</code>)             </li> <li> <code>aws_allow_unsafe_html</code>                 (<code>bool | None</code>)             </li> <li> <code>aws_region</code>                 (<code>str | None</code>)             </li> <li> <code>aws_access_key_id</code>                 (<code>str | None</code>)             </li> <li> <code>aws_role_arn</code>                 (<code>str | None</code>)             </li> <li> <code>azure_account_name</code>                 (<code>str | None</code>)             </li> <li> <code>azure_tenant_id</code>                 (<code>str | None</code>)             </li> <li> <code>azure_client_id</code>                 (<code>str | None</code>)             </li> <li> <code>endpoint_url</code>                 (<code>str | None</code>)             </li> <li> <code>verify_ssl</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class FullCloudStorageConnectionInterface(AuthSettingsInput):\n    \"\"\"API response model - no secrets exposed\"\"\"\n\n    # Public fields only\n    aws_allow_unsafe_html: bool | None = None\n    aws_region: str | None = None\n    aws_access_key_id: str | None = None\n    aws_role_arn: str | None = None\n    azure_account_name: str | None = None\n    azure_tenant_id: str | None = None\n    azure_client_id: str | None = None\n    endpoint_url: str | None = None\n    verify_ssl: bool = True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.FullCloudStorageConnectionWorkerInterface","title":"<code>FullCloudStorageConnectionWorkerInterface</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>AuthSettingsInput</code></p> <p>Internal model with decrypted secrets</p> Show JSON schema: <pre><code>{\n  \"description\": \"Internal model with decrypted secrets\",\n  \"properties\": {\n    \"storage_type\": {\n      \"enum\": [\n        \"s3\",\n        \"adls\",\n        \"gcs\"\n      ],\n      \"title\": \"Storage Type\",\n      \"type\": \"string\"\n    },\n    \"auth_method\": {\n      \"enum\": [\n        \"access_key\",\n        \"iam_role\",\n        \"service_principal\",\n        \"managed_identity\",\n        \"sas_token\",\n        \"aws-cli\",\n        \"env_vars\"\n      ],\n      \"title\": \"Auth Method\",\n      \"type\": \"string\"\n    },\n    \"connection_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": \"None\",\n      \"title\": \"Connection Name\"\n    },\n    \"aws_region\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Region\"\n    },\n    \"aws_access_key_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Access Key Id\"\n    },\n    \"aws_secret_access_key\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Secret Access Key\"\n    },\n    \"aws_role_arn\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Role Arn\"\n    },\n    \"aws_allow_unsafe_html\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Allow Unsafe Html\"\n    },\n    \"aws_session_token\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Aws Session Token\"\n    },\n    \"azure_account_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Azure Account Name\"\n    },\n    \"azure_account_key\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Azure Account Key\"\n    },\n    \"azure_tenant_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Azure Tenant Id\"\n    },\n    \"azure_client_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Azure Client Id\"\n    },\n    \"azure_client_secret\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Azure Client Secret\"\n    },\n    \"endpoint_url\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Endpoint Url\"\n    },\n    \"verify_ssl\": {\n      \"default\": true,\n      \"title\": \"Verify Ssl\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"required\": [\n    \"storage_type\",\n    \"auth_method\"\n  ],\n  \"title\": \"FullCloudStorageConnectionWorkerInterface\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>storage_type</code>                 (<code>CloudStorageType</code>)             </li> <li> <code>auth_method</code>                 (<code>AuthMethod</code>)             </li> <li> <code>connection_name</code>                 (<code>str | None</code>)             </li> <li> <code>aws_region</code>                 (<code>str | None</code>)             </li> <li> <code>aws_access_key_id</code>                 (<code>str | None</code>)             </li> <li> <code>aws_secret_access_key</code>                 (<code>str | None</code>)             </li> <li> <code>aws_role_arn</code>                 (<code>str | None</code>)             </li> <li> <code>aws_allow_unsafe_html</code>                 (<code>bool | None</code>)             </li> <li> <code>aws_session_token</code>                 (<code>str | None</code>)             </li> <li> <code>azure_account_name</code>                 (<code>str | None</code>)             </li> <li> <code>azure_account_key</code>                 (<code>str | None</code>)             </li> <li> <code>azure_tenant_id</code>                 (<code>str | None</code>)             </li> <li> <code>azure_client_id</code>                 (<code>str | None</code>)             </li> <li> <code>azure_client_secret</code>                 (<code>str | None</code>)             </li> <li> <code>endpoint_url</code>                 (<code>str | None</code>)             </li> <li> <code>verify_ssl</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class FullCloudStorageConnectionWorkerInterface(AuthSettingsInput):\n    \"\"\"Internal model with decrypted secrets\"\"\"\n\n    # AWS S3\n    aws_region: str | None = None\n    aws_access_key_id: str | None = None\n    aws_secret_access_key: str | None = None\n    aws_role_arn: str | None = None\n    aws_allow_unsafe_html: bool | None = None\n    aws_session_token: str | None = None\n\n    # Azure ADLS\n    azure_account_name: str | None = None\n    azure_account_key: str | None = None\n    azure_tenant_id: str | None = None\n    azure_client_id: str | None = None\n    azure_client_secret: str | None = None\n\n    # Common\n    endpoint_url: str | None = None\n    verify_ssl: bool = True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.WriteSettingsWorkerInterface","title":"<code>WriteSettingsWorkerInterface</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Settings for writing to cloud storage</p> Show JSON schema: <pre><code>{\n  \"description\": \"Settings for writing to cloud storage\",\n  \"properties\": {\n    \"resource_path\": {\n      \"title\": \"Resource Path\",\n      \"type\": \"string\"\n    },\n    \"write_mode\": {\n      \"default\": \"overwrite\",\n      \"enum\": [\n        \"overwrite\",\n        \"append\"\n      ],\n      \"title\": \"Write Mode\",\n      \"type\": \"string\"\n    },\n    \"file_format\": {\n      \"default\": \"parquet\",\n      \"enum\": [\n        \"csv\",\n        \"parquet\",\n        \"json\",\n        \"delta\"\n      ],\n      \"title\": \"File Format\",\n      \"type\": \"string\"\n    },\n    \"parquet_compression\": {\n      \"default\": \"snappy\",\n      \"enum\": [\n        \"snappy\",\n        \"gzip\",\n        \"brotli\",\n        \"lz4\",\n        \"zstd\"\n      ],\n      \"title\": \"Parquet Compression\",\n      \"type\": \"string\"\n    },\n    \"csv_delimiter\": {\n      \"default\": \",\",\n      \"title\": \"Csv Delimiter\",\n      \"type\": \"string\"\n    },\n    \"csv_encoding\": {\n      \"default\": \"utf8\",\n      \"title\": \"Csv Encoding\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"resource_path\"\n  ],\n  \"title\": \"WriteSettingsWorkerInterface\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>resource_path</code>                 (<code>str</code>)             </li> <li> <code>write_mode</code>                 (<code>Literal['overwrite', 'append']</code>)             </li> <li> <code>file_format</code>                 (<code>Literal['csv', 'parquet', 'json', 'delta']</code>)             </li> <li> <code>parquet_compression</code>                 (<code>Literal['snappy', 'gzip', 'brotli', 'lz4', 'zstd']</code>)             </li> <li> <code>csv_delimiter</code>                 (<code>str</code>)             </li> <li> <code>csv_encoding</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class WriteSettingsWorkerInterface(BaseModel):\n    \"\"\"Settings for writing to cloud storage\"\"\"\n\n    resource_path: str  # s3://bucket/path/to/file.csv\n\n    write_mode: Literal[\"overwrite\", \"append\"] = \"overwrite\"\n    file_format: Literal[\"csv\", \"parquet\", \"json\", \"delta\"] = \"parquet\"\n\n    parquet_compression: Literal[\"snappy\", \"gzip\", \"brotli\", \"lz4\", \"zstd\"] = \"snappy\"\n\n    csv_delimiter: str = \",\"\n    csv_encoding: str = \"utf8\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.encrypt_for_worker","title":"<code>encrypt_for_worker(secret_value, user_id)</code>","text":"<p>Encrypts a secret value for use in worker contexts using per-user key derivation.</p> <p>Parameters:</p> Name Type Description Default <code>secret_value</code> <code>SecretStr | None</code> <p>The secret value to encrypt</p> required <code>user_id</code> <code>int</code> <p>The user ID for key derivation</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Encrypted secret with embedded user_id, or None if secret_value is None</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>def encrypt_for_worker(secret_value: SecretStr | None, user_id: int) -&gt; str | None:\n    \"\"\"\n    Encrypts a secret value for use in worker contexts using per-user key derivation.\n\n    Args:\n        secret_value: The secret value to encrypt\n        user_id: The user ID for key derivation\n\n    Returns:\n        Encrypted secret with embedded user_id, or None if secret_value is None\n    \"\"\"\n    if secret_value is not None:\n        return encrypt_secret(secret_value.get_secret_value(), user_id)\n    return None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.get_cloud_storage_write_settings_worker_interface","title":"<code>get_cloud_storage_write_settings_worker_interface(write_settings, connection, lf, user_id, flowfile_flow_id=1, flowfile_node_id=-1)</code>","text":"<p>Convert to a worker interface model with encrypted secrets.</p> <p>Parameters:</p> Name Type Description Default <code>write_settings</code> <code>CloudStorageWriteSettings</code> <p>Cloud storage write settings</p> required <code>connection</code> <code>FullCloudStorageConnection</code> <p>Full cloud storage connection with secrets</p> required <code>lf</code> <code>LazyFrame</code> <p>LazyFrame to serialize</p> required <code>user_id</code> <code>int</code> <p>User ID for per-user key derivation</p> required <code>flowfile_flow_id</code> <code>int</code> <p>Flow ID for tracking</p> <code>1</code> <code>flowfile_node_id</code> <code>int | str</code> <p>Node ID for tracking</p> <code>-1</code> <p>Returns:</p> Type Description <code>CloudStorageWriteSettingsWorkerInterface</code> <p>CloudStorageWriteSettingsWorkerInterface ready for worker</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>def get_cloud_storage_write_settings_worker_interface(\n    write_settings: CloudStorageWriteSettings,\n    connection: FullCloudStorageConnection,\n    lf: pl.LazyFrame,\n    user_id: int,\n    flowfile_flow_id: int = 1,\n    flowfile_node_id: int | str = -1,\n) -&gt; CloudStorageWriteSettingsWorkerInterface:\n    \"\"\"\n    Convert to a worker interface model with encrypted secrets.\n\n    Args:\n        write_settings: Cloud storage write settings\n        connection: Full cloud storage connection with secrets\n        lf: LazyFrame to serialize\n        user_id: User ID for per-user key derivation\n        flowfile_flow_id: Flow ID for tracking\n        flowfile_node_id: Node ID for tracking\n\n    Returns:\n        CloudStorageWriteSettingsWorkerInterface ready for worker\n    \"\"\"\n    operation = base64.b64encode(lf.serialize()).decode()\n\n    return CloudStorageWriteSettingsWorkerInterface(\n        operation=operation,\n        write_settings=write_settings.get_write_setting_worker_interface(),\n        connection=connection.get_worker_interface(user_id),\n        flowfile_flow_id=flowfile_flow_id,\n        flowfile_node_id=flowfile_node_id,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#output_model","title":"<code>output_model</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model","title":"<code>flowfile_core.schemas.output_model</code>","text":"<p>Classes:</p> Name Description <code>BaseItem</code> <p>A base model for any item in a file system, like a file or directory.</p> <code>ExpressionRef</code> <p>A reference to a single Polars expression, including its name and docstring.</p> <code>ExpressionsOverview</code> <p>Represents a categorized list of available Polars expressions.</p> <code>FileColumn</code> <p>Represents detailed schema and statistics for a single column (field).</p> <code>InstantFuncResult</code> <p>Represents the result of a function that is expected to execute instantly.</p> <code>ItemInfo</code> <p>Provides detailed information about a single item in an output directory.</p> <code>NodeData</code> <p>A comprehensive model holding the complete state and data for a single node.</p> <code>NodeDescriptionResponse</code> <p>Response model for the node description endpoint.</p> <code>NodeResult</code> <p>Represents the execution result of a single node in a FlowGraph run.</p> <code>OutputDir</code> <p>Represents the contents of a single output directory.</p> <code>OutputFile</code> <p>Represents a single file in an output directory, extending BaseItem.</p> <code>OutputFiles</code> <p>Represents a collection of files, typically within a directory.</p> <code>OutputTree</code> <p>Represents a directory tree, including subdirectories.</p> <code>RunInformation</code> <p>Contains summary information about a complete FlowGraph execution.</p> <code>TableExample</code> <p>Represents a preview of a table, including schema and sample data.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.BaseItem","title":"<code>BaseItem</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A base model for any item in a file system, like a file or directory.</p> Show JSON schema: <pre><code>{\n  \"description\": \"A base model for any item in a file system, like a file or directory.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"path\": {\n      \"title\": \"Path\",\n      \"type\": \"string\"\n    },\n    \"size\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Size\"\n    },\n    \"creation_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Creation Date\"\n    },\n    \"access_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Access Date\"\n    },\n    \"modification_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Modification Date\"\n    },\n    \"source_path\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Source Path\"\n    },\n    \"number_of_items\": {\n      \"default\": -1,\n      \"title\": \"Number Of Items\",\n      \"type\": \"integer\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"path\"\n  ],\n  \"title\": \"BaseItem\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>path</code>                 (<code>str</code>)             </li> <li> <code>size</code>                 (<code>int | None</code>)             </li> <li> <code>creation_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>access_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>modification_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>source_path</code>                 (<code>str | None</code>)             </li> <li> <code>number_of_items</code>                 (<code>int</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class BaseItem(BaseModel):\n    \"\"\"A base model for any item in a file system, like a file or directory.\"\"\"\n\n    name: str\n    path: str\n    size: int | None = None\n    creation_date: datetime | None = None\n    access_date: datetime | None = None\n    modification_date: datetime | None = None\n    source_path: str | None = None\n    number_of_items: int = -1\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.ExpressionRef","title":"<code>ExpressionRef</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A reference to a single Polars expression, including its name and docstring.</p> Show JSON schema: <pre><code>{\n  \"description\": \"A reference to a single Polars expression, including its name and docstring.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"doc\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Doc\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"doc\"\n  ],\n  \"title\": \"ExpressionRef\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>doc</code>                 (<code>str | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class ExpressionRef(BaseModel):\n    \"\"\"A reference to a single Polars expression, including its name and docstring.\"\"\"\n\n    name: str\n    doc: str | None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.ExpressionsOverview","title":"<code>ExpressionsOverview</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a categorized list of available Polars expressions.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"ExpressionRef\": {\n      \"description\": \"A reference to a single Polars expression, including its name and docstring.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"doc\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"title\": \"Doc\"\n        }\n      },\n      \"required\": [\n        \"name\",\n        \"doc\"\n      ],\n      \"title\": \"ExpressionRef\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Represents a categorized list of available Polars expressions.\",\n  \"properties\": {\n    \"expression_type\": {\n      \"title\": \"Expression Type\",\n      \"type\": \"string\"\n    },\n    \"expressions\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/ExpressionRef\"\n      },\n      \"title\": \"Expressions\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"expression_type\",\n    \"expressions\"\n  ],\n  \"title\": \"ExpressionsOverview\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>expression_type</code>                 (<code>str</code>)             </li> <li> <code>expressions</code>                 (<code>list[ExpressionRef]</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class ExpressionsOverview(BaseModel):\n    \"\"\"Represents a categorized list of available Polars expressions.\"\"\"\n\n    expression_type: str\n    expressions: list[ExpressionRef]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.FileColumn","title":"<code>FileColumn</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents detailed schema and statistics for a single column (field).</p> Show JSON schema: <pre><code>{\n  \"description\": \"Represents detailed schema and statistics for a single column (field).\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"data_type\": {\n      \"title\": \"Data Type\",\n      \"type\": \"string\"\n    },\n    \"is_unique\": {\n      \"title\": \"Is Unique\",\n      \"type\": \"boolean\"\n    },\n    \"max_value\": {\n      \"title\": \"Max Value\",\n      \"type\": \"string\"\n    },\n    \"min_value\": {\n      \"title\": \"Min Value\",\n      \"type\": \"string\"\n    },\n    \"number_of_empty_values\": {\n      \"title\": \"Number Of Empty Values\",\n      \"type\": \"integer\"\n    },\n    \"number_of_filled_values\": {\n      \"title\": \"Number Of Filled Values\",\n      \"type\": \"integer\"\n    },\n    \"number_of_unique_values\": {\n      \"title\": \"Number Of Unique Values\",\n      \"type\": \"integer\"\n    },\n    \"size\": {\n      \"title\": \"Size\",\n      \"type\": \"integer\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"data_type\",\n    \"is_unique\",\n    \"max_value\",\n    \"min_value\",\n    \"number_of_empty_values\",\n    \"number_of_filled_values\",\n    \"number_of_unique_values\",\n    \"size\"\n  ],\n  \"title\": \"FileColumn\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>data_type</code>                 (<code>str</code>)             </li> <li> <code>is_unique</code>                 (<code>bool</code>)             </li> <li> <code>max_value</code>                 (<code>str</code>)             </li> <li> <code>min_value</code>                 (<code>str</code>)             </li> <li> <code>number_of_empty_values</code>                 (<code>int</code>)             </li> <li> <code>number_of_filled_values</code>                 (<code>int</code>)             </li> <li> <code>number_of_unique_values</code>                 (<code>int</code>)             </li> <li> <code>size</code>                 (<code>int</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class FileColumn(BaseModel):\n    \"\"\"Represents detailed schema and statistics for a single column (field).\"\"\"\n\n    name: str\n    data_type: str\n    is_unique: bool\n    max_value: str\n    min_value: str\n    number_of_empty_values: int\n    number_of_filled_values: int\n    number_of_unique_values: int\n    size: int\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.InstantFuncResult","title":"<code>InstantFuncResult</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the result of a function that is expected to execute instantly.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Represents the result of a function that is expected to execute instantly.\",\n  \"properties\": {\n    \"success\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Success\"\n    },\n    \"result\": {\n      \"title\": \"Result\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"result\"\n  ],\n  \"title\": \"InstantFuncResult\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>success</code>                 (<code>bool | None</code>)             </li> <li> <code>result</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class InstantFuncResult(BaseModel):\n    \"\"\"Represents the result of a function that is expected to execute instantly.\"\"\"\n\n    success: bool | None = None\n    result: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.ItemInfo","title":"<code>ItemInfo</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>OutputFile</code></p> <p>Provides detailed information about a single item in an output directory.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Provides detailed information about a single item in an output directory.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"path\": {\n      \"title\": \"Path\",\n      \"type\": \"string\"\n    },\n    \"size\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Size\"\n    },\n    \"creation_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Creation Date\"\n    },\n    \"access_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Access Date\"\n    },\n    \"modification_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Modification Date\"\n    },\n    \"source_path\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Source Path\"\n    },\n    \"number_of_items\": {\n      \"default\": -1,\n      \"title\": \"Number Of Items\",\n      \"type\": \"integer\"\n    },\n    \"ext\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Ext\"\n    },\n    \"mimetype\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Mimetype\"\n    },\n    \"id\": {\n      \"default\": -1,\n      \"title\": \"Id\",\n      \"type\": \"integer\"\n    },\n    \"type\": {\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"analysis_file_available\": {\n      \"default\": false,\n      \"title\": \"Analysis File Available\",\n      \"type\": \"boolean\"\n    },\n    \"analysis_file_location\": {\n      \"default\": null,\n      \"title\": \"Analysis File Location\",\n      \"type\": \"string\"\n    },\n    \"analysis_file_error\": {\n      \"default\": null,\n      \"title\": \"Analysis File Error\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"path\",\n    \"type\"\n  ],\n  \"title\": \"ItemInfo\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>path</code>                 (<code>str</code>)             </li> <li> <code>size</code>                 (<code>int | None</code>)             </li> <li> <code>creation_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>access_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>modification_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>source_path</code>                 (<code>str | None</code>)             </li> <li> <code>number_of_items</code>                 (<code>int</code>)             </li> <li> <code>ext</code>                 (<code>str | None</code>)             </li> <li> <code>mimetype</code>                 (<code>str | None</code>)             </li> <li> <code>id</code>                 (<code>int</code>)             </li> <li> <code>type</code>                 (<code>str</code>)             </li> <li> <code>analysis_file_available</code>                 (<code>bool</code>)             </li> <li> <code>analysis_file_location</code>                 (<code>str</code>)             </li> <li> <code>analysis_file_error</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class ItemInfo(OutputFile):\n    \"\"\"Provides detailed information about a single item in an output directory.\"\"\"\n\n    id: int = -1\n    type: str\n    analysis_file_available: bool = False\n    analysis_file_location: str = None\n    analysis_file_error: str = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.NodeData","title":"<code>NodeData</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A comprehensive model holding the complete state and data for a single node.</p> <p>This includes its input/output data previews, settings, and run status.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FileColumn\": {\n      \"description\": \"Represents detailed schema and statistics for a single column (field).\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"is_unique\": {\n          \"title\": \"Is Unique\",\n          \"type\": \"boolean\"\n        },\n        \"max_value\": {\n          \"title\": \"Max Value\",\n          \"type\": \"string\"\n        },\n        \"min_value\": {\n          \"title\": \"Min Value\",\n          \"type\": \"string\"\n        },\n        \"number_of_empty_values\": {\n          \"title\": \"Number Of Empty Values\",\n          \"type\": \"integer\"\n        },\n        \"number_of_filled_values\": {\n          \"title\": \"Number Of Filled Values\",\n          \"type\": \"integer\"\n        },\n        \"number_of_unique_values\": {\n          \"title\": \"Number Of Unique Values\",\n          \"type\": \"integer\"\n        },\n        \"size\": {\n          \"title\": \"Size\",\n          \"type\": \"integer\"\n        }\n      },\n      \"required\": [\n        \"name\",\n        \"data_type\",\n        \"is_unique\",\n        \"max_value\",\n        \"min_value\",\n        \"number_of_empty_values\",\n        \"number_of_filled_values\",\n        \"number_of_unique_values\",\n        \"size\"\n      ],\n      \"title\": \"FileColumn\",\n      \"type\": \"object\"\n    },\n    \"TableExample\": {\n      \"description\": \"Represents a preview of a table, including schema and sample data.\",\n      \"properties\": {\n        \"node_id\": {\n          \"title\": \"Node Id\",\n          \"type\": \"integer\"\n        },\n        \"number_of_records\": {\n          \"title\": \"Number Of Records\",\n          \"type\": \"integer\"\n        },\n        \"number_of_columns\": {\n          \"title\": \"Number Of Columns\",\n          \"type\": \"integer\"\n        },\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"table_schema\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FileColumn\"\n          },\n          \"title\": \"Table Schema\",\n          \"type\": \"array\"\n        },\n        \"columns\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Columns\",\n          \"type\": \"array\"\n        },\n        \"data\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"object\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": {},\n          \"title\": \"Data\"\n        },\n        \"has_example_data\": {\n          \"default\": false,\n          \"title\": \"Has Example Data\",\n          \"type\": \"boolean\"\n        },\n        \"has_run_with_current_setup\": {\n          \"default\": false,\n          \"title\": \"Has Run With Current Setup\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"node_id\",\n        \"number_of_records\",\n        \"number_of_columns\",\n        \"name\",\n        \"table_schema\",\n        \"columns\"\n      ],\n      \"title\": \"TableExample\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"A comprehensive model holding the complete state and data for a single node.\\n\\nThis includes its input/output data previews, settings, and run status.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"flow_type\": {\n      \"title\": \"Flow Type\",\n      \"type\": \"string\"\n    },\n    \"left_input\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/TableExample\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"right_input\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/TableExample\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"main_input\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/TableExample\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"main_output\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/TableExample\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"left_output\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/TableExample\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"right_output\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/TableExample\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null\n    },\n    \"has_run\": {\n      \"default\": false,\n      \"title\": \"Has Run\",\n      \"type\": \"boolean\"\n    },\n    \"is_cached\": {\n      \"default\": false,\n      \"title\": \"Is Cached\",\n      \"type\": \"boolean\"\n    },\n    \"setting_input\": {\n      \"default\": null,\n      \"title\": \"Setting Input\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_id\",\n    \"flow_type\"\n  ],\n  \"title\": \"NodeData\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>flow_type</code>                 (<code>str</code>)             </li> <li> <code>left_input</code>                 (<code>TableExample | None</code>)             </li> <li> <code>right_input</code>                 (<code>TableExample | None</code>)             </li> <li> <code>main_input</code>                 (<code>TableExample | None</code>)             </li> <li> <code>main_output</code>                 (<code>TableExample | None</code>)             </li> <li> <code>left_output</code>                 (<code>TableExample | None</code>)             </li> <li> <code>right_output</code>                 (<code>TableExample | None</code>)             </li> <li> <code>has_run</code>                 (<code>bool</code>)             </li> <li> <code>is_cached</code>                 (<code>bool</code>)             </li> <li> <code>setting_input</code>                 (<code>Any</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class NodeData(BaseModel):\n    \"\"\"A comprehensive model holding the complete state and data for a single node.\n\n    This includes its input/output data previews, settings, and run status.\n    \"\"\"\n\n    flow_id: int\n    node_id: int\n    flow_type: str\n    left_input: TableExample | None = None\n    right_input: TableExample | None = None\n    main_input: TableExample | None = None\n    main_output: TableExample | None = None\n    left_output: TableExample | None = None\n    right_output: TableExample | None = None\n    has_run: bool = False\n    is_cached: bool = False\n    setting_input: Any = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.NodeDescriptionResponse","title":"<code>NodeDescriptionResponse</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response model for the node description endpoint.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Response model for the node description endpoint.\",\n  \"properties\": {\n    \"description\": {\n      \"default\": \"\",\n      \"title\": \"Description\",\n      \"type\": \"string\"\n    },\n    \"is_auto_generated\": {\n      \"default\": false,\n      \"title\": \"Is Auto Generated\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"title\": \"NodeDescriptionResponse\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>description</code>                 (<code>str</code>)             </li> <li> <code>is_auto_generated</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class NodeDescriptionResponse(BaseModel):\n    \"\"\"Response model for the node description endpoint.\"\"\"\n\n    description: str = \"\"\n    is_auto_generated: bool = False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.NodeResult","title":"<code>NodeResult</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the execution result of a single node in a FlowGraph run.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Represents the execution result of a single node in a FlowGraph run.\",\n  \"properties\": {\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"node_name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Node Name\"\n    },\n    \"start_timestamp\": {\n      \"title\": \"Start Timestamp\",\n      \"type\": \"number\"\n    },\n    \"end_timestamp\": {\n      \"default\": 0,\n      \"title\": \"End Timestamp\",\n      \"type\": \"number\"\n    },\n    \"success\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Success\"\n    },\n    \"error\": {\n      \"default\": \"\",\n      \"title\": \"Error\",\n      \"type\": \"string\"\n    },\n    \"run_time\": {\n      \"default\": -1,\n      \"title\": \"Run Time\",\n      \"type\": \"integer\"\n    },\n    \"is_running\": {\n      \"default\": true,\n      \"title\": \"Is Running\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"required\": [\n    \"node_id\"\n  ],\n  \"title\": \"NodeResult\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>node_name</code>                 (<code>str | None</code>)             </li> <li> <code>start_timestamp</code>                 (<code>float</code>)             </li> <li> <code>end_timestamp</code>                 (<code>float</code>)             </li> <li> <code>success</code>                 (<code>bool | None</code>)             </li> <li> <code>error</code>                 (<code>str</code>)             </li> <li> <code>run_time</code>                 (<code>int</code>)             </li> <li> <code>is_running</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class NodeResult(BaseModel):\n    \"\"\"Represents the execution result of a single node in a FlowGraph run.\"\"\"\n\n    node_id: int\n    node_name: str | None = None\n    start_timestamp: float = Field(default_factory=time.time)\n    end_timestamp: float = 0\n    success: bool | None = None\n    error: str = \"\"\n    run_time: int = -1\n    is_running: bool = True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.OutputDir","title":"<code>OutputDir</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseItem</code></p> <p>Represents the contents of a single output directory.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"ItemInfo\": {\n      \"description\": \"Provides detailed information about a single item in an output directory.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"path\": {\n          \"title\": \"Path\",\n          \"type\": \"string\"\n        },\n        \"size\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Size\"\n        },\n        \"creation_date\": {\n          \"anyOf\": [\n            {\n              \"format\": \"date-time\",\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Creation Date\"\n        },\n        \"access_date\": {\n          \"anyOf\": [\n            {\n              \"format\": \"date-time\",\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Access Date\"\n        },\n        \"modification_date\": {\n          \"anyOf\": [\n            {\n              \"format\": \"date-time\",\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Modification Date\"\n        },\n        \"source_path\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Source Path\"\n        },\n        \"number_of_items\": {\n          \"default\": -1,\n          \"title\": \"Number Of Items\",\n          \"type\": \"integer\"\n        },\n        \"ext\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Ext\"\n        },\n        \"mimetype\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Mimetype\"\n        },\n        \"id\": {\n          \"default\": -1,\n          \"title\": \"Id\",\n          \"type\": \"integer\"\n        },\n        \"type\": {\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"analysis_file_available\": {\n          \"default\": false,\n          \"title\": \"Analysis File Available\",\n          \"type\": \"boolean\"\n        },\n        \"analysis_file_location\": {\n          \"default\": null,\n          \"title\": \"Analysis File Location\",\n          \"type\": \"string\"\n        },\n        \"analysis_file_error\": {\n          \"default\": null,\n          \"title\": \"Analysis File Error\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"name\",\n        \"path\",\n        \"type\"\n      ],\n      \"title\": \"ItemInfo\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Represents the contents of a single output directory.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"path\": {\n      \"title\": \"Path\",\n      \"type\": \"string\"\n    },\n    \"size\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Size\"\n    },\n    \"creation_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Creation Date\"\n    },\n    \"access_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Access Date\"\n    },\n    \"modification_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Modification Date\"\n    },\n    \"source_path\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Source Path\"\n    },\n    \"number_of_items\": {\n      \"default\": -1,\n      \"title\": \"Number Of Items\",\n      \"type\": \"integer\"\n    },\n    \"all_items\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"All Items\",\n      \"type\": \"array\"\n    },\n    \"items\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/ItemInfo\"\n      },\n      \"title\": \"Items\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"path\",\n    \"all_items\",\n    \"items\"\n  ],\n  \"title\": \"OutputDir\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>path</code>                 (<code>str</code>)             </li> <li> <code>size</code>                 (<code>int | None</code>)             </li> <li> <code>creation_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>access_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>modification_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>source_path</code>                 (<code>str | None</code>)             </li> <li> <code>number_of_items</code>                 (<code>int</code>)             </li> <li> <code>all_items</code>                 (<code>list[str]</code>)             </li> <li> <code>items</code>                 (<code>list[ItemInfo]</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class OutputDir(BaseItem):\n    \"\"\"Represents the contents of a single output directory.\"\"\"\n\n    all_items: list[str]\n    items: list[ItemInfo]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.OutputFile","title":"<code>OutputFile</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseItem</code></p> <p>Represents a single file in an output directory, extending BaseItem.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Represents a single file in an output directory, extending BaseItem.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"path\": {\n      \"title\": \"Path\",\n      \"type\": \"string\"\n    },\n    \"size\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Size\"\n    },\n    \"creation_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Creation Date\"\n    },\n    \"access_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Access Date\"\n    },\n    \"modification_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Modification Date\"\n    },\n    \"source_path\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Source Path\"\n    },\n    \"number_of_items\": {\n      \"default\": -1,\n      \"title\": \"Number Of Items\",\n      \"type\": \"integer\"\n    },\n    \"ext\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Ext\"\n    },\n    \"mimetype\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Mimetype\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"path\"\n  ],\n  \"title\": \"OutputFile\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>path</code>                 (<code>str</code>)             </li> <li> <code>size</code>                 (<code>int | None</code>)             </li> <li> <code>creation_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>access_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>modification_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>source_path</code>                 (<code>str | None</code>)             </li> <li> <code>number_of_items</code>                 (<code>int</code>)             </li> <li> <code>ext</code>                 (<code>str | None</code>)             </li> <li> <code>mimetype</code>                 (<code>str | None</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class OutputFile(BaseItem):\n    \"\"\"Represents a single file in an output directory, extending BaseItem.\"\"\"\n\n    ext: str | None = None\n    mimetype: str | None = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.OutputFiles","title":"<code>OutputFiles</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseItem</code></p> <p>Represents a collection of files, typically within a directory.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFile\": {\n      \"description\": \"Represents a single file in an output directory, extending BaseItem.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"path\": {\n          \"title\": \"Path\",\n          \"type\": \"string\"\n        },\n        \"size\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Size\"\n        },\n        \"creation_date\": {\n          \"anyOf\": [\n            {\n              \"format\": \"date-time\",\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Creation Date\"\n        },\n        \"access_date\": {\n          \"anyOf\": [\n            {\n              \"format\": \"date-time\",\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Access Date\"\n        },\n        \"modification_date\": {\n          \"anyOf\": [\n            {\n              \"format\": \"date-time\",\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Modification Date\"\n        },\n        \"source_path\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Source Path\"\n        },\n        \"number_of_items\": {\n          \"default\": -1,\n          \"title\": \"Number Of Items\",\n          \"type\": \"integer\"\n        },\n        \"ext\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Ext\"\n        },\n        \"mimetype\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Mimetype\"\n        }\n      },\n      \"required\": [\n        \"name\",\n        \"path\"\n      ],\n      \"title\": \"OutputFile\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Represents a collection of files, typically within a directory.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"path\": {\n      \"title\": \"Path\",\n      \"type\": \"string\"\n    },\n    \"size\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Size\"\n    },\n    \"creation_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Creation Date\"\n    },\n    \"access_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Access Date\"\n    },\n    \"modification_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Modification Date\"\n    },\n    \"source_path\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Source Path\"\n    },\n    \"number_of_items\": {\n      \"default\": -1,\n      \"title\": \"Number Of Items\",\n      \"type\": \"integer\"\n    },\n    \"files\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/OutputFile\"\n      },\n      \"title\": \"Files\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"path\"\n  ],\n  \"title\": \"OutputFiles\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>path</code>                 (<code>str</code>)             </li> <li> <code>size</code>                 (<code>int | None</code>)             </li> <li> <code>creation_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>access_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>modification_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>source_path</code>                 (<code>str | None</code>)             </li> <li> <code>number_of_items</code>                 (<code>int</code>)             </li> <li> <code>files</code>                 (<code>list[OutputFile]</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class OutputFiles(BaseItem):\n    \"\"\"Represents a collection of files, typically within a directory.\"\"\"\n\n    files: list[OutputFile] = Field(default_factory=list)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.OutputTree","title":"<code>OutputTree</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>OutputFiles</code></p> <p>Represents a directory tree, including subdirectories.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"OutputFile\": {\n      \"description\": \"Represents a single file in an output directory, extending BaseItem.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"path\": {\n          \"title\": \"Path\",\n          \"type\": \"string\"\n        },\n        \"size\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Size\"\n        },\n        \"creation_date\": {\n          \"anyOf\": [\n            {\n              \"format\": \"date-time\",\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Creation Date\"\n        },\n        \"access_date\": {\n          \"anyOf\": [\n            {\n              \"format\": \"date-time\",\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Access Date\"\n        },\n        \"modification_date\": {\n          \"anyOf\": [\n            {\n              \"format\": \"date-time\",\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Modification Date\"\n        },\n        \"source_path\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Source Path\"\n        },\n        \"number_of_items\": {\n          \"default\": -1,\n          \"title\": \"Number Of Items\",\n          \"type\": \"integer\"\n        },\n        \"ext\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Ext\"\n        },\n        \"mimetype\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Mimetype\"\n        }\n      },\n      \"required\": [\n        \"name\",\n        \"path\"\n      ],\n      \"title\": \"OutputFile\",\n      \"type\": \"object\"\n    },\n    \"OutputFiles\": {\n      \"description\": \"Represents a collection of files, typically within a directory.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"path\": {\n          \"title\": \"Path\",\n          \"type\": \"string\"\n        },\n        \"size\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Size\"\n        },\n        \"creation_date\": {\n          \"anyOf\": [\n            {\n              \"format\": \"date-time\",\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Creation Date\"\n        },\n        \"access_date\": {\n          \"anyOf\": [\n            {\n              \"format\": \"date-time\",\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Access Date\"\n        },\n        \"modification_date\": {\n          \"anyOf\": [\n            {\n              \"format\": \"date-time\",\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Modification Date\"\n        },\n        \"source_path\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Source Path\"\n        },\n        \"number_of_items\": {\n          \"default\": -1,\n          \"title\": \"Number Of Items\",\n          \"type\": \"integer\"\n        },\n        \"files\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/OutputFile\"\n          },\n          \"title\": \"Files\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"name\",\n        \"path\"\n      ],\n      \"title\": \"OutputFiles\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Represents a directory tree, including subdirectories.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"path\": {\n      \"title\": \"Path\",\n      \"type\": \"string\"\n    },\n    \"size\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Size\"\n    },\n    \"creation_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Creation Date\"\n    },\n    \"access_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Access Date\"\n    },\n    \"modification_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Modification Date\"\n    },\n    \"source_path\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Source Path\"\n    },\n    \"number_of_items\": {\n      \"default\": -1,\n      \"title\": \"Number Of Items\",\n      \"type\": \"integer\"\n    },\n    \"files\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/OutputFile\"\n      },\n      \"title\": \"Files\",\n      \"type\": \"array\"\n    },\n    \"directories\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/OutputFiles\"\n      },\n      \"title\": \"Directories\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"path\"\n  ],\n  \"title\": \"OutputTree\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>path</code>                 (<code>str</code>)             </li> <li> <code>size</code>                 (<code>int | None</code>)             </li> <li> <code>creation_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>access_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>modification_date</code>                 (<code>datetime | None</code>)             </li> <li> <code>source_path</code>                 (<code>str | None</code>)             </li> <li> <code>number_of_items</code>                 (<code>int</code>)             </li> <li> <code>files</code>                 (<code>list[OutputFile]</code>)             </li> <li> <code>directories</code>                 (<code>list[OutputFiles]</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class OutputTree(OutputFiles):\n    \"\"\"Represents a directory tree, including subdirectories.\"\"\"\n\n    directories: list[OutputFiles] = Field(default_factory=list)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.RunInformation","title":"<code>RunInformation</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Contains summary information about a complete FlowGraph execution.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"NodeResult\": {\n      \"description\": \"Represents the execution result of a single node in a FlowGraph run.\",\n      \"properties\": {\n        \"node_id\": {\n          \"title\": \"Node Id\",\n          \"type\": \"integer\"\n        },\n        \"node_name\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Node Name\"\n        },\n        \"start_timestamp\": {\n          \"title\": \"Start Timestamp\",\n          \"type\": \"number\"\n        },\n        \"end_timestamp\": {\n          \"default\": 0,\n          \"title\": \"End Timestamp\",\n          \"type\": \"number\"\n        },\n        \"success\": {\n          \"anyOf\": [\n            {\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Success\"\n        },\n        \"error\": {\n          \"default\": \"\",\n          \"title\": \"Error\",\n          \"type\": \"string\"\n        },\n        \"run_time\": {\n          \"default\": -1,\n          \"title\": \"Run Time\",\n          \"type\": \"integer\"\n        },\n        \"is_running\": {\n          \"default\": true,\n          \"title\": \"Is Running\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"node_id\"\n      ],\n      \"title\": \"NodeResult\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Contains summary information about a complete FlowGraph execution.\",\n  \"properties\": {\n    \"flow_id\": {\n      \"title\": \"Flow Id\",\n      \"type\": \"integer\"\n    },\n    \"start_time\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"title\": \"Start Time\"\n    },\n    \"end_time\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"End Time\"\n    },\n    \"success\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Success\"\n    },\n    \"nodes_completed\": {\n      \"default\": 0,\n      \"title\": \"Nodes Completed\",\n      \"type\": \"integer\"\n    },\n    \"number_of_nodes\": {\n      \"default\": 0,\n      \"title\": \"Number Of Nodes\",\n      \"type\": \"integer\"\n    },\n    \"node_step_result\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/NodeResult\"\n      },\n      \"title\": \"Node Step Result\",\n      \"type\": \"array\"\n    },\n    \"run_type\": {\n      \"enum\": [\n        \"fetch_one\",\n        \"full_run\",\n        \"init\"\n      ],\n      \"title\": \"Run Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"flow_id\",\n    \"node_step_result\",\n    \"run_type\"\n  ],\n  \"title\": \"RunInformation\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flow_id</code>                 (<code>int</code>)             </li> <li> <code>start_time</code>                 (<code>datetime | None</code>)             </li> <li> <code>end_time</code>                 (<code>datetime | None</code>)             </li> <li> <code>success</code>                 (<code>bool | None</code>)             </li> <li> <code>nodes_completed</code>                 (<code>int</code>)             </li> <li> <code>number_of_nodes</code>                 (<code>int</code>)             </li> <li> <code>node_step_result</code>                 (<code>list[NodeResult]</code>)             </li> <li> <code>run_type</code>                 (<code>Literal['fetch_one', 'full_run', 'init']</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class RunInformation(BaseModel):\n    \"\"\"Contains summary information about a complete FlowGraph execution.\"\"\"\n\n    flow_id: int\n    start_time: datetime | None = Field(default_factory=datetime.now)\n    end_time: datetime | None = None\n    success: bool | None = None\n    nodes_completed: int = 0\n    number_of_nodes: int = 0\n    node_step_result: list[NodeResult]\n    run_type: Literal[\"fetch_one\", \"full_run\", \"init\"]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.TableExample","title":"<code>TableExample</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a preview of a table, including schema and sample data.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FileColumn\": {\n      \"description\": \"Represents detailed schema and statistics for a single column (field).\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"data_type\": {\n          \"title\": \"Data Type\",\n          \"type\": \"string\"\n        },\n        \"is_unique\": {\n          \"title\": \"Is Unique\",\n          \"type\": \"boolean\"\n        },\n        \"max_value\": {\n          \"title\": \"Max Value\",\n          \"type\": \"string\"\n        },\n        \"min_value\": {\n          \"title\": \"Min Value\",\n          \"type\": \"string\"\n        },\n        \"number_of_empty_values\": {\n          \"title\": \"Number Of Empty Values\",\n          \"type\": \"integer\"\n        },\n        \"number_of_filled_values\": {\n          \"title\": \"Number Of Filled Values\",\n          \"type\": \"integer\"\n        },\n        \"number_of_unique_values\": {\n          \"title\": \"Number Of Unique Values\",\n          \"type\": \"integer\"\n        },\n        \"size\": {\n          \"title\": \"Size\",\n          \"type\": \"integer\"\n        }\n      },\n      \"required\": [\n        \"name\",\n        \"data_type\",\n        \"is_unique\",\n        \"max_value\",\n        \"min_value\",\n        \"number_of_empty_values\",\n        \"number_of_filled_values\",\n        \"number_of_unique_values\",\n        \"size\"\n      ],\n      \"title\": \"FileColumn\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Represents a preview of a table, including schema and sample data.\",\n  \"properties\": {\n    \"node_id\": {\n      \"title\": \"Node Id\",\n      \"type\": \"integer\"\n    },\n    \"number_of_records\": {\n      \"title\": \"Number Of Records\",\n      \"type\": \"integer\"\n    },\n    \"number_of_columns\": {\n      \"title\": \"Number Of Columns\",\n      \"type\": \"integer\"\n    },\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"table_schema\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/FileColumn\"\n      },\n      \"title\": \"Table Schema\",\n      \"type\": \"array\"\n    },\n    \"columns\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Columns\",\n      \"type\": \"array\"\n    },\n    \"data\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"object\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": {},\n      \"title\": \"Data\"\n    },\n    \"has_example_data\": {\n      \"default\": false,\n      \"title\": \"Has Example Data\",\n      \"type\": \"boolean\"\n    },\n    \"has_run_with_current_setup\": {\n      \"default\": false,\n      \"title\": \"Has Run With Current Setup\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"required\": [\n    \"node_id\",\n    \"number_of_records\",\n    \"number_of_columns\",\n    \"name\",\n    \"table_schema\",\n    \"columns\"\n  ],\n  \"title\": \"TableExample\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>node_id</code>                 (<code>int</code>)             </li> <li> <code>number_of_records</code>                 (<code>int</code>)             </li> <li> <code>number_of_columns</code>                 (<code>int</code>)             </li> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>table_schema</code>                 (<code>list[FileColumn]</code>)             </li> <li> <code>columns</code>                 (<code>list[str]</code>)             </li> <li> <code>data</code>                 (<code>list[dict] | None</code>)             </li> <li> <code>has_example_data</code>                 (<code>bool</code>)             </li> <li> <code>has_run_with_current_setup</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class TableExample(BaseModel):\n    \"\"\"Represents a preview of a table, including schema and sample data.\"\"\"\n\n    node_id: int\n    number_of_records: int\n    number_of_columns: int\n    name: str\n    table_schema: list[FileColumn]\n    columns: list[str]\n    data: list[dict] | None = {}\n    has_example_data: bool = False\n    has_run_with_current_setup: bool = False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#web-api","title":"Web API","text":"<p>This section documents the FastAPI routes that expose <code>flowfile-core</code>'s functionality over HTTP.</p>"},{"location":"for-developers/python-api-reference.html#routes","title":"<code>routes</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes","title":"<code>flowfile_core.routes.routes</code>","text":"<p>Main API router and endpoint definitions for the Flowfile application.</p> <p>This module sets up the FastAPI router, defines all the API endpoints for interacting with flows, nodes, files, and other core components of the application. It handles the logic for creating, reading, updating, and deleting these resources.</p> <p>Functions:</p> Name Description <code>add_generic_settings</code> <p>A generic endpoint to update the settings of any node.</p> <code>add_node</code> <p>Adds a new, unconfigured node (a \"promise\") to the flow graph.</p> <code>cancel_flow</code> <p>Cancels a currently running flow execution.</p> <code>clear_history</code> <p>Clear all history for a flow.</p> <code>close_flow</code> <p>Closes an active flow session for the current user.</p> <code>connect_node</code> <p>Creates a connection (edge) between two nodes in the flow graph.</p> <code>copy_node</code> <p>Copies an existing node's settings to a new node promise.</p> <code>create_db_connection</code> <p>Creates and securely stores a new database connection.</p> <code>create_directory</code> <p>Creates a new directory at the specified path.</p> <code>create_flow</code> <p>Creates a new, empty flow file at the specified path and registers a session for it.</p> <code>delete_db_connection</code> <p>Deletes a stored database connection.</p> <code>delete_node</code> <p>Deletes a node from the flow graph.</p> <code>delete_node_connection</code> <p>Deletes a connection (edge) between two nodes.</p> <code>get_active_flow_file_sessions</code> <p>Retrieves a list of all currently active flow sessions for the current user.</p> <code>get_db_connections</code> <p>Retrieves all stored database connections for the current user (without passwords).</p> <code>get_default_path</code> <p>Returns the default starting path for the file browser (user data directory).</p> <code>get_description_node</code> <p>Retrieves the description text for a specific node.</p> <code>get_directory_contents</code> <p>Gets the contents of a directory path.</p> <code>get_downstream_node_ids</code> <p>Gets a list of all node IDs that are downstream dependencies of a given node.</p> <code>get_excel_sheet_names</code> <p>Retrieves the sheet names from an Excel file.</p> <code>get_expression_doc</code> <p>Retrieves documentation for available Polars expressions.</p> <code>get_expressions</code> <p>Retrieves a list of all available Flowfile expression names.</p> <code>get_flow</code> <p>Retrieves the settings for a specific flow.</p> <code>get_flow_artifacts</code> <p>Returns artifact visualization data for the canvas.</p> <code>get_flow_frontend_data</code> <p>Retrieves the data needed to render the flow graph in the frontend.</p> <code>get_flow_settings</code> <p>Retrieves the main settings for a flow.</p> <code>get_generated_code</code> <p>Generates and returns a Python script with Polars code representing the flow.</p> <code>get_graphic_walker_input</code> <p>Gets the data and configuration for the Graphic Walker data exploration tool.</p> <code>get_history_status</code> <p>Get the current state of the history system for a flow.</p> <code>get_instant_function_result</code> <p>Executes a simple, instant function on a node's data and returns the result.</p> <code>get_list_of_saved_flows</code> <p>Scans a directory for saved flow files (<code>.flowfile</code>).</p> <code>get_local_files</code> <p>Retrieves a list of files from a specified local directory.</p> <code>get_node</code> <p>Retrieves the complete state and data preview for a single node.</p> <code>get_node_available_artifacts</code> <p>Return available artifact metadata for a node.</p> <code>get_node_list</code> <p>Retrieves the list of all available node types and their templates.</p> <code>get_node_model</code> <p>(Internal) Retrieves a node's Pydantic model from the input_schema module by its name.</p> <code>get_node_upstream_ids</code> <p>Return the transitive upstream node IDs for a given node.</p> <code>get_reference_node</code> <p>Retrieves the reference identifier for a specific node.</p> <code>get_run_status</code> <p>Retrieves the run status information for a specific flow.</p> <code>get_table_example</code> <p>Retrieves a data preview (schema and sample rows) for a node's output.</p> <code>get_vue_flow_data</code> <p>Retrieves the flow data formatted for the Vue-based frontend.</p> <code>import_saved_flow</code> <p>Imports a flow from a saved <code>.yaml</code> and registers it as a new session for the current user.</p> <code>redo_action</code> <p>Redo the last undone action on the flow graph.</p> <code>register_flow</code> <p>Registers a new flow session with the application for the current user.</p> <code>run_flow</code> <p>Executes a flow in a background task.</p> <code>save_flow</code> <p>Saves the current state of a flow to a <code>.yaml</code>.</p> <code>trigger_fetch_node_data</code> <p>Fetches and refreshes the data for a specific node.</p> <code>undo_action</code> <p>Undo the last action on the flow graph.</p> <code>update_description_node</code> <p>Updates the description text for a specific node.</p> <code>update_flow_settings</code> <p>Updates the main settings for a flow.</p> <code>update_reference_node</code> <p>Updates the reference identifier for a specific node.</p> <code>upload_file</code> <p>Uploads a file to the server's 'uploads' directory.</p> <code>validate_db_settings</code> <p>Validates that a connection can be made to a database with the given settings.</p> <code>validate_node_reference</code> <p>Validates if a reference is valid and unique for a node.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.add_generic_settings","title":"<code>add_generic_settings(input_data, node_type, current_user=Depends(get_current_active_user))</code>","text":"<p>A generic endpoint to update the settings of any node.</p> <p>This endpoint dynamically determines the correct Pydantic model and update function based on the <code>node_type</code> parameter.</p> <p>Returns:</p> Type Description <code>OperationResponse</code> <p>OperationResponse with current history state.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/update_settings/\", tags=[\"transform\"], response_model=OperationResponse)\ndef add_generic_settings(\n    input_data: dict[str, Any], node_type: str, current_user=Depends(get_current_active_user)\n) -&gt; OperationResponse:\n    \"\"\"A generic endpoint to update the settings of any node.\n\n    This endpoint dynamically determines the correct Pydantic model and update\n    function based on the `node_type` parameter.\n\n    Returns:\n        OperationResponse with current history state.\n    \"\"\"\n    input_data[\"user_id\"] = current_user.id\n    node_type = camel_case_to_snake_case(node_type)\n    flow_id = int(input_data.get(\"flow_id\"))\n    node_id = int(input_data.get(\"node_id\"))\n    logger.info(f\"Updating the data for flow: {flow_id}, node {node_id}\")\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow.flow_settings.is_running:\n        raise HTTPException(422, \"Flow is running\")\n    if flow is None:\n        raise HTTPException(404, \"could not find the flow\")\n    add_func = getattr(flow, \"add_\" + node_type)\n    parsed_input = None\n    setting_name_ref = \"node\" + node_type.replace(\"_\", \"\")\n\n    if add_func is None:\n        raise HTTPException(404, \"could not find the function\")\n    try:\n        ref = get_node_model(setting_name_ref)\n        if ref:\n            parsed_input = ref(**input_data)\n    except Exception as e:\n        raise HTTPException(421, str(e))\n    if parsed_input is None:\n        raise HTTPException(404, \"could not find the interface\")\n    try:\n        # History capture is handled by the decorator on each add_* method\n        add_func(parsed_input)\n    except Exception as e:\n        logger.error(e)\n        raise HTTPException(419, str(f\"error: {e}\"))\n\n    return OperationResponse(success=True, history=flow.get_history_state())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.add_node","title":"<code>add_node(flow_id, node_id, node_type, pos_x=0, pos_y=0)</code>","text":"<p>Adds a new, unconfigured node (a \"promise\") to the flow graph.</p> <p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>int</code> <p>The ID of the flow to add the node to.</p> required <code>node_id</code> <code>int</code> <p>The client-generated ID for the new node.</p> required <code>node_type</code> <code>str</code> <p>The type of the node to add (e.g., 'filter', 'join').</p> required <code>pos_x</code> <code>int | float</code> <p>The X coordinate for the node's position in the UI.</p> <code>0</code> <code>pos_y</code> <code>int | float</code> <p>The Y coordinate for the node's position in the UI.</p> <code>0</code> <p>Returns:</p> Type Description <code>OperationResponse | None</code> <p>OperationResponse with current history state.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/editor/add_node/\", tags=[\"editor\"], response_model=OperationResponse)\ndef add_node(\n    flow_id: int, node_id: int, node_type: str, pos_x: int | float = 0, pos_y: int | float = 0\n) -&gt; OperationResponse | None:\n    \"\"\"Adds a new, unconfigured node (a \"promise\") to the flow graph.\n\n    Args:\n        flow_id: The ID of the flow to add the node to.\n        node_id: The client-generated ID for the new node.\n        node_type: The type of the node to add (e.g., 'filter', 'join').\n        pos_x: The X coordinate for the node's position in the UI.\n        pos_y: The Y coordinate for the node's position in the UI.\n\n    Returns:\n        OperationResponse with current history state.\n    \"\"\"\n    if isinstance(pos_x, float):\n        pos_x = int(pos_x)\n    if isinstance(pos_y, float):\n        pos_y = int(pos_y)\n    flow = flow_file_handler.get_flow(flow_id)\n    logger.info(f\"Adding a promise for {node_type}\")\n    if flow.flow_settings.is_running:\n        raise HTTPException(422, \"Flow is running\")\n\n    node = flow.get_node(node_id)\n    if node is not None:\n        flow.delete_node(node_id)\n    node_promise = input_schema.NodePromise(\n        flow_id=flow_id, node_id=node_id, cache_results=False, pos_x=pos_x, pos_y=pos_y, node_type=node_type\n    )\n    if node_type == \"explore_data\":\n        flow.add_initial_node_analysis(node_promise)\n    else:\n        # Capture state BEFORE adding node (for batched history)\n        pre_snapshot = flow.get_flowfile_data() if flow.flow_settings.track_history else None\n\n        logger.info(\"Adding node\")\n        # Add node without individual history tracking\n        flow.add_node_promise(node_promise, track_history=False)\n\n        if check_if_has_default_setting(node_type):\n            logger.info(f\"Found standard settings for {node_type}, trying to upload them\")\n            setting_name_ref = \"node\" + node_type.replace(\"_\", \"\")\n            node_model = get_node_model(setting_name_ref)\n\n            # Temporarily disable history tracking for initial settings\n            original_track_history = flow.flow_settings.track_history\n            flow.flow_settings.track_history = False\n            try:\n                add_func = getattr(flow, \"add_\" + node_type)\n                initial_settings = node_model(\n                    flow_id=flow_id, node_id=node_id, cache_results=False, pos_x=pos_x, pos_y=pos_y, node_type=node_type\n                )\n                add_func(initial_settings)\n            finally:\n                flow.flow_settings.track_history = original_track_history\n\n        # Capture batched history entry for the whole add_node operation\n        if pre_snapshot is not None and flow.flow_settings.track_history:\n            from flowfile_core.schemas.history_schema import HistoryActionType\n\n            flow._history_manager.capture_if_changed(\n                flow,\n                pre_snapshot,\n                HistoryActionType.ADD_NODE,\n                f\"Add {node_type} node\",\n                node_id,\n            )\n            logger.info(f\"History: Captured batched 'Add {node_type} node' entry\")\n\n    logger.info(f\"History state after add_node: {flow.get_history_state()}\")\n    return OperationResponse(success=True, history=flow.get_history_state())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.cancel_flow","title":"<code>cancel_flow(flow_id)</code>","text":"<p>Cancels a currently running flow execution.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/flow/cancel/\", tags=[\"editor\"])\ndef cancel_flow(flow_id: int):\n    \"\"\"Cancels a currently running flow execution.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if not flow.flow_settings.is_running:\n        raise HTTPException(422, \"Flow is not running\")\n    flow.cancel()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.clear_history","title":"<code>clear_history(flow_id)</code>","text":"<p>Clear all history for a flow.</p> <p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>int</code> <p>The ID of the flow to clear history for.</p> required Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/editor/history_clear/\", tags=[\"editor\"])\ndef clear_history(flow_id: int):\n    \"\"\"Clear all history for a flow.\n\n    Args:\n        flow_id: The ID of the flow to clear history for.\n    \"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, \"Could not find the flow\")\n    flow._history_manager.clear()\n    return {\"message\": \"History cleared successfully\"}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.close_flow","title":"<code>close_flow(flow_id, current_user=Depends(get_current_active_user))</code>","text":"<p>Closes an active flow session for the current user.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/editor/close_flow/\", tags=[\"editor\"])\ndef close_flow(flow_id: int, current_user=Depends(get_current_active_user)) -&gt; None:\n    \"\"\"Closes an active flow session for the current user.\"\"\"\n    user_id = current_user.id if current_user else None\n    flow_file_handler.delete_flow(flow_id, user_id=user_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.connect_node","title":"<code>connect_node(flow_id, node_connection)</code>","text":"<p>Creates a connection (edge) between two nodes in the flow graph.</p> <p>Returns:</p> Type Description <code>OperationResponse</code> <p>OperationResponse with current history state.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/editor/connect_node/\", tags=[\"editor\"], response_model=OperationResponse)\ndef connect_node(flow_id: int, node_connection: input_schema.NodeConnection) -&gt; OperationResponse:\n    \"\"\"Creates a connection (edge) between two nodes in the flow graph.\n\n    Returns:\n        OperationResponse with current history state.\n    \"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        logger.info(\"could not find the flow\")\n        raise HTTPException(404, \"could not find the flow\")\n    if flow.flow_settings.is_running:\n        raise HTTPException(422, \"Flow is running\")\n\n    # Capture history BEFORE the change\n    from_id = node_connection.output_connection.node_id\n    to_id = node_connection.input_connection.node_id\n    flow.capture_history_snapshot(HistoryActionType.ADD_CONNECTION, f\"Connect {from_id} -&gt; {to_id}\")\n\n    add_connection(flow, node_connection)\n\n    return OperationResponse(success=True, history=flow.get_history_state())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.copy_node","title":"<code>copy_node(node_id_to_copy_from, flow_id_to_copy_from, node_promise)</code>","text":"<p>Copies an existing node's settings to a new node promise.</p> <p>Parameters:</p> Name Type Description Default <code>node_id_to_copy_from</code> <code>int</code> <p>The ID of the node to copy the settings from.</p> required <code>flow_id_to_copy_from</code> <code>int</code> <p>The ID of the flow containing the source node.</p> required <code>node_promise</code> <code>NodePromise</code> <p>A <code>NodePromise</code> representing the new node to be created.</p> required <p>Returns:</p> Type Description <code>OperationResponse</code> <p>OperationResponse with current history state.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/editor/copy_node\", tags=[\"editor\"], response_model=OperationResponse)\ndef copy_node(\n    node_id_to_copy_from: int, flow_id_to_copy_from: int, node_promise: input_schema.NodePromise\n) -&gt; OperationResponse:\n    \"\"\"Copies an existing node's settings to a new node promise.\n\n    Args:\n        node_id_to_copy_from: The ID of the node to copy the settings from.\n        flow_id_to_copy_from: The ID of the flow containing the source node.\n        node_promise: A `NodePromise` representing the new node to be created.\n\n    Returns:\n        OperationResponse with current history state.\n    \"\"\"\n    try:\n        flow_to_copy_from = flow_file_handler.get_flow(flow_id_to_copy_from)\n        flow = (\n            flow_to_copy_from\n            if flow_id_to_copy_from == node_promise.flow_id\n            else flow_file_handler.get_flow(node_promise.flow_id)\n        )\n        node_to_copy = flow_to_copy_from.get_node(node_id_to_copy_from)\n        logger.info(f\"Copying data {node_promise.node_type}\")\n\n        if flow.flow_settings.is_running:\n            raise HTTPException(422, \"Flow is running\")\n\n        # Capture history BEFORE the change\n        flow.capture_history_snapshot(\n            HistoryActionType.COPY_NODE, f\"Copy {node_promise.node_type} node\", node_id=node_promise.node_id\n        )\n\n        if flow.get_node(node_promise.node_id) is not None:\n            flow.delete_node(node_promise.node_id)\n\n        if node_promise.node_type == \"explore_data\":\n            flow.add_initial_node_analysis(node_promise)\n            return OperationResponse(success=True, history=flow.get_history_state())\n\n        flow.copy_node(node_promise, node_to_copy.setting_input, node_to_copy.node_type)\n\n        return OperationResponse(success=True, history=flow.get_history_state())\n\n    except Exception as e:\n        logger.error(e)\n        raise HTTPException(422, str(e))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.create_db_connection","title":"<code>create_db_connection(input_connection, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>","text":"<p>Creates and securely stores a new database connection.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/db_connection_lib\", tags=[\"db_connections\"])\ndef create_db_connection(\n    input_connection: input_schema.FullDatabaseConnection,\n    current_user=Depends(get_current_active_user),\n    db: Session = Depends(get_db),\n):\n    \"\"\"Creates and securely stores a new database connection.\"\"\"\n    logger.info(f\"Creating database connection {input_connection.connection_name}\")\n    try:\n        store_database_connection(db, input_connection, current_user.id)\n    except ValueError:\n        raise HTTPException(422, \"Connection name already exists\")\n    except Exception as e:\n        logger.error(e)\n        raise HTTPException(422, str(e))\n    return {\"message\": \"Database connection created successfully\"}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.create_directory","title":"<code>create_directory(new_directory)</code>","text":"<p>Creates a new directory at the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>new_directory</code> <code>NewDirectory</code> <p>An <code>input_schema.NewDirectory</code> object with the path and name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the directory was created successfully.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/files/create_directory\", response_model=output_model.OutputDir, tags=[\"file manager\"])\ndef create_directory(new_directory: input_schema.NewDirectory) -&gt; bool:\n    \"\"\"Creates a new directory at the specified path.\n\n    Args:\n        new_directory: An `input_schema.NewDirectory` object with the path and name.\n\n    Returns:\n        `True` if the directory was created successfully.\n    \"\"\"\n    result, error = create_dir(new_directory)\n    if result:\n        return True\n    else:\n        raise error\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.create_flow","title":"<code>create_flow(flow_path=None, name=None, current_user=Depends(get_current_active_user))</code>","text":"<p>Creates a new, empty flow file at the specified path and registers a session for it.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/editor/create_flow/\", tags=[\"editor\"])\ndef create_flow(flow_path: str = None, name: str = None, current_user=Depends(get_current_active_user)):\n    \"\"\"Creates a new, empty flow file at the specified path and registers a session for it.\"\"\"\n    if flow_path is not None and name is None:\n        name = Path(flow_path).stem\n    elif flow_path is not None and name is not None:\n        if name not in flow_path and (flow_path.endswith(\".yaml\") or flow_path.endswith(\".yml\")):\n            raise HTTPException(422, \"The name must be part of the flow path when a full path is provided\")\n        elif name in flow_path and not (flow_path.endswith(\".yaml\") or flow_path.endswith(\".yml\")):\n            flow_path = str(Path(flow_path) / (name + \".yaml\"))\n        elif name not in flow_path and (name.endswith(\".yaml\") or name.endswith(\".yml\")):\n            flow_path = str(Path(flow_path) / name)\n        elif name not in flow_path and not (name.endswith(\".yaml\") or name.endswith(\".yml\")):\n            flow_path = str(Path(flow_path) / (name + \".yaml\"))\n    if flow_path is not None:\n        # Validate path is within allowed sandbox\n        flow_path = validate_path_under_cwd(flow_path)\n        flow_path_ref = Path(flow_path)\n        if not flow_path_ref.parent.exists():\n            raise HTTPException(422, \"The directory does not exist\")\n    user_id = current_user.id if current_user else None\n    flow_id = flow_file_handler.add_flow(name=name, flow_path=flow_path, user_id=user_id)\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow and flow.flow_settings:\n        _auto_register_flow(flow.flow_settings.path, name or flow.flow_settings.name, user_id)\n    return flow_id\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.delete_db_connection","title":"<code>delete_db_connection(connection_name, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>","text":"<p>Deletes a stored database connection.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.delete(\"/db_connection_lib\", tags=[\"db_connections\"])\ndef delete_db_connection(\n    connection_name: str, current_user=Depends(get_current_active_user), db: Session = Depends(get_db)\n):\n    \"\"\"Deletes a stored database connection.\"\"\"\n    logger.info(f\"Deleting database connection {connection_name}\")\n    db_connection = get_database_connection(db, connection_name, current_user.id)\n    if db_connection is None:\n        raise HTTPException(404, \"Database connection not found\")\n    delete_database_connection(db, connection_name, current_user.id)\n    return {\"message\": \"Database connection deleted successfully\"}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.delete_node","title":"<code>delete_node(flow_id, node_id)</code>","text":"<p>Deletes a node from the flow graph.</p> <p>Returns:</p> Type Description <code>OperationResponse</code> <p>OperationResponse with current history state.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/editor/delete_node/\", tags=[\"editor\"], response_model=OperationResponse)\ndef delete_node(flow_id: int | None, node_id: int) -&gt; OperationResponse:\n    \"\"\"Deletes a node from the flow graph.\n\n    Returns:\n        OperationResponse with current history state.\n    \"\"\"\n    logger.info(\"Deleting node\")\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow.flow_settings.is_running:\n        raise HTTPException(422, \"Flow is running\")\n\n    # Capture history BEFORE the change\n    node = flow.get_node(node_id)\n    node_type = node.node_type if node else \"unknown\"\n    flow.capture_history_snapshot(HistoryActionType.DELETE_NODE, f\"Delete {node_type} node\", node_id=node_id)\n\n    flow.delete_node(node_id)\n\n    return OperationResponse(success=True, history=flow.get_history_state())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.delete_node_connection","title":"<code>delete_node_connection(flow_id, node_connection=None)</code>","text":"<p>Deletes a connection (edge) between two nodes.</p> <p>Returns:</p> Type Description <code>OperationResponse</code> <p>OperationResponse with current history state.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/editor/delete_connection/\", tags=[\"editor\"], response_model=OperationResponse)\ndef delete_node_connection(flow_id: int, node_connection: input_schema.NodeConnection = None) -&gt; OperationResponse:\n    \"\"\"Deletes a connection (edge) between two nodes.\n\n    Returns:\n        OperationResponse with current history state.\n    \"\"\"\n    flow_id = int(flow_id)\n    logger.info(\n        f\"Deleting connection node {node_connection.output_connection.node_id} to node {node_connection.input_connection.node_id}\"\n    )\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow.flow_settings.is_running:\n        raise HTTPException(422, \"Flow is running\")\n\n    # Capture history BEFORE the change\n    from_id = node_connection.output_connection.node_id\n    to_id = node_connection.input_connection.node_id\n    flow.capture_history_snapshot(HistoryActionType.DELETE_CONNECTION, f\"Delete connection {from_id} -&gt; {to_id}\")\n\n    delete_connection(flow, node_connection)\n\n    return OperationResponse(success=True, history=flow.get_history_state())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_active_flow_file_sessions","title":"<code>get_active_flow_file_sessions(current_user=Depends(get_current_active_user))</code>  <code>async</code>","text":"<p>Retrieves a list of all currently active flow sessions for the current user.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/active_flowfile_sessions/\", response_model=list[schemas.FlowSettings])\nasync def get_active_flow_file_sessions(current_user=Depends(get_current_active_user)) -&gt; list[schemas.FlowSettings]:\n    \"\"\"Retrieves a list of all currently active flow sessions for the current user.\"\"\"\n    user_id = current_user.id if current_user else None\n    return [flf.flow_settings for flf in flow_file_handler.get_user_flows(user_id)]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_db_connections","title":"<code>get_db_connections(db=Depends(get_db), current_user=Depends(get_current_active_user))</code>","text":"<p>Retrieves all stored database connections for the current user (without passwords).</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\n    \"/db_connection_lib\", tags=[\"db_connections\"], response_model=list[input_schema.FullDatabaseConnectionInterface]\n)\ndef get_db_connections(\n    db: Session = Depends(get_db), current_user=Depends(get_current_active_user)\n) -&gt; list[input_schema.FullDatabaseConnectionInterface]:\n    \"\"\"Retrieves all stored database connections for the current user (without passwords).\"\"\"\n    return get_all_database_connections_interface(db, current_user.id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_default_path","title":"<code>get_default_path()</code>  <code>async</code>","text":"<p>Returns the default starting path for the file browser (user data directory).</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/files/default_path/\", response_model=str, tags=[\"file manager\"])\nasync def get_default_path() -&gt; str:\n    \"\"\"Returns the default starting path for the file browser (user data directory).\"\"\"\n    return str(storage.user_data_directory)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_description_node","title":"<code>get_description_node(flow_id, node_id)</code>","text":"<p>Retrieves the description text for a specific node.</p> <p>Returns the user-provided description if set, otherwise falls back to an auto-generated description based on the node's configuration. The response includes an <code>is_auto_generated</code> flag so the frontend knows whether to refresh the description after settings changes.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/node/description\", response_model=output_model.NodeDescriptionResponse, tags=[\"editor\"])\ndef get_description_node(flow_id: int, node_id: int):\n    \"\"\"Retrieves the description text for a specific node.\n\n    Returns the user-provided description if set, otherwise falls back\n    to an auto-generated description based on the node's configuration.\n    The response includes an `is_auto_generated` flag so the frontend\n    knows whether to refresh the description after settings changes.\n    \"\"\"\n    try:\n        node = flow_file_handler.get_flow(flow_id).get_node(node_id)\n    except:\n        raise HTTPException(404, \"Could not find the node\")\n    if node is None:\n        raise HTTPException(404, \"Could not find the node\")\n    user_description = node.setting_input.description if hasattr(node.setting_input, \"description\") else \"\"\n    if user_description:\n        return output_model.NodeDescriptionResponse(description=user_description, is_auto_generated=False)\n    if hasattr(node.setting_input, \"get_default_description\"):\n        auto_desc = node.setting_input.get_default_description()\n        return output_model.NodeDescriptionResponse(description=auto_desc, is_auto_generated=True)\n    return output_model.NodeDescriptionResponse(description=\"\", is_auto_generated=True)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_directory_contents","title":"<code>get_directory_contents(directory, file_types=None, include_hidden=False)</code>  <code>async</code>","text":"<p>Gets the contents of a directory path.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The absolute path to the directory.</p> required <code>file_types</code> <code>list[str]</code> <p>An optional list of file extensions to filter by.</p> <code>None</code> <code>include_hidden</code> <code>bool</code> <p>If True, includes hidden files and directories.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[FileInfo]</code> <p>A list of <code>FileInfo</code> objects representing the directory's contents.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/files/directory_contents/\", response_model=list[FileInfo], tags=[\"file manager\"])\nasync def get_directory_contents(\n    directory: str, file_types: list[str] = None, include_hidden: bool = False\n) -&gt; list[FileInfo]:\n    \"\"\"Gets the contents of a directory path.\n\n    Args:\n        directory: The absolute path to the directory.\n        file_types: An optional list of file extensions to filter by.\n        include_hidden: If True, includes hidden files and directories.\n\n    Returns:\n        A list of `FileInfo` objects representing the directory's contents.\n    \"\"\"\n    from flowfile_core.configs.settings import is_electron_mode\n\n    # In Electron mode, allow browsing the entire filesystem (no sandbox).\n    # In other modes, sandbox to the user data directory.\n    sandbox_root = None if is_electron_mode() else storage.user_data_directory\n    try:\n        directory_explorer = SecureFileExplorer(directory, sandbox_root)\n        return directory_explorer.list_contents(show_hidden=include_hidden, file_types=file_types)\n    except PermissionError:\n        raise HTTPException(403, \"Access denied: path is outside the allowed directory\")\n    except Exception as e:\n        logger.error(e)\n        raise HTTPException(404, \"Could not access the directory\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_downstream_node_ids","title":"<code>get_downstream_node_ids(flow_id, node_id)</code>  <code>async</code>","text":"<p>Gets a list of all node IDs that are downstream dependencies of a given node.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/node/downstream_node_ids\", response_model=list[int], tags=[\"editor\"])\nasync def get_downstream_node_ids(flow_id: int, node_id: int) -&gt; list[int]:\n    \"\"\"Gets a list of all node IDs that are downstream dependencies of a given node.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    node = flow.get_node(node_id)\n    return list(node.get_all_dependent_node_ids())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_excel_sheet_names","title":"<code>get_excel_sheet_names(path)</code>  <code>async</code>","text":"<p>Retrieves the sheet names from an Excel file.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/api/get_xlsx_sheet_names\", tags=[\"excel_reader\"], response_model=list[str])\nasync def get_excel_sheet_names(path: str) -&gt; list[str] | None:\n    \"\"\"Retrieves the sheet names from an Excel file.\"\"\"\n    validated_path = validate_path_under_cwd(path)\n    sheet_names = excel_file_manager.get_sheet_names(validated_path)\n    if sheet_names:\n        return sheet_names\n    else:\n        raise HTTPException(404, \"File not found\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_expression_doc","title":"<code>get_expression_doc()</code>","text":"<p>Retrieves documentation for available Polars expressions.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/editor/expression_doc\", tags=[\"editor\"], response_model=list[output_model.ExpressionsOverview])\ndef get_expression_doc() -&gt; list[output_model.ExpressionsOverview]:\n    \"\"\"Retrieves documentation for available Polars expressions.\"\"\"\n    return get_expression_overview()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_expressions","title":"<code>get_expressions()</code>","text":"<p>Retrieves a list of all available Flowfile expression names.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/editor/expressions\", tags=[\"editor\"], response_model=list[str])\ndef get_expressions() -&gt; list[str]:\n    \"\"\"Retrieves a list of all available Flowfile expression names.\"\"\"\n    return get_all_expressions()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_flow","title":"<code>get_flow(flow_id)</code>","text":"<p>Retrieves the settings for a specific flow.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/editor/flow\", tags=[\"editor\"], response_model=schemas.FlowSettings)\ndef get_flow(flow_id: int):\n    \"\"\"Retrieves the settings for a specific flow.\"\"\"\n    flow_id = int(flow_id)\n    result = get_flow_settings(flow_id)\n    return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_flow_artifacts","title":"<code>get_flow_artifacts(flow_id)</code>","text":"<p>Returns artifact visualization data for the canvas.</p> <p>Includes per-node artifact summaries (for badges/tooltips) and artifact edges (for dashed-line connections between publisher and consumer nodes).</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/flow/artifacts\", tags=[\"editor\"])\ndef get_flow_artifacts(flow_id: int):\n    \"\"\"Returns artifact visualization data for the canvas.\n\n    Includes per-node artifact summaries (for badges/tooltips) and\n    artifact edges (for dashed-line connections between publisher and\n    consumer nodes).\n    \"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, \"Could not find the flow\")\n    ctx = flow.artifact_context\n    return {\n        \"nodes\": ctx.get_node_summaries(),\n        \"edges\": ctx.get_artifact_edges(),\n    }\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_flow_frontend_data","title":"<code>get_flow_frontend_data(flow_id=1)</code>","text":"<p>Retrieves the data needed to render the flow graph in the frontend.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/flow_data\", tags=[\"manager\"])\ndef get_flow_frontend_data(flow_id: int | None = 1):\n    \"\"\"Retrieves the data needed to render the flow graph in the frontend.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, \"could not find the flow\")\n    return flow.get_frontend_data()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_flow_settings","title":"<code>get_flow_settings(flow_id=1)</code>","text":"<p>Retrieves the main settings for a flow.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/flow_settings\", tags=[\"manager\"], response_model=schemas.FlowSettings)\ndef get_flow_settings(flow_id: int | None = 1) -&gt; schemas.FlowSettings:\n    \"\"\"Retrieves the main settings for a flow.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, \"could not find the flow\")\n    return flow.flow_settings\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_generated_code","title":"<code>get_generated_code(flow_id)</code>","text":"<p>Generates and returns a Python script with Polars code representing the flow.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/editor/code_to_polars\", tags=[], response_model=str)\ndef get_generated_code(flow_id: int) -&gt; str:\n    \"\"\"Generates and returns a Python script with Polars code representing the flow.\"\"\"\n    flow_id = int(flow_id)\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, \"could not find the flow\")\n    return export_flow_to_polars(flow)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_graphic_walker_input","title":"<code>get_graphic_walker_input(flow_id, node_id)</code>","text":"<p>Gets the data and configuration for the Graphic Walker data exploration tool.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/analysis_data/graphic_walker_input\", tags=[\"analysis\"], response_model=input_schema.NodeExploreData)\ndef get_graphic_walker_input(flow_id: int, node_id: int):\n    \"\"\"Gets the data and configuration for the Graphic Walker data exploration tool.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    node = flow.get_node(node_id)\n    if node.results.analysis_data_generator is None:\n        logger.error(\"The data is not refreshed and available for analysis\")\n        raise HTTPException(422, \"The data is not refreshed and available for analysis\")\n    return AnalyticsProcessor.process_graphic_walker_input(node)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_history_status","title":"<code>get_history_status(flow_id)</code>","text":"<p>Get the current state of the history system for a flow.</p> <p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>int</code> <p>The ID of the flow to get history status for.</p> required <p>Returns:</p> Type Description <code>HistoryState</code> <p>HistoryState with information about available undo/redo operations.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/editor/history_status/\", tags=[\"editor\"], response_model=HistoryState)\ndef get_history_status(flow_id: int) -&gt; HistoryState:\n    \"\"\"Get the current state of the history system for a flow.\n\n    Args:\n        flow_id: The ID of the flow to get history status for.\n\n    Returns:\n        HistoryState with information about available undo/redo operations.\n    \"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, \"Could not find the flow\")\n    return flow.get_history_state()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_instant_function_result","title":"<code>get_instant_function_result(flow_id, node_id, func_string)</code>  <code>async</code>","text":"<p>Executes a simple, instant function on a node's data and returns the result.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/custom_functions/instant_result\", tags=[])\nasync def get_instant_function_result(flow_id: int, node_id: int, func_string: str):\n    \"\"\"Executes a simple, instant function on a node's data and returns the result.\"\"\"\n    try:\n        node = flow_file_handler.get_node(flow_id, node_id)\n        result = await asyncio.to_thread(get_instant_func_results, node, func_string)\n        return result\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_list_of_saved_flows","title":"<code>get_list_of_saved_flows(path)</code>","text":"<p>Scans a directory for saved flow files (<code>.flowfile</code>).</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/files/available_flow_files\", tags=[\"editor\"], response_model=list[FileInfo])\ndef get_list_of_saved_flows(path: str):\n    \"\"\"Scans a directory for saved flow files (`.flowfile`).\"\"\"\n    try:\n        # Validate path is within sandbox before proceeding\n        explorer = SecureFileExplorer(start_path=storage.user_data_directory, sandbox_root=storage.user_data_directory)\n        validated_path = explorer.get_absolute_path(path)\n        if validated_path is None:\n            return []\n        return get_files_from_directory(\n            str(validated_path), types=[\"flowfile\"], sandbox_root=storage.user_data_directory\n        )\n    except:\n        return []\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_local_files","title":"<code>get_local_files(directory)</code>  <code>async</code>","text":"<p>Retrieves a list of files from a specified local directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The absolute path of the directory to scan.</p> required <p>Returns:</p> Type Description <code>list[FileInfo]</code> <p>A list of <code>FileInfo</code> objects for each item in the directory.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if the directory does not exist.</p> <code>HTTPException</code> <p>403 if access is denied (path outside sandbox).</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/files/files_in_local_directory/\", response_model=list[FileInfo], tags=[\"file manager\"])\nasync def get_local_files(directory: str) -&gt; list[FileInfo]:\n    \"\"\"Retrieves a list of files from a specified local directory.\n\n    Args:\n        directory: The absolute path of the directory to scan.\n\n    Returns:\n        A list of `FileInfo` objects for each item in the directory.\n\n    Raises:\n        HTTPException: 404 if the directory does not exist.\n        HTTPException: 403 if access is denied (path outside sandbox).\n    \"\"\"\n    # Validate path is within sandbox before proceeding\n    explorer = SecureFileExplorer(start_path=storage.user_data_directory, sandbox_root=storage.user_data_directory)\n    validated_path = explorer.get_absolute_path(directory)\n    if validated_path is None:\n        raise HTTPException(403, \"Access denied or directory does not exist\")\n    if not validated_path.exists() or not validated_path.is_dir():\n        raise HTTPException(404, \"Directory does not exist\")\n    files = get_files_from_directory(str(validated_path), sandbox_root=storage.user_data_directory)\n    if files is None:\n        raise HTTPException(403, \"Access denied or directory does not exist\")\n    return files\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_node","title":"<code>get_node(flow_id, node_id, get_data=False)</code>","text":"<p>Retrieves the complete state and data preview for a single node.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/node\", response_model=output_model.NodeData, tags=[\"editor\"])\ndef get_node(flow_id: int, node_id: int, get_data: bool = False):\n    \"\"\"Retrieves the complete state and data preview for a single node.\"\"\"\n    logging.info(f\"Getting node {node_id} from flow {flow_id}\")\n    flow = flow_file_handler.get_flow(flow_id)\n    node = flow.get_node(node_id)\n    if node is None:\n        raise HTTPException(422, \"Not found\")\n    v = node.get_node_data(flow_id=flow.flow_id, include_example=get_data)\n    return v\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_node_available_artifacts","title":"<code>get_node_available_artifacts(flow_id, node_id, kernel_id=None)</code>","text":"<p>Return available artifact metadata for a node.</p> <p>Used by the frontend to populate artifact selector UI components.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/flow/node_available_artifacts\", tags=[\"editor\"])\ndef get_node_available_artifacts(flow_id: int, node_id: int, kernel_id: str | None = None):\n    \"\"\"Return available artifact metadata for a node.\n\n    Used by the frontend to populate artifact selector UI components.\n    \"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, \"Could not find the flow\")\n    node = flow.get_node(node_id)\n    if node is None:\n        raise HTTPException(404, \"Could not find the node\")\n    resolved_kernel_id = kernel_id or getattr(node.setting_input, \"kernel_id\", None)\n    if not resolved_kernel_id:\n        return {\"artifacts\": []}\n\n    upstream_ids = flow._get_upstream_node_ids(node_id)\n    available = flow.artifact_context.compute_available(\n        node_id=node_id,\n        kernel_id=resolved_kernel_id,\n        upstream_node_ids=upstream_ids,\n    )\n    return {\"artifacts\": [ref.to_dict() for ref in available.values()]}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_node_list","title":"<code>get_node_list()</code>","text":"<p>Retrieves the list of all available node types and their templates.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/node_list\", response_model=list[schemas.NodeTemplate])\ndef get_node_list() -&gt; list[schemas.NodeTemplate]:\n    \"\"\"Retrieves the list of all available node types and their templates.\"\"\"\n    return nodes_list\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_node_model","title":"<code>get_node_model(setting_name_ref)</code>","text":"<p>(Internal) Retrieves a node's Pydantic model from the input_schema module by its name.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>def get_node_model(setting_name_ref: str):\n    \"\"\"(Internal) Retrieves a node's Pydantic model from the input_schema module by its name.\"\"\"\n    logger.info(\"Getting node model for: \" + setting_name_ref)\n    for ref_name, ref in inspect.getmodule(input_schema).__dict__.items():\n        if ref_name.lower() == setting_name_ref:\n            return ref\n    logger.error(f\"Could not find node model for: {setting_name_ref}\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_node_upstream_ids","title":"<code>get_node_upstream_ids(flow_id, node_id)</code>","text":"<p>Return the transitive upstream node IDs for a given node.</p> <p>Used by the frontend to determine which artifacts are actually reachable (via the DAG) from a specific python_script node.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/flow/node_upstream_ids\", tags=[\"editor\"])\ndef get_node_upstream_ids(flow_id: int, node_id: int):\n    \"\"\"Return the transitive upstream node IDs for a given node.\n\n    Used by the frontend to determine which artifacts are actually\n    reachable (via the DAG) from a specific python_script node.\n    \"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, \"Could not find the flow\")\n    return {\"upstream_node_ids\": flow._get_upstream_node_ids(node_id)}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_reference_node","title":"<code>get_reference_node(flow_id, node_id)</code>","text":"<p>Retrieves the reference identifier for a specific node.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/node/reference\", tags=[\"editor\"])\ndef get_reference_node(flow_id: int, node_id: int):\n    \"\"\"Retrieves the reference identifier for a specific node.\"\"\"\n    try:\n        node = flow_file_handler.get_flow(flow_id).get_node(node_id)\n    except:\n        raise HTTPException(404, \"Could not find the node\")\n    if node is None:\n        raise HTTPException(404, \"Could not find the node\")\n    return node.setting_input.node_reference or \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_run_status","title":"<code>get_run_status(flow_id, response)</code>","text":"<p>Retrieves the run status information for a specific flow.</p> <p>Returns a 202 Accepted status while the flow is running, and 200 OK when finished.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/flow/run_status/\", tags=[\"editor\"], response_model=output_model.RunInformation)\ndef get_run_status(flow_id: int, response: Response):\n    \"\"\"Retrieves the run status information for a specific flow.\n\n    Returns a 202 Accepted status while the flow is running, and 200 OK when finished.\n    \"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if not flow:\n        raise HTTPException(status_code=404, detail=\"Flow not found\")\n    if flow.flow_settings.is_running:\n        response.status_code = status.HTTP_202_ACCEPTED\n    else:\n        response.status_code = status.HTTP_200_OK\n    return flow.get_run_info()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_table_example","title":"<code>get_table_example(flow_id, node_id)</code>","text":"<p>Retrieves a data preview (schema and sample rows) for a node's output.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/node/data\", response_model=output_model.TableExample, tags=[\"editor\"])\ndef get_table_example(flow_id: int, node_id: int):\n    \"\"\"Retrieves a data preview (schema and sample rows) for a node's output.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    node = flow.get_node(node_id)\n    return node.get_table_example(True)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_vue_flow_data","title":"<code>get_vue_flow_data(flow_id)</code>","text":"<p>Retrieves the flow data formatted for the Vue-based frontend.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/flow_data/v2\", tags=[\"manager\"])\ndef get_vue_flow_data(flow_id: int) -&gt; schemas.VueFlowInput:\n    \"\"\"Retrieves the flow data formatted for the Vue-based frontend.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, \"could not find the flow\")\n    data = flow.get_vue_flow_input()\n    return data\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.import_saved_flow","title":"<code>import_saved_flow(flow_path, current_user=Depends(get_current_active_user))</code>","text":"<p>Imports a flow from a saved <code>.yaml</code> and registers it as a new session for the current user.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/import_flow/\", tags=[\"editor\"], response_model=int)\ndef import_saved_flow(flow_path: str, current_user=Depends(get_current_active_user)) -&gt; int:\n    \"\"\"Imports a flow from a saved `.yaml` and registers it as a new session for the current user.\"\"\"\n    validated_path = validate_path_under_cwd(flow_path)\n    if not os.path.exists(validated_path):\n        raise HTTPException(404, \"File not found\")\n    user_id = current_user.id if current_user else None\n    flow_id = flow_file_handler.import_flow(Path(validated_path), user_id=user_id)\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow and flow.flow_settings:\n        _auto_register_flow(validated_path, flow.flow_settings.name, user_id)\n    return flow_id\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.redo_action","title":"<code>redo_action(flow_id)</code>","text":"<p>Redo the last undone action on the flow graph.</p> <p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>int</code> <p>The ID of the flow to redo.</p> required <p>Returns:</p> Type Description <code>UndoRedoResult</code> <p>UndoRedoResult indicating success or failure.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/editor/redo/\", tags=[\"editor\"], response_model=UndoRedoResult)\ndef redo_action(flow_id: int) -&gt; UndoRedoResult:\n    \"\"\"Redo the last undone action on the flow graph.\n\n    Args:\n        flow_id: The ID of the flow to redo.\n\n    Returns:\n        UndoRedoResult indicating success or failure.\n    \"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, \"Could not find the flow\")\n    if flow.flow_settings.is_running:\n        raise HTTPException(422, \"Flow is running\")\n    return flow.redo()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.register_flow","title":"<code>register_flow(flow_data, current_user=Depends(get_current_active_user))</code>","text":"<p>Registers a new flow session with the application for the current user.</p> <p>Parameters:</p> Name Type Description Default <code>flow_data</code> <code>FlowSettings</code> <p>The <code>FlowSettings</code> for the new flow.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The ID of the newly registered flow.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/flow/register/\", tags=[\"editor\"])\ndef register_flow(flow_data: schemas.FlowSettings, current_user=Depends(get_current_active_user)) -&gt; int:\n    \"\"\"Registers a new flow session with the application for the current user.\n\n    Args:\n        flow_data: The `FlowSettings` for the new flow.\n\n    Returns:\n        The ID of the newly registered flow.\n    \"\"\"\n    user_id = current_user.id if current_user else None\n    return flow_file_handler.register_flow(flow_data, user_id=user_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.run_flow","title":"<code>run_flow(flow_id, background_tasks, current_user=Depends(get_current_active_user))</code>  <code>async</code>","text":"<p>Executes a flow in a background task.</p> <p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>int</code> <p>The ID of the flow to execute.</p> required <code>background_tasks</code> <code>BackgroundTasks</code> <p>FastAPI's background task runner.</p> required <p>Returns:</p> Type Description <code>JSONResponse</code> <p>A JSON response indicating that the flow has started.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/flow/run/\", tags=[\"editor\"])\nasync def run_flow(\n    flow_id: int, background_tasks: BackgroundTasks, current_user=Depends(get_current_active_user)\n) -&gt; JSONResponse:\n    \"\"\"Executes a flow in a background task.\n\n    Args:\n        flow_id: The ID of the flow to execute.\n        background_tasks: FastAPI's background task runner.\n\n    Returns:\n        A JSON response indicating that the flow has started.\n    \"\"\"\n    logger.info(\"starting to run...\")\n    flow = flow_file_handler.get_flow(flow_id)\n    lock = get_flow_run_lock(flow_id)\n    user_id = current_user.id if current_user else None\n    async with lock:\n        if flow.flow_settings.is_running:\n            raise HTTPException(422, \"Flow is already running\")\n        background_tasks.add_task(_run_and_track, flow, user_id)\n    return JSONResponse(content={\"message\": \"Data started\", \"flow_id\": flow_id}, status_code=status.HTTP_200_OK)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.save_flow","title":"<code>save_flow(flow_id, flow_path=None)</code>","text":"<p>Saves the current state of a flow to a <code>.yaml</code>.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/save_flow\", tags=[\"editor\"])\ndef save_flow(flow_id: int, flow_path: str = None):\n    \"\"\"Saves the current state of a flow to a `.yaml`.\"\"\"\n    if flow_path is not None:\n        flow_path = validate_path_under_cwd(flow_path)\n    flow = flow_file_handler.get_flow(flow_id)\n    flow.save_flow(flow_path=flow_path)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.trigger_fetch_node_data","title":"<code>trigger_fetch_node_data(flow_id, node_id, background_tasks)</code>  <code>async</code>","text":"<p>Fetches and refreshes the data for a specific node.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/node/trigger_fetch_data\", tags=[\"editor\"])\nasync def trigger_fetch_node_data(flow_id: int, node_id: int, background_tasks: BackgroundTasks):\n    \"\"\"Fetches and refreshes the data for a specific node.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    lock = get_flow_run_lock(flow_id)\n    async with lock:\n        if flow.flow_settings.is_running:\n            raise HTTPException(422, \"Flow is already running\")\n        try:\n            flow.validate_if_node_can_be_fetched(node_id)\n        except Exception as e:\n            raise HTTPException(422, str(e))\n        background_tasks.add_task(flow.trigger_fetch_node, node_id)\n    return JSONResponse(\n        content={\"message\": \"Data started\", \"flow_id\": flow_id, \"node_id\": node_id}, status_code=status.HTTP_200_OK\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.undo_action","title":"<code>undo_action(flow_id)</code>","text":"<p>Undo the last action on the flow graph.</p> <p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>int</code> <p>The ID of the flow to undo.</p> required <p>Returns:</p> Type Description <code>UndoRedoResult</code> <p>UndoRedoResult indicating success or failure.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/editor/undo/\", tags=[\"editor\"], response_model=UndoRedoResult)\ndef undo_action(flow_id: int) -&gt; UndoRedoResult:\n    \"\"\"Undo the last action on the flow graph.\n\n    Args:\n        flow_id: The ID of the flow to undo.\n\n    Returns:\n        UndoRedoResult indicating success or failure.\n    \"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, \"Could not find the flow\")\n    if flow.flow_settings.is_running:\n        raise HTTPException(422, \"Flow is running\")\n    return flow.undo()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.update_description_node","title":"<code>update_description_node(flow_id, node_id, description=Body(...))</code>","text":"<p>Updates the description text for a specific node.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/node/description/\", tags=[\"editor\"])\ndef update_description_node(flow_id: int, node_id: int, description: str = Body(...)):\n    \"\"\"Updates the description text for a specific node.\"\"\"\n    try:\n        node = flow_file_handler.get_flow(flow_id).get_node(node_id)\n    except:\n        raise HTTPException(404, \"Could not find the node\")\n    node.setting_input.description = description\n    return True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.update_flow_settings","title":"<code>update_flow_settings(flow_settings)</code>","text":"<p>Updates the main settings for a flow.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/flow_settings\", tags=[\"manager\"])\ndef update_flow_settings(flow_settings: schemas.FlowSettings):\n    \"\"\"Updates the main settings for a flow.\"\"\"\n    flow = flow_file_handler.get_flow(flow_settings.flow_id)\n    if flow is None:\n        raise HTTPException(404, \"could not find the flow\")\n    flow.flow_settings = flow_settings\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.update_reference_node","title":"<code>update_reference_node(flow_id, node_id, reference=Body(...))</code>","text":"<p>Updates the reference identifier for a specific node.</p> <p>The reference must be: - Lowercase only - No spaces allowed - Unique across all nodes in the flow</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/node/reference/\", tags=[\"editor\"])\ndef update_reference_node(flow_id: int, node_id: int, reference: str = Body(...)):\n    \"\"\"Updates the reference identifier for a specific node.\n\n    The reference must be:\n    - Lowercase only\n    - No spaces allowed\n    - Unique across all nodes in the flow\n    \"\"\"\n    try:\n        flow = flow_file_handler.get_flow(flow_id)\n        node = flow.get_node(node_id)\n    except:\n        raise HTTPException(404, \"Could not find the node\")\n    if node is None:\n        raise HTTPException(404, \"Could not find the node\")\n\n    # Handle empty reference (allow clearing)\n    if reference == \"\" or reference is None:\n        node.setting_input.node_reference = None\n        return True\n\n    # Validate: lowercase only, no spaces\n    if \" \" in reference:\n        raise HTTPException(422, \"Reference cannot contain spaces\")\n    if reference != reference.lower():\n        raise HTTPException(422, \"Reference must be lowercase\")\n\n    # Validate: unique across all nodes in the flow\n    for other_node in flow.nodes:\n        if other_node.node_id != node_id:\n            other_ref = getattr(other_node.setting_input, \"node_reference\", None)\n            if other_ref and other_ref == reference:\n                raise HTTPException(422, f'Reference \"{reference}\" is already used by another node')\n\n    node.setting_input.node_reference = reference\n    return True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.upload_file","title":"<code>upload_file(file=File(...))</code>  <code>async</code>","text":"<p>Uploads a file to the server's 'uploads' directory.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>UploadFile</code> <p>The file to be uploaded.</p> <code>File(...)</code> <p>Returns:</p> Type Description <code>JSONResponse</code> <p>A JSON response containing the filename and the path where it was saved.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/upload/\")\nasync def upload_file(file: UploadFile = File(...)) -&gt; JSONResponse:\n    \"\"\"Uploads a file to the server's 'uploads' directory.\n\n    Args:\n        file: The file to be uploaded.\n\n    Returns:\n        A JSON response containing the filename and the path where it was saved.\n    \"\"\"\n    safe_name = Path(file.filename).name.replace(\"..\", \"\")\n    if not safe_name:\n        raise HTTPException(400, \"Invalid filename\")\n    uploads_dir = Path(\"uploads\")\n    uploads_dir.mkdir(exist_ok=True)\n    file_location = uploads_dir / safe_name\n    with open(file_location, \"wb+\") as file_object:\n        file_object.write(file.file.read())\n    return JSONResponse(content={\"filename\": safe_name, \"filepath\": str(file_location)})\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.validate_db_settings","title":"<code>validate_db_settings(database_settings, current_user=Depends(get_current_active_user))</code>  <code>async</code>","text":"<p>Validates that a connection can be made to a database with the given settings.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/validate_db_settings\")\nasync def validate_db_settings(\n    database_settings: input_schema.DatabaseSettings, current_user=Depends(get_current_active_user)\n):\n    \"\"\"Validates that a connection can be made to a database with the given settings.\"\"\"\n    # Validate the query settings\n    try:\n        sql_source = create_sql_source_from_db_settings(database_settings, user_id=current_user.id)\n        sql_source.validate()\n        return {\"message\": \"Query settings are valid\"}\n    except Exception as e:\n        raise HTTPException(status_code=422, detail=str(e))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.validate_node_reference","title":"<code>validate_node_reference(flow_id, node_id, reference)</code>","text":"<p>Validates if a reference is valid and unique for a node.</p> <p>Returns:</p> Type Description <p>Dict with 'valid' (bool) and 'error' (str or None) fields.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/node/validate_reference\", tags=[\"editor\"])\ndef validate_node_reference(flow_id: int, node_id: int, reference: str):\n    \"\"\"Validates if a reference is valid and unique for a node.\n\n    Returns:\n        Dict with 'valid' (bool) and 'error' (str or None) fields.\n    \"\"\"\n    try:\n        flow = flow_file_handler.get_flow(flow_id)\n    except:\n        raise HTTPException(404, \"Could not find the flow\")\n\n    # Handle empty reference (always valid - means use default)\n    if reference == \"\" or reference is None:\n        return {\"valid\": True, \"error\": None}\n\n    # Validate: lowercase only\n    if reference != reference.lower():\n        return {\"valid\": False, \"error\": \"Reference must be lowercase\"}\n\n    # Validate: no spaces\n    if \" \" in reference:\n        return {\"valid\": False, \"error\": \"Reference cannot contain spaces\"}\n\n    # Validate: unique across all nodes in the flow\n    for other_node in flow.nodes:\n        if other_node.node_id != node_id:\n            other_ref = getattr(other_node.setting_input, \"node_reference\", None)\n            if other_ref and other_ref == reference:\n                return {\"valid\": False, \"error\": f'Reference \"{reference}\" is already used by another node'}\n\n    return {\"valid\": True, \"error\": None}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#auth","title":"<code>auth</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.auth","title":"<code>flowfile_core.routes.auth</code>","text":"<p>Functions:</p> Name Description <code>change_own_password</code> <p>Change the current user's password</p> <code>create_user</code> <p>Create a new user (admin only)</p> <code>delete_user</code> <p>Delete a user (admin only)</p> <code>get_password_requirements</code> <p>Get password requirements for client-side validation</p> <code>list_users</code> <p>List all users (admin only)</p> <code>update_user</code> <p>Update a user (admin only)</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.auth.change_own_password","title":"<code>change_own_password(password_data, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>  <code>async</code>","text":"<p>Change the current user's password</p> Source code in <code>flowfile_core/flowfile_core/routes/auth.py</code> <pre><code>@router.post(\"/users/me/change-password\", response_model=User)\nasync def change_own_password(\n    password_data: ChangePassword,\n    current_user: User = Depends(get_current_active_user),\n    db: Session = Depends(get_db)\n):\n    \"\"\"Change the current user's password\"\"\"\n    user = db.query(db_models.User).filter(db_models.User.id == current_user.id).first()\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=\"User not found\"\n        )\n\n    # Verify current password\n    if not verify_password(password_data.current_password, user.hashed_password):\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Current password is incorrect\"\n        )\n\n    # Validate new password requirements\n    is_valid, error_message = validate_password(password_data.new_password)\n    if not is_valid:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=error_message\n        )\n\n    # Update password and clear must_change_password flag\n    user.hashed_password = get_password_hash(password_data.new_password)\n    user.must_change_password = False\n    db.commit()\n    db.refresh(user)\n\n    return User(\n        username=user.username,\n        id=user.id,\n        email=user.email,\n        full_name=user.full_name,\n        disabled=user.disabled,\n        is_admin=user.is_admin,\n        must_change_password=user.must_change_password\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.auth.create_user","title":"<code>create_user(user_data, current_user=Depends(get_current_admin_user), db=Depends(get_db))</code>  <code>async</code>","text":"<p>Create a new user (admin only)</p> Source code in <code>flowfile_core/flowfile_core/routes/auth.py</code> <pre><code>@router.post(\"/users\", response_model=User)\nasync def create_user(\n    user_data: UserCreate,\n    current_user: User = Depends(get_current_admin_user),\n    db: Session = Depends(get_db)\n):\n    \"\"\"Create a new user (admin only)\"\"\"\n    # Check if username already exists\n    existing_user = db.query(db_models.User).filter(\n        db_models.User.username == user_data.username\n    ).first()\n    if existing_user:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Username already exists\"\n        )\n\n    # Check if email already exists (if provided)\n    if user_data.email:\n        existing_email = db.query(db_models.User).filter(\n            db_models.User.email == user_data.email\n        ).first()\n        if existing_email:\n            raise HTTPException(\n                status_code=status.HTTP_400_BAD_REQUEST,\n                detail=\"Email already exists\"\n            )\n\n    # Validate password requirements\n    is_valid, error_message = validate_password(user_data.password)\n    if not is_valid:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=error_message\n        )\n\n    # Create new user with must_change_password=True\n    hashed_password = get_password_hash(user_data.password)\n    new_user = db_models.User(\n        username=user_data.username,\n        email=user_data.email or f\"{user_data.username}@flowfile.app\",\n        full_name=user_data.full_name,\n        hashed_password=hashed_password,\n        is_admin=user_data.is_admin,\n        must_change_password=True\n    )\n    db.add(new_user)\n    db.commit()\n    db.refresh(new_user)\n\n    return User(\n        username=new_user.username,\n        id=new_user.id,\n        email=new_user.email,\n        full_name=new_user.full_name,\n        disabled=new_user.disabled,\n        is_admin=new_user.is_admin,\n        must_change_password=new_user.must_change_password\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.auth.delete_user","title":"<code>delete_user(user_id, current_user=Depends(get_current_admin_user), db=Depends(get_db))</code>  <code>async</code>","text":"<p>Delete a user (admin only)</p> Source code in <code>flowfile_core/flowfile_core/routes/auth.py</code> <pre><code>@router.delete(\"/users/{user_id}\")\nasync def delete_user(\n    user_id: int,\n    current_user: User = Depends(get_current_admin_user),\n    db: Session = Depends(get_db)\n):\n    \"\"\"Delete a user (admin only)\"\"\"\n    user = db.query(db_models.User).filter(db_models.User.id == user_id).first()\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=\"User not found\"\n        )\n\n    # Prevent admin from deleting themselves\n    if user.id == current_user.id:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Cannot delete your own account\"\n        )\n\n    # Delete user's secrets and connections first (cascade)\n    db.query(db_models.Secret).filter(db_models.Secret.user_id == user_id).delete()\n    db.query(db_models.DatabaseConnection).filter(db_models.DatabaseConnection.user_id == user_id).delete()\n    db.query(db_models.CloudStorageConnection).filter(db_models.CloudStorageConnection.user_id == user_id).delete()\n\n    db.delete(user)\n    db.commit()\n\n    return {\"message\": f\"User '{user.username}' deleted successfully\"}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.auth.get_password_requirements","title":"<code>get_password_requirements()</code>  <code>async</code>","text":"<p>Get password requirements for client-side validation</p> Source code in <code>flowfile_core/flowfile_core/routes/auth.py</code> <pre><code>@router.get(\"/password-requirements\")\nasync def get_password_requirements():\n    \"\"\"Get password requirements for client-side validation\"\"\"\n    return PASSWORD_REQUIREMENTS\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.auth.list_users","title":"<code>list_users(current_user=Depends(get_current_admin_user), db=Depends(get_db))</code>  <code>async</code>","text":"<p>List all users (admin only)</p> Source code in <code>flowfile_core/flowfile_core/routes/auth.py</code> <pre><code>@router.get(\"/users\", response_model=list[User])\nasync def list_users(\n    current_user: User = Depends(get_current_admin_user),\n    db: Session = Depends(get_db)\n):\n    \"\"\"List all users (admin only)\"\"\"\n    users = db.query(db_models.User).all()\n    return [\n        User(\n            username=u.username,\n            id=u.id,\n            email=u.email,\n            full_name=u.full_name,\n            disabled=u.disabled,\n            is_admin=u.is_admin,\n            must_change_password=u.must_change_password\n        )\n        for u in users\n    ]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.auth.update_user","title":"<code>update_user(user_id, user_data, current_user=Depends(get_current_admin_user), db=Depends(get_db))</code>  <code>async</code>","text":"<p>Update a user (admin only)</p> Source code in <code>flowfile_core/flowfile_core/routes/auth.py</code> <pre><code>@router.put(\"/users/{user_id}\", response_model=User)\nasync def update_user(\n    user_id: int,\n    user_data: UserUpdate,\n    current_user: User = Depends(get_current_admin_user),\n    db: Session = Depends(get_db)\n):\n    \"\"\"Update a user (admin only)\"\"\"\n    user = db.query(db_models.User).filter(db_models.User.id == user_id).first()\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=\"User not found\"\n        )\n\n    # Prevent admin from disabling themselves\n    if user.id == current_user.id and user_data.disabled:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Cannot disable your own account\"\n        )\n\n    # Prevent admin from removing their own admin status\n    if user.id == current_user.id and user_data.is_admin is False:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Cannot remove your own admin privileges\"\n        )\n\n    # Update fields\n    if user_data.email is not None:\n        # Check if email already exists for another user\n        existing_email = db.query(db_models.User).filter(\n            db_models.User.email == user_data.email,\n            db_models.User.id != user_id\n        ).first()\n        if existing_email:\n            raise HTTPException(\n                status_code=status.HTTP_400_BAD_REQUEST,\n                detail=\"Email already exists\"\n            )\n        user.email = user_data.email\n\n    if user_data.full_name is not None:\n        user.full_name = user_data.full_name\n\n    if user_data.disabled is not None:\n        user.disabled = user_data.disabled\n\n    if user_data.is_admin is not None:\n        user.is_admin = user_data.is_admin\n\n    if user_data.password is not None:\n        # Validate password requirements\n        is_valid, error_message = validate_password(user_data.password)\n        if not is_valid:\n            raise HTTPException(\n                status_code=status.HTTP_400_BAD_REQUEST,\n                detail=error_message\n            )\n        user.hashed_password = get_password_hash(user_data.password)\n        # Reset must_change_password when admin sets a new password\n        user.must_change_password = True\n\n    if user_data.must_change_password is not None:\n        user.must_change_password = user_data.must_change_password\n\n    db.commit()\n    db.refresh(user)\n\n    return User(\n        username=user.username,\n        id=user.id,\n        email=user.email,\n        full_name=user.full_name,\n        disabled=user.disabled,\n        is_admin=user.is_admin,\n        must_change_password=user.must_change_password\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#cloud_connections","title":"<code>cloud_connections</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.cloud_connections","title":"<code>flowfile_core.routes.cloud_connections</code>","text":"<p>Functions:</p> Name Description <code>create_cloud_storage_connection</code> <p>Create a new cloud storage connection.</p> <code>delete_cloud_connection_with_connection_name</code> <p>Delete a cloud connection.</p> <code>get_cloud_connections</code> <p>Get all cloud storage connections for the current user.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.cloud_connections.create_cloud_storage_connection","title":"<code>create_cloud_storage_connection(input_connection, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>","text":"<p>Create a new cloud storage connection. Parameters     input_connection: FullCloudStorageConnection schema containing connection details     current_user: User obtained from Depends(get_current_active_user)     db: Session obtained from Depends(get_db) Returns     Dict with a success message</p> Source code in <code>flowfile_core/flowfile_core/routes/cloud_connections.py</code> <pre><code>@router.post(\"/cloud_connection\", tags=[\"cloud_connections\"])\ndef create_cloud_storage_connection(\n    input_connection: FullCloudStorageConnection,\n    current_user=Depends(get_current_active_user),\n    db: Session = Depends(get_db),\n):\n    \"\"\"\n    Create a new cloud storage connection.\n    Parameters\n        input_connection: FullCloudStorageConnection schema containing connection details\n        current_user: User obtained from Depends(get_current_active_user)\n        db: Session obtained from Depends(get_db)\n    Returns\n        Dict with a success message\n    \"\"\"\n    logger.info(f\"Create cloud connection {input_connection.connection_name}\")\n    try:\n        store_cloud_connection(db, input_connection, current_user.id)\n    except ValueError:\n        raise HTTPException(422, \"Connection name already exists\")\n    except Exception as e:\n        logger.error(e)\n        raise HTTPException(422, str(e))\n    return {\"message\": \"Cloud connection created successfully\"}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.cloud_connections.delete_cloud_connection_with_connection_name","title":"<code>delete_cloud_connection_with_connection_name(connection_name, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>","text":"<p>Delete a cloud connection.</p> Source code in <code>flowfile_core/flowfile_core/routes/cloud_connections.py</code> <pre><code>@router.delete(\"/cloud_connection\", tags=[\"cloud_connections\"])\ndef delete_cloud_connection_with_connection_name(\n    connection_name: str, current_user=Depends(get_current_active_user), db: Session = Depends(get_db)\n):\n    \"\"\"\n    Delete a cloud connection.\n    \"\"\"\n    logger.info(f\"Deleting cloud connection {connection_name}\")\n    cloud_storage_connection = get_cloud_connection_schema(db, connection_name, current_user.id)\n    if cloud_storage_connection is None:\n        raise HTTPException(404, \"Cloud connection connection not found\")\n    delete_cloud_connection(db, connection_name, current_user.id)\n    return {\"message\": \"Cloud connection deleted successfully\"}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.cloud_connections.get_cloud_connections","title":"<code>get_cloud_connections(db=Depends(get_db), current_user=Depends(get_current_active_user))</code>","text":"<p>Get all cloud storage connections for the current user. Parameters     db: Session obtained from Depends(get_db)     current_user: User obtained from Depends(get_current_active_user)</p> <p>Returns     List[FullCloudStorageConnectionInterface]</p> Source code in <code>flowfile_core/flowfile_core/routes/cloud_connections.py</code> <pre><code>@router.get(\"/cloud_connections\", tags=[\"cloud_connection\"], response_model=list[FullCloudStorageConnectionInterface])\ndef get_cloud_connections(\n    db: Session = Depends(get_db), current_user=Depends(get_current_active_user)\n) -&gt; list[FullCloudStorageConnectionInterface]:\n    \"\"\"\n    Get all cloud storage connections for the current user.\n    Parameters\n        db: Session obtained from Depends(get_db)\n        current_user: User obtained from Depends(get_current_active_user)\n\n    Returns\n        List[FullCloudStorageConnectionInterface]\n    \"\"\"\n    return get_all_cloud_connections_interface(db, current_user.id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#logs","title":"<code>logs</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.logs","title":"<code>flowfile_core.routes.logs</code>","text":"<p>Functions:</p> Name Description <code>add_log</code> <p>Adds a log message to the log file for a given flow_id.</p> <code>add_raw_log</code> <p>Adds a log message to the log file for a given flow_id.</p> <code>format_sse_message</code> <p>Format the data as a proper SSE message</p> <code>stream_logs</code> <p>Streams logs for a given flow_id using Server-Sent Events.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.logs.add_log","title":"<code>add_log(flow_id, log_message)</code>  <code>async</code>","text":"<p>Adds a log message to the log file for a given flow_id.</p> Source code in <code>flowfile_core/flowfile_core/routes/logs.py</code> <pre><code>@router.post(\"/logs/{flow_id}\", tags=[\"flow_logging\"])\nasync def add_log(flow_id: int, log_message: str):\n    \"\"\"Adds a log message to the log file for a given flow_id.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if not flow:\n        raise HTTPException(status_code=404, detail=\"Flow not found\")\n    flow.flow_logger.info(log_message)\n    return {\"message\": \"Log added successfully\"}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.logs.add_raw_log","title":"<code>add_raw_log(raw_log_input)</code>  <code>async</code>","text":"<p>Adds a log message to the log file for a given flow_id.</p> Source code in <code>flowfile_core/flowfile_core/routes/logs.py</code> <pre><code>@router.post(\"/raw_logs\", tags=[\"flow_logging\"])\nasync def add_raw_log(raw_log_input: schemas.RawLogInput):\n    \"\"\"Adds a log message to the log file for a given flow_id.\"\"\"\n    flow = flow_file_handler.get_flow(raw_log_input.flowfile_flow_id)\n    if not flow:\n        raise HTTPException(status_code=404, detail=\"Flow not found\")\n    flow_logger = flow.flow_logger\n    node_id = raw_log_input.node_id if raw_log_input.node_id is not None else -1\n    if raw_log_input.log_type == \"INFO\":\n        flow_logger.info(raw_log_input.log_message, extra=raw_log_input.extra, node_id=node_id)\n    elif raw_log_input.log_type == \"WARNING\":\n        flow_logger.warning(raw_log_input.log_message, extra=raw_log_input.extra, node_id=node_id)\n    elif raw_log_input.log_type == \"ERROR\":\n        flow_logger.error(raw_log_input.log_message, extra=raw_log_input.extra, node_id=node_id)\n    return {\"message\": \"Log added successfully\"}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.logs.format_sse_message","title":"<code>format_sse_message(data)</code>  <code>async</code>","text":"<p>Format the data as a proper SSE message</p> Source code in <code>flowfile_core/flowfile_core/routes/logs.py</code> <pre><code>async def format_sse_message(data: str) -&gt; str:\n    \"\"\"Format the data as a proper SSE message\"\"\"\n    return f\"data: {json.dumps(data)}\\n\\n\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.logs.stream_logs","title":"<code>stream_logs(flow_id, idle_timeout=300, current_user=Depends(get_current_user_from_query))</code>  <code>async</code>","text":"<p>Streams logs for a given flow_id using Server-Sent Events. Requires authentication via token in query parameter. The connection will close gracefully if the server shuts down.</p> Source code in <code>flowfile_core/flowfile_core/routes/logs.py</code> <pre><code>@router.get(\"/logs/{flow_id}\", tags=[\"flow_logging\"])\nasync def stream_logs(flow_id: int, idle_timeout: int = 300, current_user=Depends(get_current_user_from_query)):\n    \"\"\"\n    Streams logs for a given flow_id using Server-Sent Events.\n    Requires authentication via token in query parameter.\n    The connection will close gracefully if the server shuts down.\n    \"\"\"\n    logger.info(f\"Starting log stream for flow_id: {flow_id} by user: {current_user.username}\")\n    await asyncio.sleep(0.3)\n    flow = flow_file_handler.get_flow(flow_id)\n    logger.info(\"Streaming logs\")\n    if not flow:\n        raise HTTPException(status_code=404, detail=\"Flow not found\")\n\n    log_file_path = flow.flow_logger.get_log_filepath()\n    if not Path(log_file_path).exists():\n        raise HTTPException(status_code=404, detail=\"Log file not found\")\n\n    class RunningState:\n        def __init__(self):\n            self.has_started = False\n\n        def is_running(self):\n            if flow.flow_settings.is_running:\n                self.has_started = True\n            return flow.flow_settings.is_running or not self.has_started\n\n    running_state = RunningState()\n\n    return StreamingResponse(\n        stream_log_file(log_file_path, running_state.is_running, idle_timeout),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"Content-Type\": \"text/event-stream\",\n        },\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#public","title":"<code>public</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.public","title":"<code>flowfile_core.routes.public</code>","text":"<p>Classes:</p> Name Description <code>GeneratedKey</code> <p>Response model for the generate key endpoint.</p> <code>SetupStatus</code> <p>Response model for the setup status endpoint.</p> <p>Functions:</p> Name Description <code>docs_redirect</code> <p>Redirects to the documentation page.</p> <code>generate_key</code> <p>Generate a new master encryption key.</p> <code>get_setup_status</code> <p>Get the current setup status of the application.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.public.GeneratedKey","title":"<code>GeneratedKey</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response model for the generate key endpoint.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Response model for the generate key endpoint.\",\n  \"properties\": {\n    \"key\": {\n      \"title\": \"Key\",\n      \"type\": \"string\"\n    },\n    \"instructions\": {\n      \"title\": \"Instructions\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"key\",\n    \"instructions\"\n  ],\n  \"title\": \"GeneratedKey\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>key</code>                 (<code>str</code>)             </li> <li> <code>instructions</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/routes/public.py</code> <pre><code>class GeneratedKey(BaseModel):\n    \"\"\"Response model for the generate key endpoint.\"\"\"\n\n    key: str\n    instructions: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.public.SetupStatus","title":"<code>SetupStatus</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response model for the setup status endpoint.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Response model for the setup status endpoint.\",\n  \"properties\": {\n    \"setup_required\": {\n      \"title\": \"Setup Required\",\n      \"type\": \"boolean\"\n    },\n    \"master_key_configured\": {\n      \"title\": \"Master Key Configured\",\n      \"type\": \"boolean\"\n    },\n    \"mode\": {\n      \"title\": \"Mode\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"setup_required\",\n    \"master_key_configured\",\n    \"mode\"\n  ],\n  \"title\": \"SetupStatus\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>setup_required</code>                 (<code>bool</code>)             </li> <li> <code>master_key_configured</code>                 (<code>bool</code>)             </li> <li> <code>mode</code>                 (<code>str</code>)             </li> </ul> Source code in <code>flowfile_core/flowfile_core/routes/public.py</code> <pre><code>class SetupStatus(BaseModel):\n    \"\"\"Response model for the setup status endpoint.\"\"\"\n\n    setup_required: bool\n    master_key_configured: bool\n    mode: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.public.docs_redirect","title":"<code>docs_redirect()</code>  <code>async</code>","text":"<p>Redirects to the documentation page.</p> Source code in <code>flowfile_core/flowfile_core/routes/public.py</code> <pre><code>@router.get(\"/\", tags=[\"admin\"])\nasync def docs_redirect():\n    \"\"\"Redirects to the documentation page.\"\"\"\n    return RedirectResponse(url=\"/docs\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.public.generate_key","title":"<code>generate_key()</code>  <code>async</code>","text":"<p>Generate a new master encryption key.</p> Source code in <code>flowfile_core/flowfile_core/routes/public.py</code> <pre><code>@router.post(\"/setup/generate-key\", response_model=GeneratedKey, tags=[\"setup\"])\nasync def generate_key():\n    \"\"\"Generate a new master encryption key.\"\"\"\n    key = generate_master_key()\n    instructions = (\n        f'Add to your .env file:\\n  FLOWFILE_MASTER_KEY=\"{key}\"\\n\\n'\n        \"Then restart: docker-compose down &amp;&amp; docker-compose up\"\n    )\n    return GeneratedKey(key=key, instructions=instructions)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.public.get_setup_status","title":"<code>get_setup_status()</code>  <code>async</code>","text":"<p>Get the current setup status of the application.</p> Source code in <code>flowfile_core/flowfile_core/routes/public.py</code> <pre><code>@router.get(\"/health/status\", response_model=SetupStatus, tags=[\"health\"])\nasync def get_setup_status():\n    \"\"\"Get the current setup status of the application.\"\"\"\n    mode = os.environ.get(\"FLOWFILE_MODE\", \"electron\")\n    master_key_ok = is_master_key_configured()\n    return SetupStatus(\n        setup_required=not master_key_ok,\n        master_key_configured=master_key_ok,\n        mode=mode,\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#secrets","title":"<code>secrets</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.secrets","title":"<code>flowfile_core.routes.secrets</code>","text":"<p>Manages CRUD (Create, Read, Update, Delete) operations for secrets.</p> <p>This router provides secure endpoints for creating, retrieving, and deleting sensitive credentials for the authenticated user. Secrets are encrypted before being stored and are associated with the user's ID.</p> <p>Functions:</p> Name Description <code>create_secret</code> <p>Creates a new secret for the authenticated user.</p> <code>delete_secret</code> <p>Deletes a secret by name for the authenticated user.</p> <code>get_secret</code> <p>Retrieves a specific secret by name for the authenticated user.</p> <code>get_secrets</code> <p>Retrieves all secret names for the currently authenticated user.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.secrets.create_secret","title":"<code>create_secret(secret, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>  <code>async</code>","text":"<p>Creates a new secret for the authenticated user.</p> <p>The secret value is encrypted before being stored in the database. A secret name must be unique for a given user.</p> <p>Parameters:</p> Name Type Description Default <code>secret</code> <code>SecretInput</code> <p>A <code>SecretInput</code> object containing the name and plaintext value of the secret.</p> required <code>current_user</code> <p>The authenticated user object, injected by FastAPI.</p> <code>Depends(get_current_active_user)</code> <code>db</code> <code>Session</code> <p>The database session, injected by FastAPI.</p> <code>Depends(get_db)</code> <p>Raises:</p> Type Description <code>HTTPException</code> <p>400 if a secret with the same name already exists for the user.</p> <p>Returns:</p> Type Description <code>Secret</code> <p>A <code>Secret</code> object containing the name and the encrypted value.</p> Source code in <code>flowfile_core/flowfile_core/routes/secrets.py</code> <pre><code>@router.post(\"/secrets\", response_model=Secret)\nasync def create_secret(\n    secret: SecretInput, current_user=Depends(get_current_active_user), db: Session = Depends(get_db)\n) -&gt; Secret:\n    \"\"\"Creates a new secret for the authenticated user.\n\n    The secret value is encrypted before being stored in the database. A secret\n    name must be unique for a given user.\n\n    Args:\n        secret: A `SecretInput` object containing the name and plaintext value of the secret.\n        current_user: The authenticated user object, injected by FastAPI.\n        db: The database session, injected by FastAPI.\n\n    Raises:\n        HTTPException: 400 if a secret with the same name already exists for the user.\n\n    Returns:\n        A `Secret` object containing the name and the *encrypted* value.\n    \"\"\"\n    # Get user ID\n    user_id = 1 if os.environ.get(\"FLOWFILE_MODE\") == \"electron\" else current_user.id\n\n    existing_secret = (\n        db.query(db_models.Secret)\n        .filter(db_models.Secret.user_id == user_id, db_models.Secret.name == secret.name)\n        .first()\n    )\n\n    if existing_secret:\n        raise HTTPException(status_code=400, detail=\"Secret with this name already exists\")\n\n    # The store_secret function handles encryption and DB storage\n    stored_secret = store_secret(db, secret, user_id)\n    return Secret(name=stored_secret.name, value=stored_secret.encrypted_value, user_id=str(user_id))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.secrets.delete_secret","title":"<code>delete_secret(secret_name, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>  <code>async</code>","text":"<p>Deletes a secret by name for the authenticated user.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>The name of the secret to delete.</p> required <code>current_user</code> <p>The authenticated user object, injected by FastAPI.</p> <code>Depends(get_current_active_user)</code> <code>db</code> <code>Session</code> <p>The database session, injected by FastAPI.</p> <code>Depends(get_db)</code> <p>Returns:</p> Type Description <code>None</code> <p>An empty response with a 204 No Content status code upon success.</p> Source code in <code>flowfile_core/flowfile_core/routes/secrets.py</code> <pre><code>@router.delete(\"/secrets/{secret_name}\", status_code=204)\nasync def delete_secret(\n    secret_name: str, current_user=Depends(get_current_active_user), db: Session = Depends(get_db)\n) -&gt; None:\n    \"\"\"Deletes a secret by name for the authenticated user.\n\n    Args:\n        secret_name: The name of the secret to delete.\n        current_user: The authenticated user object, injected by FastAPI.\n        db: The database session, injected by FastAPI.\n\n    Returns:\n        An empty response with a 204 No Content status code upon success.\n    \"\"\"\n    # Get user ID\n    user_id = 1 if os.environ.get(\"FLOWFILE_MODE\") == \"electron\" else current_user.id\n    delete_secret_action(db, secret_name, user_id)\n    return None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.secrets.get_secret","title":"<code>get_secret(secret_name, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>  <code>async</code>","text":"<p>Retrieves a specific secret by name for the authenticated user.</p> <p>Note: This endpoint returns the secret name and metadata but does not expose the decrypted secret value.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>The name of the secret to retrieve.</p> required <code>current_user</code> <p>The authenticated user object, injected by FastAPI.</p> <code>Depends(get_current_active_user)</code> <code>db</code> <code>Session</code> <p>The database session, injected by FastAPI.</p> <code>Depends(get_db)</code> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if the secret is not found.</p> <p>Returns:</p> Type Description <code>Secret</code> <p>A <code>Secret</code> object containing the name and encrypted value.</p> Source code in <code>flowfile_core/flowfile_core/routes/secrets.py</code> <pre><code>@router.get(\"/secrets/{secret_name}\", response_model=Secret)\nasync def get_secret(\n    secret_name: str, current_user=Depends(get_current_active_user), db: Session = Depends(get_db)\n) -&gt; Secret:\n    \"\"\"Retrieves a specific secret by name for the authenticated user.\n\n    Note: This endpoint returns the secret name and metadata but does not\n    expose the decrypted secret value.\n\n    Args:\n        secret_name: The name of the secret to retrieve.\n        current_user: The authenticated user object, injected by FastAPI.\n        db: The database session, injected by FastAPI.\n\n    Raises:\n        HTTPException: 404 if the secret is not found.\n\n    Returns:\n        A `Secret` object containing the name and encrypted value.\n    \"\"\"\n    # Get user ID\n    user_id = 1 if os.environ.get(\"FLOWFILE_MODE\") == \"electron\" else current_user.id\n\n    # Get secret from database\n    db_secret = (\n        db.query(db_models.Secret)\n        .filter(db_models.Secret.user_id == user_id, db_models.Secret.name == secret_name)\n        .first()\n    )\n\n    if not db_secret:\n        raise HTTPException(status_code=404, detail=\"Secret not found\")\n\n    return Secret(name=db_secret.name, value=db_secret.encrypted_value, user_id=str(db_secret.user_id))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.secrets.get_secrets","title":"<code>get_secrets(current_user=Depends(get_current_active_user), db=Depends(get_db))</code>  <code>async</code>","text":"<p>Retrieves all secret names for the currently authenticated user.</p> <p>Note: This endpoint returns the secret names and metadata but does not expose the decrypted secret values.</p> <p>Parameters:</p> Name Type Description Default <code>current_user</code> <p>The authenticated user object, injected by FastAPI.</p> <code>Depends(get_current_active_user)</code> <code>db</code> <code>Session</code> <p>The database session, injected by FastAPI.</p> <code>Depends(get_db)</code> <p>Returns:</p> Type Description <p>A list of <code>Secret</code> objects, each containing the name and encrypted value.</p> Source code in <code>flowfile_core/flowfile_core/routes/secrets.py</code> <pre><code>@router.get(\"/secrets\", response_model=list[Secret])\nasync def get_secrets(current_user=Depends(get_current_active_user), db: Session = Depends(get_db)):\n    \"\"\"Retrieves all secret names for the currently authenticated user.\n\n    Note: This endpoint returns the secret names and metadata but does not\n    expose the decrypted secret values.\n\n    Args:\n        current_user: The authenticated user object, injected by FastAPI.\n        db: The database session, injected by FastAPI.\n\n    Returns:\n        A list of `Secret` objects, each containing the name and encrypted value.\n    \"\"\"\n    user_id = current_user.id\n\n    # Get secrets from database\n    db_secrets = db.query(db_models.Secret).filter(db_models.Secret.user_id == user_id).all()\n\n    # Prepare response model (without decrypting)\n    secrets = []\n    for db_secret in db_secrets:\n        secrets.append(Secret(name=db_secret.name, value=db_secret.encrypted_value, user_id=str(db_secret.user_id)))\n\n    return secrets\n</code></pre>"},{"location":"users/index.html","title":"Flowfile User Guides","text":"<p>Welcome to Flowfile! Whether you prefer visual drag-and-drop or writing code, we've got you covered.</p>"},{"location":"users/index.html#choose-your-path","title":"Choose Your Path","text":""},{"location":"users/index.html#visual-editor","title":"\ud83c\udfa8 Visual Editor","text":"<p>Perfect for analysts and business users who want to build data pipelines visually.</p> <p>You'll learn:</p> <ul> <li>Drag and drop nodes to build flows</li> <li>Configure transformations with forms</li> <li>Connect to databases and cloud storage</li> <li>Export your flows as Python code</li> </ul> <p>Get Started with Visual Editor \u2192</p>"},{"location":"users/index.html#python-api","title":"\ud83d\udc0d Python API","text":"<p>Perfect for developers and data scientists who prefer code.</p> <p>You'll learn:</p> <ul> <li>Build pipelines with Polars-compatible API</li> <li>Seamlessly integrate with existing code</li> <li>Visualize your code as flow graphs</li> <li>Use advanced features and optimizations</li> </ul> <p>Get Started with Python \u2192</p>"},{"location":"users/index.html#the-best-of-both-worlds","title":"The Best of Both Worlds","text":"<p>The beauty of Flowfile is that you don't have to choose. You can:</p> <ul> <li>Write code and visualize it instantly with <code>open_graph_in_editor()</code></li> <li>Build visually and export as Python code</li> <li>Switch between visual and code at any time</li> <li>Collaborate across technical and non-technical teams</li> </ul>"},{"location":"users/index.html#quick-examples","title":"Quick Examples","text":""},{"location":"users/index.html#visual-approach","title":"Visual Approach","text":"<ol> <li>Drag a \"Read Data\" node onto canvas</li> <li>Add a \"Filter\" node and connect them</li> <li>Configure filter conditions in the form</li> <li>Run and see results instantly</li> </ol>"},{"location":"users/index.html#code-approach","title":"Code Approach","text":"<pre><code>import flowfile as ff\n\ndf = ff.read_csv(\"data.csv\")\nresult = df.filter(ff.col(\"amount\") &gt; 100)\nff.open_graph_in_editor(result.flow_graph)  # See it visually!\n</code></pre>"},{"location":"users/index.html#where-to-start","title":"Where to Start?","text":"<ul> <li>New to Flowfile? Start with our Quick Start Guide</li> <li>Coming from Excel/Tableau? Try the Visual Editor</li> <li>Know Python/Pandas/Polars? Jump into the Python API</li> <li>Want to see real examples? Check out our tutorials in either section</li> </ul> <p>Remember: Every visual flow can become code, and every code pipeline can be visualized. Choose what feels natural and switch whenever you want!</p>"},{"location":"users/secrets.html","title":"Secrets","text":"<p>Store sensitive credentials like database passwords and API keys securely.</p>"},{"location":"users/secrets.html#how-it-works","title":"How It Works","text":"<p>Secrets are encrypted using a master key before storage. When a flow needs a credential, Flowfile decrypts it on-demand. The actual values never appear in flow definitions or logs.</p>"},{"location":"users/secrets.html#master-key","title":"Master Key","text":"<p>The master key encrypts all secrets. Without it, secrets cannot be decrypted.</p>"},{"location":"users/secrets.html#configuration-by-mode","title":"Configuration by Mode","text":"Mode Configuration Desktop (Electron) Auto-generated on first open, stored at <code>~/.config/flowfile/</code> Python API Auto-generated on first use, stored at <code>~/.config/flowfile/</code> Docker Generate via setup wizard, set as <code>FLOWFILE_MASTER_KEY</code> env variable"},{"location":"users/secrets.html#desktop-python-api","title":"Desktop &amp; Python API","text":"<p>The master key is automatically generated on first use and stored securely. No manual configuration needed.</p> <p>Backup recommended</p> <p>The key is stored in <code>~/.config/flowfile/</code>. Back up this directory to preserve access to your encrypted secrets.</p>"},{"location":"users/secrets.html#docker","title":"Docker","text":"<p>On first start without a master key, Flowfile shows a setup screen:</p> <ol> <li>Click Generate Master Key</li> <li>Copy the generated key</li> <li>Add to your <code>.env</code> file: <code>FLOWFILE_MASTER_KEY=&lt;your-key&gt;</code></li> <li>Restart the containers</li> </ol> <p></p> <p>Protect your master key</p> <ul> <li>Back up your <code>.env</code> file securely</li> <li>Never commit to version control</li> <li>Losing it = losing access to all encrypted secrets</li> </ul>"},{"location":"users/secrets.html#creating-secrets","title":"Creating Secrets","text":"<ol> <li>Open Settings \u2192 Secrets</li> <li>Click Add Secret</li> <li>Enter name (e.g., <code>prod_database_password</code>)</li> <li>Enter value</li> <li>Save</li> </ol>"},{"location":"users/secrets.html#using-secrets","title":"Using Secrets","text":"<p>Reference secrets by name when configuring connections. The encrypted value is decrypted at runtime.</p>"},{"location":"users/secrets.html#encryption","title":"Encryption","text":"<ul> <li>Algorithm: Fernet (AES-128-CBC + HMAC-SHA256)</li> <li>Isolation: Each user's secrets stored separately</li> <li>Storage: Encrypted in SQLite database</li> </ul>"},{"location":"users/deployment/index.html","title":"Deployment","text":"<p>Choose how to run Flowfile based on your needs.</p> Method Best For Multi-user Desktop App Getting started, local development No Python Package Scripting, CI/CD, automation No Docker Teams, servers, production Yes"},{"location":"users/deployment/index.html#quick-comparison","title":"Quick Comparison","text":""},{"location":"users/deployment/index.html#desktop-app","title":"Desktop App","text":"<p>Download and run. No setup required. Best for exploring Flowfile or building flows locally on macOS or Windows.</p>"},{"location":"users/deployment/index.html#python-package","title":"Python Package","text":"<p><code>pip install flowfile</code>. Run flows programmatically or integrate into existing Python projects.</p>"},{"location":"users/deployment/index.html#docker","title":"Docker","text":"<p>Full deployment with authentication, secrets management, and centralized storage. Best for teams and production environments.</p>"},{"location":"users/deployment/desktop.html","title":"Desktop App","text":"<p>The easiest way to get started with Flowfile.</p>"},{"location":"users/deployment/desktop.html#download","title":"Download","text":"<p>Download the latest release for your platform:</p> <p>Download for macOS / Windows \u2192</p>"},{"location":"users/deployment/desktop.html#installation","title":"Installation","text":""},{"location":"users/deployment/desktop.html#macos","title":"macOS","text":"<ol> <li>Download the <code>.dmg</code> file</li> <li>Open it and drag Flowfile to Applications</li> <li>Launch from Applications</li> </ol> <p>First launch</p> <p>macOS may ask you to confirm opening an app from an unidentified developer. Go to System Preferences \u2192 Security &amp; Privacy and click \"Open Anyway\".</p>"},{"location":"users/deployment/desktop.html#windows","title":"Windows","text":"<ol> <li>Download the <code>.exe</code> installer</li> <li>Run the installer and follow the wizard</li> <li>Launch from the Start menu</li> </ol>"},{"location":"users/deployment/desktop.html#why-desktop","title":"Why Desktop?","text":"<p>Pros</p> <ul> <li>Zero configuration - just download and run</li> <li>No Docker, no terminal commands</li> <li>Works offline</li> <li>Data stays on your machine</li> <li>Master key managed automatically</li> </ul> <p>Cons</p> <ul> <li>Single user only (no team sharing)</li> <li>No centralized secrets management</li> <li>Flows stored locally (manual backup needed)</li> </ul>"},{"location":"users/deployment/desktop.html#when-to-use-docker-instead","title":"When to Use Docker Instead","text":"<p>Consider Docker deployment if you need:</p> <ul> <li>Multiple users with separate accounts</li> <li>Centralized flow storage</li> <li>Team collaboration</li> <li>Server/production deployment</li> </ul>"},{"location":"users/deployment/docker.html","title":"Docker Reference","text":"<p>Run Flowfile with Docker using pre-built images from Docker Hub.</p>"},{"location":"users/deployment/docker.html#quick-start","title":"Quick Start","text":"<pre><code>git clone https://github.com/edwardvaneechoud/Flowfile.git\ncd Flowfile\ndocker compose up -d\n</code></pre> <p>Access at <code>http://localhost:8080</code>. The setup wizard will guide you through master key configuration.</p> <p></p>"},{"location":"users/deployment/docker.html#docker-images","title":"Docker Images","text":"Image Description <code>edwardvaneechoud/flowfile-frontend</code> Web UI <code>edwardvaneechoud/flowfile-core</code> API server <code>edwardvaneechoud/flowfile-worker</code> Data processing <p>Tags: <code>latest</code>, <code>0.5.3</code>, or specific version</p>"},{"location":"users/deployment/docker.html#docker-composeyml","title":"docker-compose.yml","text":"<pre><code>services:\n  flowfile-frontend:\n    image: edwardvaneechoud/flowfile-frontend:latest\n    ports:\n      - \"8080:8080\"\n    networks:\n      - flowfile-network\n    depends_on:\n      - flowfile-core\n      - flowfile-worker\n\n  flowfile-core:\n    image: edwardvaneechoud/flowfile-core:latest\n    ports:\n      - \"63578:63578\"\n    environment:\n      - FLOWFILE_MODE=docker\n      - FLOWFILE_ADMIN_USER=${FLOWFILE_ADMIN_USER:-admin}\n      - FLOWFILE_ADMIN_PASSWORD=${FLOWFILE_ADMIN_PASSWORD:-changeme}\n      - JWT_SECRET_KEY=${JWT_SECRET_KEY}\n      - FLOWFILE_MASTER_KEY=${FLOWFILE_MASTER_KEY:-}\n      - WORKER_HOST=flowfile-worker\n    volumes:\n      - ./flowfile_data:/app/user_data\n      - ./saved_flows:/app/flowfile_core/saved_flows\n      - flowfile-storage:/app/internal_storage\n    networks:\n      - flowfile-network\n\n  flowfile-worker:\n    image: edwardvaneechoud/flowfile-worker:latest\n    ports:\n      - \"63579:63579\"\n    environment:\n      - FLOWFILE_MODE=docker\n      - CORE_HOST=flowfile-core\n      - FLOWFILE_MASTER_KEY=${FLOWFILE_MASTER_KEY:-}\n    volumes:\n      - ./flowfile_data:/app/user_data\n      - flowfile-storage:/app/internal_storage\n    networks:\n      - flowfile-network\n\nnetworks:\n  flowfile-network:\n\nvolumes:\n  flowfile-storage:\n</code></pre>"},{"location":"users/deployment/docker.html#environment-variables","title":"Environment Variables","text":"Variable Description Default <code>FLOWFILE_MODE</code> Set to <code>docker</code> for multi-user auth <code>docker</code> <code>FLOWFILE_ADMIN_USER</code> Admin username <code>admin</code> <code>FLOWFILE_ADMIN_PASSWORD</code> Admin password <code>changeme</code> <code>JWT_SECRET_KEY</code> Token signing secret Required <code>FLOWFILE_MASTER_KEY</code> Encryption key for secrets Via setup wizard <code>WORKER_HOST</code> Worker hostname <code>flowfile-worker</code> <code>CORE_HOST</code> Core hostname <code>flowfile-core</code>"},{"location":"users/deployment/docker.html#env-example","title":".env Example","text":"<pre><code>FLOWFILE_ADMIN_USER=admin\nFLOWFILE_ADMIN_PASSWORD=YourSecurePassword123!\nJWT_SECRET_KEY=generate-with-openssl-rand-hex-32\nFLOWFILE_MASTER_KEY=generated-from-setup-wizard\n</code></pre>"},{"location":"users/deployment/docker.html#volumes","title":"Volumes","text":"Path Purpose <code>./flowfile_data</code> User data <code>./saved_flows</code> Flow definitions <code>flowfile-storage</code> Internal storage"},{"location":"users/deployment/docker.html#commands","title":"Commands","text":"<pre><code>docker compose up -d      # Start\ndocker compose down       # Stop\ndocker compose pull       # Update images\ndocker compose logs -f    # View logs\n</code></pre>"},{"location":"users/deployment/docker.html#health-checks","title":"Health Checks","text":"Service Endpoint Core <code>http://localhost:63578/health</code> Worker <code>http://localhost:63579/health</code> Frontend <code>http://localhost:8080</code>"},{"location":"users/deployment/python.html","title":"Python Package","text":"<p>Install Flowfile as a Python package for programmatic use.</p>"},{"location":"users/deployment/python.html#installation","title":"Installation","text":"<pre><code>pip install flowfile\n</code></pre>"},{"location":"users/deployment/python.html#usage","title":"Usage","text":"<pre><code>import flowfile as ff\n\n# Read data\ndf = ff.read_csv(\"data.csv\")\n\n# Transform\ndf = df.with_formula(\"total\", \"price * quantity\")\n\n# Write\ndf.write_csv(\"output.csv\")\n</code></pre>"},{"location":"users/deployment/python.html#running-the-visual-editor","title":"Running the Visual Editor","text":"<p>Launch the visual editor from Python:</p> <pre><code>import flowfile as ff\n\n# Open the visual editor\nff.open_editor()\n</code></pre>"},{"location":"users/deployment/python.html#features","title":"Features","text":"<ul> <li>Full Python API for building flows programmatically</li> <li>Export visual flows to Python code</li> <li>Master key auto-generated and stored securely</li> <li>Integrates with existing Python projects</li> </ul>"},{"location":"users/deployment/python.html#documentation","title":"Documentation","text":"<p>See the Python API Guide for detailed documentation.</p>"},{"location":"users/python-api/index.html","title":"Python API","text":"<p>Build data pipelines programmatically with Flowfile's Polars-compatible API.</p> <p>If You Know Polars, You Know Flowfile</p> <p>Our API is designed to be a seamless extension of Polars. The majority of the methods are identical, so you can leverage your existing knowledge to be productive from day one. The main additions are features that connect your code to the broader Flowfile ecosystem, like cloud integrations and UI visualization.</p>"},{"location":"users/python-api/index.html#who-this-is-for","title":"Who This Is For","text":"<ul> <li>Python developers who prefer code over drag-and-drop</li> <li>Data scientists familiar with Polars or Pandas</li> <li>Engineers building automated data pipelines</li> <li>Anyone who needs version control and programmatic pipeline generation</li> </ul>"},{"location":"users/python-api/index.html#quick-example","title":"Quick Example","text":"<pre><code>import flowfile as ff\n\ndf = ff.read_csv(\"sales.csv\")\nresult = df.filter(ff.col(\"amount\") &gt; 100).group_by(\"region\").agg(\n    ff.col(\"amount\").sum()\n)\n\n# Visualize your pipeline\nff.open_graph_in_editor(result.flow_graph)\n</code></pre>"},{"location":"users/python-api/index.html#documentation","title":"Documentation","text":""},{"location":"users/python-api/index.html#quick-start","title":"Quick Start","text":"<p>Get up and running in 5 minutes with your first pipeline.</p>"},{"location":"users/python-api/index.html#core-concepts","title":"Core Concepts","text":"<ul> <li>FlowFrame and FlowGraph - Fundamental building blocks</li> <li>Formula Syntax - Flowfile's Excel-like expressions</li> </ul>"},{"location":"users/python-api/index.html#api-reference","title":"API Reference","text":"<ul> <li>Reading Data</li> <li>Writing Data</li> <li>Data Types</li> <li>DataFrame Operations</li> <li>Aggregations</li> <li>Joins</li> <li>Cloud Storage</li> <li>Visual UI Integration</li> </ul>"},{"location":"users/python-api/index.html#tutorials","title":"Tutorials","text":"<ul> <li>Building Flows with Code</li> </ul>"},{"location":"users/python-api/index.html#for-contributors","title":"For Contributors","text":"<p>Want to understand how Flowfile works internally or contribute to the project? See the Developer Documentation for architecture details and internal API reference.</p> <p>Prefer visual workflows? Check out the Visual Editor Guide.</p>"},{"location":"users/python-api/quickstart.html","title":"Python API Quick Start","text":"<p>Get up and running with Flowfile's Python API in 5 minutes.</p>"},{"location":"users/python-api/quickstart.html#installation","title":"Installation","text":"<pre><code>pip install flowfile\n</code></pre>"},{"location":"users/python-api/quickstart.html#your-first-pipeline","title":"Your First Pipeline","text":"<pre><code>import flowfile as ff\n\n# Load data\ndf = ff.read_csv(\"sales.csv\", description=\"Load sales data\")\n\n# Transform\nresult = (\n    df\n    .filter(ff.col(\"amount\") &gt; 100, description=\"Filter large sales\")\n    .with_columns([\n        (ff.col(\"amount\") * 1.1).alias(\"amount_with_tax\")\n    ], description=\"Add tax calculation\")\n    .group_by(\"region\")\n    .agg([\n        ff.col(\"amount\").sum().alias(\"total_sales\"),\n        ff.col(\"amount\").mean().alias(\"avg_sale\")\n    ])\n)\n\n# Get results as Polars DataFrame\ndata = result.collect()\nprint(data)\n\n# Visualize in the UI\nff.open_graph_in_editor(result.flow_graph)\n</code></pre>"},{"location":"users/python-api/quickstart.html#key-concepts","title":"Key Concepts","text":""},{"location":"users/python-api/quickstart.html#flowframe","title":"FlowFrame","text":"<p>Your data container - like a Polars LazyFrame but tracks all operations:</p> <pre><code># Create from various sources\ndf = ff.FlowFrame({\"col1\": [1, 2, 3]})  # From dict\ndf = ff.read_csv(\"file.csv\")            # From CSV\ndf = ff.read_parquet(\"file.parquet\")    # From Parquet\n</code></pre>"},{"location":"users/python-api/quickstart.html#always-lazy","title":"Always Lazy","text":"<p>Operations don't execute until you call <code>.collect()</code>:</p> <pre><code># These operations just build the plan\ndf = ff.read_csv(\"huge_file.csv\")\ndf = df.filter(ff.col(\"status\") == \"active\")\ndf = df.select([\"id\", \"name\", \"amount\"])\n\n# Now it executes everything efficiently\nresult = df.collect()\n</code></pre>"},{"location":"users/python-api/quickstart.html#descriptions","title":"Descriptions","text":"<p>Document your pipeline as you build:</p> <pre><code>df = (\n    ff.read_csv(\"input.csv\", description=\"Raw customer data\")\n    .filter(ff.col(\"active\") == True, description=\"Keep active only\")\n    .drop_duplicates(description=\"Remove duplicates\")\n)\n</code></pre>"},{"location":"users/python-api/quickstart.html#common-operations","title":"Common Operations","text":""},{"location":"users/python-api/quickstart.html#filtering","title":"Filtering","text":"<pre><code># Polars style\ndf.filter(ff.col(\"age\") &gt; 21)\n\n# Flowfile formula style\ndf.filter(flowfile_formula=\"[age] &gt; 21 AND [status] = 'active'\")\n</code></pre>"},{"location":"users/python-api/quickstart.html#adding-columns","title":"Adding Columns","text":"<pre><code># Standard way\ndf.with_columns([\n    (ff.col(\"price\") * ff.col(\"quantity\")).alias(\"total\")\n])\n\n# Formula syntax\ndf.with_columns(\n    flowfile_formulas=[\"[price] * [quantity]\"],\n    output_column_names=[\"total\"]\n)\n</code></pre>"},{"location":"users/python-api/quickstart.html#grouping-aggregation","title":"Grouping &amp; Aggregation","text":"<pre><code>df.group_by(\"category\").agg([\n    ff.col(\"sales\").sum().alias(\"total_sales\"),\n    ff.col(\"sales\").mean().alias(\"avg_sales\"),\n    ff.col(\"id\").count().alias(\"count\")\n])\n</code></pre>"},{"location":"users/python-api/quickstart.html#joining","title":"Joining","text":"<pre><code>customers = ff.read_csv(\"customers.csv\")\norders = ff.read_csv(\"orders.csv\")\n\nresult = customers.join(\n    orders,\n    left_on=\"customer_id\",\n    right_on=\"cust_id\",\n    how=\"left\"\n)\n</code></pre>"},{"location":"users/python-api/quickstart.html#cloud-storage","title":"Cloud Storage","text":"<pre><code>from pydantic import SecretStr\n\n# Set up S3 connection\nff.create_cloud_storage_connection_if_not_exists(\n    ff.FullCloudStorageConnection(\n        connection_name=\"my-s3\",\n        storage_type=\"s3\",\n        auth_method=\"access_key\",\n        aws_region=\"us-east-1\",\n        aws_access_key_id=\"your-key\",\n        aws_secret_access_key=SecretStr(\"your-secret\")\n    )\n)\n\n# Read from S3\ndf = ff.scan_parquet_from_cloud_storage(\n    \"s3://bucket/data.parquet\",\n    connection_name=\"my-s3\"\n)\n\n# Write to S3\ndf.write_parquet_to_cloud_storage(\n    \"s3://bucket/output.parquet\",\n    connection_name=\"my-s3\"\n)\n</code></pre>"},{"location":"users/python-api/quickstart.html#visual-integration","title":"Visual Integration","text":""},{"location":"users/python-api/quickstart.html#open-in-editor","title":"Open in Editor","text":"<pre><code># Build pipeline in code\npipeline = ff.read_csv(\"data.csv\").filter(ff.col(\"value\") &gt; 100)\n\n# Open in visual editor\nff.open_graph_in_editor(pipeline.flow_graph)\n</code></pre>"},{"location":"users/python-api/quickstart.html#start-web-ui","title":"Start Web UI","text":"<pre><code># Launch the web interface\nff.start_web_ui()  # Opens browser automatically\n</code></pre>"},{"location":"users/python-api/quickstart.html#complete-example","title":"Complete Example","text":"<pre><code>import flowfile as ff\n\n# Build a complete ETL pipeline\npipeline = (\n    ff.read_csv(\"raw_sales.csv\", description=\"Load raw sales\")\n    .filter(ff.col(\"amount\") &gt; 0, description=\"Remove invalid\")\n    .with_columns([\n        ff.col(\"date\").str.strptime(ff.Date, \"%Y-%m-%d\"),\n        (ff.col(\"amount\") * ff.col(\"quantity\")).alias(\"total\")\n    ], description=\"Parse dates and calculate totals\")\n    .group_by([ff.col(\"date\").dt.year().alias(\"year\"), \"product\"])\n    .agg([\n        ff.col(\"total\").sum().alias(\"revenue\"),\n        ff.col(\"quantity\").sum().alias(\"units_sold\")\n    ])\n    .sort(\"revenue\", descending=True)\n)\n\n# Execute and get results\nresults = pipeline.collect()\nprint(results)\n\n# Visualize the pipeline\nff.open_graph_in_editor(pipeline.flow_graph)\n\n# Save results\npipeline.write_parquet(\"yearly_sales.parquet\")\n</code></pre>"},{"location":"users/python-api/quickstart.html#next-steps","title":"Next Steps","text":"<ul> <li>\ud83d\udcd6 Core Concepts - Understand FlowFrame and FlowGraph</li> <li>\ud83d\udcda API Reference - Detailed documentation</li> <li>\ud83c\udfaf Tutorials - Real-world examples</li> <li>\ud83d\udd04 Visual Integration - Working with the UI</li> </ul>"},{"location":"users/python-api/quickstart.html#tips","title":"Tips","text":"<ol> <li>Use descriptions - They appear in the visual editor</li> <li>Think lazy - Build your entire pipeline before collecting</li> <li>Leverage formulas - Use <code>[column]</code> syntax for simpler expressions</li> <li>Visualize often - <code>open_graph_in_editor()</code> helps debug</li> <li>Check schemas - Use <code>df.schema</code> to see structure without running</li> </ol> <p>Ready for more? Check out the full API reference or learn about core concepts. Or want to see another example? Checkout the quickstart guide!</p>"},{"location":"users/python-api/concepts/index.html","title":"Core Concepts","text":"<p>Understanding the key concepts behind Flowfile's Python API will help you build better pipelines.</p>"},{"location":"users/python-api/concepts/index.html#available-guides","title":"Available Guides","text":""},{"location":"users/python-api/concepts/index.html#flowframe-and-flowgraph","title":"FlowFrame and FlowGraph","text":"<p>The fundamental building blocks of Flowfile pipelines.</p> <p>You'll learn: - What FlowFrame is and how it differs from DataFrames - How FlowGraph tracks your operations - Why everything is lazy by default - How visual and code representations connect</p> <p>Key takeaways: - FlowFrame = Your data + its transformation history - FlowGraph = The complete pipeline blueprint - Every operation creates a node in the graph</p>"},{"location":"users/python-api/concepts/index.html#formula-syntax","title":"Formula Syntax","text":"<p>Flowfile's Excel-like formula syntax for expressions.</p> <p>You'll learn: - When to use <code>[column]</code> vs <code>ff.col(\"column\")</code> - Supported operations and functions - How formulas translate to Polars - Best practices for each syntax</p> <p>Key takeaways: - Formulas make simple operations more readable - Great for users coming from Excel/Tableau - Both syntaxes can be mixed in the same pipeline</p>"},{"location":"users/python-api/concepts/index.html#quick-overview","title":"Quick Overview","text":""},{"location":"users/python-api/concepts/index.html#flowframe-vs-dataframe","title":"FlowFrame vs DataFrame","text":"DataFrame (Pandas/Polars) FlowFrame (Flowfile) Holds data in memory Always lazy (data not loaded) Operations execute immediately Operations build a plan No operation history Full operation history in graph Can't visualize workflow Can open in visual editor"},{"location":"users/python-api/concepts/index.html#the-lazy-advantage","title":"The Lazy Advantage","text":"<pre><code># This doesn't load the 10GB file!\ndf = ff.read_csv(\"huge_file.csv\")\n\n# Still no data loaded - just building the plan\ndf = df.filter(ff.col(\"country\") == \"USA\")\ndf = df.select([\"id\", \"amount\"])\n\n# NOW it loads only what's needed\nresult = df.collect()  # Might only read 100MB!\n</code></pre>"},{"location":"users/python-api/concepts/index.html#visual-integration","title":"Visual Integration","text":"<p>Every FlowFrame knows its history:</p> <pre><code># Build a complex pipeline\npipeline = (\n    ff.read_csv(\"input.csv\")\n    .filter(ff.col(\"active\") == True)\n    .group_by(\"category\")\n    .agg(ff.col(\"revenue\").sum())\n)\n\n# See the entire pipeline visually\nff.open_graph_in_editor(pipeline.flow_graph)\n\n# The graph shows all 4 operations as connected nodes\n</code></pre>"},{"location":"users/python-api/concepts/index.html#why-these-concepts-matter","title":"Why These Concepts Matter","text":"<p>Understanding these concepts helps you:</p> <ol> <li>Write efficient code - Leverage lazy evaluation</li> <li>Debug effectively - Visualize your pipeline</li> <li>Collaborate better - Share visual representations</li> <li>Optimize performance - Understand what executes when</li> </ol>"},{"location":"users/python-api/concepts/index.html#learn-more","title":"Learn More","text":"<ul> <li>Deep dive: Read the full FlowFrame and FlowGraph guide</li> <li>Expressions: Master the Formula Syntax</li> <li>Practice: Try the tutorials</li> </ul> <p>These concepts are the foundation of Flowfile. Understanding them will make everything else click!</p>"},{"location":"users/python-api/concepts/design-concepts.html","title":"FlowFrame and FlowGraph Design Concepts","text":"<p>Understanding how FlowFrame and FlowGraph work together is key to mastering Flowfile. This guide explains the core design principles that make Flowfile both powerful and intuitive.</p> <p>Related Reading</p> <ul> <li>Practical Implementation: See these concepts in action in our Code to Flow guide</li> <li>Architecture Overview: Learn about the system design in Technical Architecture</li> <li>Visual Building: Compare with Building Flows visually</li> </ul>"},{"location":"users/python-api/concepts/design-concepts.html#flowframe-always-lazy-always-connected","title":"FlowFrame: Always Lazy, Always Connected","text":""},{"location":"users/python-api/concepts/design-concepts.html#what-is-flowframe","title":"What is FlowFrame?","text":"<p>FlowFrame is Flowfile's version of a Polars DataFrame with a crucial difference: it's always lazy and always connected to a graph.</p> <pre><code>import flowfile as ff\n\n# This creates a FlowFrame, not a regular DataFrame\ndf = ff.FlowFrame({\n    \"id\": [1, 2, 3, 4, 5],\n    \"amount\": [100, 250, 80, 300, 150],\n    \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\"]\n})\nprint(type(df))  # &lt;class 'FlowFrame'&gt;\nprint(type(df.data))  # &lt;class 'polars.LazyFrame'&gt;\n</code></pre>"},{"location":"users/python-api/concepts/design-concepts.html#key-properties-of-flowframe","title":"Key Properties of FlowFrame","text":""},{"location":"users/python-api/concepts/design-concepts.html#1-always-lazy-evaluation","title":"1. Always Lazy Evaluation","text":"<p>A <code>FlowFrame</code> never loads your actual data into memory until you explicitly call <code>.collect()</code>. This means you can build complex transformations on massive datasets without consuming memory:</p> <pre><code># None of this processes any data yet\ndf = (\n    ff.FlowFrame({\n        \"id\": [1, 2, 3, 4, 5],\n        \"amount\": [500, 1200, 800, 1500, 900], \n        \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\"]\n    })                                # Creates manual input node\n    .filter(ff.col(\"amount\") &gt; 1000)  # No filtering happens yet\n    .group_by(\"category\")             # No grouping happens yet\n    .agg(ff.col(\"amount\").sum())     # No aggregation happens yet\n)\n\n# Only now does the data get processed\nresult = df.collect()  # Everything executes at once, optimized\n</code></pre> <p>Performance Benefits</p> <p>This lazy evaluation is powered by Polars and explained in detail in our Technical Architecture guide. </p>"},{"location":"users/python-api/concepts/design-concepts.html#2-connected-to-a-dag-directed-acyclic-graph","title":"2. Connected to a DAG (Directed Acyclic Graph)","text":"<p>Every FlowFrame has a reference to a FlowGraph that tracks every operation as a node:</p> <pre><code>df = ff.FlowFrame({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"active\": [True, False, True]\n})\nprint(df.flow_graph)  # Shows the graph this FlowFrame belongs to\nprint(df.node_id)     # Shows which node in the graph this FlowFrame represents\n</code></pre> <p>For a deeper understanding of how this DAG works internally, see FlowGraph in the Developers Guide).</p>"},{"location":"users/python-api/concepts/design-concepts.html#3-linear-operation-tracking","title":"3. Linear Operation Tracking","text":"<p>Each operation creates a new node in the graph, even if you repeat the same operation:</p> <pre><code>df = ff.FlowFrame({\n    \"id\": [1, 2, 3, 4],\n    \"amount\": [50, 150, 75, 200],\n    \"status\": [\"active\", \"inactive\", \"active\", \"active\"]\n})\nprint(f\"Initial graph has {len(df.flow_graph.nodes)} nodes\")\n\n# First filter - creates node 1\ndf1 = df.filter(ff.col(\"amount\") &gt; 100)\nprint(f\"After first filter: {len(df1.flow_graph.nodes)} nodes\")\n\n# Second identical filter - creates node 2 (not reused!)\ndf2 = df.filter(ff.col(\"amount\") &gt; 100)  \nprint(f\"After second filter: {len(df2.flow_graph.nodes)} nodes\")\n\n# Both operations are tracked separately in the graph\n</code></pre>"},{"location":"users/python-api/concepts/design-concepts.html#flowgraph-the-pipelines-blueprint","title":"FlowGraph: The Pipeline's Blueprint","text":""},{"location":"users/python-api/concepts/design-concepts.html#what-is-flowgraph","title":"What is FlowGraph?","text":"<p>FlowGraph is the \"brain\" behind FlowFrame - it's a Directed Acyclic Graph (DAG) that tracks every step in your data transformation pipeline.</p> <pre><code># Access the graph from any FlowFrame\ndf = ff.FlowFrame({\n    \"product\": [\"Widget\", \"Gadget\", \"Tool\"],\n    \"price\": [10.99, 25.50, 8.75],\n    \"quantity\": [100, 50, 200]\n})\ngraph = df.flow_graph\n\nprint(f\"Graph ID: {graph.flow_id}\")\nprint(f\"Number of operations: {len(graph.nodes)}\")\nprint(f\"Node connections: {graph.node_connections}\")\n</code></pre>"},{"location":"users/python-api/concepts/design-concepts.html#visual-integration","title":"Visual Integration","text":""},{"location":"users/python-api/concepts/design-concepts.html#viewing-your-graph","title":"Viewing Your Graph","text":"<p>Every FlowFrame can show its complete operation history in the visual editor:</p> <pre><code># Build a pipeline\nresult = (\n    ff.FlowFrame({\n        \"region\": [\"North\", \"South\", \"North\", \"East\", \"South\"],\n        \"amount\": [1000, 0, 1500, 800, 1200],\n        \"product\": [\"A\", \"B\", \"A\", \"C\", \"B\"]\n    }, description=\"Load sales data\")\n    .filter(ff.col(\"amount\") &gt; 0, description=\"Remove invalid amounts\")\n    .group_by(\"region\", description=\"Group by sales region\")\n    .agg(ff.col(\"amount\").sum().alias(\"total_sales\"))\n)\n\n# Open the entire pipeline in the visual editor\nff.open_graph_in_editor(result.flow_graph)\n</code></pre> <p>Learn More</p> <p>See Visual UI Integration for details on launching and controlling the visual editor from Python.</p> <p>This opens a visual representation showing: - Each operation as a node - Data flow between operations - Descriptions you added for documentation - Schema changes at each step</p>"},{"location":"users/python-api/concepts/design-concepts.html#real-time-schema-prediction","title":"Real-time Schema Prediction","text":"<p>The DAG enables instant schema prediction without processing data:</p> <pre><code>df = ff.FlowFrame({\n    \"product\": [\"Widget\", \"Gadget\"],\n    \"price\": [10.50, 25.00],\n    \"quantity\": [2, 3]\n})\nprint(\"Original schema:\", df.schema)\n\n# Schema is predicted instantly, no data processed\ntransformed = df.with_columns([\n    (ff.col(\"price\") * ff.col(\"quantity\")).alias(\"total\")\n])\nprint(\"New schema:\", transformed.schema)  # Shows new 'total' column immediately\n</code></pre> <p>How Schema Prediction Works</p> <p>Learn about the closure pattern that enables this in The Magic of Closures.</p>"},{"location":"users/python-api/concepts/design-concepts.html#practical-implications","title":"Practical Implications","text":""},{"location":"users/python-api/concepts/design-concepts.html#memory-efficiency","title":"Memory Efficiency","text":"<p>Since FlowFrame is always lazy:</p> <pre><code># This can handle large datasets efficiently through lazy evaluation\nlarge_pipeline = (\n    ff.FlowFrame({\n        \"id\": list(range(10000)),\n        \"quality_score\": [0.1, 0.9, 0.8, 0.95] * 2500,  # Simulating large data\n        \"value\": list(range(10000)),\n        \"category\": [\"A\", \"B\", \"C\", \"D\"] * 2500\n    })\n    .filter(ff.col(\"quality_score\") &gt; 0.95)  # Reduces data early\n    .select([\"id\", \"value\", \"category\"])     # Reduces columns early  \n    .group_by(\"category\")\n    .agg(ff.col(\"value\").mean())\n)\n\n# Only processes what's needed when you collect\nresult = large_pipeline.collect()  # Optimized execution plan\n</code></pre> <p>Performance Guide</p> <p>For more on optimization strategies, see Execution Methods in our philosophy guide.</p>"},{"location":"users/python-api/concepts/design-concepts.html#graph-reuse-and-copying","title":"Graph Reuse and Copying","text":"<p>You can work with the same graph across multiple FlowFrames:</p> <pre><code># Start with common base\nbase = ff.FlowFrame({\n    \"region\": [\"North\", \"South\", \"East\"],\n    \"year\": [2024, 2024, 2023],\n    \"sales\": [1000, 1500, 800],\n    \"product\": [\"Widget\", \"Gadget\", \"Tool\"],\n    \"quantity\": [10, 15, 8]\n}).filter(ff.col(\"year\") == 2024)\n\n# Create different branches (same graph, different endpoints)\nsales_summary = base.group_by(\"region\").agg(ff.col(\"sales\").sum())\nproduct_summary = base.group_by(\"product\").agg(ff.col(\"quantity\").sum())\n\n# Both share the same underlying graph\nassert sales_summary.flow_graph is product_summary.flow_graph\n</code></pre>"},{"location":"users/python-api/concepts/design-concepts.html#best-practices","title":"Best Practices","text":""},{"location":"users/python-api/concepts/design-concepts.html#1-use-descriptions-for-complex-pipelines","title":"1. Use Descriptions for Complex Pipelines","text":"<pre><code>import flowfile as ff\npipeline = (\n    ff.FlowFrame({\n        \"customer_id\": [1, 2, 3, 4, 5],\n        \"status\": [\"active\", \"inactive\", \"active\", \"active\", \"inactive\"],\n        \"signup_date\": [\"2024-01-15\", \"2023-12-10\", \"2024-02-20\", \"2023-11-05\", \"2024-03-01\"],\n        \"customer_segment\": [\"premium\", \"basic\", \"premium\", \"basic\", \"premium\"],\n        \"revenue\": [1000, 500, 1500, 300, 2000]\n    }, description=\"Load raw customer data\")\n    .filter(ff.col(\"status\") == \"active\", description=\"Keep only active customers\")\n    .with_columns([\n        ff.col(\"signup_date\").str.strptime(ff.Date, \"%Y-%m-%d\").alias(\"signup_date\")\n    ], description=\"Parse signup dates\")\n    .group_by(\"customer_segment\", description=\"Aggregate by customer segment\")\n    .agg([\n        ff.col(\"revenue\").sum().alias(\"total_revenue\"),\n        ff.col(\"customer_id\").count().alias(\"customer_count\")\n    ], description=\"Calculate segment metrics\")\n)\n</code></pre>"},{"location":"users/python-api/concepts/design-concepts.html#2-visualize-during-development","title":"2. Visualize During Development","text":"<pre><code># Check your pipeline structure frequently\nff.open_graph_in_editor(pipeline.flow_graph)\n</code></pre> <p>Complete Examples</p> <ul> <li>Database Pipeline: See PostgreSQL Integration for a real-world ui example</li> <li>Cloud Pipeline: Check Cloud Connections for S3 workflows</li> <li>Export to Code: Learn how your pipelines convert to pure Python in Flow to Code</li> </ul>"},{"location":"users/python-api/concepts/design-concepts.html#summary","title":"Summary","text":"<p>FlowFrame and FlowGraph work together to provide:</p> <ul> <li>Lazy evaluation: No memory waste, optimal performance</li> <li>Complete lineage: Every operation is tracked and visualizable  </li> <li>Real-time feedback: Instant schema prediction and error detection</li> <li>Seamless integration: Switch between code and visual editing</li> <li>Polars compatibility: Very identical API with additional features</li> <li>Automatic adaptation: Complex operations automatically fall back to code nodes</li> </ul> <p>Understanding this design helps you build efficient, maintainable data pipelines that scale from quick analyses to production ETL workflows.</p>"},{"location":"users/python-api/concepts/design-concepts.html#related-documentation","title":"Related Documentation","text":"<ul> <li>FlowFrame Operations - Available transformations and methods</li> <li>Expressions - Column operations and formula syntax</li> <li>Joins - Combining datasets</li> <li>Aggregations - Group by and summarization</li> <li>Visual UI Integration - Working with the visual editor</li> <li>Developers guide - Core architecture and design philosophy</li> </ul>"},{"location":"users/python-api/concepts/expressions.html","title":"Expressions","text":"<p>Column expressions for data transformations. Flowfile follows the Polars expressions API, with additional features for Flowfile-formula syntax.</p>"},{"location":"users/python-api/concepts/expressions.html#flowfile-formula-syntax","title":"Flowfile formula syntax","text":"<p>Flowfile supports a simplified formula syntax for expressions, allowing you to use bracket notation for column references. This will render nicely in the Flowfile UI and was implemented to decrease the learning curve for users coming from Excel, PowerBI, Alteryx and Tableau. You can try it out here: Flowfile Formula Playground.</p> <p>For example, instead of using <code>ff.col(\"price\")</code>, you can use <code>[price]</code> in supported operations. Operations that support this syntax are documented below.</p> <ul> <li>ff.with_columns     You can use flowfile_formulas: Optional[List[str]] = None, output_column_names: Optional[List[str]] = None, to create new columns based on Flowfile-formula syntax. The number of columns in <code>output_column_names</code> must match the number of formulas in <code>flowfile_formulas</code>.</li> <li>ff.filter     You can use the flowfile_formula parameter to filter rows based on Flowfile Formula syntax.</li> </ul>"},{"location":"users/python-api/concepts/expressions.html#column-references","title":"Column References","text":"<pre><code>import flowfile as ff\n\n# Polars style\nff.col(\"price\")\n\n# Flowfile formula syntax (in supported operations)\n\"[price]\"  # Equivalent to ff.col(\"price\")\n</code></pre>"},{"location":"users/python-api/concepts/expressions.html#example-flowfile-formula-syntax","title":"Example Flowfile formula syntax","text":"<pre><code>import flowfile as ff\n\ndf = ff.FlowFrame({\n    \"product\": [\"Widget\", \"Gadget\", \"Tool\"],\n    \"price\": [10.50, 25.00, 8.75],\n    \"quantity\": [100, 50, 200],\n    \"discount\": [0.1, 0.15, 0.05]\n})\n\n# Using Flowfile formula syntax (Excel-like)\nresult = df.with_columns(\n    flowfile_formulas=[\n        \"[price] * [quantity]\",                    # Simple multiplication\n        \"[price] * (1 - [discount])\",              # With parentheses\n        \"if [quantity] &gt; 75 then 'High' else 'Low' endif\",      # Conditional\n        \"round([price] * [discount], 2)\"\n    ],\n    output_column_names=[\"revenue\", \"discounted_price\", \"volume_category\", \"discount_amount\"],\n    description=\"Calculate derived metrics\"\n)\n\n# Mix formulas with regular Polars expressions\noutput = df.with_columns([\n    ff.col(\"price\").round(0).alias(\"price_rounded\")  # Polars style\n]).with_columns(\n    flowfile_formulas=[\"[price_rounded] * [quantity]\"],  # Formula style\n    output_column_names=[\"estimated_revenue\"]\n)\n</code></pre>"},{"location":"users/python-api/concepts/expressions.html#arithmetic-operations","title":"Arithmetic Operations","text":"<pre><code># Standard expressions\ndf = df.with_columns([\n    (ff.col(\"price\") * ff.col(\"quantity\")).alias(\"revenue\"),\n    (ff.col(\"price\") * 1.1).alias(\"price_with_tax\"),\n    (ff.col(\"total\") / ff.col(\"count\")).alias(\"average\")\n])\n\n# Formula syntax\ndf = df.with_columns(\n    flowfile_formulas=[\n        \"[price] * [quantity]\",\n        \"[price] * 1.1\",\n        \"[total] / [count]\"\n    ],\n    output_column_names=[\"revenue\", \"price_with_tax\", \"average\"]\n)\n</code></pre> <p>Formula Syntax</p> <p>Use <code>[column_name]</code> in formula strings for simpler syntax when supported by the operation.</p> <p>Want to see more about the Flowfile python api? Check out the reference documentation.</p>"},{"location":"users/python-api/reference/index.html","title":"Python API Flowfile Reference","text":"<p>This section documents Flowfile's Python API, focusing on extensions and differences from Polars. For standard Polars operations, see the Polars documentation.</p>"},{"location":"users/python-api/reference/index.html#core-api","title":"Core API","text":""},{"location":"users/python-api/reference/index.html#data-inputoutput","title":"Data Input/Output","text":"<ul> <li>Reading Data - File formats and cloud storage</li> <li>Writing Data - Saving results</li> <li>Data Types - Supported data types</li> </ul>"},{"location":"users/python-api/reference/index.html#transformations","title":"Transformations","text":"<ul> <li>FlowFrame Operations - Filter, select, sort</li> <li>Aggregations - Group by and summarize</li> <li>Joins - Combining datasets</li> </ul>"},{"location":"users/python-api/reference/index.html#flowfile-specific-features","title":"Flowfile-Specific Features","text":"<ul> <li>Cloud Storage - S3 integration</li> <li>visualize pipelines - Working with the visual editor</li> </ul>"},{"location":"users/python-api/reference/index.html#key-extensions-to-polars","title":"Key Extensions to Polars","text":""},{"location":"users/python-api/reference/index.html#description-parameter","title":"Description Parameter","text":"<p>Every operation accepts <code>description</code> for visual documentation: <pre><code>df = df.filter(ff.col(\"active\") == True, description=\"Keep active records\")\n</code></pre></p>"},{"location":"users/python-api/reference/index.html#flowfile-formula-syntax","title":"Flowfile Formula Syntax","text":"<p>Alternative bracket-based syntax for expressions: <pre><code>df.filter(flowfile_formula=\"[price] &gt; 100 AND [quantity] &gt;= 10\")\n</code></pre> Read more about the formula syntax here: Flowfile Formula Syntax. Or try it out here: Flowfile Formula Playground</p>"},{"location":"users/python-api/reference/index.html#automatic-node-types","title":"Automatic Node Types","text":"<p>Operations map to UI nodes when possible, otherwise fall back to <code>polars_code</code>: <pre><code># Simple \u2192 UI node\ndf.group_by(\"category\").agg(ff.col(\"value\").sum())\n\n# Complex \u2192 polars_code node\ndf.group_by([ff.col(\"category\").str.to_uppercase()]).agg(ff.col(\"value\").sum())\n</code></pre></p>"},{"location":"users/python-api/reference/index.html#graph-access","title":"Graph Access","text":"<p>Inspect and visualize the pipeline DAG: <pre><code>ff.open_graph_in_editor(df.flow_graph)\n</code></pre></p>"},{"location":"users/python-api/reference/index.html#architecture-deep-dives","title":"Architecture Deep Dives","text":"<p>For understanding how Flowfile works internally:</p> <ul> <li>Core Architecture - FlowGraph, FlowNode, and FlowDataEngine internals</li> <li>Design Philosophy - The dual interface approach</li> </ul>"},{"location":"users/python-api/reference/index.html#getting-help","title":"Getting Help","text":"<ul> <li>Not finding a method? Check the Polars documentation - most methods work identically</li> <li>Need examples? See our tutorials</li> <li>Understanding concepts? Read about FlowFrame and FlowGraph</li> </ul> <p>This reference covers Flowfile-specific features. For standard Polars operations, see the Polars API Reference.</p>"},{"location":"users/python-api/reference/aggregations.html","title":"Aggregations","text":"<p>Group by and aggregate operations for summarizing data.</p>"},{"location":"users/python-api/reference/aggregations.html#basic-group-by","title":"Basic Group By","text":"<pre><code>import flowfile as ff\n\ndf = ff.FlowFrame({\n    \"category\": [\"A\", \"B\", \"A\", \"B\", \"A\"],\n    \"value\": [10, 20, 30, 40, 50],\n    \"quantity\": [1, 2, 3, 4, 5]\n})\n\n# Simple aggregation\nresult = df.group_by(\"category\").agg([\n    ff.col(\"value\").sum().alias(\"total_value\"),\n    ff.col(\"value\").mean().alias(\"avg_value\"),\n    ff.col(\"quantity\").count().alias(\"count\")\n])\n\n# With description\nresult = df.group_by(\"category\", description=\"Group by product category\").agg([\n    ff.col(\"value\").sum().alias(\"total_value\")\n])\n</code></pre>"},{"location":"users/python-api/reference/aggregations.html#multiple-grouping-columns","title":"Multiple Grouping Columns","text":"<pre><code>result = df.group_by([\"region\", \"category\"]).agg([\n    ff.col(\"sales\").sum().alias(\"total_sales\"),\n    ff.col(\"sales\").mean().alias(\"avg_sales\")\n])\n</code></pre>"},{"location":"users/python-api/reference/aggregations.html#complex-group-by","title":"Complex Group By","text":"<pre><code># Group by expression (creates polars_code node)\nresult = df.group_by([\n    ff.col(\"date\").dt.year().alias(\"year\")\n]).agg([\n    ff.col(\"amount\").sum()\n])\n\n# Dynamic aggregation\nresult = df.group_by(\"category\").agg([\n    ff.all().sum()  # Sum all numeric columns\n])\n</code></pre>"},{"location":"users/python-api/reference/aggregations.html#available-aggregations","title":"Available Aggregations","text":"Function Description <code>sum()</code> Sum of values <code>mean()</code> Average value <code>median()</code> Median value <code>min()</code> Minimum value <code>max()</code> Maximum value <code>count()</code> Count of non-null values <code>std()</code> Standard deviation <code>var()</code> Variance <code>first()</code> First value in group <code>last()</code> Last value in group <code>list()</code> Collect values into list"},{"location":"users/python-api/reference/aggregations.html#window-functions","title":"Window Functions","text":"<pre><code># Running calculations\ndf = df.with_columns([\n    ff.col(\"value\").cumsum().over(\"category\").alias(\"running_total\"),\n    ff.col(\"value\").rank().over(\"category\").alias(\"rank\")\n])\n</code></pre> <p>Node Type Selection</p> <p>Simple group_by operations create UI nodes. Complex expressions in group_by create <code>polars_code</code> nodes.</p> <p>\u2190 Previous: Expressions | Next: Joins \u2192</p>"},{"location":"users/python-api/reference/cloud-connections.html","title":"Cloud Connection Management","text":"<p>Flowfile provides secure, centralized management for cloud storage connections. Connections can be created through code or the UI\u2014both store credentials in an encrypted database.  On this page we will cover how to create and manage them in Python. If you want to learn how to create them in the UI,  check out the UI guide.</p>"},{"location":"users/python-api/reference/cloud-connections.html#creating-connections","title":"Creating Connections","text":""},{"location":"users/python-api/reference/cloud-connections.html#code-approach","title":"Code Approach","text":"<pre><code>import flowfile as ff\nfrom pydantic import SecretStr\n\n# Create a new S3 connection\nff.create_cloud_storage_connection(\n    ff.FullCloudStorageConnection(\n        connection_name=\"data-lake\",\n        storage_type=\"s3\",\n        auth_method=\"access_key\",\n        aws_region=\"us-east-1\",\n        aws_access_key_id=\"AKIAIOSFODNN7EXAMPLE\",\n        aws_secret_access_key=SecretStr(\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\")\n    )\n)\n</code></pre>"},{"location":"users/python-api/reference/cloud-connections.html#visual-editor-integration","title":"Visual Editor Integration","text":"<p>Connections created through code are immediately available in the Flowfile visual editor:</p> <pre><code># Create connection in code\nff.create_cloud_storage_connection(\n    ff.FullCloudStorageConnection(\n        connection_name=\"data-lake\",\n        # ... parameters\n    )\n)\n\n# This connection now appears in:\n# - Cloud Storage Reader node's connection dropdown\n# - Cloud Storage Writer node's connection dropdown\n# - Any other nodes that use cloud connections\n</code></pre> <p>Seamless Integration</p> <p>There's no difference between connections created via code or UI. Both are stored in the same encrypted database and are instantly available across all interfaces.</p>"},{"location":"users/python-api/reference/cloud-connections.html#connection-types","title":"Connection Types","text":""},{"location":"users/python-api/reference/cloud-connections.html#s3-connection-access-key","title":"S3 Connection (Access Key)","text":"<pre><code>ff.FullCloudStorageConnection(\n    connection_name=\"my-s3\",\n    storage_type=\"s3\",\n    auth_method=\"access_key\",\n    aws_region=\"us-east-1\",\n    aws_access_key_id=\"AKIAIOSFODNN7EXAMPLE\",\n    aws_secret_access_key=SecretStr(\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"),\n    endpoint_url=\"https://s3.amazonaws.com\"  # Optional, for custom endpoints\n)\n</code></pre> <pre><code>ff.FullCloudStorageConnection(\n    connection_name=\"my-s3-cli\",\n    storage_type=\"s3\",\n    auth_method=\"aws_cli\",  # Uses local AWS CLI credentials\n    aws_region=\"us-east-1\"\n)\n</code></pre>"},{"location":"users/python-api/reference/cloud-connections.html#managing-connections","title":"Managing Connections","text":""},{"location":"users/python-api/reference/cloud-connections.html#create-if-not-exists","title":"Create If Not Exists","text":"<p>Safely create connections without duplicates:</p> <pre><code># Only creates if \"data-lake\" doesn't exist\nff.create_cloud_storage_connection_if_not_exists(\n    ff.FullCloudStorageConnection(\n        connection_name=\"data-lake\",\n        storage_type=\"s3\",\n        # ... other parameters\n    )\n)\n</code></pre>"},{"location":"users/python-api/reference/cloud-connections.html#list-all-connections","title":"List All Connections","text":"<pre><code># Get all available connections for current user\nconnections = ff.get_all_available_cloud_storage_connections()\n\nfor conn in connections:\n    print(f\"Name: {conn.connection_name}\")\n    print(f\"Type: {conn.storage_type}\")\n    print(f\"Region: {conn.aws_region}\")\n</code></pre>"},{"location":"users/python-api/reference/cloud-connections.html#delete-connection","title":"Delete Connection","text":"<pre><code># Remove a connection by name\nff.del_cloud_storage_connection(\"old-connection\")\n</code></pre>"},{"location":"users/python-api/reference/cloud-connections.html#using-connections","title":"Using Connections","text":"<p>Once created, use connections in read/write operations:</p> <pre><code># Reading with connection\ndf = ff.scan_parquet_from_cloud_storage(\n    \"s3://bucket/data.parquet\",\n    connection_name=\"data-lake\"  # Use the connection name\n)\n\n# Writing with connection\ndf.write_parquet_to_cloud_storage(\n    \"s3://bucket/output.parquet\",\n    connection_name=\"data-lake\"\n)\n</code></pre>"},{"location":"users/python-api/reference/cloud-connections.html#security-features","title":"Security Features","text":""},{"location":"users/python-api/reference/cloud-connections.html#credential-encryption","title":"Credential Encryption","text":"<ul> <li>All credentials are encrypted before storage</li> <li>Secrets never appear in logs or error messages</li> <li>Use <code>SecretStr</code> wrapper for sensitive values</li> </ul>"},{"location":"users/python-api/reference/cloud-connections.html#user-isolation","title":"User Isolation","text":"<ul> <li>Connections are scoped to the current user</li> <li>Each user manages their own connections</li> <li>No cross-user credential access</li> </ul>"},{"location":"users/python-api/reference/cloud-connections.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"users/python-api/reference/cloud-connections.html#common-issues","title":"Common Issues","text":"Issue Solution \"Connection not found\" Ensure connection exists with <code>get_all_available_cloud_storage_connections()</code> \"Access denied\" Verify credentials and permissions \"Invalid endpoint\" Check <code>endpoint_url</code> for custom S3 services \"SSL verification failed\" Use <code>aws_allow_unsafe_html=True</code> for local/dev endpoints only"},{"location":"users/python-api/reference/cloud-connections.html#debug-connection","title":"Debug Connection","text":"<pre><code># List all connections to verify\nconns = ff.get_all_available_cloud_storage_connections()\nprint(f\"Available connections: {[c.connection_name for c in conns]}\")\n\n# Check specific connection details\nmy_conn = next((c for c in conns if c.connection_name == \"data-lake\"), None)\nif my_conn:\n    print(f\"Storage type: {my_conn.storage_type}\")\n    print(f\"Auth method: {my_conn.auth_method}\")\n    print(f\"Region: {my_conn.aws_region}\")\n</code></pre> <p>UI Integration</p> <p>All connections created via code are immediately available in the UI's connection dropdown when configuring nodes.</p> <p>\u2190 Previous: Joins | Next: visual Ui \u2192</p>"},{"location":"users/python-api/reference/data-types.html","title":"Data Types","text":"<p>Flowfile supports all Polars data types. This page covers the most commonly used types and conversions.</p>"},{"location":"users/python-api/reference/data-types.html#supported-types","title":"Supported Types","text":"Type Description Example <code>Int8</code>, <code>Int16</code>, <code>Int32</code>, <code>Int64</code> Signed integers <code>123</code> <code>UInt8</code>, <code>UInt16</code>, <code>UInt32</code>, <code>UInt64</code> Unsigned integers <code>456</code> <code>Float32</code>, <code>Float64</code> Floating point <code>12.34</code> <code>Boolean</code> True/False values <code>True</code> <code>Utf8</code> / <code>String</code> Text data <code>\"hello\"</code> <code>Date</code> Date without time <code>2024-01-15</code> <code>Datetime</code> Date with time <code>2024-01-15 14:30:00</code> <code>Time</code> Time without date <code>14:30:00</code> <code>Duration</code> Time delta <code>2 days</code> <code>List</code> Nested arrays <code>[1, 2, 3]</code> <code>Struct</code> Nested objects <code>{\"a\": 1, \"b\": 2}</code>"},{"location":"users/python-api/reference/data-types.html#type-casting","title":"Type Casting","text":"<pre><code>import flowfile as ff\n\ndf = ff.FlowFrame({\n    \"int_col\": [1, 2, 3],\n    \"str_col\": [\"10\", \"20\", \"30\"],\n    \"date_str\": [\"2024-01-01\", \"2024-01-02\", \"2024-01-03\"]\n})\n\n# Cast types\ndf = df.with_columns([\n    ff.col(\"int_col\").cast(ff.Float64).alias(\"float_col\"),\n    ff.col(\"str_col\").cast(ff.Int32).alias(\"parsed_int\"),\n    ff.col(\"date_str\").str.strptime(ff.Date, \"%Y-%m-%d\").alias(\"date_col\")\n])\n</code></pre>"},{"location":"users/python-api/reference/data-types.html#schema-inspection","title":"Schema Inspection","text":"<pre><code># Get schema without processing data\nprint(df.schema)\n# [Column(name='int_col', dtype=Int64), ...]\n\n# Check specific column type\nprint(df.schema[0].dtype)\n# Int64\n</code></pre> <p>\u2190 Previous: Writing data | Next: FlowFile Operations \u2192</p>"},{"location":"users/python-api/reference/flowframe-operations.html","title":"DataFrame Operations","text":"<p>Core operations for transforming data. All standard Polars operations are supported with additional Flowfile features.</p>"},{"location":"users/python-api/reference/flowframe-operations.html#filtering","title":"Filtering","text":"<pre><code>import flowfile as ff\n\ndf = ff.FlowFrame({\"price\": [10, 20, 30], \"qty\": [5, 0, 10]})\n\n# Standard Polars filter\ndf = df.filter(ff.col(\"price\") &gt; 15)\n\n# With description\ndf = df.filter(ff.col(\"price\") &gt; 15, description=\"Keep items over $15\")\n\n# Flowfile formula syntax\ndf = df.filter(flowfile_formula=\"[price] &gt; 15 AND [qty] &gt; 0\")\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#selecting-columns","title":"Selecting Columns","text":"<pre><code># Select specific columns\ndf = df.select([\"price\", \"qty\"])\n\n# Select with expressions\ndf = df.select([\n    ff.col(\"price\"),\n    ff.col(\"qty\").alias(\"quantity\")\n])\n\n# Exclude columns\ndf = df.select(ff.exclude(\"internal_id\"))\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#addingmodifying-columns","title":"Adding/Modifying Columns","text":"<pre><code># Standard with_columns\ndf = df.with_columns([\n    (ff.col(\"price\") * ff.col(\"qty\")).alias(\"total\")\n])\n\n# Flowfile formula syntax\ndf = df.with_columns(\n    flowfile_formulas=[\"[price] * [qty]\"],\n    output_column_names=[\"total\"],\n    description=\"Calculate line totals\"\n)\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#sorting","title":"Sorting","text":"<pre><code># Sort by column\ndf = df.sort(\"price\")\ndf = df.sort(\"price\", descending=True)\n\n# Multi-column sort\ndf = df.sort([\"category\", \"price\"], descending=[False, True])\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#unique-operations","title":"Unique Operations","text":"<pre><code># Get unique rows\ndf = df.unique()\n\n# Unique by specific columns\ndf = df.unique(subset=[\"product_id\"])\n\n# Drop duplicates (alias)\ndf = df.drop_duplicates(subset=[\"product_id\"])\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#string-operations","title":"String Operations","text":"<pre><code>df = df.with_columns([\n    ff.col(\"name\").str.to_uppercase().alias(\"name_upper\"),\n    ff.col(\"code\").str.slice(0, 3).alias(\"prefix\"),\n    ff.col(\"text\").str.contains(\"pattern\").alias(\"has_pattern\")\n])\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#conditional-logic","title":"Conditional Logic","text":"<pre><code># When/then/otherwise\ndf = df.with_columns([\n    ff.when(ff.col(\"price\") &gt; 100)\n    .then(ff.lit(\"Premium\"))\n    .when(ff.col(\"price\") &gt; 50)\n    .then(ff.lit(\"Standard\"))\n    .otherwise(ff.lit(\"Budget\"))\n    .alias(\"tier\")\n])\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#date-operations","title":"Date Operations","text":"<pre><code>df = df.with_columns([\n    ff.col(\"date\").dt.year().alias(\"year\"),\n    ff.col(\"date\").dt.month().alias(\"month\"),\n    ff.col(\"date\").dt.day().alias(\"day\"),\n    ff.col(\"date\").dt.weekday().alias(\"weekday\")\n])\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#list-operations","title":"List Operations","text":"<pre><code>df = df.with_columns([\n    ff.col(\"tags\").list.len().alias(\"tag_count\"),\n    ff.col(\"values\").list.sum().alias(\"total\"),\n    ff.col(\"items\").list.first().alias(\"first_item\")\n])\n</code></pre> <p>Polars Compatibility</p> <p>All standard Polars DataFrame methods work identically. See Polars docs for complete reference.</p> <p>\u2190 Previous: Data Types | Next: Aggregations \u2192</p>"},{"location":"users/python-api/reference/joins.html","title":"Joins","text":"<p>Combining data from multiple FlowFrames.</p>"},{"location":"users/python-api/reference/joins.html#basic-join","title":"Basic Join","text":"<pre><code>import flowfile as ff\n\ncustomers = ff.FlowFrame({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n})\n\norders = ff.FlowFrame({\n    \"order_id\": [101, 102, 103],\n    \"customer_id\": [1, 2, 1],\n    \"amount\": [100, 200, 150]\n})\n\n# Inner join\nresult = customers.join(\n    orders,\n    left_on=\"id\",\n    right_on=\"customer_id\",\n    how=\"inner\",\n    description=\"Join customers with orders\"\n)\n</code></pre>"},{"location":"users/python-api/reference/joins.html#join-types","title":"Join Types","text":"<pre><code># Inner join (default)\ndf1.join(df2, on=\"key\", how=\"inner\")\n\n# Left join\ndf1.join(df2, on=\"key\", how=\"left\")\n\n# Outer join\ndf1.join(df2, on=\"key\", how=\"outer\")\n\n# Semi join (filter df1 by df2)\ndf1.join(df2, on=\"key\", how=\"semi\")\n\n# Anti join (exclude matches)\ndf1.join(df2, on=\"key\", how=\"anti\")\n</code></pre>"},{"location":"users/python-api/reference/joins.html#multiple-join-keys","title":"Multiple Join Keys","text":"<pre><code>result = df1.join(\n    df2,\n    on=[\"region\", \"year\"],  # Join on multiple columns\n    how=\"inner\"\n)\n\n# Different column names\nresult = df1.join(\n    df2,\n    left_on=[\"region_code\", \"period\"],\n    right_on=[\"region\", \"year\"],\n    how=\"left\"\n)\n</code></pre>"},{"location":"users/python-api/reference/joins.html#cross-join","title":"Cross Join","text":"<pre><code># Cartesian product\nresult = df1.join(df2, how=\"cross\")\n</code></pre>"},{"location":"users/python-api/reference/joins.html#unionconcatenation","title":"Union/Concatenation","text":"<pre><code># Vertical concatenation\ncombined = ff.concat([df1, df2, df3])\n\n# Union (removes duplicates)\nunion_df = df1.unique().vstack(df2.unique()).unique()\n\n# Diagonal concatenation (handles different schemas)\ncombined = ff.concat([df1, df2], how=\"diagonal\")\n</code></pre>"},{"location":"users/python-api/reference/joins.html#join-validation","title":"Join Validation","text":"<pre><code># Check for duplicates before joining\nif df2.select(\"customer_id\").n_unique() &lt; len(df2):\n    print(\"Warning: duplicate keys in right table\")\n\n# Validate join results\nresult = df1.join(df2, on=\"id\", how=\"left\")\nunmatched = result.filter(ff.col(\"amount\").is_null())\nprint(f\"Unmatched records: {len(unmatched)}\")\n</code></pre> <p>Unsupported Join Types</p> <p>Currently, <code>join_asof</code> and <code>join_where</code> are not supported in Flowfile. These operations will need to be implemented using alternative approaches or raw Polars code.</p> <p>\u2190 Previous: Aggregations | Next: Cloud Connection \u2192</p>"},{"location":"users/python-api/reference/reading-data.html","title":"Reading Data","text":"<p>Flowfile provides Polars-compatible readers with additional cloud storage integration and visual workflow features.</p> <p>Polars Compatibility</p> <p>All Flowfile readers accept the same parameters as Polars, plus optional <code>description</code> for visual documentation.</p>"},{"location":"users/python-api/reference/reading-data.html#local-file-reading","title":"Local File Reading","text":""},{"location":"users/python-api/reference/reading-data.html#csv-files","title":"CSV Files","text":"<pre><code>import flowfile as ff\n\n# Basic usage (same as Polars)\ndf = ff.read_csv(\"data.csv\")\n\n# With Flowfile description\ndf = ff.read_csv(\"data.csv\", description=\"Load customer data\")\n\n# Polars parameters work identically\ndf = ff.read_csv(\n    \"data.csv\",\n    separator=\",\",\n    has_header=True,\n    skip_rows=1,\n    n_rows=1000,\n    description=\"Sample first 1000 customer records\"\n)\n</code></pre> <p>Key Parameters (same as Polars):</p> <ul> <li><code>separator</code>: Field delimiter (default: <code>,</code>)</li> <li><code>has_header</code>: First row contains column names (default: <code>True</code>)</li> <li><code>skip_rows</code>: Skip rows at start of file</li> <li><code>n_rows</code>: Maximum rows to read</li> <li><code>encoding</code>: File encoding (default: <code>utf8</code>)</li> <li><code>null_values</code>: Values to treat as null</li> <li><code>schema_overrides</code>: Override column types</li> </ul>"},{"location":"users/python-api/reference/reading-data.html#parquet-files","title":"Parquet Files","text":"<pre><code># Basic usage\ndf = ff.read_parquet(\"data.parquet\")\n\n# With description\ndf = ff.read_parquet(\"sales_data.parquet\", description=\"Q4 sales results\")\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#scanning-vs-reading","title":"Scanning vs Reading","text":"<p>Flowfile provides both <code>read_*</code> and <code>scan_*</code> functions for Polars compatibility:</p> <pre><code># These are identical in Flowfile\ndf1 = ff.read_csv(\"data.csv\")\ndf2 = ff.scan_csv(\"data.csv\")  # Alias for read_csv\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#cloud-storage-reading","title":"Cloud Storage Reading","text":"<p>Flowfile extends Polars with specialized cloud storage readers that integrate with secure connection management.</p>"},{"location":"users/python-api/reference/reading-data.html#cloud-csv-reading","title":"Cloud CSV Reading","text":"<pre><code># Read from S3 with connection\ndf = ff.scan_csv_from_cloud_storage(\n    \"s3://my-bucket/data.csv\",\n    connection_name=\"my-aws-connection\",\n    delimiter=\",\",\n    has_header=True,\n    encoding=\"utf8\"\n)\n\n# Directory scanning (reads all CSV files)\ndf = ff.scan_csv_from_cloud_storage(\n    \"s3://my-bucket/csv-files/\",\n    connection_name=\"my-aws-connection\"\n)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#cloud-parquet-reading","title":"Cloud Parquet Reading","text":"<pre><code># Single file\ndf = ff.scan_parquet_from_cloud_storage(\n    \"s3://data-lake/sales.parquet\",\n    connection_name=\"data-lake-connection\"\n)\n\n# Directory of files\ndf = ff.scan_parquet_from_cloud_storage(\n    \"s3://data-lake/partitioned-data/\",\n    connection_name=\"data-lake-connection\",\n    scan_mode=\"directory\"\n)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#cloud-json-reading","title":"Cloud JSON Reading","text":"<pre><code>df = ff.scan_json_from_cloud_storage(\n    \"s3://my-bucket/data.json\",\n    connection_name=\"my-aws-connection\"\n)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#delta-lake-reading","title":"Delta Lake Reading","text":"<pre><code># Latest version\ndf = ff.scan_delta(\n    \"s3://data-lake/delta-table\",\n    connection_name=\"data-lake-connection\"\n)\n\n# Specific version (if supported)\ndf = ff.scan_delta(\n    \"s3://data-lake/delta-table\",\n    connection_name=\"data-lake-connection\"\n    # Note: version parameter support depends on implementation\n)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#database-reading","title":"Database Reading","text":"<p>Read data from SQL databases using stored connections.</p>"},{"location":"users/python-api/reference/reading-data.html#setup-connection","title":"Setup Connection","text":"<pre><code>import flowfile as ff\n\nff.create_database_connection(\n    connection_name=\"my_db\",\n    database_type=\"postgresql\",\n    host=\"localhost\",\n    port=5432,\n    database=\"mydb\",\n    username=\"user\",\n    password=\"pass\"\n)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#read-a-table","title":"Read a Table","text":"<pre><code>df = ff.read_database(\n    \"my_db\",\n    table_name=\"users\",\n    schema_name=\"public\"\n)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#read-with-sql-query","title":"Read with SQL Query","text":"<pre><code>df = ff.read_database(\n    \"my_db\",\n    query=\"SELECT id, name FROM users WHERE active = true\"\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>connection_name</code>: Name of a stored database connection (required)</li> <li><code>table_name</code>: Table to read from</li> <li><code>schema_name</code>: Database schema (e.g., \"public\")</li> <li><code>query</code>: Custom SQL query (takes precedence over <code>table_name</code>)</li> </ul>"},{"location":"users/python-api/reference/reading-data.html#connection-management","title":"Connection Management","text":"<p>Before reading from cloud storage, set up connections:</p> <pre><code>import flowfile as ff\nfrom pydantic import SecretStr\n\n# Create S3 connection\nff.create_cloud_storage_connection_if_not_exists(\n    ff.FullCloudStorageConnection(\n        connection_name=\"my-aws-connection\",\n        storage_type=\"s3\",\n        auth_method=\"access_key\",\n        aws_region=\"us-east-1\",\n        aws_access_key_id=\"your-access-key\",\n        aws_secret_access_key=SecretStr(\"your-secret-key\")\n    )\n)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#flowfile-specific-features","title":"Flowfile-Specific Features","text":""},{"location":"users/python-api/reference/reading-data.html#description-parameter","title":"Description Parameter","text":"<p>Every reader accepts an optional <code>description</code> for visual documentation:</p> <pre><code>df = ff.read_csv(\n    \"quarterly_sales.csv\",\n    description=\"Load Q4 2024 sales data for analysis\"\n)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#automatic-scan-mode-detection","title":"Automatic Scan Mode Detection","text":"<p>Cloud storage readers automatically detect scan mode:</p> <pre><code># Automatically detects single file\ndf = ff.scan_parquet_from_cloud_storage(\"s3://bucket/file.parquet\")\n\n# Automatically detects directory scan\ndf = ff.scan_parquet_from_cloud_storage(\"s3://bucket/folder/\")\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#integration-with-visual-ui","title":"Integration with Visual UI","text":"<p>All reading operations create nodes in the visual workflow:</p> <pre><code>df = ff.read_csv(\"data.csv\", description=\"Source data\")\n\n# Open in visual editor\nff.open_graph_in_editor(df.flow_graph)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#examples","title":"Examples","text":""},{"location":"users/python-api/reference/reading-data.html#standard-data-pipeline","title":"Standard Data Pipeline","text":"<pre><code>import flowfile as ff\n\n# Read local file\ncustomers = ff.read_csv(\"customers.csv\", description=\"Customer master data\")\n\n# Read from cloud\norders = ff.scan_parquet_from_cloud_storage(\n    \"s3://data-warehouse/orders/\",\n    connection_name=\"warehouse\",\n    description=\"Order history from data warehouse\"\n)\n\n# Continue processing...\nresult = customers.join(orders, on=\"customer_id\")\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#multi-format-cloud-pipeline","title":"Multi-Format Cloud Pipeline","text":"<pre><code># Different formats from same connection\nconfig_data = ff.scan_json_from_cloud_storage(\n    \"s3://configs/settings.json\",\n    connection_name=\"app-data\"\n)\n\nsales_data = ff.scan_parquet_from_cloud_storage(\n    \"s3://analytics/sales/\",\n    connection_name=\"app-data\"\n)\n\ndelta_data = ff.scan_delta(\n    \"s3://warehouse/customer_dim\",\n    connection_name=\"app-data\"\n)\n</code></pre> <p>\u2190 Previous: Introduction | Next: Writing Data \u2192</p>"},{"location":"users/python-api/reference/visual-ui.html","title":"Visual UI Integration","text":"<p>Flowfile provides a web-based visual interface that can be launched directly from Python. This allows seamless transitions between code and visual pipeline development.</p>"},{"location":"users/python-api/reference/visual-ui.html#starting-the-web-ui","title":"Starting the Web UI","text":""},{"location":"users/python-api/reference/visual-ui.html#quick-start","title":"Quick Start","text":"<pre><code>import flowfile as ff\n\n# Start the web UI (opens browser automatically)\nff.start_web_ui()\n\n# Start without opening browser\nff.start_web_ui(open_browser=False)\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#command-line","title":"Command Line","text":"<pre><code># Start with default settings\nflowfile run ui\n\n# Start without opening browser\nflowfile run ui --no-browser\n</code></pre> <p>Unified Mode</p> <p>The web UI runs in \"unified mode\" - a single service that combines the Core API, Worker, and Web UI. No separate services or Docker required!</p>"},{"location":"users/python-api/reference/visual-ui.html#opening-pipelines-in-the-editor","title":"Opening Pipelines in the Editor","text":""},{"location":"users/python-api/reference/visual-ui.html#basic-usage","title":"Basic Usage","text":"<pre><code>import flowfile as ff\n\n# Build a pipeline in code\ndf = ff.FlowFrame({\n    \"product\": [\"Widget\", \"Gadget\", \"Tool\"],\n    \"price\": [19.99, 39.99, 15.99],\n    \"quantity\": [100, 50, 200]\n})\n\nresult = df.filter(ff.col(\"price\") &gt; 20).with_columns([\n    (ff.col(\"price\") * ff.col(\"quantity\")).alias(\"revenue\")\n])\n\n# Open in visual editor (auto-starts server if needed)\nff.open_graph_in_editor(result.flow_graph)\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#what-happens-behind-the-scenes","title":"What Happens Behind the Scenes","text":"<p>When you call <code>open_graph_in_editor()</code>:</p> <ol> <li>Saves the graph to a temporary <code>.flowfile</code> </li> <li>Checks if server is running at <code>http://localhost:63578</code></li> <li>Starts server if needed using <code>flowfile run ui --no-browser</code></li> <li>Imports the flow via API endpoint</li> <li>Opens browser tab at <code>http://localhost:63578/ui/flow/{id}</code></li> </ol>"},{"location":"users/python-api/reference/visual-ui.html#advanced-options","title":"Advanced Options","text":"<pre><code># Save to specific location instead of temp file\nff.open_graph_in_editor(\n    result.flow_graph,\n    storage_location=\"./my_pipeline.flowfile\"\n)\n\n# Don't automatically open browser\nff.open_graph_in_editor(\n    result.flow_graph,\n    automatically_open_browser=False\n)\n\n# Use custom module name (for development)\nff.open_graph_in_editor(\n    result.flow_graph,\n    module_name=\"my_custom_flowfile\"\n)\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#server-management","title":"Server Management","text":""},{"location":"users/python-api/reference/visual-ui.html#checking-server-status","title":"Checking Server Status","text":"<pre><code># All server management functions are in flowfile.api\nfrom flowfile.api import (\n    is_flowfile_running,\n    start_flowfile_server_process, \n    stop_flowfile_server_process,\n    get_auth_token\n)\n\nif is_flowfile_running():\n    print(\"Server is running\")\nelse:\n    print(\"Server is not running\")\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#manual-server-control","title":"Manual Server Control","text":"<pre><code>from flowfile.api import start_flowfile_server_process, stop_flowfile_server_process\n\n# Start server manually\nsuccess, single_mode = start_flowfile_server_process()\n\n# Stop server when done\nstop_flowfile_server_process()\n</code></pre> <p>Auto-cleanup</p> <p>The server process is automatically stopped when your Python script exits. No need to manually stop it unless you want to free resources earlier.</p>"},{"location":"users/python-api/reference/visual-ui.html#configuration","title":"Configuration","text":""},{"location":"users/python-api/reference/visual-ui.html#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>FLOWFILE_HOST</code> <code>127.0.0.1</code> Host to bind server to <code>FLOWFILE_PORT</code> <code>63578</code> Port for the server <code>FLOWFILE_MODULE_NAME</code> <code>flowfile</code> Module name to run"},{"location":"users/python-api/reference/visual-ui.html#urls-and-endpoints","title":"URLs and Endpoints","text":"<p>Once running, the following are available:</p> <ul> <li>Web UI: <code>http://localhost:63578/ui</code></li> <li>API Docs: <code>http://localhost:63578/docs</code></li> <li>Health Check: <code>http://localhost:63578/docs</code> (used to verify server is running)</li> </ul>"},{"location":"users/python-api/reference/visual-ui.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"users/python-api/reference/visual-ui.html#server-wont-start","title":"Server Won't Start","text":"<pre><code># Check if port is already in use\nimport socket\n\ndef is_port_in_use(port):\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        return s.connect_ex(('localhost', port)) == 0\n\nif is_port_in_use(63578):\n    print(\"Port 63578 is already in use\")\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#server-starts-but-ui-doesnt-open","title":"Server Starts but UI Doesn't Open","text":"<ul> <li>Manually navigate to <code>http://localhost:63578/ui</code></li> <li>Check server logs in terminal</li> <li>Verify no firewall blocking localhost connections</li> </ul>"},{"location":"users/python-api/reference/visual-ui.html#import-fails","title":"Import Fails","text":"<pre><code># Verify authentication is working\nfrom flowfile.api import get_auth_token\n\ntoken = get_auth_token()\nif token:\n    print(\"Auth successful\")\nelse:\n    print(\"Auth failed - check server logs\")\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#poetry-environment-issues","title":"Poetry Environment Issues","text":"<p>If using Poetry for development:</p> <pre><code># Force Poetry detection\nimport os\nos.environ[\"FORCE_POETRY\"] = \"1\"\n\n# Or specify Poetry path\nos.environ[\"POETRY_PATH\"] = \"/path/to/poetry\"\n\nff.open_graph_in_editor(df.flow_graph)\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#best-practices","title":"Best Practices","text":""},{"location":"users/python-api/reference/visual-ui.html#1-let-auto-start-handle-it","title":"1. Let Auto-start Handle It","text":"<pre><code># \u2705 Good: Let open_graph_in_editor start server\nff.open_graph_in_editor(df.flow_graph)\n\n# \u274c Avoid: Manual server management unless necessary\nff.start_web_ui()\ntime.sleep(5)\nff.open_graph_in_editor(df.flow_graph)\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#2-use-temporary-files","title":"2. Use Temporary Files","text":"<pre><code># \u2705 Good: Let Flowfile handle temp files\nff.open_graph_in_editor(df.flow_graph)\n\n# Only specify path if you need to keep the file\nff.open_graph_in_editor(\n    df.flow_graph,\n    storage_location=\"./important_pipeline.flowfile\"\n)\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#3-single-server-instance","title":"3. Single Server Instance","text":"<p>The server is designed to be a singleton - multiple calls to <code>open_graph_in_editor()</code> will reuse the same server instance.</p> <pre><code># First call starts server\nff.open_graph_in_editor(pipeline1.flow_graph)\n\n# Subsequent calls reuse server\nff.open_graph_in_editor(pipeline2.flow_graph)  # No new server started\nff.open_graph_in_editor(pipeline3.flow_graph)  # Still same server\n</code></pre> <p>Where to Go Next</p> <ul> <li>Explore Visual Nodes: Learn the details of each node available in the Visual Editor.</li> <li>Convert Code to Visual: See how your code translates into a visual workflow in the Conversion Guide.</li> <li>Build with Code: Dive deeper into the code-first approach for building pipelines.</li> <li>Back to Index: Return to the main Python API Index.</li> </ul>"},{"location":"users/python-api/reference/writing-data.html","title":"Writing Data","text":"<p>Flowfile provides Polars-compatible writers with additional cloud storage integration and visual workflow features.</p> <p>Polars Compatibility</p> <p>Local file writers work identically to Polars, plus optional <code>description</code> for visual documentation.</p>"},{"location":"users/python-api/reference/writing-data.html#local-file-writing","title":"Local File Writing","text":""},{"location":"users/python-api/reference/writing-data.html#csv-files","title":"CSV Files","text":"<pre><code>import flowfile as ff\n\n# Basic usage (same as Polars)\ndf = ff.read_csv(\"input.csv\")\ndf.write_csv(\"output.csv\")\n\n# With Flowfile description\ndf.write_csv(\"processed_data.csv\", description=\"Save cleaned customer data\")\n\n# Polars parameters work identically\ndf.write_csv(\n    \"output.csv\",\n    separator=\";\",\n    encoding=\"utf-8\",\n    description=\"Export with semicolon delimiter\"\n)\n</code></pre> <p>Key Parameters (same as Polars):</p> <ul> <li><code>separator</code>: Field delimiter (default: <code>,</code>)</li> <li><code>encoding</code>: File encoding (default: <code>utf-8</code>)</li> </ul>"},{"location":"users/python-api/reference/writing-data.html#parquet-files","title":"Parquet Files","text":"<pre><code># Basic usage\ndf.write_parquet(\"output.parquet\")\n\n# With description and compression\ndf.write_parquet(\n    \"compressed_data.parquet\",\n    description=\"Save with high compression\",\n    compression=\"gzip\"\n)\n</code></pre>"},{"location":"users/python-api/reference/writing-data.html#cloud-storage-writing","title":"Cloud Storage Writing","text":"<p>Flowfile extends writing capabilities with specialized cloud storage writers that integrate with secure connection management.</p>"},{"location":"users/python-api/reference/writing-data.html#cloud-csv-writing","title":"Cloud CSV Writing","text":"<pre><code># Write to S3\ndf.write_csv_to_cloud_storage(\n    \"s3://my-bucket/output.csv\",\n    connection_name=\"my-aws-connection\",\n    delimiter=\",\",\n    encoding=\"utf8\",\n    description=\"Export processed data to S3\"\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>path</code>: Full S3 path including bucket and file name</li> <li><code>connection_name</code>: Name of configured cloud storage connection</li> <li><code>delimiter</code>: CSV field separator (default: <code>;</code>)</li> <li><code>encoding</code>: File encoding (<code>utf8</code> or <code>utf8-lossy</code>)</li> </ul>"},{"location":"users/python-api/reference/writing-data.html#cloud-parquet-writing","title":"Cloud Parquet Writing","text":"<pre><code># Write to S3 with compression\ndf.write_parquet_to_cloud_storage(\n    \"s3://data-lake/processed/results.parquet\",\n    connection_name=\"data-lake-connection\",\n    compression=\"snappy\",\n    description=\"Save analysis results to data lake\"\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>path</code>: Full S3 path for the output file</li> <li><code>connection_name</code>: Name of configured cloud storage connection  </li> <li><code>compression</code>: Compression algorithm (<code>snappy</code>, <code>gzip</code>, <code>brotli</code>, <code>lz4</code>, <code>zstd</code>)</li> </ul>"},{"location":"users/python-api/reference/writing-data.html#cloud-json-writing","title":"Cloud JSON Writing","text":"<pre><code># Write JSON to cloud storage\ndf.write_json_to_cloud_storage(\n    \"s3://api-data/export.json\", \n    connection_name=\"api-storage\",\n    description=\"Export for API consumption\"\n)\n</code></pre>"},{"location":"users/python-api/reference/writing-data.html#delta-lake-writing","title":"Delta Lake Writing","text":"<pre><code># Write Delta table (supports append mode)\ndf.write_delta(\n    \"s3://warehouse/customer_dim\",\n    connection_name=\"warehouse-connection\",\n    write_mode=\"overwrite\",\n    description=\"Update customer dimension table\"\n)\n\n# Append to existing Delta table\nnew_data.write_delta(\n    \"s3://warehouse/customer_dim\",\n    connection_name=\"warehouse-connection\", \n    write_mode=\"append\",\n    description=\"Add new customers to dimension\"\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>path</code>: S3 path for the Delta table</li> <li><code>connection_name</code>: Name of configured cloud storage connection</li> <li><code>write_mode</code>: <code>overwrite</code> (replace) or <code>append</code> (add to existing)</li> </ul>"},{"location":"users/python-api/reference/writing-data.html#write-modes","title":"Write Modes","text":""},{"location":"users/python-api/reference/writing-data.html#overwrite-vs-append","title":"Overwrite vs Append","text":"<pre><code># Overwrite existing data (default)\ndf.write_parquet_to_cloud_storage(\n    \"s3://bucket/data.parquet\",\n    connection_name=\"conn\",\n    write_mode=\"overwrite\"  # Default for most formats\n)\n\n# Append to existing (Delta Lake only)\ndf.write_delta(\n    \"s3://warehouse/events\",\n    connection_name=\"conn\",\n    write_mode=\"append\"\n)\n</code></pre> <p>Append Mode</p> <p>Currently only supported for Delta Lake format. Other formats always overwrite.</p>"},{"location":"users/python-api/reference/writing-data.html#connection-requirements","title":"Connection Requirements","text":"<p>All cloud storage writing requires a configured connection:</p> <pre><code>import flowfile as ff\nfrom pydantic import SecretStr\n\n# Set up connection before writing\nff.create_cloud_storage_connection_if_not_exists(\n    ff.FullCloudStorageConnection(\n        connection_name=\"data-lake\",\n        storage_type=\"s3\", \n        auth_method=\"access_key\",\n        aws_region=\"us-east-1\",\n        aws_access_key_id=\"your-key\",\n        aws_secret_access_key=SecretStr(\"your-secret\")\n    )\n)\n\n# Now you can write to cloud storage\ndf.write_parquet_to_cloud_storage(\n    \"s3://data-lake/output.parquet\",\n    connection_name=\"data-lake\"\n)\n</code></pre> <p>\u2190 Previous: Reading Data | Next: Data Types \u2192</p>"},{"location":"users/python-api/tutorials/index.html","title":"Python API Tutorials","text":"<p>Learn to build powerful data pipelines with code through practical, hands-on examples.</p>"},{"location":"users/python-api/tutorials/index.html#available-tutorials","title":"Available Tutorials","text":""},{"location":"users/python-api/tutorials/index.html#building-flows-with-code","title":"Building Flows with Code","text":"<p>The complete guide to creating data pipelines programmatically while maintaining visual compatibility.</p> <p>You'll learn: - Creating pipelines with the FlowFrame API - Using Polars-compatible operations - Automatically generating visual graphs - Switching between code and visual editing</p> <p>Perfect for: - Python developers new to Flowfile - Data scientists wanting reproducible pipelines - Anyone preferring code over drag-and-drop</p>"},{"location":"users/python-api/tutorials/index.html#coming-soon","title":"Coming Soon","text":""},{"location":"users/python-api/tutorials/index.html#data-pipeline-patterns","title":"Data Pipeline Patterns","text":"<p>Common patterns for ETL, data cleaning, and analysis.</p>"},{"location":"users/python-api/tutorials/index.html#performance-optimization","title":"Performance Optimization","text":"<p>Advanced techniques for handling large datasets efficiently.</p>"},{"location":"users/python-api/tutorials/index.html#integration-examples","title":"Integration Examples","text":"<p>Connecting Flowfile with pandas, scikit-learn, and other tools.</p>"},{"location":"users/python-api/tutorials/index.html#tutorial-style","title":"Tutorial Style","text":"<p>Our Python tutorials focus on: - Real-world examples - Practical use cases you'll actually encounter - Code-first approach - Everything done programmatically - Visual integration - How to leverage the UI when helpful - Best practices - Production-ready patterns</p>"},{"location":"users/python-api/tutorials/index.html#quick-examples","title":"Quick Examples","text":""},{"location":"users/python-api/tutorials/index.html#etl-pipeline","title":"ETL Pipeline","text":"<pre><code>import flowfile as ff\n\n# Extract\nraw_data = ff.read_csv(\"sales.csv\")\n\n# Transform\ntransformed = (\n    raw_data\n    .filter(ff.col(\"amount\") &gt; 0)\n    .with_columns([\n        ff.col(\"date\").str.strptime(ff.Date, \"%Y-%m-%d\")\n    ])\n    .group_by(\"region\")\n    .agg(ff.col(\"amount\").sum())\n)\n\n# Load\ntransformed.write_parquet(\"output.parquet\")\n</code></pre>"},{"location":"users/python-api/tutorials/index.html#data-validation","title":"Data Validation","text":"<pre><code># Check for data quality issues\ndf = ff.read_csv(\"input.csv\")\n\n# Find duplicates\nduplicates = df.group_by(\"id\").agg(\n    ff.count().alias(\"count\")\n).filter(ff.col(\"count\") &gt; 1)\n\n# Find nulls\nnull_counts = df.select([\n    ff.col(c).is_null().sum().alias(f\"{c}_nulls\")\n    for c in df.columns\n])\n</code></pre>"},{"location":"users/python-api/tutorials/index.html#resources","title":"Resources","text":"<ul> <li>API Reference - Complete method documentation</li> <li>Core Concepts - Understand the architecture</li> <li>Quick Start - Get running in 5 minutes</li> </ul> <p>Want more tutorials? Let us know what you'd like to see in our GitHub Discussions!</p>"},{"location":"users/python-api/tutorials/flowfile_frame_api.html","title":"Building Flows with code","text":"<p>The <code>flowfile_frame</code> module provides a powerful, Polars-like API that allows you to define and execute data transformation pipelines in Python while automatically generating a visual ETL graph. <sub><sup>[Source: readme.md]</sup></sub></p>"},{"location":"users/python-api/tutorials/flowfile_frame_api.html#overview","title":"Overview","text":"<p><code>flowfile_frame</code> is designed to bridge the gap between writing code and visual workflow design. It offers:</p> <ul> <li>A familiar API for those accustomed to Pandas or Polars.  </li> <li>Automatic generation of an ETL graph from your Python code.  </li> <li>The ability to visualize, save, and share your data pipelines in the Flowfile Designer UI.  </li> <li>The performance benefits of the Polars engine.  </li> </ul>"},{"location":"users/python-api/tutorials/flowfile_frame_api.html#installation","title":"Installation","text":"<p>The <code>flowfile_frame</code> module is included with the standard <code>flowfile</code> package.</p> <pre><code>pip install flowfile\n</code></pre>"},{"location":"users/python-api/tutorials/flowfile_frame_api.html#quick-start","title":"Quick Start","text":"<p>You can create a data pipeline programmatically and see the results:</p> <pre><code>import flowfile as ff\nfrom flowfile import col, open_graph_in_editor\n\ndf = ff.from_dict({\n    \"id\": [1, 2, 2],\n    \"value\": [10, 20, 15]\n})\n\nresult = df.filter(col(\"value\") &gt; 12, description=\"filter value &gt; 12\").with_columns(\n    (col(\"value\") * 10).alias(\"scaled_value\"), description=\"get a scaled value\"\n).group_by(col(\"id\")).agg(col(\"value\").sum().alias(\"sum_value\"),\n                        col(\"value\").max().alias(\"max_value\"),\n                        col(\"value\").min().alias(\"min_value\"))\ndf = result.collect()  # provides a polars dataframe\nopen_graph_in_editor(result.flow_graph)\n</code></pre> Generated Flow in Flowfile UI <p></p>"},{"location":"users/python-api/tutorials/flowfile_frame_api.html#visualizing-your-pipeline","title":"Visualizing Your Pipeline","text":"<p>One of the most powerful features of <code>flowfile_frame</code> is its ability to convert your code into a visual graph that can be opened in the Flowfile UI.</p> <p>You can build more advanced pipelines with conditional logic, grouping, and aggregation \u2014 and then instantly visualize them.</p> <pre><code>import flowfile as ff\nfrom flowfile import open_graph_in_editor\n\n# Create a more complex data pipeline\ndf = ff.from_dict({\n    \"id\": [1, 2, 3, 4, 5],\n    \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\"],\n    \"value\": [100, 200, 150, 300, 250]\n})\n\naggregated_df = (\n    df\n    .filter(ff.col(\"value\") &gt; 120, description='Filter on value greater then 120')\n    .with_columns([\n        (ff.col(\"value\") * 1.1).alias(\"adjusted_value\"),\n        ff.when(ff.col(\"category\") == \"A\").then(ff.lit(\"Premium\"))\n          .when(ff.col(\"category\") == \"B\").then(ff.lit(\"Standard\"))\n          .otherwise(ff.lit(\"Basic\")).alias(\"tier\")\n    ], description='Calculate the thier')\n    .group_by(\"tier\")\n    .agg([\n        ff.col(\"adjusted_value\").sum().alias(\"total_value\"),\n        ff.col(\"id\").count().alias(\"count\")\n    ])\n)\n\n# This will launch the Flowfile Designer UI and render your pipeline\nopen_graph_in_editor(aggregated_df.flow_graph)\n</code></pre> Generated Flow in Flowfile UI <p></p> <p>When you run <code>open_graph_in_editor(...)</code>, the Flowfile Designer UI will open and display a visual graph of your pipeline. You can:</p> <ul> <li>Inspect each transformation node</li> <li>Continue modifying your logic visually</li> <li>Share or export your pipeline</li> </ul>"},{"location":"users/python-api/tutorials/flowfile_frame_api.html#benefits-summary","title":"Benefits Summary","text":"<p>By combining the declarative power of a Polars-like API with Flowfile\u2019s interactive designer, <code>flowfile_frame</code> gives you:</p> <ul> <li>Code-first development with automatic visualization</li> <li>Zero-config ETL graph generation</li> <li>Easy debugging and collaboration</li> </ul>"},{"location":"users/visual-editor/index.html","title":"Visual Editor Guide","text":"<p>Build powerful data pipelines without writing code using Flowfile's intuitive drag-and-drop interface.</p>"},{"location":"users/visual-editor/index.html#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Build flows visually - Drag, drop, and connect nodes</li> <li>Transform data - Filter, aggregate, join, and more</li> <li>Connect to data sources - Databases, files, and cloud storage</li> <li>Preview results - See data at each step</li> <li>Export to code - Generate Python code from your visual flows</li> </ul>"},{"location":"users/visual-editor/index.html#getting-started","title":"Getting Started","text":""},{"location":"users/visual-editor/index.html#your-first-flow","title":"Your First Flow","text":"<ol> <li>Create a new flow - Click \"Create\" in the toolbar</li> <li>Add an input node - Drag a \"Read Data\" node from the left panel</li> <li>Configure the node - Click it and set file path in the right panel</li> <li>Add transformations - Connect filter, sort, or other nodes</li> <li>Run the flow - Click \"Run\" and see your results</li> </ol>"},{"location":"users/visual-editor/index.html#interface-overview","title":"Interface Overview","text":"<ul> <li>Left Panel: Node library organized by category</li> <li>Center Canvas: Build your flow here</li> <li>Right Panel: Configure selected nodes</li> <li>Bottom Panel: Preview data and logs</li> </ul>"},{"location":"users/visual-editor/index.html#core-concepts","title":"Core Concepts","text":""},{"location":"users/visual-editor/index.html#nodes","title":"Nodes","text":"<p>Each node represents a data operation:</p> <ul> <li>Input nodes - Load data from files, databases, APIs</li> <li>Transform nodes - Modify and clean your data</li> <li>Combine nodes - Join and merge datasets</li> <li>Aggregate nodes - Summarize and group data</li> <li>Output nodes - Save or export results</li> </ul>"},{"location":"users/visual-editor/index.html#connections","title":"Connections","text":"<p>Draw lines between nodes to define data flow. Data moves from top to bottom, left to right.</p>"},{"location":"users/visual-editor/index.html#execution","title":"Execution","text":"<ul> <li>Development mode - See data at every step (great for debugging)</li> <li>Performance mode - Optimized execution for large datasets</li> </ul>"},{"location":"users/visual-editor/index.html#learn-more","title":"Learn More","text":""},{"location":"users/visual-editor/index.html#more-resources","title":"More Resources","text":"<ul> <li>Building Flows - Detailed workflow guide</li> <li>Node Reference - Complete documentation of all nodes</li> </ul>"},{"location":"users/visual-editor/index.html#tutorials","title":"Tutorials","text":"<ul> <li>Connect to Databases - PostgreSQL, MySQL, and more</li> <li>Cloud Storage Setup - Work with S3 data</li> <li>Export to Python - Convert visual flows to code</li> </ul>"},{"location":"users/visual-editor/index.html#tips-for-success","title":"Tips for Success","text":"<ol> <li>Start simple - Build basic flows before adding complexity</li> <li>Use descriptions - Document nodes for your future self</li> <li>Preview often - Check data at each transformation</li> <li>Save regularly - Flows are saved as <code>.flowfile</code> files</li> <li>Try both modes - Development for testing, Performance for production</li> </ol>"},{"location":"users/visual-editor/index.html#visual-vs-code","title":"Visual vs Code","text":"<p>Wondering when to use visual vs Python? Here's a quick guide:</p> <p>Use Visual Editor when:</p> <ul> <li>Exploring new datasets</li> <li>Building one-off analyses</li> <li>Collaborating with non-technical users</li> <li>Creating documented workflows</li> <li>Learning data transformations</li> </ul> <p>Consider Python API when:</p> <ul> <li>Integrating with existing code</li> <li>Building programmatic pipelines</li> <li>Need version control</li> <li>Require advanced custom logic</li> <li>Automating workflows</li> </ul> <p>Remember, you can always switch between them!</p> <p>Ready to build? Start with Building Flows or explore the Node Reference.</p>"},{"location":"users/visual-editor/building-flows.html","title":"Building Flows","text":"<p>Flowfile allows you to create data pipelines visually by connecting nodes that represent different data operations. This guide will walk you through the process of creating and running flows.</p> <p>Looking for a quickstart overview?</p> <p>Check out our Quick Start Guide to get up and running in minutes.</p>"},{"location":"users/visual-editor/building-flows.html#interface-overview","title":"Interface Overview","text":"<p>The complete Flowfile interface showing:</p> <ul> <li>Left sidebar: Browse and select from available nodes </li> <li>Center canvas: Build your flow by arranging and connecting nodes</li> <li>Right sidebar: Configure node settings and parameters</li> <li>Bottom panel: Preview data at each step</li> </ul>"},{"location":"users/visual-editor/building-flows.html#creating-a-flow","title":"Creating a Flow","text":"<p> The Flowfile landing page when no flows are active, showing options to create a new flow or open an existing one </p>"},{"location":"users/visual-editor/building-flows.html#starting-a-new-flow","title":"Starting a New Flow","text":"<ol> <li>Click the Create button in the top toolbar</li> <li>A new empty canvas will open</li> <li>Save your flow at any time using the Save button</li> <li>Flows are saved as <code>.yaml</code> or <code>.json</code> files (human-readable formats)</li> </ol>"},{"location":"users/visual-editor/building-flows.html#adding-nodes","title":"Adding Nodes","text":"<ol> <li>Browse nodes in the left sidebar, organized by category:<ul> <li>Input Sources (for loading data)</li> <li>Transformations (for modifying data)</li> <li>Combine Operations (for joining data)</li> <li>Aggregations (for summarizing data)</li> <li>Output Destinations (for saving data)</li> </ul> </li> <li>Drag any node onto the canvas</li> <li>Connect nodes to create a flow</li> </ol>"},{"location":"users/visual-editor/building-flows.html#configuring-nodes","title":"Configuring Nodes","text":""},{"location":"users/visual-editor/building-flows.html#node-settings","title":"Node Settings","text":"<p>Click any node on the canvas to open its settings in the right sidebar. Each node type has unique configuration options tailored to its function.</p> <p>For example, the \"Formula\" node shown here includes sections for:</p> <ul> <li>\ud83c\udf9b\ufe0f General: Add a custom description via general settings</li> <li>\u2699\ufe0f Performance tweaking: Define if the data needs to be cached for better performance via general settings</li> <li>\u2194\ufe0f Transformations: Define the formula to be applied on the incoming data </li> </ul> <p>The settings panel for a \"Formula\" node.</p>"},{"location":"users/visual-editor/building-flows.html#data-preview","title":"Data Preview","text":"<ol> <li>After configuration, each node shows the output schema of the action</li> <li>Click on the run button to execute the node</li> <li>The preview panel will show the output data</li> </ol>"},{"location":"users/visual-editor/building-flows.html#running-your-flow","title":"Running Your Flow","text":""},{"location":"users/visual-editor/building-flows.html#1-execution-options","title":"1. Execution Options","text":"<p>Choose your execution mode from the settings panel:</p> <ul> <li>Development: Lets you view the data in every step of the process, at the cost of performance</li> <li>Performance: Only executes steps needed for the output (e.g., writing data), allowing for query optimizations and better performance</li> </ul>"},{"location":"users/visual-editor/building-flows.html#2-running-the-flow","title":"2. Running the Flow","text":"<ol> <li>Click the Run button in the top toolbar</li> <li>Watch the execution progress:<ul> <li>\ud83d\udfe2 Green: Success</li> <li>\ud83d\udd34 Red: Error</li> <li>\ud83d\udfe1 Yellow: Warning</li> <li>\u26aa White: Not executed</li> </ul> </li> </ol>"},{"location":"users/visual-editor/building-flows.html#3-viewing-results","title":"3. Viewing Results","text":"<ol> <li>Click any node after execution to see its output data</li> <li>Review the results in the preview panel</li> <li>Check for any errors or warnings</li> <li>Export results using output nodes</li> </ol>"},{"location":"users/visual-editor/building-flows.html#saving-and-loading-flows","title":"Saving and Loading Flows","text":"<p>Flowfile saves your pipelines as human-readable YAML or JSON files, making them easy to version control, share, and collaborate on.</p>"},{"location":"users/visual-editor/building-flows.html#supported-formats","title":"Supported Formats","text":"Format Extension Best For YAML <code>.yaml</code>, <code>.yml</code> Human readability, version control JSON <code>.json</code> Programmatic access, API integration"},{"location":"users/visual-editor/building-flows.html#saving-a-flow","title":"Saving a Flow","text":"<ol> <li>Click the Save button in the toolbar</li> <li>Choose a filename with <code>.yaml</code> or <code>.json</code> extension</li> <li>Your flow is saved with all nodes, connections, and settings</li> </ol>"},{"location":"users/visual-editor/building-flows.html#what-gets-saved","title":"What Gets Saved","text":"<ul> <li>Flow settings: Name, description, execution mode</li> <li>All nodes: Type, position, and configuration</li> <li>Connections: How nodes are linked together</li> <li>Node settings: All parameters and options</li> </ul>"},{"location":"users/visual-editor/building-flows.html#loading-a-flow","title":"Loading a Flow","text":"<ol> <li>Click Open in the toolbar</li> <li>Select a <code>.yaml</code>, <code>.json</code>, or legacy <code>.flowfile</code></li> <li>The flow is fully restored and ready to run</li> </ol>"},{"location":"users/visual-editor/building-flows.html#version-control","title":"Version Control","text":"<p>YAML files work great with Git:</p> <ul> <li>Track changes to your pipelines over time</li> <li>Review diffs to see what changed</li> <li>Collaborate with team members</li> <li>Roll back to previous versions</li> </ul> <p>Migrating from Legacy Format</p> <p>If you have flows saved in the old <code>.flowfile</code> format, simply open them and save as <code>.yaml</code> to convert.</p>"},{"location":"users/visual-editor/building-flows.html#example-flow","title":"Example Flow","text":"<p>Here's a typical flow that demonstrates common operations: </p>"},{"location":"users/visual-editor/building-flows.html#best-practices","title":"Best Practices","text":""},{"location":"users/visual-editor/building-flows.html#organization","title":"Organization","text":"<ul> <li>Give your flows clear, descriptive names</li> <li>Arrange nodes logically from left to right</li> <li>Group related operations together</li> <li>Use comments or node labels for documentation</li> </ul>"},{"location":"users/visual-editor/building-flows.html#development","title":"Development","text":"<ul> <li>Save your work frequently</li> <li>Test with a subset of data first</li> <li>Use the auto-run mode during development</li> <li>Break complex flows into smaller, manageable parts</li> </ul>"},{"location":"users/visual-editor/building-flows.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Check node configurations if errors occur</li> <li>Review data previews to understand transformations</li> <li>Ensure data types match between connected nodes</li> <li>Look for error messages in node status</li> </ul>"},{"location":"users/visual-editor/building-flows.html#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li> <p>Node Management:</p> <ul> <li>Double-click canvas to pan</li> <li>Use mouse wheel to zoom</li> <li>Hold Shift to select multiple nodes</li> <li>Right-click for context menu</li> <li>Right-click on the text to add notes</li> </ul> </li> <li> <p>Data Handling:</p> <ul> <li>Use sample nodes during development</li> <li>Preview data frequently</li> <li>Check column types early with select nodes</li> </ul> </li> </ul>"},{"location":"users/visual-editor/building-flows.html#want-to-see-another-example","title":"Want to see another example?","text":"<p>Checkout the quickstart guide!</p>"},{"location":"users/visual-editor/building-flows.html#next-steps","title":"Next Steps","text":"<p>After mastering basic flows, explore:</p> <ul> <li>Input sources</li> <li>Complex transformations</li> <li>Data aggregation techniques</li> <li>Advanced joining methods</li> <li>Output options</li> </ul>"},{"location":"users/visual-editor/kernels.html","title":"Kernel Execution","text":"<p>Run custom Python code in isolated Docker containers with full access to your flow's data.</p> <p>Beta Feature</p> <p>Kernel execution is currently in beta. The core functionality is working, but some features are still under active development and optimization. See Known Limitations for details.</p> <p>Kernels provide a sandboxed execution environment for Python Script nodes. Each kernel runs inside its own Docker container with configurable resources (CPU, memory, GPU), persistent namespaces across executions, and access to the <code>flowfile</code> API for reading inputs, writing outputs, and managing artifacts.</p>"},{"location":"users/visual-editor/kernels.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker must be installed and running on the host machine</li> <li>The flowfile-kernel Docker image must be built:</li> </ul> <pre><code>docker compose build flowfile-kernel\n</code></pre> <p>Desktop App</p> <p>When running Flowfile as a desktop application, Docker must be available on your local machine. Verify with <code>docker info</code>.</p>"},{"location":"users/visual-editor/kernels.html#kernel-manager","title":"Kernel Manager","text":"<p>The Kernel Manager is the central dashboard for creating, starting, stopping, and monitoring kernels. Open it from the sidebar menu.  The Kernel Manager showing configured kernels with status, resource usage, and actions</p> <p>When Docker is not running or the kernel image has not been built, a status banner appears at the top of the page with instructions on how to resolve the issue.</p> <p> Warning banner shown when Docker is unavailable or the kernel image is missing</p>"},{"location":"users/visual-editor/kernels.html#creating-a-kernel","title":"Creating a Kernel","text":"<ol> <li>In the Kernel Manager, click Create Kernel to expand the creation form</li> <li>Fill in the configuration fields:</li> </ol> <p> The kernel creation form with resource configuration options</p> Setting Description Default Kernel ID Unique identifier (alphanumeric) \u2014 Name A human-readable display label \u2014 Packages Comma-separated pip packages to install at startup (none) Memory (GB) Maximum memory the container can use (0.5\u201364 GB) <code>2</code> CPU Cores Number of CPU cores allocated (0.5\u201332) <code>2</code> GPU Enable GPU passthrough (requires NVIDIA Docker) <code>false</code> <ol> <li>Click Create Kernel to save the configuration</li> <li>Click Start on the kernel card to launch the container</li> </ol>"},{"location":"users/visual-editor/kernels.html#kernel-cards","title":"Kernel Cards","text":"<p>Each kernel is displayed as a card showing its current state, resource allocation, and live memory usage.</p> <p> A kernel card showing status badge, CPU/memory allocation, installed packages, and memory usage bar</p> <p>The status badge indicates the kernel's current state:</p> Status Badge Meaning Stopped Gray Container is not running Starting Blue (animated) Container is initializing Ready Green Idle and ready for execution Executing Orange (animated) Currently running code Error Red Failed \u2014 check error message on the card <p>The memory usage bar shows real-time consumption, color-coded green (normal), orange (warning, &gt;80%), or red (critical, &gt;95%).</p>"},{"location":"users/visual-editor/kernels.html#python-script-node","title":"Python Script Node","text":"<p>Add a Python Script node to your flow to write and execute Python code in a kernel.</p>"},{"location":"users/visual-editor/kernels.html#selecting-a-kernel","title":"Selecting a Kernel","text":"<p>In the node settings panel, the kernel dropdown shows all available kernels with their current state.</p> <p> Kernel dropdown in the Python Script node settings, showing available kernels and their state</p> <p>Kernel Required</p> <p>A running kernel is required to execute Python code. If no kernel is selected or the selected kernel is stopped, a warning message appears with instructions.</p>"},{"location":"users/visual-editor/kernels.html#notebook-editor","title":"Notebook Editor","text":"<p>The code editor uses a Jupyter-style notebook interface with multiple cells. Each cell can be executed independently.</p> <p> The notebook editor showing multiple code cells with execution counters, a toolbar, and output</p> <p>Toolbar actions:</p> Button Description Run All Execute all cells in order Clear Erase all cell outputs Restart Clear all kernel variables for this flow <p>Cell actions (visible on hover):</p> Action Shortcut Description Run cell <code>Shift+Enter</code> Execute the cell Run and advance <code>Cmd/Ctrl+Enter</code> Execute and move to next cell Move up/down \u2014 Reorder cells Delete \u2014 Remove the cell"},{"location":"users/visual-editor/kernels.html#cell-output","title":"Cell Output","text":"<p>After executing a cell, the output area shows results, stdout, and any errors.</p> <p> Cell output showing a rendered matplotlib chart, execution time, and stdout</p> <p>Output types rendered:</p> <ul> <li>Charts \u2014 matplotlib and plotly figures rendered inline</li> <li>Images \u2014 PIL images displayed as PNG</li> <li>HTML \u2014 rendered in a sandboxed iframe</li> <li>Text \u2014 plain text from <code>print()</code> statements or <code>flowfile.display()</code></li> <li>Errors \u2014 tracebacks displayed in a red block</li> </ul>"},{"location":"users/visual-editor/kernels.html#expanded-editor","title":"Expanded Editor","text":"<p>Click Expand Editor to open a fullscreen code editing view. The expanded editor shows the kernel status and memory usage in the header bar.</p>"},{"location":"users/visual-editor/kernels.html#artifacts-panel","title":"Artifacts Panel","text":"<p>The node settings panel shows artifacts available from upstream nodes and artifacts published by the current node.</p> <p> Artifacts panel showing available upstream artifacts and published artifacts for the current node</p>"},{"location":"users/visual-editor/kernels.html#api-reference","title":"API Reference","text":"<p>Click the ? button in the code editor header to open the built-in API reference.</p>"},{"location":"users/visual-editor/kernels.html#writing-code","title":"Writing Code","text":"<p>Inside a Python Script node connected to a kernel, you write standard Python code. The <code>flowfile</code> module is available automatically \u2014 no imports needed.</p>"},{"location":"users/visual-editor/kernels.html#reading-input-data","title":"Reading Input Data","text":"<pre><code># Read the main input as a Polars LazyFrame\ndf = flowfile.read_input()\n\n# Read a named input (when multiple inputs are connected)\norders = flowfile.read_input(\"orders\")\ncustomers = flowfile.read_input(\"customers\")\n\n# Read all inputs at once\nall_inputs = flowfile.read_inputs()\n# Returns: {\"main\": [LazyFrame, ...], \"orders\": [LazyFrame, ...]}\n</code></pre>"},{"location":"users/visual-editor/kernels.html#writing-output-data","title":"Writing Output Data","text":"<pre><code># Process data and publish the result\nresult = df.filter(pl.col(\"amount\") &gt; 100).select(\"id\", \"amount\", \"date\")\nflowfile.publish_output(result)\n\n# Publish a named output\nflowfile.publish_output(summary, name=\"summary\")\n</code></pre> <p>Both <code>pl.LazyFrame</code> and <code>pl.DataFrame</code> are accepted by <code>publish_output</code>.</p>"},{"location":"users/visual-editor/kernels.html#displaying-results","title":"Displaying Results","text":"<p>Use <code>flowfile.display()</code> to render rich output in the node's output panel:</p> <pre><code># Display a matplotlib chart\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.bar([\"A\", \"B\", \"C\"], [10, 20, 15])\nax.set_title(\"Sales by Category\")\nflowfile.display(fig, title=\"Sales Chart\")\n</code></pre> <p>Supported display types:</p> Object Type Rendering <code>matplotlib.figure.Figure</code> PNG image <code>plotly.graph_objects.Figure</code> Interactive HTML <code>PIL.Image.Image</code> PNG image HTML string (e.g. <code>\"&lt;b&gt;hello&lt;/b&gt;\"</code>) Rendered HTML Any other object Plain text via <code>str()</code> <p>Interactive mode</p> <p>In cell-execution mode, the last expression in your code is automatically displayed \u2014 similar to Jupyter notebooks.</p>"},{"location":"users/visual-editor/kernels.html#logging","title":"Logging","text":"<p>Send real-time log messages to the flow viewer:</p> <pre><code>flowfile.log(\"Processing started\")\nflowfile.log_info(\"Loaded 1,234 rows\")\nflowfile.log_warning(\"Column 'price' has 5 null values\")\nflowfile.log_error(\"Failed to parse date column\")\n</code></pre>"},{"location":"users/visual-editor/kernels.html#artifacts","title":"Artifacts","text":"<p>Artifacts let you persist Python objects (models, arrays, DataFrames) across executions within the same flow. They are scoped to the flow that created them.</p>"},{"location":"users/visual-editor/kernels.html#local-artifacts-flow-scoped","title":"Local Artifacts (Flow-scoped)","text":"<pre><code># Save a trained model\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier().fit(X_train, y_train)\nflowfile.publish_artifact(\"rf_model\", model)\n\n# In a later execution or different node in the same flow:\nmodel = flowfile.read_artifact(\"rf_model\")\npredictions = model.predict(X_test)\n\n# List all artifacts in this flow\nartifacts = flowfile.list_artifacts()\nfor a in artifacts:\n    print(f\"{a.name} (node {a.node_id})\")\n\n# Delete an artifact\nflowfile.delete_artifact(\"rf_model\")\n</code></pre> <p>Artifacts are automatically serialized using the best format for the object type:</p> Object Type Format Polars / Pandas DataFrame Parquet scikit-learn, NumPy, XGBoost, LightGBM Joblib Everything else Cloudpickle"},{"location":"users/visual-editor/kernels.html#global-artifacts-catalog","title":"Global Artifacts (Catalog)","text":"<p>Global artifacts are stored in the Flowfile catalog and persist beyond the current flow. They can be retrieved from any flow or session.</p> <pre><code># Publish to the global catalog\nartifact_id = flowfile.publish_global(\n    \"sales_model_v2\",\n    model,\n    description=\"Random Forest trained on Q4 data\",\n    tags=[\"ml\", \"classification\"],\n)\n\n# Retrieve from the global catalog\nmodel = flowfile.get_global(\"sales_model_v2\")\n\n# Get a specific version\nmodel_v1 = flowfile.get_global(\"sales_model_v2\", version=1)\n\n# List all global artifacts\nartifacts = flowfile.list_global_artifacts(tags=[\"ml\"])\nfor a in artifacts:\n    print(f\"{a.name} v{a.version} \u2014 {a.python_type}\")\n\n# Delete a global artifact\nflowfile.delete_global_artifact(\"sales_model_v2\")\n</code></pre> <p>Registered Flows Required</p> <p><code>publish_global</code> requires the flow to be registered in the catalog. It is not available in interactive (cell) mode.</p>"},{"location":"users/visual-editor/kernels.html#persistence-recovery","title":"Persistence &amp; Recovery","text":"<p>When persistence is enabled, local artifacts are automatically saved to disk inside the container. If the kernel restarts, artifacts are recovered based on the configured recovery mode:</p> Recovery Mode Behavior Lazy (default) Artifacts are indexed on disk but loaded into memory only when accessed Eager All artifacts are loaded into memory immediately on startup Clear All persisted artifacts are deleted on startup"},{"location":"users/visual-editor/kernels.html#shared-files","title":"Shared Files","text":"<p>Use <code>flowfile.get_shared_location()</code> to write files that are accessible across all Flowfile services and survive container restarts:</p> <pre><code># Write a CSV to the shared directory\noutput_path = flowfile.get_shared_location(\"reports/monthly.csv\")\ndf.collect().write_csv(output_path)\n\n# The file is now accessible from other nodes and services\n</code></pre>"},{"location":"users/visual-editor/kernels.html#cancelling-execution","title":"Cancelling Execution","text":"<p>To interrupt a long-running code execution:</p> <ol> <li>Click the Stop button in the node's execution panel</li> <li>The kernel receives a <code>KeyboardInterrupt</code> signal</li> <li>Execution halts and control returns to the idle state</li> </ol> <p>This works the same way as pressing <code>Ctrl+C</code> in a Python terminal.</p>"},{"location":"users/visual-editor/kernels.html#resource-monitoring","title":"Resource Monitoring","text":"<p>The kernel tracks memory usage via the container's cgroup filesystem. You can view current memory consumption both on the kernel card in the Kernel Manager and in the Python Script node header.</p> <p>If a kernel exceeds its memory limit, Docker terminates the container and Flowfile reports an out-of-memory error.</p>"},{"location":"users/visual-editor/kernels.html#using-kernels-in-the-node-designer","title":"Using Kernels in the Node Designer","text":"<p>Custom nodes built with the Node Designer can also run on kernels. This lets you create reusable nodes that depend on third-party libraries (e.g. scikit-learn, XGBoost) or that need artifact support.</p>"},{"location":"users/visual-editor/kernels.html#enabling-kernel-mode","title":"Enabling Kernel Mode","text":"<p>In the Node Designer, check Require Kernel Execution in the metadata section. This reveals a kernel selector and output name configuration.</p> <p>The Node Designer with kernel execution enabled, showing the kernel dropdown and output names</p> <p>When a user drops your kernel-enabled custom node into a flow, the node settings panel shows a kernel dropdown so they can choose which kernel runs it.</p> <p>A kernel-enabled custom node in a flow with the kernel selector visible</p>"},{"location":"users/visual-editor/kernels.html#what-changes","title":"What Changes","text":"<p>Your <code>process</code> method code stays the same \u2014 the <code>self.settings_schema</code> access pattern works identically. Behind the scenes, the Node Designer generates a self-contained kernel script that:</p> <ol> <li>Creates proxy classes replicating <code>self.settings_schema.section.component.value</code></li> <li>Reads inputs via <code>flowfile.read_input()</code></li> <li>Runs your process method body</li> <li>Publishes outputs via <code>flowfile.publish_output()</code> for each named output</li> </ol> <p>The full <code>flowfile</code> API (artifacts, display, logging) is available inside kernel-enabled custom nodes.</p> <p>For details on building custom nodes, see Node Designer.</p>"},{"location":"users/visual-editor/kernels.html#known-limitations","title":"Known Limitations","text":"<p>Kernel execution is in beta. The following limitations are known and being worked on:</p> <ul> <li>Named reads from inputs \u2014 Currently all inputs are named <code>main</code> and provided as a list of DataFrames. Ideally inputs would be named based on their source connections, allowing you to read them by name (e.g. <code>flowfile.read_input(\"orders\")</code>). This is not yet implemented.</li> <li>Flow-to-code export \u2014 Python Script nodes that use kernel execution are not yet included in the Export to Python code generator. Kernel nodes will be skipped in the generated code.</li> <li>Artifact state visibility \u2014 There is currently no UI to browse or inspect the contents of stored artifacts. You can list artifacts via <code>flowfile.list_artifacts()</code> in code, but there is no visual artifact explorer yet.</li> <li>Python package versioning \u2014 Packages specified during kernel creation are installed via <code>pip install</code> at container startup without version pinning. There is no lock file or reproducible environment mechanism yet. To pin versions, specify them explicitly in the packages field (e.g. <code>scikit-learn==1.4.0, pandas==2.1.0</code>).</li> </ul>"},{"location":"users/visual-editor/kernels.html#flowfile-api-reference","title":"<code>flowfile</code> API Reference","text":"<p>The following functions are available inside kernel code via the <code>flowfile</code> module:</p>"},{"location":"users/visual-editor/kernels.html#data-io","title":"Data I/O","text":"Function Description <code>read_input(name=\"main\")</code> Read input data as a <code>pl.LazyFrame</code> <code>read_first(name=\"main\")</code> Read only the first input file <code>read_inputs()</code> Read all named inputs as <code>dict[str, list[LazyFrame]]</code> <code>publish_output(df, name=\"main\")</code> Write a DataFrame/LazyFrame as output"},{"location":"users/visual-editor/kernels.html#local-artifacts","title":"Local Artifacts","text":"Function Description <code>publish_artifact(name, obj)</code> Store a Python object in the flow's artifact store <code>read_artifact(name)</code> Retrieve a stored artifact <code>delete_artifact(name)</code> Remove an artifact <code>list_artifacts()</code> List all artifacts in the current flow"},{"location":"users/visual-editor/kernels.html#global-artifacts","title":"Global Artifacts","text":"Function Description <code>publish_global(name, obj, ...)</code> Persist an object to the global catalog <code>get_global(name, version=None)</code> Retrieve from the global catalog <code>list_global_artifacts(...)</code> List available global artifacts <code>delete_global_artifact(name, ...)</code> Delete a global artifact"},{"location":"users/visual-editor/kernels.html#display-logging","title":"Display &amp; Logging","text":"Function Description <code>display(obj, title=\"\")</code> Render rich output (charts, images, HTML, text) <code>log(message, level=\"INFO\")</code> Send a log message to the flow viewer <code>log_info(message)</code> Shortcut for <code>log(message, \"INFO\")</code> <code>log_warning(message)</code> Shortcut for <code>log(message, \"WARNING\")</code> <code>log_error(message)</code> Shortcut for <code>log(message, \"ERROR\")</code>"},{"location":"users/visual-editor/kernels.html#utilities","title":"Utilities","text":"Function Description <code>get_shared_location(filename)</code> Get a path in the shared directory"},{"location":"users/visual-editor/kernels.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Node Designer \u2014 Create custom nodes with kernel support</li> <li>Building Flows \u2014 Using nodes in workflows</li> <li>Transform Nodes \u2014 Built-in transformation nodes</li> <li>Docker Deployment \u2014 Running Flowfile with Docker</li> <li>Kernel Architecture \u2014 Technical deep-dive for developers</li> </ul>"},{"location":"users/visual-editor/node-designer.html","title":"Node Designer","text":"<p>Create custom transformation nodes visually\u2014no Python files required.</p> <p>The Node Designer lets you build reusable nodes by dragging UI components onto a canvas, configuring their properties, and writing transformation logic. Your custom nodes appear in the node palette alongside built-in nodes.</p> <p> The Node Designer with a \"Prefixer\" node being created</p>"},{"location":"users/visual-editor/node-designer.html#quick-start","title":"Quick Start","text":"<ol> <li>Open Node Designer from the sidebar menu</li> <li>Set your node's name and category</li> <li>Drag components from the left panel into a section</li> <li>Write your transformation code in the Process Method editor</li> <li>Click Save to add your node to the palette</li> </ol> <p>Restart Required</p> <p>After saving a new node, refresh <code>cmd/cntr + r</code> Flowfile to load it into the node palette.</p>"},{"location":"users/visual-editor/node-designer.html#interface-overview","title":"Interface Overview","text":"<p>The Node Designer uses a three-panel layout:</p> <ul> <li> <p>Component Palette (Left)</p> <p>Draggable UI components for building your node's settings interface</p> </li> <li> <p>Design Canvas (Center)</p> <p>Visual preview of your node's configuration panel with metadata settings</p> </li> <li> <p>Properties Panel (Right)</p> <p>Configuration options for the currently selected component</p> </li> </ul>"},{"location":"users/visual-editor/node-designer.html#design-canvas","title":"Design Canvas","text":"<p>The center panel is where you define your node's identity and structure.</p>"},{"location":"users/visual-editor/node-designer.html#node-metadata","title":"Node Metadata","text":"Field Description Example Node Name Internal identifier (no spaces) <code>Prefixer</code> Category Where it will appears in the palette <code>Custom</code>, <code>Text</code>, <code>Transform</code> Title Display name shown on the node <code>Add prefixes to columns</code> Description Tooltip text explaining the node <code>Make columns easy to recognize...</code> Number of Inputs How many input connections <code>1</code> (most common) Number of Outputs How many output connections <code>1</code> (most common) Node Icon Visual identifier in the palette Select from icon library <p>Features Under Development</p> <p>Category and Number of Outputs are currently under development.  For now, custom nodes will appear in the default category and support single outputs.</p>"},{"location":"users/visual-editor/node-designer.html#ui-sections","title":"UI Sections","text":"<p>Sections group related components together. Each section:</p> <ul> <li>Has a Variable Name (used in code, e.g., <code>main_section</code>)</li> <li>Has a Display Title (shown in UI, e.g., \"Section 1\")</li> <li>Can contain multiple components</li> <li>Appears as a collapsible group in the node's settings panel</li> </ul> <p>Click + Add Section to create a new section, then drag components into it.</p> <p></p>"},{"location":"users/visual-editor/node-designer.html#component-palette","title":"Component Palette","text":"<p>Drag these components from the left panel into your sections:</p>"},{"location":"users/visual-editor/node-designer.html#input-components","title":"Input Components","text":"Component Use Case Value Type Text Input Names, patterns, custom strings <code>str</code> Numeric Input Thresholds, counts, percentages <code>int</code> or <code>float</code> Toggle Switch Enable/disable features <code>bool</code> Single Select Choose one option from a list <code>str</code> Slider Select a value within a range <code>int</code> or <code>float</code>"},{"location":"users/visual-editor/node-designer.html#column-components","title":"Column Components","text":"Component Use Case Value Type Column Selector Pick one column from input data <code>str</code> Multi Select Select multiple columns <code>list[str]</code> Column Action Column with operation choice <code>dict</code>"},{"location":"users/visual-editor/node-designer.html#special-components","title":"Special Components","text":"Component Use Case Value Type Secret Selector API keys, passwords, credentials <code>str</code> (SecretStr) <p>No Secret Usage Validation</p> <p>There is currently no scanning to verify that secrets are handled securely  in your process code. You are responsible for ensuring secrets are not  logged, exposed in error messages, or written to output data.</p>"},{"location":"users/visual-editor/node-designer.html#properties-panel","title":"Properties Panel","text":"<p>When you select a component on the canvas, configure it in the right panel.</p>"},{"location":"users/visual-editor/node-designer.html#basic-properties","title":"Basic Properties","text":"<p>All components share these basic properties:</p> Property Description Field Name Internal identifier used in code (e.g., <code>columns_to_change</code>) Label Display text shown to users (e.g., \"Columns To Prefix\") <p>Additional options vary depending on the selected UI component (e.g., min/max for Numeric Input, data type filters for Column Selector, options list for Single Select).</p> <p>Click Insert Variable to copy the accessor path for use in your process code.</p>"},{"location":"users/visual-editor/node-designer.html#process-method","title":"Process Method","text":"<p>The bottom section contains the code editor where you write your transformation logic.</p> <p></p>"},{"location":"users/visual-editor/node-designer.html#function-signature","title":"Function Signature","text":"<pre><code>def process(self, *inputs: pl.LazyFrame) -&gt; pl.LazyFrame:\n</code></pre> <p>Your function receives Polars LazyFrames and must return a LazyFrame.</p>"},{"location":"users/visual-editor/node-designer.html#accessing-component-values","title":"Accessing Component Values","text":"<p>Use <code>self.settings_schema</code> to access values from your UI components:</p> <pre><code># Pattern: self.settings_schema.&lt;section_name&gt;.&lt;component_name&gt;.value\n\n# Get text input value\nprefix_text: str = self.settings_schema.main_section.prefix_text.value\n\n# Get selected columns (list)\ncolumns_to_change: list[str] = self.settings_schema.main_section.columns_to_change.value\n\n# Get numeric value\nthreshold: float = self.settings_schema.options.threshold.value\n\n# Get toggle state\nis_enabled: bool = self.settings_schema.options.is_enabled.value\n</code></pre> <p>Use Insert Variable</p> <p>Click Insert Variable in the Properties panel to automatically insert the correct accessor path for any component.</p>"},{"location":"users/visual-editor/node-designer.html#complete-example","title":"Complete Example","text":"<p>Here's a full example that adds a prefix to selected column names:</p> <pre><code>def process(self, *inputs: pl.LazyFrame) -&gt; pl.LazyFrame:\n    # Get the first input LazyFrame\n    lf = inputs[0]\n    prefix_text: str = self.settings_schema.main_section.prefix_text.value\n    columns_to_change: list[str] = self.settings_schema.main_section.columns_to_change.value\n\n    # Build expressions: rename selected columns, keep others unchanged\n    exprs: list[pl.Expr] = [\n        pl.col(col_name).alias(f\"{prefix_text}_{col_name}\") \n        if col_name in columns_to_change else pl.col(col_name) \n        for col_name in lf.columns\n    ]\n    return lf.select(exprs)\n</code></pre>"},{"location":"users/visual-editor/node-designer.html#generated-code","title":"Generated Code","text":"<p>When you click Preview, the Node Designer shows the complete Python class that will be generated:</p> <pre><code># Auto-generated custom node\n# Generated by Node Designer\n\nimport polars as pl\nfrom flowfile_core.flowfile.node_designer import (\n    CustomNodeBase, Section, NodeSettings, TextInput, ColumnSelector\n)\n\n# Section 1\nmain_section = Section(\n    title=\"Section 1\",\n    prefix_text=TextInput(\n        label=\"PrefixText\",\n    ),\n    columns_to_change=ColumnSelector(\n        label=\"Columns To Prefix\",\n        required=True,\n        multiple=True,\n    ),\n)\n\nclass PrefixerSettings(NodeSettings):\n    main_section: Section = main_section\n\n\nclass Prefixer(CustomNodeBase):\n    node_name: str = \"Prefixer\"\n    node_category: str = \"Custom\"\n    node_icon: str = \"ruler-plus.svg\"\n    title: str = \"Add prefixes to columns\"\n    intro: str = \"Make columns easy to recognize by adding a prefix value to them\"\n    number_of_inputs: int = 1\n    number_of_outputs: int = 1\n    settings_schema: PrefixerSettings = PrefixerSettings()\n\n    def process(self, *inputs: pl.LazyFrame) -&gt; pl.LazyFrame:\n        # Get the first input LazyFrame\n        lf = inputs[0]\n        prefix_text: str = self.settings_schema.main_section.prefix_text.value\n        columns_to_change: list[str] = self.settings_schema.main_section.columns_to_change.value\n\n        exprs: list[pl.Expr] = [\n            pl.col(col_name).alias(f\"{prefix_text}_{col_name}\") \n            if col_name in columns_to_change else pl.col(col_name) \n            for col_name in lf.columns\n        ]\n        return lf.select(exprs)\n</code></pre> <p>This generated class is what gets saved to your user-defined nodes directory.</p>"},{"location":"users/visual-editor/node-designer.html#kernel-execution","title":"Kernel Execution","text":"<p>Custom nodes can run their process method inside a Docker-based kernel instead of the local Polars engine. This is useful when your node needs third-party libraries (e.g. scikit-learn, XGBoost), requires more isolation, or produces artifacts.</p>"},{"location":"users/visual-editor/node-designer.html#enabling-kernel-mode","title":"Enabling Kernel Mode","text":"<ol> <li>In the Node Metadata section, check Require Kernel Execution</li> </ol> <p> Enabling kernel execution in the Node Designer metadata section</p> <ol> <li>A new Execution section appears below the metadata. Select a kernel from the dropdown.</li> </ol> <p> The kernel execution section showing the kernel dropdown and output name configuration</p> <ol> <li>Configure Output Names \u2014 when a kernel is selected, you can define multiple named outputs (e.g. <code>main</code>, <code>predictions</code>, <code>metrics</code>). Each output name maps to a separate output handle on the node.</li> </ol> <p>Create kernels first</p> <p>Kernels must be created and started in the Kernel Manager before they appear in the dropdown. If no kernels are available, the dropdown only shows \"Local (default)\".</p>"},{"location":"users/visual-editor/node-designer.html#how-it-works","title":"How It Works","text":"<p>When kernel execution is enabled:</p> <ul> <li>The Node Designer auto-generates a kernel script from your <code>process</code> method</li> <li>Your <code>self.settings_schema</code> values are baked into the script as a lightweight proxy</li> <li>Inputs are read via <code>flowfile.read_input()</code> instead of being passed as LazyFrame arguments</li> <li>Return values are published via <code>flowfile.publish_output()</code> for each named output</li> <li>The full <code>flowfile</code> API is available \u2014 artifacts, display, logging, and more</li> </ul> <p>Your process method code stays the same. The <code>self.settings_schema</code> pattern works identically in kernel mode \u2014 the generated script creates proxy classes that replicate the same access pattern.</p>"},{"location":"users/visual-editor/node-designer.html#custom-node-with-kernel-using-it-in-a-flow","title":"Custom Node with Kernel \u2014 Using It in a Flow","text":"<p>When you use a kernel-enabled custom node in a flow, the node settings panel shows a Kernel dropdown to select (or change) which kernel runs the node.</p> <p> A kernel-enabled custom node in a flow, showing the kernel selector in the node settings panel</p>"},{"location":"users/visual-editor/node-designer.html#example-ml-scoring-node","title":"Example: ML Scoring Node","text":"<p>Here's a process method that trains a model and publishes it as an artifact:</p> <pre><code>def process(self, *inputs: pl.LazyFrame) -&gt; pl.LazyFrame:\n    import flowfile\n    from sklearn.ensemble import RandomForestClassifier\n\n    lf = inputs[0]\n    target_col: str = self.settings_schema.main_section.target_column.value\n\n    df = lf.collect()\n    X = df.drop(target_col).to_numpy()\n    y = df[target_col].to_numpy()\n\n    model = RandomForestClassifier(n_estimators=100).fit(X, y)\n    flowfile.publish_artifact(\"trained_model\", model)\n    flowfile.log_info(f\"Model trained with accuracy: {model.score(X, y):.3f}\")\n\n    predictions = model.predict(X)\n    return df.with_columns(pl.Series(\"prediction\", predictions)).lazy()\n</code></pre> <p>For more details on the <code>flowfile</code> API available inside kernels, see Kernel Execution.</p>"},{"location":"users/visual-editor/node-designer.html#toolbar-actions","title":"Toolbar Actions","text":"Button Shortcut Description Help \u2014 Open documentation modal Browse \u2014 Load an existing node definition New \u2014 Create a blank node definition Preview \u2014 View the generated Python code Save \u2014 Save the node to your user-defined nodes directory"},{"location":"users/visual-editor/node-designer.html#programmatic-alternative","title":"Programmatic Alternative","text":"<p>For more control or version-controlled node definitions, you can create nodes as Python files. See Creating Custom Nodes for the code-based approach.</p>"},{"location":"users/visual-editor/node-designer.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Kernel Execution \u2014 Run code in isolated Docker containers</li> <li>Building Flows \u2014 Using nodes in workflows</li> <li>Transform Nodes \u2014 Built-in transformation nodes</li> <li>Creating Custom Nodes \u2014 Python-based node creation</li> </ul>"},{"location":"users/visual-editor/settings.html","title":"Settings","text":"<p>Configure Flowfile's appearance and behavior.</p>"},{"location":"users/visual-editor/settings.html#theme","title":"Theme","text":"<p>Click the theme icon in the top navigation bar to switch between:</p> Mode Description Light White backgrounds Dark Reduced brightness for low-light System Follows OS preference <p>Your preference persists across sessions.</p>"},{"location":"users/visual-editor/settings.html#user-management","title":"User Management","text":"<p>Available in Docker mode only.</p> <p>Manage team access through the Admin panel. Click your username \u2192 Admin.</p> <p></p>"},{"location":"users/visual-editor/settings.html#creating-users","title":"Creating Users","text":"<ol> <li>Click Add User</li> <li>Enter username, email, full name</li> <li>Set temporary password</li> <li>Optionally grant admin privileges</li> <li>Click Create</li> </ol> <p>New users must change their password on first login.</p>"},{"location":"users/visual-editor/settings.html#password-requirements","title":"Password Requirements","text":"<ul> <li>Minimum 8 characters</li> <li>At least one uppercase letter</li> <li>At least one lowercase letter</li> <li>At least one number</li> </ul>"},{"location":"users/visual-editor/settings.html#admin-privileges","title":"Admin Privileges","text":"<p>Admins can:</p> <ul> <li>Create, modify, and delete users</li> <li>View all users in the system</li> <li>Grant/revoke admin status</li> </ul>"},{"location":"users/visual-editor/settings.html#deleting-users","title":"Deleting Users","text":"<p>Deleting a user removes their:</p> <ul> <li>Account and login access</li> <li>Stored secrets</li> <li>Saved connections</li> </ul> <p>Flow definitions are preserved.</p>"},{"location":"users/visual-editor/settings.html#related","title":"Related","text":"<ul> <li>Secrets - Encrypted credential storage</li> <li>Docker - Docker deployment</li> </ul>"},{"location":"users/visual-editor/nodes/index.html","title":"Nodes Overview","text":"<p>Flowfile's nodes are the building blocks of your data pipeline. Each node performs a specific operation, allowing you to load, transform, combine, summarize, and output data without writing code.  </p> <p>By connecting nodes together, you can create flexible and scalable workflows to process data efficiently.  </p>"},{"location":"users/visual-editor/nodes/index.html#node-categories","title":"Node Categories","text":"<p>Flowfile nodes are grouped into five main categories:  </p> <ul> <li>Input Nodes \u2013 Load data from files, databases, or external sources.  </li> <li>Transform Nodes \u2013 Modify, clean, and reshape your data.  </li> <li>Combine Nodes \u2013 Merge, join, or match multiple datasets.  </li> <li>Aggregate Nodes \u2013 Summarize, group, and compute metrics.  </li> <li>Output Nodes \u2013 Save data to files or explore it interactively.  </li> </ul>"},{"location":"users/visual-editor/nodes/index.html#how-nodes-work","title":"How Nodes Work","text":"<p>Each node is designed to be intuitive and consistent, ensuring a smooth workflow experience:  </p> <p>\u2714 Clear inputs and outputs \u2013 Easily understand what data flows into and out of each node. \u2714 Visual feedback \u2013 Monitor the data transformation process. \u2714 Preview capability \u2013 Inspect results before applying changes. \u2714 Error handling \u2013 Validate and debug data issues efficiently.  </p> <p>Flowfile\u2019s node-based approach makes data processing fast, flexible, and code-free, helping you build powerful data pipelines with ease.</p>"},{"location":"users/visual-editor/nodes/aggregate.html","title":"Aggregate Nodes","text":"<p>Aggregate nodes help you summarize and analyze your data by grouping and calculating statistics. These nodes are essential for creating summaries and transforming data structure.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#node-details","title":"Node Details","text":""},{"location":"users/visual-editor/nodes/aggregate.html#group-by","title":"Group By","text":"<p>The Group By node aggregates data based on selected columns, allowing calculations such as sums, averages, counts, and more.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#key-features","title":"Key Features","text":"<ul> <li>Group by one or more columns  </li> <li>Apply aggregation functions to other columns  </li> <li>Rename output columns  </li> </ul>"},{"location":"users/visual-editor/nodes/aggregate.html#usage","title":"Usage","text":"<ol> <li>Select one or more columns to group by.  </li> <li>Choose aggregation functions for other columns.  </li> <li>Set custom output column names if needed.  </li> </ol>"},{"location":"users/visual-editor/nodes/aggregate.html#configuration-options","title":"Configuration Options","text":"Parameter Description Group By Columns Columns used to define groups. Aggregations Functions like <code>sum</code>, <code>count</code>, <code>avg</code>, <code>min</code>, <code>max</code>. Output Column Name Custom name for the aggregated result (optional). <p>This node is essential for summarizing datasets and preparing structured outputs.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#pivot-data","title":"Pivot Data","text":"<p>The Pivot Data node converts data from a long format to a wide format by creating new columns based on unique values in a pivot column.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#key-features_1","title":"Key Features","text":"<ul> <li>Transform long format into wide format  </li> <li>Select multiple index columns  </li> <li>Aggregate values during pivoting  </li> </ul>"},{"location":"users/visual-editor/nodes/aggregate.html#usage_1","title":"Usage","text":"<ol> <li>Select index columns to retain in the final output.  </li> <li>Choose a pivot column whose unique values will become new columns.  </li> <li>Select a value column containing the data to fill the new columns.  </li> <li>Apply aggregation functions (e.g., <code>sum</code>, <code>count</code>, <code>avg</code>) if needed.  </li> </ol>"},{"location":"users/visual-editor/nodes/aggregate.html#configuration-options_1","title":"Configuration Options","text":"Parameter Description Index Columns Columns that define the groups in the final table. Pivot Column Unique values from this column become new column names. Value Column The column containing values to be placed in the new columns. Aggregations Functions applied when multiple values exist per pivot column entry. <p>This node is useful for restructuring datasets into a summary-friendly format.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#unpivot-data","title":"Unpivot Data","text":"<p>The Unpivot Data node transforms data from wide format to long format, making it easier for analysis and reporting.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#key-features_2","title":"Key Features","text":"<ul> <li>Convert multiple columns into key-value pairs  </li> <li>Select index columns to retain  </li> <li>Use dynamic data type selection  </li> </ul>"},{"location":"users/visual-editor/nodes/aggregate.html#usage_2","title":"Usage","text":"<ol> <li>Select index columns to keep unchanged.  </li> <li>Choose value columns to transform into key-value pairs.  </li> <li>(Optional) Enable dynamic data type selection to filter columns automatically.  </li> </ol>"},{"location":"users/visual-editor/nodes/aggregate.html#configuration-options_2","title":"Configuration Options","text":"Parameter Description Index Columns Columns that remain unchanged in the final structure. Value Columns Columns that will be unpivoted into key-value pairs. Data Type Selector Automatically select columns based on data type (e.g., <code>string</code>). Selection Mode Choose between <code>column</code> or <code>data_type</code> for unpivot selection. <p>This node helps in restructuring datasets, especially when working with reporting or analytical tools.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#count-records","title":"Count Records","text":"<p>The Count Records node calculates the total number of rows in the dataset.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#key-features_3","title":"Key Features","text":"<ul> <li>Simple row count operation  </li> <li>No configuration required  </li> <li>Adds a new column <code>number_of_records</code> </li> </ul>"},{"location":"users/visual-editor/nodes/aggregate.html#usage_3","title":"Usage","text":"<ol> <li>Add the Count Records node to your workflow.  </li> <li>It will automatically count the total number of rows.  </li> </ol>"},{"location":"users/visual-editor/nodes/aggregate.html#configuration-options_3","title":"Configuration Options","text":"<p>This node has no additional settings\u2014it simply returns the record count.</p> <p>This transformation is useful for quick dataset validation and workflow monitoring.</p> <p>\u2190 Combine data | Next: Write data \u2192</p>"},{"location":"users/visual-editor/nodes/combine.html","title":"Combine Nodes","text":"<p>Combine nodes allow you to merge multiple datasets in different ways, enabling data integration and enrichment. These nodes help in aligning, linking, and structuring data from various sources to create a unified dataset.  </p> <p>Depending on the method used, datasets can be merged by matching values, stacking rows, finding similar records, generating all possible combinations, or grouping related elements in a network.  </p> <p>These transformations are essential for tasks like data preparation, consolidation, and relationship mapping across datasets.</p>"},{"location":"users/visual-editor/nodes/combine.html#node-details","title":"Node Details","text":""},{"location":"users/visual-editor/nodes/combine.html#join","title":"Join","text":"<p>The Join node merges two datasets based on matching values in selected columns.</p>"},{"location":"users/visual-editor/nodes/combine.html#key-features","title":"Key Features","text":"<ul> <li>Supports multiple join types: Inner, Left, Right, Outer </li> <li>Join on one or more columns </li> <li>Handles duplicate column names with automatic renaming  </li> </ul>"},{"location":"users/visual-editor/nodes/combine.html#usage","title":"Usage","text":"<ol> <li>Connect two input datasets (left and right).  </li> <li>Select join type (<code>inner</code>, <code>left</code>, <code>right</code>, <code>anti</code> or <code>outer</code>).  </li> <li>Choose columns to join on.  </li> <li>Select which columns to keep from each dataset.  </li> </ol>"},{"location":"users/visual-editor/nodes/combine.html#configuration-options","title":"Configuration Options","text":"Parameter Description Join Type Choose <code>inner</code>, <code>left</code>, <code>right</code>, <code>anti</code> or <code>outer</code> join. Join Columns Columns used to match records between datasets. <p>This node is useful for merging related datasets, such as combining customer data with orders or linking product details with inventory.</p>"},{"location":"users/visual-editor/nodes/combine.html#fuzzy-match","title":"Fuzzy Match","text":"<p>The Fuzzy Match node joins datasets based on similar values instead of exact matches, using various matching algorithms.</p>"},{"location":"users/visual-editor/nodes/combine.html#key-features_1","title":"Key Features","text":"<ul> <li>Supports fuzzy matching algorithms (e.g., Levenshtein)  </li> <li>Configurable similarity threshold </li> <li>Calculates match scores </li> <li>Joins datasets based on approximate values  </li> </ul>"},{"location":"users/visual-editor/nodes/combine.html#usage_1","title":"Usage","text":"<ol> <li>Connect two datasets (left and right).  </li> <li>Select columns to match on.  </li> <li>Choose a fuzzy matching algorithm.  </li> <li>Set a similarity threshold (e.g., 75%).  </li> </ol>"},{"location":"users/visual-editor/nodes/combine.html#configuration-options_1","title":"Configuration Options","text":"Parameter Description Join Columns Columns used for fuzzy matching. Fuzzy Algorithm Choose an algorithm (e.g., <code>Levenshtein</code>). Threshold Score Minimum similarity score for a match (0-100). <p>This node is useful for handling typos, name variations, and inconsistent formatting when merging datasets.</p>"},{"location":"users/visual-editor/nodes/combine.html#union-data","title":"Union Data","text":"<p>The Union Data node merges multiple datasets by stacking rows together.</p>"},{"location":"users/visual-editor/nodes/combine.html#key-features_2","title":"Key Features","text":"<ul> <li>Combines multiple datasets into one  </li> <li>Automatically aligns columns based on names  </li> <li>Uses diagonal relaxed mode, allowing flexible column matching  </li> </ul>"},{"location":"users/visual-editor/nodes/combine.html#usage_2","title":"Usage","text":"<ol> <li>Connect multiple input datasets.  </li> <li>The node will automatically align and stack the data.  </li> </ol> <p>This node is useful for combining similar datasets, such as monthly reports or regional data.</p>"},{"location":"users/visual-editor/nodes/combine.html#cross-join","title":"Cross Join","text":"<p>The Cross Join node creates all possible combinations between two datasets.</p>"},{"location":"users/visual-editor/nodes/combine.html#key-features_3","title":"Key Features","text":"<ul> <li>Generates a Cartesian product of two datasets  </li> <li>Automatically aligns columns  </li> <li>Handles duplicate column names  </li> </ul>"},{"location":"users/visual-editor/nodes/combine.html#usage_3","title":"Usage","text":"<ol> <li>Connect two datasets (left and right).</li> <li>Select the columns that you would like to keep and their output names</li> <li>The node will generate all possible row combinations.  </li> </ol> <p>This node is useful for creating test scenarios, generating all possible product combinations, or building comparison matrices.</p>"},{"location":"users/visual-editor/nodes/combine.html#graph-solver","title":"Graph Solver","text":"<p>The Graph Solver node groups related records based on connections in a graph-structured dataset.</p>"},{"location":"users/visual-editor/nodes/combine.html#key-features_4","title":"Key Features","text":"<ul> <li>Identifies connected components in graph-like data  </li> <li>Groups related nodes into the same category  </li> <li>Supports custom output column names </li> </ul>"},{"location":"users/visual-editor/nodes/combine.html#usage_4","title":"Usage","text":"<ol> <li>Select From and To columns to define relationships.  </li> <li>The node assigns a group identifier to connected nodes.  </li> </ol>"},{"location":"users/visual-editor/nodes/combine.html#configuration-options_2","title":"Configuration Options","text":"Parameter Description From Column Defines the starting point of each connection. To Column Defines the endpoint of each connection. Output Column Stores the assigned group identifier. <p>This node is useful for detecting dependencies, clustering related entities, and analyzing network connections.</p> <p>\u2190 Transform data | Next: Aggregate data \u2192</p>"},{"location":"users/visual-editor/nodes/input.html","title":"Input Nodes","text":"<p>Input nodes are the starting point for any data flow. Flowfile currently supports reading from local files, cloud storage (S3), and manual input.</p>"},{"location":"users/visual-editor/nodes/input.html#node-details","title":"Node Details","text":""},{"location":"users/visual-editor/nodes/input.html#read-data","title":"Read Data","text":"<p>The Read Data node allows you to load local data into your flow. It currently supports CSV, Excel, and Parquet file formats, each with specific configuration options.</p>"},{"location":"users/visual-editor/nodes/input.html#supported-formats","title":"Supported Formats:","text":"<ul> <li>CSV files (<code>.csv</code>)</li> <li>Excel files (<code>.xlsx</code>, <code>.xls</code>)</li> <li>Parquet files (<code>.parquet</code>)</li> </ul>"},{"location":"users/visual-editor/nodes/input.html#usage","title":"Usage:","text":"<ol> <li>Select your input file.  </li> <li>Configure any format-specific options.  </li> <li>Preview and confirm your data.  </li> </ol>"},{"location":"users/visual-editor/nodes/input.html#csv","title":"CSV","text":"<p>When a CSV file is selected, the following setup options are available:  </p> Parameter Description Has Headers Determines whether the first row is used as headers. If <code>\"yes\"</code>, the first row is treated as column names. If <code>\"no\"</code>, default column names like <code>\"Column 1, Column 2, ...\"</code> are assigned. Delimiter Specifies the character used to separate values (e.g., comma <code>,</code>, semicolon <code>;</code>, tab <code>\\t</code>). Encoding Defines the file encoding (e.g., <code>UTF-8</code>, <code>ISO-8859-1</code>). Quote Character Character used to enclose text fields, preventing delimiter conflicts (e.g., <code>\"</code>, <code>'</code>). New Line Delimiter Specifies how new lines are detected (e.g., <code>\\n</code>, <code>\\r\\n</code>). Schema Infer Length Determines how many rows are scanned to infer column types. Truncate Long Lines If enabled, long lines are truncated instead of causing errors. Ignore Errors If enabled, the process continues even if some rows cause errors."},{"location":"users/visual-editor/nodes/input.html#excel","title":"Excel","text":"<p>When an Excel file is selected, you can specify the sheet, select specific rows and columns, and configure headers and type inference options to tailor data loading to your needs.</p> Parameter Description Sheet Name The name of the sheet to be read. If not specified, the first sheet is used. Start Row The row index (zero-based) from which reading starts. Default is <code>0</code> (beginning of the sheet). Start Column The column index (zero-based) from which reading starts. Default is <code>0</code> (first column). End Row The row index (zero-based) at which reading stops. Default is <code>0</code> (read all rows). End Column The column index (zero-based) at which reading stops. Default is <code>0</code> (read all columns). Has Headers Determines whether the first row is treated as headers. If <code>true</code>, the first row is used as column names. If <code>false</code>, default column names are assigned. Type Inference If <code>true</code>, the engine attempts to infer data types. If <code>false</code>, data types are not automatically inferred."},{"location":"users/visual-editor/nodes/input.html#parquet","title":"Parquet","text":"<p>When a Parquet file is selected, no additional setup options are required. Parquet is a columnar storage format optimized for efficiency and performance. It retains schema information and data types, enabling faster reads and writes without manual configuration.</p>"},{"location":"users/visual-editor/nodes/input.html#cloud-storage-reader","title":"Cloud Storage Reader","text":"<p>The Cloud Storage Reader node allows you to read data directly from AWS S3.</p> Screenshot: Cloud Storage Reader Configuration <p></p>"},{"location":"users/visual-editor/nodes/input.html#connection-options","title":"Connection Options:","text":"<ul> <li>Use existing S3 connections configured in your workspace (see Manage Cloud Connections)</li> <li>Use local AWS CLI credentials or environment variables</li> </ul>"},{"location":"users/visual-editor/nodes/input.html#file-settings","title":"File Settings:","text":"Parameter Description File Path Path to the file or directory (e.g., <code>bucket-name/folder/file.csv</code>) File Format Supported formats: CSV, Parquet, JSON, Delta Lake Scan Mode Single file or directory scan (reads all matching files in a directory)"},{"location":"users/visual-editor/nodes/input.html#format-specific-options","title":"Format-Specific Options:","text":"<p>CSV Options: - Has Headers: First row contains column headers - Delimiter: Character separating values (default: <code>,</code>) - Encoding: File encoding (UTF-8 or UTF-8 Lossy)</p> <p>Delta Lake Options: - Version: Specify a specific version to read (optional, defaults to latest)</p>"},{"location":"users/visual-editor/nodes/input.html#manual-input","title":"Manual Input","text":"<p>The Manual Input node allows you to create data directly within Flowfile or paste data from your clipboard.</p> <p>\u2190 Node overview | Next: Transform data \u2192</p>"},{"location":"users/visual-editor/nodes/output.html","title":"Output Nodes","text":"<p>Output nodes represent the final steps in your data pipeline, allowing you to save your transformed data or explore it visually. These nodes help you deliver your results in the desired format or analyze them directly.</p>"},{"location":"users/visual-editor/nodes/output.html#node-details","title":"Node Details","text":""},{"location":"users/visual-editor/nodes/output.html#node-details_1","title":"Node Details","text":""},{"location":"users/visual-editor/nodes/output.html#write-data","title":"Write Data","text":"<p>The Write Data node allows you to save your processed data in different formats. It supports CSV, Excel, and Parquet, each with specific configuration options.  </p>"},{"location":"users/visual-editor/nodes/output.html#supported-formats","title":"Supported Formats","text":"<ul> <li>CSV files (<code>.csv</code>)  </li> <li>Excel files (<code>.xlsx</code>)  </li> <li>Parquet files (<code>.parquet</code>)  </li> </ul>"},{"location":"users/visual-editor/nodes/output.html#usage","title":"Usage","text":"<ol> <li>Configure the output file path.  </li> <li>Select the file format.  </li> <li>Set writing options (e.g., delimiter, compression).  </li> </ol>"},{"location":"users/visual-editor/nodes/output.html#csv","title":"CSV","text":"<p>When a CSV file is selected, the following setup options are available:  </p> Parameter Description Delimiter Specifies the character used to separate values (default: <code>,</code>). Encoding Defines the file encoding (default: <code>UTF-8</code>). Write Mode Determines how the file is saved (<code>overwrite</code>, <code>new file</code> or <code>append</code>)."},{"location":"users/visual-editor/nodes/output.html#excel","title":"Excel","text":"<p>When an Excel file is selected, additional configurations allow customizing the output.</p> Parameter Description Sheet Name Name of the sheet where data will be written (default: <code>Sheet1</code>). Write Mode Determines how the file is saved (<code>overwrite</code> or <code>new file</code>)."},{"location":"users/visual-editor/nodes/output.html#parquet","title":"Parquet","text":"<p>When a Parquet file is selected, no additional setup options are required. Parquet is a columnar storage format, optimized for efficient reading and writing.</p> Parameter Description Write Mode Determines how the file is saved (<code>overwrite</code> or <code>new file</code>)."},{"location":"users/visual-editor/nodes/output.html#general-configuration-options","title":"General Configuration Options","text":"Parameter Description File Path Directory and filename for the output file. File Format Selects the output format (<code>CSV</code>, <code>Excel</code>, <code>Parquet</code>). Overwrite Mode Controls whether to replace or append data. When <code>new file</code> is selected it will throw an error when the file already exists <p>This node ensures that your transformed data is saved in the correct format, ready for further use or analysis.</p>"},{"location":"users/visual-editor/nodes/output.html#cloud-storage-writer","title":"Cloud Storage Writer","text":"<p>The Cloud Storage Writer node allows you to save your processed data directly to cloud storage services like AWS S3.</p> Screenshot: Cloud Storage Writer Configuration <p></p>"},{"location":"users/visual-editor/nodes/output.html#connection-options","title":"Connection Options:","text":"<ul> <li>Use existing cloud storage connections configured in your workspace (see Manage Cloud Connections)</li> <li>Use local AWS CLI credentials or environment variables for authentication</li> </ul>"},{"location":"users/visual-editor/nodes/output.html#file-settings","title":"File Settings:","text":"Parameter Description File Path Full path including bucket/container and file name (e.g., <code>bucket-name/folder/output.parquet</code>) File Format Supported formats: CSV, Parquet, JSON, Delta Lake Write Mode <code>overwrite</code> (replace existing) or <code>append</code> (Delta Lake only)"},{"location":"users/visual-editor/nodes/output.html#format-specific-options","title":"Format-Specific Options:","text":"<p>CSV Options: - Delimiter: Character to separate values (default: <code>,</code>) - Encoding: File encoding (UTF-8 or UTF-8 Lossy)</p> <p>Parquet Options: - Compression: Choose from Snappy (default), Gzip, Brotli, LZ4, or Zstd</p> <p>Delta Lake Options: - Supports both <code>overwrite</code> and <code>append</code> write modes - Automatically handles schema evolution when appending</p> <p>Overwrite Mode</p> <p>When using <code>overwrite</code> mode, any existing file or data at the target path will be replaced. Make sure to verify the path before executing.</p> <p>Append Mode</p> <p>Available only for Delta Lake format.</p>"},{"location":"users/visual-editor/nodes/output.html#explore-data","title":"Explore Data","text":"<p>The Explore Data node provides interactive data exploration and analysis capabilities.</p>"},{"location":"users/visual-editor/nodes/transform.html","title":"Transform Nodes","text":"<p>Transform nodes modify and shape your data. These nodes handle everything from basic operations like filtering and sorting to more complex transformations like custom formulas and text manipulation.</p>"},{"location":"users/visual-editor/nodes/transform.html#node-details","title":"Node Details","text":""},{"location":"users/visual-editor/nodes/transform.html#add-record-id","title":"Add Record ID","text":"<p>The Add Record ID transformation generates a unique identifier for each record in your dataset. You can create a simple sequential ID or generate grouped IDs based on one or more columns.</p>"},{"location":"users/visual-editor/nodes/transform.html#usage","title":"Usage:","text":"<ol> <li>Add the Add Record ID node to your flow.  </li> <li>Configure the settings:</li> <li>Define the output column name.</li> <li>Set an optional offset for ID numbering.</li> <li>(Optional) Enable grouping and specify grouping columns.  </li> <li>Apply the transformation.  </li> </ol>"},{"location":"users/visual-editor/nodes/transform.html#configuration-options","title":"Configuration Options","text":"Parameter Description Output Column Name Name of the new column where the record ID will be stored. Default is <code>\"record_id\"</code>. Offset Starting value for the record ID. Default is <code>1</code>. Group By If <code>true</code>, record IDs are assigned within groups instead of sequentially across all records. Default is <code>false</code>. Group By Columns List of columns to group by when assigning record IDs. Only applies when Group By is enabled."},{"location":"users/visual-editor/nodes/transform.html#behavior","title":"Behavior","text":"<ul> <li>Sequential Record ID (Default)  </li> <li>A new column is added with a simple incremental ID starting from the defined offset.  </li> <li>Grouped Record ID </li> <li>When grouping is enabled, the record ID resets within each group based on the specified columns.  </li> </ul> <p>This transformation helps in creating unique keys, tracking row order, or structuring data for downstream processing.</p>"},{"location":"users/visual-editor/nodes/transform.html#formula","title":"Formula","text":"<p>The Formula node allows you to create new columns or modify existing ones using custom expressions. It supports a wide range of operations, including mathematical calculations, string manipulations, and conditional logic.</p>"},{"location":"users/visual-editor/nodes/transform.html#key-features","title":"Key Features","text":"<ul> <li>Create new columns dynamically  </li> <li>Modify existing columns using expressions  </li> <li>Perform mathematical operations (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>)  </li> <li>Apply string functions (<code>concat</code>, <code>uppercase</code>, <code>lowercase</code>)  </li> <li>Use conditional logic</li> <li>Use date time transformations</li> </ul>"},{"location":"users/visual-editor/nodes/transform.html#usage_1","title":"Usage","text":"<ol> <li>Drag the Formula node onto your canvas.  </li> <li>Connect input data.  </li> <li>Write your formula expression.  </li> <li>Preview the results.  </li> <li>Configure column names and data types.  </li> </ol>"},{"location":"users/visual-editor/nodes/transform.html#configuration-options_1","title":"Configuration Options","text":"Parameter Description Column Name The name of the new or modified column. Formula The expression used to compute values for the column. Data Type The expected data type of the resulting column (optional)."},{"location":"users/visual-editor/nodes/transform.html#behavior_1","title":"Behavior","text":"<ul> <li>If a new column is created, it will be added to the dataset.  </li> <li>If an existing column is modified, its values will be updated based on the formula.  </li> <li>If no data type is specified, the result defaults to <code>String</code>.  </li> </ul> <p>This transformation is useful for feature engineering, data cleaning, and enriching datasets with computed values.</p>"},{"location":"users/visual-editor/nodes/transform.html#select-data","title":"Select Data","text":"<p>The Select Data node allows you to choose which columns to keep, rename, and reorder. This transformation is useful for refining datasets, dropping unnecessary fields, and ensuring a structured column layout.</p>"},{"location":"users/visual-editor/nodes/transform.html#key-features_1","title":"Key Features","text":"<ul> <li>Select specific columns to retain in the dataset  </li> <li>Reorder columns by dragging them into the desired order or by ordering them alphabetically</li> <li>Rename columns to provide meaningful names  </li> <li>Automatically remove missing fields </li> </ul>"},{"location":"users/visual-editor/nodes/transform.html#configuration-options_2","title":"Configuration Options","text":"Parameter Description Column Selection Choose which columns to keep in the dataset. Reordering Drag and drop to change the column order. Rename Column Assign a new name to any selected column. Keep Missing Fields If enabled, columns that are missing from input data are retained in the selection list."},{"location":"users/visual-editor/nodes/transform.html#behavior_2","title":"Behavior","text":"<ul> <li>If a selected column is missing from the input, it is marked as unavailable.  </li> <li>Columns can be renamed without affecting their original data.  </li> <li>Changing the order affects how the columns appear in downstream processing.  </li> </ul> <p>This transformation ensures that datasets are structured efficiently before further analysis or processing.</p>"},{"location":"users/visual-editor/nodes/transform.html#filter-data","title":"Filter Data","text":"<p>The Filter Data node keeps only rows that match a specified condition. Enter a formula that evaluates to <code>true</code> or <code>false</code>, and only <code>true</code> rows remain.</p>"},{"location":"users/visual-editor/nodes/transform.html#key-features_2","title":"Key Features","text":"<ul> <li>Apply custom conditions to filter data  </li> <li>Use operators (<code>or</code>, <code>and</code>, <code>&lt;</code>,)  </li> <li>Support for string, numeric, and date filtering </li> </ul>"},{"location":"users/visual-editor/nodes/transform.html#usage_2","title":"Usage","text":"<ol> <li>Drag the Filter Data node onto your canvas.  </li> <li>Connect input data.  </li> <li>Enter a filter formula (e.g., <code>[City] = 'Amsterdam'</code>).  </li> <li>Apply the filter to keep matching rows.  </li> </ol>"},{"location":"users/visual-editor/nodes/transform.html#example-filters","title":"Example Filters","text":"Expression Description <code>[City] = 'Amsterdam'</code> Keep rows where <code>City</code> is \"Amsterdam\". <code>[Age] &gt; 30</code> Keep rows where <code>Age</code> is greater than 30. <code>[Country] = 'USA' &amp;&amp;[Sales] &gt; 100</code> Keep rows where <code>Country</code> is \"USA\" and <code>Sales</code> is over 100. <p>Use this node to refine datasets efficiently.</p>"},{"location":"users/visual-editor/nodes/transform.html#sort-data","title":"Sort Data","text":"<p>The Sort Data node orders your data based on one or more columns.</p>"},{"location":"users/visual-editor/nodes/transform.html#key-features_3","title":"Key Features","text":"<ul> <li>Sort by multiple columns  </li> <li>Choose ascending or descending order</li> </ul>"},{"location":"users/visual-editor/nodes/transform.html#usage_3","title":"Usage","text":"<ol> <li>Select columns to sort by.  </li> <li>Choose sort direction (ascending or descending) for each column.  </li> </ol>"},{"location":"users/visual-editor/nodes/transform.html#configuration-options_3","title":"Configuration Options","text":"Parameter Description Sort Columns Columns used to sort the dataset. Sort Order Set ascending (<code>asc</code>) or descending (<code>desc</code>). <p>This node ensures structured and ordered data for better analysis.</p>"},{"location":"users/visual-editor/nodes/transform.html#take-sample","title":"Take Sample","text":"<p>The Take Sample node lets you work with a subset of your data.</p>"},{"location":"users/visual-editor/nodes/transform.html#drop-duplicates","title":"Drop Duplicates","text":"<p>The Drop Duplicates node removes duplicate rows based on selected columns. Only the first occurrence is kept by default.</p>"},{"location":"users/visual-editor/nodes/transform.html#key-features_4","title":"Key Features","text":"<ul> <li>Remove duplicate rows  </li> <li>Select columns to check for duplicates  </li> </ul>"},{"location":"users/visual-editor/nodes/transform.html#usage_4","title":"Usage","text":"<ol> <li>Select columns to check for duplicates.  </li> <li>Choose whether to keep the first or last occurrence.  </li> </ol>"},{"location":"users/visual-editor/nodes/transform.html#configuration-options_4","title":"Configuration Options","text":"Parameter Description Columns Columns used to check for duplicates. <p>This node ensures a clean dataset by eliminating redundant rows.</p>"},{"location":"users/visual-editor/nodes/transform.html#text-to-rows","title":"Text to Rows","text":"<p>The Text to Rows node splits text from a selected column into multiple rows based on a delimiter.</p>"},{"location":"users/visual-editor/nodes/transform.html#key-features_5","title":"Key Features","text":"<ul> <li>Split a column into multiple rows  </li> <li>Use a fixed delimiter (e.g., <code>,</code>, <code>;</code>, <code>|</code>)  </li> <li>Split using values from another column  </li> </ul>"},{"location":"users/visual-editor/nodes/transform.html#usage_5","title":"Usage","text":"<ol> <li>Select the column to split.  </li> <li>Choose a delimiter or use another column for splitting.  </li> <li>(Optional) Set an output column name. </li> </ol>"},{"location":"users/visual-editor/nodes/transform.html#configuration-options_5","title":"Configuration Options","text":"Parameter Description Column to Split The column containing text to be split. Output Column Name Name of the new column after splitting (defaults to the original column). Split by Fixed Value If <code>true</code>, use a fixed delimiter (default: <code>,</code>). Delimiter The character used to split text (e.g., <code>,</code>, <code>|</code>, <code>;</code>). Split by Column Instead of a fixed delimiter, use values from another column. <p>This transformation helps normalize datasets by converting text lists into structured rows.</p>"},{"location":"users/visual-editor/nodes/transform.html#polars-code","title":"Polars Code","text":"<p>The Polars Code node allows you to write custom Polars DataFrame transformations directly in your workflow.</p>"},{"location":"users/visual-editor/nodes/transform.html#key-features_6","title":"Key Features","text":"<ul> <li>Write custom Polars expressions </li> <li>Apply advanced transformations not covered by standard nodes  </li> <li>Filter, aggregate, or modify data using Polars API </li> </ul>"},{"location":"users/visual-editor/nodes/transform.html#usage_6","title":"Usage","text":"<ol> <li>Write a single-line or multi-line Polars expression.  </li> <li>Use <code>input_df</code> as the DataFrame reference.  </li> <li>Assign results to <code>output_df</code> for multi-line operations.  </li> </ol>"},{"location":"users/visual-editor/nodes/transform.html#example-code","title":"Example Code","text":""},{"location":"users/visual-editor/nodes/transform.html#single-line-transformation","title":"Single-line transformation","text":"<pre><code>input_df.filter(pl.col('Age') &gt; 30)\n</code></pre>"},{"location":"users/visual-editor/nodes/transform.html#multi-line-transformation","title":"Multi-line transformation","text":"<pre><code>result = input_df.select(['Name', 'City'])\nfiltered = result.filter(pl.col('City') == 'Amsterdam')\noutput_df = filtered.with_columns(pl.col('Name').alias('Customer_Name')) # this will be the output of the node\n</code></pre> <p>\u2190 Read data | Next: Combine data \u2192</p>"},{"location":"users/visual-editor/tutorials/index.html","title":"Tutorials","text":"<p>Hands-on guides for building data pipelines with Flowfile.</p>"},{"location":"users/visual-editor/tutorials/index.html#available-tutorials","title":"Available Tutorials","text":""},{"location":"users/visual-editor/tutorials/index.html#database-connectivity","title":"Database Connectivity","text":"<p>Connect to PostgreSQL databases, read data, apply transformations, and write results back.</p>"},{"location":"users/visual-editor/tutorials/index.html#cloud-storage","title":"Cloud Storage","text":"<p>Connect to AWS S3 and other cloud storage services.</p>"},{"location":"users/visual-editor/tutorials/index.html#export-to-python","title":"Export to Python","text":"<p>Convert visual pipelines into executable Python code.</p>"},{"location":"users/visual-editor/tutorials/index.html#reference-documentation","title":"Reference Documentation","text":"<ul> <li>Docker - Docker deployment</li> <li>Secrets - Encrypted credential storage</li> <li>Settings - Theme and user management</li> </ul>"},{"location":"users/visual-editor/tutorials/cloud-connections.html","title":"Manage S3 Connections","text":"<p>This guide walks you through creating AWS S3 connections in Flowfile to access your cloud data.</p>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#overview","title":"Overview","text":"<p>Cloud storage connections securely store your AWS credentials and configuration, allowing you to reuse them across multiple workflows without re-entering credentials.</p>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#steps-to-create-an-s3-connection","title":"Steps to Create an S3 Connection","text":""},{"location":"users/visual-editor/tutorials/cloud-connections.html#1-access-cloud-storage-connections","title":"1. Access Cloud Storage Connections","text":"<p>Click the Cloud icon in the left sidebar to access the Cloud Storage Connections page.</p> Screenshot: Cloud Storage Connections Page <p></p>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#2-add-new-connection","title":"2. Add New Connection","text":"<p>Click the \"+ Add Connection\" button to open the connection configuration dialog.</p> Screenshot: Add Connection Dialog <p></p>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#3-configure-connection-settings","title":"3. Configure Connection Settings","text":""},{"location":"users/visual-editor/tutorials/cloud-connections.html#basic-settings","title":"Basic Settings","text":"Field Description Connection Name A unique identifier for this connection (e.g., <code>my_s3_storage</code>) Storage Type Select AWS S3"},{"location":"users/visual-editor/tutorials/cloud-connections.html#authentication-methods","title":"Authentication Methods","text":"<p>Choose one of the following authentication methods:</p>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#access-key","title":"Access Key","text":"<ul> <li>AWS Access Key ID: Your AWS access key (e.g., <code>AKIAIOSFODNN7EXAMPLE</code>)</li> <li>AWS Secret Access Key: Your AWS secret access key</li> <li>AWS Region: The AWS region where your S3 buckets are located (e.g., <code>us-east-1</code>)</li> </ul>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#aws-cli","title":"AWS CLI","text":"<ul> <li>Uses credentials from your local AWS CLI configuration</li> <li>AWS Region: The AWS region where your S3 buckets are located</li> </ul>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#advanced-settings-optional","title":"Advanced Settings (Optional)","text":"Field Description Custom Endpoint URL For S3-compatible services (e.g., MinIO) Allow Unsafe HTML Enable if your S3 data contains HTML content Verify SSL Disable only for testing with self-signed certificates"},{"location":"users/visual-editor/tutorials/cloud-connections.html#4-save-connection","title":"4. Save Connection","text":"<p>Click \"Create Connection\" to save your configuration.</p>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#using-s3-connections-in-workflows","title":"Using S3 Connections in Workflows","text":"<p>Once created, your S3 connection will appear in the Cloud Storage Reader and Writer node's connection dropdown. Simply:</p> <ol> <li>Add a Cloud Storage Reader node to your workflow</li> <li>Select your connection from the dropdown</li> <li>Enter the S3 path (e.g., <code>s3://my-bucket/data/file.csv</code>)</li> <li>Configure file format options</li> <li>Run your workflow</li> </ol>"},{"location":"users/visual-editor/tutorials/code-generator.html","title":"Building code with flows","text":"<p>Flowfile's Code Generator allows you to export your visually designed data pipelines as clean, executable Python code. This feature is designed to empower users who wish to inspect the underlying data transformation logic, integrate Flowfile pipelines into existing Python projects, or extend their workflows with custom scripts.</p> <p>The generated code is entirely self-contained and relies mostly on the Polars library, ensuring it can run independently of Flowfile. </p>"},{"location":"users/visual-editor/tutorials/code-generator.html#key-characteristics-of-the-generated-code","title":"Key Characteristics of the Generated Code","text":"<p>When you generate code from your Flowfile graph, you can expect the output to be:</p> <ul> <li>Standalone: The code functions independently, requiring only Polars and common Python libraries.</li> <li>Readable: The structure mirrors your visual flow, making it easy to understand the sequence of operations.</li> <li>Direct Translation: Each Flowfile node and its configured settings are directly translated into equivalent Polars operations.</li> <li>Ready for Integration: You can copy, modify, and embed this code into other Python applications or scripts.</li> </ul>"},{"location":"users/visual-editor/tutorials/code-generator.html#examples-of-generated-code","title":"Examples of Generated Code","text":"<p>Here are some simplified examples illustrating what the generated Polars code looks like for common Flowfile operations. These examples highlight how your visual workflow seamlessly translates into Python.</p>"},{"location":"users/visual-editor/tutorials/code-generator.html#example-1-reading-a-csv-and-selecting-columns","title":"Example 1: Reading a CSV and Selecting Columns","text":"<p>This example shows how a pipeline that reads a CSV file and then selects/renames specific columns translates into Polars code.</p> <p>Flowfile Pipeline:</p> <ol> <li>Read CSV (e.g., <code>customers.csv</code>)</li> <li>Select (e.g., keep <code>name</code> as <code>customer_name</code>, <code>age</code>)</li> </ol> Generated Polars Code <pre><code># Example 1: Reading a CSV and Selecting Columns\nimport polars as pl\n\ndef run_etl_pipeline():\n    \"\"\"\n    ETL Pipeline: Example CSV Read and Select\n    Generated from Flowfile\n    \"\"\"\n    df_1 = pl.scan_csv(\"/path/to/your/customers.csv\")\n    df_2 = df_1.select(\n        pl.col(\"name\").alias(\"customer_name\"),\n        pl.col(\"age\")\n    )\n    return df_2\n\nif __name__ == \"__main__\":\n    pipeline_output = run_etl_pipeline()\n</code></pre>"},{"location":"users/visual-editor/tutorials/code-generator.html#example-2-grouping-and-aggregating-data","title":"Example 2: Grouping and Aggregating Data","text":"<p>This example demonstrates the code generated for a pipeline that processes a dataset and performs a group by operation with aggregations.</p> <p>Flowfile Pipeline:</p> <ol> <li>Manual Input (sample sales data with <code>product</code> and <code>revenue</code>)</li> <li>Group By (e.g., group by <code>product</code>, sum <code>revenue</code> as <code>total_revenue</code>)</li> </ol> Generated Polars Code <pre><code># Example 2: Grouping and Aggregating Data\nimport polars as pl\n\ndef run_etl_pipeline():\n    \"\"\"\n    ETL Pipeline: Example Grouping and Aggregating\n    Generated from Flowfile\n    \"\"\"\n    # Simplified manual input example\n    df_1 = pl.LazyFrame(\n        {\n            \"product\": [\"A\", \"B\", \"A\", \"B\", \"C\"],\n            \"revenue\": [100.0, 200.0, 100.0, 200.0, 150.0],\n        }\n    )\n    df_2 = df_1.group_by([\"product\"]).agg([\n        pl.col(\"revenue\").sum().alias(\"total_revenue\"),\n    ])\n    return df_2\n\nif __name__ == \"__main__\":\n    pipeline_output = run_etl_pipeline()\n</code></pre>"},{"location":"users/visual-editor/tutorials/code-generator.html#example-3-custom-polars-code-execution","title":"Example 3: Custom Polars Code Execution","text":"<p>For advanced users, Flowfile offers a \"Polars Code\" node where you can write custom Polars expressions. Here's how that custom code is integrated into the generated script.</p> <p>Flowfile Pipeline:</p> <ol> <li>Manual Input (a basic DataFrame)</li> <li>Polars Code (a node containing custom Polars logic, e.g., adding a new column)</li> </ol> Generated Polars Code <pre><code># Example 3: Custom Polars Code Execution\nimport polars as pl\n\ndef run_etl_pipeline():\n    \"\"\"\n    ETL Pipeline: Custom Polars Code Example\n    Generated from Flowfile\n    \"\"\"\n    df_1 = pl.LazyFrame({\"value\": [1, 2, 3]})\n\n    # Custom Polars code as defined in the Flowfile node\n    def _custom_code_node_name(input_df: pl.LazyFrame):\n        return input_df.with_columns((pl.col('value') * 10).alias('scaled_value'))\n\n    df_2 = _custom_code_node_name(df_1)\n    return df_2\n\nif __name__ == \"__main__\":\n    pipeline_output = run_etl_pipeline()\n</code></pre> <p>These examples provide a clear overview of the type of high-quality, executable Python code produced by Flowfile's Code Generator.</p>"},{"location":"users/visual-editor/tutorials/database-connectivity.html","title":"How to Connect and Work with PostgreSQL Databases in Flowfile","text":""},{"location":"users/visual-editor/tutorials/database-connectivity.html#full-flow-overview","title":"Full flow overview","text":"<p>Flowfile's latest release introduces powerful database connectivity features that allow you to seamlessly integrate with PostgreSQL databases like Supabase. In this guide, I'll walk you through the entire process of connecting to a database, reading data, transforming it, and writing it back.</p>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#prerequisites","title":"Prerequisites","text":"<p>Before diving in, make sure you have:</p> <ul> <li>A Flowfile account (free tier works fine)</li> <li>A Supabase account (sign up here if needed)</li> <li>Sample data to work with (we're using the Sales Forecasting Dataset from Kaggle so you can easily follow along with the transformation examples)</li> </ul>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-1-set-up-your-supabase-database","title":"Step 1: Set Up Your Supabase Database","text":"<ol> <li>Create a new project in Supabase.</li> <li>Download the sample dataset from Kaggle.</li> <li>Create a new table in your Supabase project (e.g., <code>superstore_sales_data</code>).</li> <li>Import the dataset into your table (hint: use Supabase's built-in CSV import feature via the Table Editor).</li> <li>Note your database connection details (host, port, username, password).</li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-2-configure-your-database-connection-in-flowfile","title":"Step 2: Configure Your Database Connection in Flowfile","text":"<ol> <li>Open Flowfile and navigate to the database connection manager (often found under a \"Connections\" icon or within the main \"Settings\" area).</li> <li>Click \"Create New Connection\".</li> <li>Fill in your connection details:<ul> <li>Connection Name: <code>supa_base_connection</code> (or any name you prefer)</li> <li>Database Type: PostgreSQL</li> <li>Host: Your Supabase host (e.g., <code>aws-0-eu-central-1.pooler.supabase.com</code>)</li> <li>Port: <code>5432</code></li> <li>Database: <code>postgres</code></li> <li>Username: Your Supabase username</li> <li>Password: Your Supabase password</li> <li>Enable SSL: Check if required by your database (Supabase typically requires it).</li> </ul> </li> <li>Click \"Update Connection\" to save.</li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#connection-overview-in-flowfile","title":"Connection overview in Flowfile","text":""},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-3-create-a-new-data-flow","title":"Step 3: Create a New Data Flow","text":"<ol> <li>Click \"Create\" or \"New Flow\" to start a fresh workflow.</li> <li>Navigate to the \"Data actions\" panel on the left sidebar.</li> <li>Find the \"Read from Database\" node (look for the database icon).</li> <li>Drag and drop this node onto your canvas.</li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-4-configure-your-database-read-operation","title":"Step 4: Configure Your Database Read Operation","text":"<ol> <li>Click on the \"Read from Database\" node to open its settings panel.</li> <li>Select \"reference\" for Connection Mode (this tells Flowfile to use the connection you configured in Step 2).</li> <li>Choose your <code>supa_base_connection</code> from the Connection dropdown.</li> <li>Configure the table settings:<ul> <li>Schema: <code>public</code> (or your specific schema)</li> <li>Table: <code>superstore_sales_data</code></li> </ul> </li> <li>Click \"Validate Settings\" to ensure everything is working.</li> <li>You should see a green confirmation message: \"Query settings are valid\".</li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#node-settings-panel-showing-database-read-configuration","title":"Node Settings panel showing database read configuration","text":""},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-5-run-your-initial-flow","title":"Step 5: Run Your Initial Flow","text":"<ol> <li>Click the \"Run\" button in the top toolbar.</li> <li>Watch the flow execution in the log panel at the bottom.</li> <li>When completed, you'll see a success message and the number of records processed.</li> <li>You can now click on the node output dot to preview the data that was read from your database.</li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#flow-execution-logs-showing-successful-database-read-operation","title":"Flow execution logs showing successful database read operation","text":"<p>Ensure the image <code>initial_run.png</code> shows the successful run log/node status, not the configuration panel again.</p>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-6-add-data-transformations","title":"Step 6: Add Data Transformations","text":"<p>Now that you've successfully read data from your database, you can add transformation steps:</p> <ol> <li>Add transformation nodes from the \"Data actions\" panel to your workflow.<ul> <li>For example, creating time-to-ship metrics by category:<ul> <li>Add formula nodes to transform the <code>shipping_date</code> and <code>delivery_date</code> columns to a proper date type if needed.</li> <li>Add a \"Formula\" node to calculate shipping time (e.g., <code>delivery_date - shipping_date</code>). Name the new column <code>shipping_time_days</code>.</li> <li>Add a \"Group by\" node to aggregate by product category.</li> <li>In the \"Group by\" node, calculate <code>min</code>, <code>max</code>, and <code>median</code> of the <code>shipping_time_days</code> column.</li> </ul> </li> </ul> </li> <li>Connect these nodes in sequence by dragging from the output dot of one node to the input dot of the next.</li> <li>Configure each node with the specific transformations you need (refer to the Flowfile documentation for details on specific node configurations if needed).</li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#overview-of-connected-transformation-nodes-read-formula-group-by","title":"Overview of connected transformation nodes (Read -&gt; Formula -&gt; Group By)","text":""},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-7-add-a-write-to-database-node","title":"Step 7: Add a Write to Database Node","text":"<ol> <li>From the \"Data actions\" panel, find and drag the \"Write to Database\" node onto your canvas.</li> <li>Connect it to the output of the last transformation node (e.g., the \"Group by\" node).</li> <li>Configure the write operation in its settings panel:<ul> <li>Connection Mode: Select \"reference\".</li> <li>Connection: Choose your <code>supa_base_connection</code>.</li> <li>Schema: <code>public</code> (or your desired schema).</li> <li>Table: Enter a name for your new output table (e.g., <code>time_to_ship_per_category</code>).</li> <li>Write Mode: Select how to handle the table if it already exists:<ul> <li>\"Append\": Add new data to the table.</li> <li>\"Replace\": Delete the existing table and create a new one with the output data.</li> <li>\"Fail\": Abort the flow if the table already exists.</li> </ul> </li> </ul> </li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#setup-write-to-database-node-configuration-panel","title":"Setup write to database node configuration panel","text":"<p>Ensure the image <code>configure_write_db.png</code> actually shows the \"Write to Database\" node's configuration panel.</p>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-8-run-your-complete-workflow","title":"Step 8: Run Your Complete Workflow","text":"<ol> <li>Click \"Run\" to execute the full workflow from start to finish.</li> <li>The system will:<ul> <li>Read data from your source Supabase table (<code>superstore_sales_data</code>).</li> <li>Apply all the transformation steps (calculate shipping time, group by category).</li> <li>Write the aggregated results to your destination Supabase table (<code>time_to_ship_per_category</code>).</li> </ul> </li> <li>Check the logs to confirm successful execution, including messages about records read and written.</li> <li>Navigate to your Supabase project, open the SQL Editor or Table Editor, and check the <code>public.time_to_ship_per_category</code> table to see your newly created data!</li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#overview-of-the-final-result-table-in-supabase","title":"Overview of the final result table in Supabase","text":"<p>Ensure the image <code>result.png</code> shows the data in the newly created Supabase table.</p>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#conclusion","title":"Conclusion","text":"<p>Flowfile's database integration capabilities make it incredibly simple to build professional-grade data pipelines without writing code. By connecting to Supabase or other PostgreSQL databases, you can easily extract, transform, and load data in a visual, intuitive environment.</p> <p>Whether you're creating business dashboards, data warehousing solutions, or just exploring your data, the combination of Flowfile's visual workflow and Supabase's powerful PostgreSQL hosting gives you a robust platform for all your data needs.</p> <p>Feel free to experiment with different transformation nodes and workflow patterns to build increasingly sophisticated data pipelines!</p> <p>This guide is based on Flowfile v0.2.0, which introduced database connectivity features including PostgreSQL support, secure credential storage, and flexible connection management options.</p>"}]}