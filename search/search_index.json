{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"quickstart.html","title":"Quick Start Guide","text":"Get Started with Flowfile in 5 Minutes"},{"location":"quickstart.html#installation","title":"Installation","text":"Recommended quickstart: Install from PyPI <pre><code>pip install flowfile</code></pre> <p>This installs everything you need - the Python API, visual editor, and all services.</p>"},{"location":"quickstart.html#alternative-installation-methods","title":"Alternative Installation Methods","text":"Desktop Application (Pre-built Installer) <p>Download the latest installer for your platform: - Windows: Flowfile-Setup.exe - macOS: Flowfile.dmg </p> <p>Note: You may see security warnings since the installer isn't signed. On Windows, click \"More info\" \u2192 \"Run anyway\". On macOS, right-click \u2192 \"Open\" \u2192 confirm.</p> Development Setup (From Source) <pre><code># Clone repository\ngit clone https://github.com/edwardvaneechoud/Flowfile.git\ncd Flowfile\n\n# Install with Poetry\npoetry install\n\n# Start services\npoetry run flowfile_worker  # Terminal 1 (port 63579)\npoetry run flowfile_core    # Terminal 2 (port 63578)\n\n# Start frontend\ncd flowfile_frontend\nnpm install\nnpm run dev:web  # Terminal 3 (port 8080)\n</code></pre>"},{"location":"quickstart.html#choose-your-path","title":"Choose Your Path","text":"Non-Technical Users <p>Perfect for: Analysts, business users, Excel power users</p> <p>No coding required!</p> <ul> <li>\u2705 Drag and drop interface</li> <li>\u2705 Visual data preview</li> <li>\u2705 Export to Excel/CSV</li> <li>\u2705 Built-in transformations</li> </ul> Start Visual Tutorial \u2192 Technical Users <p>Perfect for: Developers, data scientists, engineers</p> <p>Full programmatic control!</p> <ul> <li>\u2705 Polars-compatible API</li> <li>\u2705 Cloud storage integration</li> <li>\u2705 Version control friendly</li> <li>\u2705 Complex dynamic logic</li> </ul> Start Python Tutorial \u2192"},{"location":"quickstart.html#non-technical-quickstart","title":"Quick Start for Non-Technical Users","text":"Goal: Clean and analyze sales data without writing any code"},{"location":"quickstart.html#step-1-start-flowfile-and-create-a-flow","title":"Step 1: Start Flowfile, and create a Flow","text":"<p>Open your terminal (Command Prompt on Windows, Terminal on Mac) and type:</p> <p><pre><code>flowfile run ui\n</code></pre> Your browser should automatically open to the Flowfile UI.</p> <p>If the browser does not open automatically</p> <p>If the browser does not open automatically, you can manually navigate to http://127.0.0.1:63578/ui#/main/designer in your web browser.</p> <p>Creating your First Flow:</p> <ol> <li>Click \"Create\" to create a new data pipeline</li> <li>Click \"Create New File Here\"</li> <li>Name your flow (e.g., \"Sales Data Analysis\")</li> <li>Click on \"Settings\" in the top right to configure your flow</li> <li>Set the Execution mode to \"Development\"</li> </ol> <p>Your should see now an empty flow: New clean flow interface</p>"},{"location":"quickstart.html#step-2-load-your-data","title":"Step 2: Load Your Data","text":"<p>Loading a CSV or Excel file:</p> <ol> <li>Find the \"Read Data\" node in the left panel under \"Input\"</li> <li>Drag it onto the canvas (center area)</li> <li>Click the node to open settings on the right</li> <li>Click \"Browse\" and select your file</li> <li>Configure options (if needed):<ul> <li>For CSV: Check \"Has Headers\" if your file has column names</li> <li>For Excel: Select the sheet name</li> </ul> </li> <li>Click \"Run\" (top toolbar) to load the data</li> <li>Click the node to preview your data in the bottom panel</li> </ol>"},{"location":"quickstart.html#step-3-clean-your-data","title":"Step 3: Clean Your Data","text":"<p>Let's remove duplicate records and filter for high-value transactions:</p> Remove Duplicates <ol> <li>Drag \"Drop Duplicates\" node from Transform section</li> <li>Connect it to your Read Data node</li> <li>Select columns to check for duplicates</li> <li>Click Run</li> </ol> Filter Data <ol> <li>Drag \"Filter Data\" node from Transform section</li> <li>Connect it to Drop Duplicates node</li> <li>Enter formula: <code>[Quantity] &gt; 7</code></li> <li>Click Run</li> </ol>"},{"location":"quickstart.html#step-4-analyze-your-data","title":"Step 4: Analyze Your Data","text":"<p>Create a summary by city:</p> <ol> <li>Add a Group By node from the Aggregate section</li> <li>Connect it to your Filter node</li> <li>Configure the aggregation:</li> <li>Group by: <code>city</code></li> <li>Aggregations:<ul> <li><code>gross income</code> \u2192 Sum \u2192 Name it <code>total_sales</code></li> <li><code>gross income</code> \u2192 Average \u2192 Name it <code>avg_sale</code></li> <li><code>gross income</code> \u2192 Count \u2192 Name it <code>number_of_sales</code></li> </ul> </li> <li>Click Run to see your summary</li> </ol> Data after group by <p></p>"},{"location":"quickstart.html#step-5-save-your-results","title":"Step 5: Save Your Results","text":"<p>Export your cleaned data:</p> <ol> <li>Add a \"Write Data\" node from Output section</li> <li>Connect it to your final transformation</li> <li>Choose format:<ul> <li>Excel: Best for sharing with colleagues</li> <li>CSV: Best for Excel/Google Sheets</li> <li>Parquet: Best for large datasets</li> </ul> </li> <li>Set file path (e.g., <code>cleaned_sales.xlsx</code>)</li> <li>Click Run to save</li> </ol>"},{"location":"quickstart.html#heres-what-your-complete-flow-should-look-like","title":"Here's what your complete flow should look like:","text":""},{"location":"quickstart.html#congratulations","title":"Congratulations!","text":"<p>You've just built your first data pipeline! You can: - Save this flow using File \u2192 Save (creates a <code>.flowfile</code>) - Share it with colleagues who can run it without any setup - Schedule it to run automatically (coming soon) - Export as Python code if you want to see what's happening behind the scenes</p>"},{"location":"quickstart.html#pro-tips-for-non-technical-users","title":"Pro Tips for Non-Technical Users:","text":"<ul> <li>Use descriptions: Right-click nodes and add descriptions to document your work</li> <li>Preview often: Click nodes after running to see data at each step</li> <li>Start small: Test with a sample of your data first</li> <li>Save versions: Save different versions of your flow as you build</li> </ul>"},{"location":"quickstart.html#next-steps","title":"Next Steps","text":"<p>Complete Visual Guide Learn All Nodes Connect to Databases</p>"},{"location":"quickstart.html#technical-quickstart","title":"Quick Start for Technical Users","text":"Goal: Build a production-ready ETL pipeline with cloud integration"},{"location":"quickstart.html#step-1-install-and-import","title":"Step 1: Install and Import","text":"<pre><code>pip install flowfile\n</code></pre> <pre><code>import flowfile as ff\nfrom flowfile import col, when, lit\nimport polars as pl  # Flowfile returns Polars DataFrames\n</code></pre>"},{"location":"quickstart.html#step-2-build-a-real-world-etl-pipeline","title":"Step 2: Build a Real-World ETL Pipeline","text":"<p>Let's build a production pipeline that reads from S3, transforms data, and writes results:</p> <pre><code># Configure S3 connection (one-time setup)\nfrom pydantic import SecretStr\n\nimport flowfile as ff\n\nff.create_cloud_storage_connection_if_not_exists(\n    ff.FullCloudStorageConnection(\n        connection_name=\"production-data\",\n        storage_type=\"s3\",\n        auth_method=\"access_key\",\n        aws_region=\"us-east-1\",\n        aws_access_key_id=\"AKIAIOSFODNN7EXAMPLE\",\n        aws_secret_access_key=SecretStr(\"wJalrXUtnFEMI/K7MDENG\")\n    )\n)\n</code></pre>"},{"location":"quickstart.html#step-3-extract-and-transform","title":"Step 3: Extract and Transform","text":"<pre><code># Build the pipeline (lazy evaluation - no data loaded yet!)\nimport flowfile as ff\npipeline = (\n    # Extract: Read partitioned parquet files from S3\n    ff.scan_parquet_from_cloud_storage(\n        \"s3://data-lake/sales/year=2024/month=*\", \n        connection_name=\"production-data\",\n        description=\"Load Q1-Q4 2024 sales data\"\n    )\n\n    # Transform: Clean and enrich\n    .filter(\n        (ff.col(\"status\") == \"completed\") &amp; \n        (ff.col(\"amount\") &gt; 0),\n        description=\"Keep only valid completed transactions\"\n    )\n\n    # Add calculated fields\n    .with_columns([\n        # Business logic\n        (ff.col(\"amount\") * ff.col(\"quantity\")).alias(\"line_total\"),\n        (ff.col(\"amount\") * ff.col(\"quantity\") * 0.1).alias(\"tax\"),\n\n        # Date features for analytics\n        ff.col(\"order_date\").dt.quarter().alias(\"quarter\"),\n        ff.col(\"order_date\").dt.day_of_week().alias(\"day_of_week\"),\n\n        # Customer segmentation\n        ff.when(ff.col(\"customer_lifetime_value\") &gt; 10000)\n            .then(ff.lit(\"VIP\"))\n            .when(ff.col(\"customer_lifetime_value\") &gt; 1000)\n            .then(ff.lit(\"Regular\"))\n            .otherwise(ff.lit(\"New\"))\n            .alias(\"customer_segment\"),\n\n        # Region mapping\n        ff.when(ff.col(\"state\").is_in([\"CA\", \"OR\", \"WA\"]))\n            .then(ff.lit(\"West\"))\n            .when(ff.col(\"state\").is_in([\"NY\", \"NJ\", \"PA\"]))\n            .then(ff.lit(\"Northeast\"))\n            .when(ff.col(\"state\").is_in([\"TX\", \"FL\", \"GA\"]))\n            .then(ff.lit(\"South\"))\n            .otherwise(ff.lit(\"Midwest\"))\n            .alias(\"region\")\n    ], description=\"Add business metrics and segments\")\n\n    # Complex aggregation\n    .group_by([\"region\", \"quarter\", \"customer_segment\"])\n    .agg([\n        # Revenue metrics\n        ff.col(\"line_total\").sum().alias(\"total_revenue\"),\n        ff.col(\"tax\").sum().alias(\"total_tax\"),\n\n        # Order metrics\n        ff.col(\"order_id\").n_unique().alias(\"unique_orders\"),\n        ff.col(\"customer_id\").n_unique().alias(\"unique_customers\"),\n\n        # Performance metrics\n        ff.col(\"line_total\").mean().round(2).alias(\"avg_order_value\"),\n        ff.col(\"quantity\").sum().alias(\"units_sold\"),\n\n        # Statistical metrics\n        ff.col(\"line_total\").std().round(2).alias(\"revenue_std\"),\n        ff.col(\"line_total\").quantile(0.5).alias(\"median_order_value\")\n    ])\n\n    # Final cleanup\n    .sort([\"region\", \"quarter\", \"total_revenue\"], descending=[False, False, True])\n    .filter(ff.col(\"total_revenue\") &gt; 1000)  # Remove noise\n)\n\n# Check the execution plan (no data processed yet!)\nprint(pipeline.explain())  # Shows optimized Polars query plan\n</code></pre>"},{"location":"quickstart.html#step-4-load-and-monitor","title":"Step 4: Load and Monitor","text":"<pre><code># Option 1: Write to cloud storage\npipeline.write_parquet_to_cloud_storage(\n    \"s3://data-warehouse/aggregated/sales_summary_2024.parquet\",\n    connection_name=\"production-data\",\n    compression=\"snappy\",\n    description=\"Save aggregated results for BI tools\"\n)\n\n# Option 2: Write to Delta Lake for versioning\npipeline.write_delta(\n    \"s3://data-warehouse/delta/sales_summary\",\n    connection_name=\"production-data\",\n    write_mode=\"append\",  # or \"overwrite\"\n    description=\"Append to Delta table\"\n)\n\n# Option 3: Collect for analysis\ndf_result = pipeline.collect()  # NOW it executes everything!\nprint(f\"Processed {len(df_result):,} aggregated records\")\nprint(df_result.head())\n</code></pre>"},{"location":"quickstart.html#step-5-advanced-features","title":"Step 5: Advanced Features","text":""},{"location":"quickstart.html#visualize-your-pipeline","title":"Visualize Your Pipeline","text":"<pre><code># Open in visual editor\nff.open_graph_in_editor(pipeline.flow_graph)\n\n# This shows your entire pipeline\n# as a visual flow diagram!\n</code></pre>  Visual overview of pipeline"},{"location":"quickstart.html#export-as-pure-python","title":"Export as Pure Python","text":"<pre><code># Generate standalone code\ncode = pipeline.flow_graph.generate_code()\n\n# Deploy without Flowfile dependency!\n# Uses only Polars\n</code></pre>"},{"location":"quickstart.html#step-6-production-patterns","title":"Step 6: Production Patterns","text":"<p>Pattern 1: Data Quality Checks</p> <pre><code>from datetime import datetime\nimport flowfile as ff\n\ndef data_quality_pipeline(df: ff.FlowFrame) -&gt; ff.FlowFrame:\n    \"\"\"Reusable data quality check pipeline\"\"\"\n\n    # Record initial count\n    initial_count = df.select(ff.col(\"*\").count().alias(\"count\"))\n\n    # Apply quality filters\n    clean_df = (\n        df\n        # Remove nulls in critical fields\n        .drop_nulls(subset=[\"order_id\", \"customer_id\", \"amount\"])\n\n        # Validate data ranges\n        .filter(\n            (ff.col(\"amount\").is_between(0, 1000000)) &amp;\n            (ff.col(\"quantity\") &gt; 0) &amp;\n            (ff.col(\"order_date\") &lt;= datetime.now())\n        )\n\n        # Remove duplicates\n        .unique(subset=[\"order_id\"], keep=\"first\")\n    )\n\n    # Log quality metrics\n    final_count = clean_df.select(ff.col(\"*\").count().alias(\"count\"))\n    print(f\"Initial count: {initial_count.collect()[0]['count']}\")\n    print(f\"Final count after quality checks: {final_count.collect()[0]['count']}\")\n    return clean_df\n</code></pre> <p>Pattern 2: Incremental Processing</p> <pre><code># Read only new data since last run\nfrom datetime import datetime\nimport flowfile as ff\nlast_processed = datetime(2024, 10, 1)\n\nincremental_pipeline = (\n    ff.scan_parquet_from_cloud_storage(\n        \"s3://data-lake/events/\",\n        connection_name=\"production-data\"\n    )\n    .filter(ff.col(\"event_timestamp\") &gt; last_processed)\n    .group_by(ff.col(\"event_timestamp\").dt.date().alias(\"date\"))\n    .agg([\n        ff.col(\"event_id\").count().alias(\"event_count\"),\n        ff.col(\"user_id\").n_unique().alias(\"unique_users\")\n    ])\n)\n\n# Process and append to existing data\nincremental_pipeline.write_delta(\n    \"s3://data-warehouse/delta/daily_metrics\",\n    connection_name=\"production-data\",\n    write_mode=\"append\"\n)\n</code></pre> <p>Pattern 3: Multi-Source Join</p> <pre><code># Combine data from multiple sources\nimport flowfile as ff\n\ncustomers = ff.scan_parquet_from_cloud_storage(\n    \"s3://data-lake/customers/\",\n    connection_name=\"production-data\"\n)\n\norders = ff.scan_csv_from_cloud_storage(\n    \"s3://raw-data/orders/\",\n    connection_name=\"production-data\",\n    delimiter=\"|\",\n    has_header=True\n)\n\nproducts = ff.read_parquet(\"local_products.parquet\")\n\n# Complex multi-join pipeline\nenriched_orders = (\n    orders\n    .join(customers, on=\"customer_id\", how=\"left\")\n    .join(products, on=\"product_id\", how=\"left\")\n    .with_columns([\n        # Handle missing values from left joins\n        ff.col(\"customer_segment\").fill_null(\"Unknown\"),\n        ff.col(\"product_category\").fill_null(\"Other\"),\n\n        # Calculate metrics\n        (ff.col(\"unit_price\") * ff.col(\"quantity\") * \n         (ff.lit(1) - ff.col(\"discount_rate\").fill_null(0))).alias(\"net_revenue\")\n    ])\n)\n\n# Materialize results\nresults = enriched_orders.collect()\n</code></pre>"},{"location":"quickstart.html#next-steps-for-technical-users","title":"Next Steps for Technical Users","text":"Complete API Reference Architecture Deep Dive Core Internals Polars Documentation"},{"location":"quickstart.html#why-flowfile","title":"\ud83c\udf1f Why Flowfile?","text":"\u26a1 Performance <p>Built on Polars - Uses the speed of Polars</p> \ud83d\udd04 Dual Interface <p>Same pipeline works in both visual and code. Switch anytime, no lock-in.</p> \ud83d\udce6 Export to Production <p>Generate pure Python/Polars code. Deploy anywhere without Flowfile.</p> \u2601\ufe0f Cloud Support <p>Direct S3/cloud storage support, no need for expensive clusters to analyse your data</p>"},{"location":"quickstart.html#troubleshooting","title":"Troubleshooting","text":"Installation Issues <pre><code># If pip install fails, try:\npip install --upgrade pip\npip install flowfile\n\n# For M1/M2 Macs:\npip install flowfile --no-binary :all:\n\n# Behind corporate proxy:\npip install --proxy http://proxy.company.com:8080 flowfile\n</code></pre> Port Already in Use <pre><code># Find what's using port 63578\nlsof -i :63578  # Mac/Linux\nnetstat -ano | findstr :63578  # Windows\n\n# Kill the process or use different port:\nFLOWFILE_PORT=8080 flowfile run ui\n</code></pre>"},{"location":"quickstart.html#get-help","title":"Get Help","text":"Documentation Full Documentation Discussions GitHub Discussions Issues GitHub Issues Ready to Transform Your Data? <p>Join thousands of users building data pipelines with Flowfile</p> Start Visual (No Code) \u2192 Start Coding (Python) \u2192"},{"location":"for-developers/index.html","title":"Flowfile: For Developers","text":"<p>Welcome to the developer documentation for Flowfile. This is the home for anyone who wants to contribute to the platform or understand its internal architecture.</p> <p>Looking to use the Python API?</p> <p>If you want to use Flowfile's Python API to build data pipelines, check out the Python API User Guide. This developer section focuses on Flowfile's internal architecture and design philosophy.</p>"},{"location":"for-developers/index.html#the-core-philosophy-code-and-ui-are-the-same-thing","title":"The Core Philosophy: Code and UI are the Same Thing","text":"<p>Flowfile is built on an architecture where the Python API and the visual editor are two interfaces to the exact same underlying objects: the <code>FlowGraph</code> and its <code>FlowNodes</code>.</p> <p>When you write <code>df.filter(...)</code>, you programmatically construct a <code>FlowNode</code> and attach it to the <code>FlowGraph</code>. When a user drags a \"Filter\" node in the UI, they create the identical object. This dual interface philosophy means your work is never locked into one paradigm.</p> <ul> <li><code>FlowGraph</code>: The central orchestrator that holds the complete definition of your pipeline\u2014every node, setting, and connection.</li> <li><code>FlowNode</code>: An individual, executable step in your pipeline that wraps settings and logic.</li> <li><code>FlowDataEngine</code>: A smart wrapper around a Polars <code>LazyFrame</code> that carries the data and its schema between nodes.</li> </ul> <p>Learn more about this in our Dual Interface Philosophy guide.</p>"},{"location":"for-developers/index.html#getting-started-with-development","title":"Getting Started with Development","text":""},{"location":"for-developers/index.html#1-prerequisites","title":"1. Prerequisites","text":"<p>To contribute to Flowfile, you should be familiar with:</p> <ul> <li>Required Knowledge: Python 3.10+, and a basic familiarity with Polars or Pandas.</li> <li>Helpful Knowledge: Experience with Polars LazyFrames, Directed Acyclic Graphs (DAGs), and Pydantic.</li> </ul>"},{"location":"for-developers/index.html#2-set-up-your-environment","title":"2. Set Up Your Environment","text":"<p>Before diving in, clone the repository and install the dependencies using Poetry:</p> <pre><code># For development/contributing\ngit clone [https://github.com/edwardvaneechoud/Flowfile](https://github.com/edwardvaneechoud/Flowfile)\ncd Flowfile\npoetry install\n</code></pre>"},{"location":"for-developers/index.html#3-see-it-in-action-a-quick-example","title":"3. See It in Action: A Quick Example","text":"<p>The following code builds a data pipeline using the Python API. This same pipeline can be generated visually in the UI.</p> <pre><code>import flowfile as ff\nfrom flowfile import col\n\n# Create a FlowFrame from a local CSV\ndf = ff.read_csv(\"sales_data.csv\", description=\"Load raw sales data\")\n\n# Build a transformation pipeline with a familiar, chainable API\nprocessed_sales = (\n    df.filter(col(\"amount\") &gt; 100, description=\"Filter for significant sales\")\n    .with_columns(\n        (col(\"quantity\") * col(\"price\")).alias(\"total_revenue\")\n    )\n    .group_by(\"region\", description=\"Aggregate sales by region\")\n    .agg(\n        col(\"total_revenue\").sum()\n    )\n)\n\n# Get your results as a Polars DataFrame\nresults_df = processed_sales.collect()\nprint(results_df)\n</code></pre>"},{"location":"for-developers/index.html#documentation-guides","title":"Documentation Guides","text":"<ul> <li>Core Architecture: A deep dive into how <code>FlowGraph</code>, <code>FlowNode</code>, and <code>FlowDataEngine</code> work together.</li> <li>Technical Architecture: An overview of the system design, including the three-service architecture and performance optimizations.</li> <li>Python API Reference: The complete, auto-generated API reference for all core classes and methods.</li> <li>Visual UI Integration: Learn how to launch and control the visual editor from Python.</li> </ul>"},{"location":"for-developers/index.html#contributing-to-flowfile","title":"Contributing to Flowfile","text":"<p>We welcome contributions! Adding a new node requires changes across the stack:</p> <ul> <li>Backend: You'll need to define Pydantic setting models, implement the transformation logic in the <code>FlowDataEngine</code>, and register the new node in the <code>FlowGraph</code>.</li> <li>Frontend: Currently, you must also manually create a Vue component for the node's configuration form in the visual editor.</li> </ul> <p>For a more detailed breakdown, please read the Contributing section in our Design Philosophy guide.</p>"},{"location":"for-developers/architecture.html","title":"Technical Architecture","text":"<p>Flowfile's architecture integrates visual design with high-performance data processing through three interconnected services and utilizes Polars' lazy evaluation. This design provides real-time feedback, processes large datasets efficiently, and maintains UI responsiveness during intensive computations. On this page, the three-service architecture, key technical features such as real-time schema prediction and efficient data exchange, and the role of Polars' lazy evaluation are explained.</p> <p></p>"},{"location":"for-developers/architecture.html#core-components","title":"Core Components","text":""},{"location":"for-developers/architecture.html#three-service-architecture","title":"Three-Service Architecture","text":"<p>Designer (Electron + Vue) The visual interface where data pipelines are build through drag-and-drop operations. It communicates with the Core service to provide real-time feedback and displays data previews.</p> <p>Core Service (FastAPI) The orchestration engine that manages workflows, predicts schemas, and coordinates execution. It maintains the Directed Acyclic Graph (DAG) structure and handles all UI interactions and overall flow logic.</p> <p>Worker Service (FastAPI) Handles heavy data computations in isolated processes. It executes Polars transformations, materializes data, and manages data caching using Apache Arrow IPC format, preventing large datasets from overwhelming the Core service.</p>"},{"location":"for-developers/architecture.html#key-technical-features","title":"Key Technical Features","text":""},{"location":"for-developers/architecture.html#real-time-schema-prediction","title":"Real-time Schema Prediction","text":"<p>When you add or configure a node, Flowfile immediately shows how your data structure will change \u2014 without executing any transformations. This continuous feedback significantly reduces user errors and increases predictability. This happens through:</p> <ul> <li>Schema Callbacks: Custom functions, defined per node type, that calculate output schemas based on node settings and input schemas.</li> <li>Lazy Evaluation: Leveraging Polars' ability to determine the schema of a planned transformation (<code>LazyFrame</code>) without processing the full dataset.</li> </ul> View Schema Prediction Python Example <pre><code># Example: Schema prediction for a Group By operation\ndef schema_callback():\n    output_columns = [(c.old_name, c.new_name, c.output_type) for c in group_by_settings.groupby_input.agg_cols]\n    depends_on = node.node_inputs.main_inputs[0]\n    input_schema_dict: Dict[str, str] = {s.name: s.data_type for s in depends_on.schema}\n    output_schema = []\n    for old_name, new_name, data_type in output_columns:\n        data_type = input_schema_dict[old_name] if data_type is None else data_type\n        output_schema.append(FlowfileColumn.from_input(data_type=data_type, column_name=new_name))\n    return output_schema\n</code></pre>"},{"location":"for-developers/architecture.html#the-directed-acyclic-graph-dag-the-foundation-of-workflows","title":"The Directed Acyclic Graph (DAG): The Foundation of Workflows","text":"<p>As you add and connect nodes, Flowfile builds a Directed Acyclic Graph (DAG) where:</p> <ul> <li>Nodes represent data operations (read file, filter, join, write to database, etc.).</li> <li>Edges represent the flow of data between operations.</li> </ul> <p>The DAG is managed by the <code>EtlGraph</code> class in the Core service, which orchestrates the entire workflow:</p> View EtlGraph Python Implementation <pre><code>class EtlGraph:\n    \"\"\"\n    Manages the ETL workflow as a DAG. Stores nodes, dependencies,\n    and settings, and handles the execution order.\n    \"\"\"\n    uuid: str\n    _node_db: Dict[Union[str, int], NodeStep]  # Internal storage for all node steps\n    _flow_starts: List[NodeStep]               # Nodes that initiate data flow (e.g., readers)\n    _node_ids: List[Union[str, int]]           # Tracking node identifiers\n    flow_settings: schemas.FlowSettings        # Global configuration for the flow\n\n    def add_node_step(self, node_id: Union[int, str], function: Callable,\n                      node_type: str, **kwargs) -&gt; None:\n        \"\"\"Adds a new processing node (NodeStep) to the graph.\"\"\"\n        node_step = NodeStep(node_id=node_id, function=function, node_type=node_type, **kwargs)\n        self._node_db[node_id] = node_step\n        # Additional logic to manage dependencies and flow starts...\n\n    def run_graph(self) -&gt; RunInformation:\n        \"\"\"Executes the entire flow in the correct topological order.\"\"\"\n        execution_order = self.topological_sort() # Determine correct sequence\n        run_info = RunInformation()\n        for node in execution_order:\n            # Execute node based on mode (Development/Performance)\n            node_results = node.execute_node() # Simplified representation\n            run_info.add_result(node.node_id, node_results)\n        return run_info\n\n    def topological_sort(self) -&gt; List[NodeStep]:\n        \"\"\"Determines the correct order to execute nodes based on dependencies.\"\"\"\n        # Standard DAG topological sort algorithm...\n        pass\n</code></pre> <p>Each <code>NodeStep</code> in the graph encapsulates information about its dependencies, transformation logic, and output schema. This structure allows Flowfile to determine execution order, track data lineage, optimize performance, and provide schema predictions throughout the pipeline.</p>"},{"location":"for-developers/architecture.html#execution-modes","title":"Execution Modes","text":"<p>By clicking on settings \u2192 execution modes you can set how the flow will be executed the next time you run the flow.</p> <p></p> <p>Flowfile offers two execution modes tailored for different needs:</p> Feature Development Mode Performance Mode Purpose Interactive debugging, step inspection Optimized execution for production/speed Execution Executes node-by-node Builds full plan, executes minimally Data Caching Caches intermediate results per step Minimal caching (only if specified/needed) Preview Data Available for all nodes Only for final/cached nodes Memory Usage Potentially higher Generally lower Speed Moderate Faster for complex flows <p>Development Mode In Development mode, each node's transformation is triggered sequentially within the Worker service. Its intermediate result is typically serialized using Apache Arrow IPC format and cached to disk. This allows you to inspect the data at each step in the Designer via small samples fetched from the cache.</p> <p>Performance Mode In Performance mode, Flowfile fully embraces Polars' lazy evaluation. The Core service constructs the entire Polars execution plan based on the DAG. This plan (<code>LazyFrame</code>) is passed to the Worker service. The Worker only materializes (executes <code>.collect()</code> or <code>.sink_*()</code>) the plan when an output node (like writing to a file) requires the final result, or if a node is explicitly configured to cache its results (<code>node.cache_results</code>). This minimizes computation and memory usage by avoiding unnecessary intermediate materializations.</p> View Performance Mode Python Example <pre><code># Execution logic in Performance Mode (simplified)\ndef execute_performance_mode(self, node: NodeStep, is_output_node: bool):\n    \"\"\"Handles execution in performance mode, leveraging lazy evaluation.\"\"\"\n    if is_output_node or node.cache_results:\n        # If result is needed (output or caching), trigger execution in Worker\n        external_df_fetcher = ExternalDfFetcher(\n            lf=node.get_resulting_data().data_frame, # Pass the LazyFrame plan\n            file_ref=node.hash, # Unique reference for caching\n            wait_on_completion=False # Usually async\n        )\n        # Worker executes .collect() or .sink_*() and caches if needed\n        result = external_df_fetcher.get_result() # May return LazyFrame or trigger compute\n        return result # Or potentially just confirmation if sinking\n    else:\n        # If not output/cached, just pass the LazyFrame plan along\n        # No computation happens here for intermediate nodes\n        return node.get_resulting_data().data_frame\n</code></pre> <p>Crucially, all actual data processing and materialization of Polars DataFrames/LazyFrames happens in the Worker service. This separation prevents large datasets from overwhelming the Core service, ensuring the UI remains responsive.</p>"},{"location":"for-developers/architecture.html#efficient-data-exchange","title":"Efficient Data Exchange","text":"<p>Flowfile uses Apache Arrow IPC format for efficient inter-process communication between the Core and Worker services:</p> <ol> <li>Worker Processing &amp; Serialization: When the Worker needs to materialize data (either for intermediate caching in Development mode or final results), it computes the Polars DataFrame. The resulting DataFrame is serialized into the efficient Arrow IPC binary format.</li> <li>Disk Caching: This serialized data is saved to a temporary file on disk. This file acts as a cache, identified by a unique hash (<code>file_ref</code>). The Worker informs the Core that the result is ready at this <code>file_ref</code>.</li> <li>Core Fetching: If the Core (or subsequently, another Worker task) needs this data, it uses the <code>file_ref</code> to access the cached Arrow file directly. This avoids sending large datasets over network sockets between processes.</li> <li>UI Sampling: For UI previews, the Core requests a small sample (e.g., the first 100 rows) from the Worker. The Worker reads just the sample from the Arrow IPC file and sends only that lightweight data back to the Core, which forwards it to the Designer.</li> </ol> <p>This ensures responsiveness, memory isolation, and efficiency.</p> <p>Here\u2019s how the Core might offload computation to the Worker, and how the Worker manages the separate process execution:</p> View Core-Side Python Example <pre><code># Core side - Initiating remote execution in the Worker (simplified)\ndef execute_remote(self, performance_mode: bool = False) -&gt; None:\n    \"\"\"Offloads the execution of a node's LazyFrame to the Worker service.\"\"\"\n    # Create a fetcher instance to manage communication with the Worker\n    external_df_fetcher = ExternalDfFetcher(\n        lf=self.get_resulting_data().data_frame, # The Polars LazyFrame plan\n        file_ref=self.hash,                      # Unique identifier for the result/cache\n        wait_on_completion=False,                # Operate asynchronously\n    )\n\n    # Store the fetcher to potentially retrieve results later\n    self._fetch_cached_df = external_df_fetcher\n\n    # Request the Worker to start processing (this returns quickly)\n    # The actual computation happens asynchronously in the Worker\n    external_df_fetcher.start_processing_in_worker() # Hypothetical method name\n\n    # For UI updates, request a sample separately\n    self.store_example_data_generator(external_df_fetcher) # Fetches sample async\n</code></pre> View Worker-Side Python Example <pre><code># Worker side - Managing computation in a separate process (simplified)\ndef start_process(\n    polars_serializable_object: bytes, # Serialized LazyFrame plan\n    task_id: str,\n    file_ref: str, # Path for cached output (Arrow IPC file)\n    # ... other args like operation type\n) -&gt; None:\n    \"\"\"Launches a separate OS process to handle the heavy computation.\"\"\"\n    # Use multiprocessing context for safety\n    mp_context = multiprocessing.get_context('spawn') # or 'fork' depending on OS/needs\n\n    # Shared memory/queue for progress tracking and results/errors\n    progress = mp_context.Value('i', 0) # Shared integer for progress %\n    error_message = mp_context.Array('c', 1024) # Shared buffer for error messages\n    queue = mp_context.Queue(maxsize=1) # For potentially passing back results (or file ref)\n\n    # Define the target function and arguments for the new process\n    process = mp_context.Process(\n        target=process_task, # The function that runs Polars .collect()/.sink()\n        kwargs={\n            'polars_serializable_object': polars_serializable_object,\n            'progress': progress,\n            'error_message': error_message,\n            'queue': queue,\n            'file_path': file_ref, # Where to save the Arrow IPC output\n            # ... other necessary kwargs\n        }\n    )\n    process.start() # Launch the independent process\n\n    # Monitor the task (e.g., update status in a database, check progress)\n    handle_task(task_id, process, progress, error_message, queue)\n</code></pre>"},{"location":"for-developers/architecture.html#the-power-of-lazy-evaluation","title":"The Power of Lazy Evaluation","text":"<p>By building on Polars' lazy evaluation, Flowfile achieves:</p> <ul> <li>Memory Efficiency: Data is loaded and processed only when necessary, often streaming through operations without loading entire datasets into memory at once. This allows processing datasets larger than RAM.</li> <li>Query Optimization: Polars analyzes the entire execution plan and can reorder, combine, or eliminate operations for maximum efficiency.</li> <li>Parallel Execution: Polars automatically parallelizes operations across all available CPU cores during execution.</li> <li>Predicate Pushdown: Filters and selections are applied as early as possible in the plan, often directly at the data source level (like during file reading), minimizing the amount of data that needs to be processed downstream.</li> </ul>"},{"location":"for-developers/architecture.html#summary","title":"Summary","text":"<p>This design enables Flowfile to:</p> <ul> <li>Provide instant feedback without processing data.</li> <li>Handle datasets of any size efficiently.</li> <li>Keep the UI responsive during heavy computations.</li> <li>Scale from simple transformations to complex ETL pipelines.</li> </ul> <p>For a deep dive into the implementation details, see the full technical article.</p>"},{"location":"for-developers/creating-custom-nodes.html","title":"Creating Custom Nodes","text":"<p>Build your own data transformation nodes with custom UI components and processing logic.</p> <p>Beta Feature</p> <p>Custom nodes are currently in beta. Some features like changing the icon are still in development.</p>"},{"location":"for-developers/creating-custom-nodes.html#what-are-custom-nodes","title":"What Are Custom Nodes?","text":"<p>Custom nodes let you extend Flowfile with your own data transformations that appear as native nodes in the visual editor. Each custom node includes:</p> <ul> <li>Custom UI - Automatically generated settings panels with dropdowns, inputs, and toggles</li> <li>Data Processing - Polars-based transformation logic</li> <li>Visual Integration - Appears seamlessly in the node palette</li> </ul>"},{"location":"for-developers/creating-custom-nodes.html#quick-start","title":"Quick Start","text":""},{"location":"for-developers/creating-custom-nodes.html#1-create-your-first-node","title":"1. Create Your First Node","text":"<p>Create a new Python file in your custom nodes directory:</p> <pre><code>~/.flowfile/user_defined_nodes/my_first_node.py\n</code></pre> <p>Custom Node Location</p> <p>The <code>~/.flowfile/user_defined_nodes/</code> directory is automatically created when you first run Flowfile. Place all your custom nodes here.</p> <p>Here's a simple example that adds a greeting column:</p> <pre><code>import polars as pl\nfrom flowfile_core.flowfile.node_designer import (\n    CustomNodeBase,\n    Section,\n    NodeSettings,\n    TextInput,\n    SingleSelect,\n    ColumnSelector,\n    Types\n)\n\nclass GreetingSettings(NodeSettings):\n    main_config: Section = Section(\n        title=\"Greeting Configuration\",\n        description=\"Configure how to greet your data\",\n        name_column=ColumnSelector(\n            label=\"Name Column\",\n            data_types=Types.String,\n            required=True\n        ),\n        greeting_style=SingleSelect(\n            label=\"Greeting Style\",\n            options=[\n                (\"formal\", \"Formal (Hello, Mr/Ms)\"),\n                (\"casual\", \"Casual (Hey there!)\"),\n                (\"enthusiastic\", \"Enthusiastic (OMG HI!!!)\"),\n            ],\n            default=\"casual\"\n        ),\n        custom_message=TextInput(\n            label=\"Custom Message\",\n            default=\"Nice to meet you!\",\n            placeholder=\"Enter your custom greeting...\"\n        )\n    )\n\nclass GreetingNode(CustomNodeBase):\n    node_name: str = \"Greeting Generator\"\n    node_category: str = \"Text Processing\"\n    title: str = \"Add Personal Greetings\"\n    intro: str = \"Transform names into personalized greetings\"\n\n    settings_schema: GreetingSettings = GreetingSettings()\n\n    def process(self, input_df: pl.LazyFrame) -&gt; pl.LazyFrame:\n        # Get values from the UI\n        name_col = self.settings_schema.main_config.name_column.value\n        style = self.settings_schema.main_config.greeting_style.value\n        custom = self.settings_schema.main_config.custom_message.value\n\n        # Define greeting logic\n        if style == \"formal\":\n            greeting_expr = pl.concat_str([\n                pl.lit(\"Hello, \"), \n                pl.col(name_col), \n                pl.lit(f\". {custom}\")\n            ])\n        elif style == \"casual\":\n            greeting_expr = pl.concat_str([\n                pl.lit(\"Hey \"), \n                pl.col(name_col), \n                pl.lit(f\"! {custom}\")\n            ])\n        else:  # enthusiastic\n            greeting_expr = pl.concat_str([\n                pl.lit(\"OMG HI \"), \n                pl.col(name_col).str.to_uppercase(), \n                pl.lit(f\"!!! {custom} \ud83c\udf89\")\n            ])\n\n        return input_df.with_columns([\n            greeting_expr.alias(\"greeting\")\n        ])\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#2-use-your-node","title":"2. Use Your Node","text":"<ol> <li>Restart Flowfile to load your new node</li> <li>Open the visual editor</li> <li>Find your node in the \"Text Processing\" category</li> <li>Drag it onto the canvas</li> <li>Configure the settings in the right panel</li> <li>Run your flow and see the results!</li> </ol>  Visual overview of the result!"},{"location":"for-developers/creating-custom-nodes.html#understanding-the-architecture","title":"Understanding the Architecture","text":""},{"location":"for-developers/creating-custom-nodes.html#node-structure","title":"Node Structure","text":"<p>Every custom node has three main parts:</p> <pre><code>class MyCustomNode(CustomNodeBase):\n    # 1. Metadata - How the node appears in Flowfile\n    node_name: str = \"My Amazing Node\"\n    node_category: str = \"Data Enhancement\"\n    title: str = \"Add Personal Greetings\"\n    intro: str = \"Transform names into personalized greetings\"\n\n    # 2. Settings Schema - The UI configuration\n    settings_schema: MySettings = MySettings()\n\n    # 3. Processing Logic - What the node actually does\n    def process(self, input_df: pl.LazyFrame) -&gt; pl.LazyFrame:\n        # Your transformation logic here\n        return modified_df\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#3-process","title":"3. Process","text":"<p>Only one input supported for now</p> <p>Currently, custom nodes only support a single input DataFrame. Support for multiple inputs is planned for future releases.</p> <p>The process method is the engine of your node. This is where you write your Polars code to transform the data.</p> <ul> <li> <p>Input: The method receives the incoming data as a Polars LazyFrame. If your node has multiple inputs, they will be passed as separate arguments (e.g., process(self, df1)).</p> </li> <li> <p>Accessing Settings: Inside this method, you can get the current values from your UI components using self.settings_schema...value. <li> <p>Output: The method must return a single Polars LazyFrame, which becomes the output of your node.</p> </li>"},{"location":"for-developers/creating-custom-nodes.html#settings-architecture","title":"Settings Architecture","text":"<p>Settings are organized in sections for clean UI organization:</p> <pre><code>class MyNodeSettings(NodeSettings):\n    # Each section becomes a collapsible panel in the UI\n    basic_config: Section = Section(\n        title=\"Basic Settings\",\n        description=\"Core functionality options\",\n        # Components go here as keyword arguments\n        input_column=ColumnSelector(...),\n        operation_type=SingleSelect(...)\n    )\n\n    advanced_options: Section = Section(\n        title=\"Advanced Options\",\n        description=\"Fine-tune behavior\",\n        enable_caching=ToggleSwitch(...),\n        max_iterations=NumericInput(...)\n    )\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#available-ui-components","title":"Available UI Components","text":""},{"location":"for-developers/creating-custom-nodes.html#text-input","title":"Text Input","text":"<p>For capturing string values:</p> <pre><code>text_field = TextInput(\n    label=\"Enter a value\",\n    default=\"Default text\",\n    placeholder=\"Hint text here...\"\n)\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#numeric-input","title":"Numeric Input","text":"<p>For numbers with optional validation:</p> <pre><code>number_field = NumericInput(\n    label=\"Count\",\n    default=10,\n    min_value=1,\n    max_value=100\n)\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#single-select","title":"Single Select","text":"<p>Dropdown for one choice:</p> <pre><code>choice_field = SingleSelect(\n    label=\"Choose option\",\n    options=[\n        (\"value1\", \"Display Name 1\"),\n        (\"value2\", \"Display Name 2\"),\n        (\"simple\", \"Simple String Option\")\n    ],\n    default=\"value1\"\n)\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#multi-select","title":"Multi Select","text":"<p>For selecting multiple options:</p> <pre><code>multi_field = MultiSelect(\n    label=\"Select multiple\",\n    options=[\n        (\"opt1\", \"Option 1\"),\n        (\"opt2\", \"Option 2\"),\n        (\"opt3\", \"Option 3\")\n    ],\n    default=[\"opt1\", \"opt2\"]\n)\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#toggle-switch","title":"Toggle Switch","text":"<p>Boolean on/off control:</p> <pre><code>toggle_field = ToggleSwitch(\n    label=\"Enable feature\",\n    default=True,\n    description=\"Turn this on to enable the feature\"\n)\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#column-selector","title":"Column Selector","text":"<p>Smart column picker with type filtering:</p> <pre><code># Select any column\nany_column = ColumnSelector(\n    label=\"Pick a column\",\n    data_types=Types.All\n)\n\n# Select only numeric columns\nnumeric_column = ColumnSelector(\n    label=\"Numeric column only\",\n    data_types=Types.Numeric,\n    required=True\n)\n\n# Select multiple string columns\ntext_columns = ColumnSelector(\n    label=\"Text columns\",\n    data_types=[Types.String, Types.Categorical],\n    multiple=True\n)\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#dynamic-column-options","title":"Dynamic Column Options","text":"<p>Use <code>IncomingColumns</code> for dropdowns that populate with input columns:</p> <pre><code>column_dropdown = SingleSelect(\n    label=\"Choose input column\",\n    options=IncomingColumns  # Automatically filled with column names\n)\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#type-filtering-in-column-selector","title":"Type Filtering in Column Selector","text":"<p>The <code>Types</code> object provides convenient type filtering:</p> <pre><code>from flowfile_core.flowfile.node_designer import Types\n\n# Type groups\nTypes.Numeric    # All numeric types\nTypes.String     # String and categorical\nTypes.Date       # Date, datetime, time\nTypes.Boolean    # Boolean columns\nTypes.All        # All column types\n\n# Specific types\nTypes.Int64      # 64-bit integers\nTypes.Float      # Float64\nTypes.Decimal    # Decimal types\n\n# Mix and match\ndata_types=[Types.Numeric, Types.Date]  # Numbers and dates only\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#real-world-examples","title":"Real-World Examples","text":""},{"location":"for-developers/creating-custom-nodes.html#data-quality-node","title":"Data Quality Node","text":"<pre><code>class DataQualityNode(CustomNodeBase):\n    node_name: str = \"Data Quality Checker\"\n    node_category: str = \"Data Validation\"\n\n    settings_schema: DataQualitySettings = DataQualitySettings(\n        validation_rules=Section(\n            title=\"Validation Rules\",\n            columns_to_check=ColumnSelector(\n                label=\"Columns to Validate\",\n                data_types=Types.All,\n                multiple=True\n            ),\n            null_threshold=NumericInput(\n                label=\"Max Null Percentage\",\n                default=5.0,\n                min_value=0,\n                max_value=100\n            ),\n            add_summary=ToggleSwitch(\n                label=\"Add Quality Summary\",\n                default=True\n            )\n        )\n    )\n\n    def process(self, input_df: pl.LazyFrame) -&gt; pl.LazyFrame:\n        columns = self.settings_schema.validation_rules.columns_to_check.value\n        threshold = self.settings_schema.validation_rules.null_threshold.value\n\n        # Calculate quality metrics\n        quality_checks = []\n        for col in columns:\n            null_pct = (input_df[col].is_null().sum() / len(input_df)) * 100\n            quality_checks.append({\n                \"column\": col,\n                \"null_percentage\": null_pct,\n                \"quality_flag\": \"PASS\" if null_pct &lt;= threshold else \"FAIL\"\n            })\n\n        # Add quality flags to original data\n        result_df = input_df\n        for check in quality_checks:\n            if check[\"quality_flag\"] == \"FAIL\":\n                result_df = result_df.with_columns([\n                    pl.col(check[\"column\"]).is_null().alias(f\"{check['column']}_has_issues\")\n                ])\n\n        return result_df\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#text-processing-node","title":"Text Processing Node","text":"<pre><code>class TextCleanerNode(CustomNodeBase):\n    node_name: str = \"Text Cleaner\"\n    node_category: str = \"Text Processing\"\n\n    settings_schema: TextCleanerSettings = TextCleanerSettings(\n        cleaning_options=Section(\n            title=\"Cleaning Options\",\n            text_column=ColumnSelector(\n                label=\"Text Column\",\n                data_types=Types.String,\n                required=True\n            ),\n            operations=MultiSelect(\n                label=\"Cleaning Operations\",\n                options=[\n                    (\"lowercase\", \"Convert to lowercase\"),\n                    (\"remove_punctuation\", \"Remove punctuation\"),\n                    (\"remove_extra_spaces\", \"Remove extra spaces\"),\n                    (\"remove_numbers\", \"Remove numbers\"),\n                    (\"trim\", \"Trim whitespace\")\n                ],\n                default=[\"lowercase\", \"trim\"]\n            ),\n            output_column=TextInput(\n                label=\"Output Column Name\",\n                default=\"cleaned_text\"\n            )\n        )\n    )\n\n    def process(self, input_df: pl.LazyFrame) -&gt; pl.LazyFrame:\n        text_col = self.settings_schema.cleaning_options.text_column.value\n        operations = self.settings_schema.cleaning_options.operations.value\n        output_col = self.settings_schema.cleaning_options.output_column.value\n\n        # Start with original text\n        expr = pl.col(text_col)\n\n        # Apply selected operations\n        if \"lowercase\" in operations:\n            expr = expr.str.to_lowercase()\n        if \"remove_punctuation\" in operations:\n            expr = expr.str.replace_all(r\"[^\\w\\s]\", \"\")\n        if \"remove_extra_spaces\" in operations:\n            expr = expr.str.replace_all(r\"\\s+\", \" \")\n        if \"remove_numbers\" in operations:\n            expr = expr.str.replace_all(r\"\\d+\", \"\")\n        if \"trim\" in operations:\n            expr = expr.str.strip_chars()\n\n        return input_df.with_columns([expr.alias(output_col)])\n</code></pre>"},{"location":"for-developers/creating-custom-nodes.html#best-practices","title":"Best Practices","text":""},{"location":"for-developers/creating-custom-nodes.html#1-performance","title":"1. Performance","text":"<p>Try to use Polars expressions and lazy evaluation to keep your nodes efficient.  A collect will be executed in the core process and can cause issues when using remote compute.</p>"},{"location":"for-developers/creating-custom-nodes.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"for-developers/creating-custom-nodes.html#node-doesnt-appear","title":"Node Doesn't Appear","text":"<ol> <li>Check the file is in <code>~/.flowfile/user_defined_nodes/</code></li> <li>Restart Flowfile completely</li> <li>Check for Python syntax errors in terminal</li> <li>Ensure your class inherits from <code>CustomNodeBase</code></li> </ol>"},{"location":"for-developers/creating-custom-nodes.html#settings-dont-work","title":"Settings Don't Work","text":"<ol> <li>Verify <code>settings_schema</code> is properly assigned</li> <li>Check component imports</li> <li>Ensure section structure is correct</li> <li>Use <code>.value</code> to access component values in <code>process()</code></li> </ol>"},{"location":"for-developers/creating-custom-nodes.html#processing-errors","title":"Processing Errors","text":"<ol> <li>Check input DataFrame exists and has expected columns</li> <li>Use <code>print()</code> or logging for debugging</li> <li>Handle null values and edge cases</li> <li>Ensure return type is <code>pl.LazyFrame</code></li> </ol>"},{"location":"for-developers/creating-custom-nodes.html#coming-soon","title":"Coming Soon","text":"<p>The following features are planned for future releases:</p> <ul> <li>Node Templates - Quick-start templates for common patterns</li> <li>Custom Icons - Upload custom icons for your nodes</li> <li>Node Categories - Create your own node categories</li> <li>Testing Framework - Built-in testing for custom nodes</li> <li>Node Publishing - Share nodes with the community</li> </ul> <p>Ready to build? Start with the Custom Node Tutorial for a step-by-step walkthrough!</p>"},{"location":"for-developers/custom-node-tutorial.html","title":"Custom Node Tutorial: Build an Emoji Generator","text":"<p>Learn to create custom nodes by building a fun emoji generator that transforms boring data into expressive datasets with visual flair.</p> <p>What You'll Build</p> <p>By the end of this tutorial, you'll have created a fully functional \"Emoji Generator\" node that: - Takes numeric data and converts it into mood-based emojis - Provides 7 different emoji themes (performance, temperature, money, etc.) - Includes intensity controls and random sparkle effects - Features a sophisticated multi-section UI</p>"},{"location":"for-developers/custom-node-tutorial.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Flowfile installed and working (<code>pip install flowfile</code>)</li> <li>Basic understanding of Python</li> <li>Familiarity with Polars DataFrames (helpful but not required)</li> </ul>"},{"location":"for-developers/custom-node-tutorial.html#step-1-set-up-your-development-environment","title":"Step 1: Set Up Your Development Environment","text":"<p>First, locate your custom nodes directory:</p> <pre><code># Check if the directory exists\nls ~/.flowfile/user_defined_nodes/\n\n# If it doesn't exist, start Flowfile once to create it\nflowfile run ui\n</code></pre> <p>Create your node file:</p> <pre><code>touch ~/.flowfile/user_defined_nodes/emoji_generator.py\n</code></pre>"},{"location":"for-developers/custom-node-tutorial.html#step-2-import-required-components","title":"Step 2: Import Required Components","text":"<p>Start by importing all the components you'll need:</p> <pre><code>import polars as pl\nimport random\nfrom typing import List\n\nfrom flowfile_core.flowfile.node_designer import (\n    CustomNodeBase,\n    Section, \n    NodeSettings,\n    TextInput,\n    NumericInput,\n    SingleSelect,\n    ToggleSwitch,\n    ColumnSelector,\n    MultiSelect,\n    Types\n)\n</code></pre>"},{"location":"for-developers/custom-node-tutorial.html#step-3-design-your-settings-schema","title":"Step 3: Design Your Settings Schema","text":"<p>We'll create a two-section UI: one for mood detection and one for styling options.</p>"},{"location":"for-developers/custom-node-tutorial.html#create-the-first-section","title":"Create the First Section","text":"<pre><code>class EmojiMoodSection(Section):\n    source_column: ColumnSelector = ColumnSelector(\n        label=\"Analyze This Column\",\n        multiple=False,\n        required=True,\n        data_types=Types.Numeric  # Only show numeric columns\n    )\n\n    mood_type: SingleSelect = SingleSelect(\n        label=\"Emoji Mood Logic\",\n        options=[\n            (\"performance\", \"\ud83d\udcc8 Performance Based (High = \ud83d\ude0e, Low = \ud83d\ude30)\"),\n            (\"temperature\", \"\ud83c\udf21\ufe0f Temperature (Hot = \ud83d\udd25, Cold = \ud83e\uddca)\"),\n            (\"money\", \"\ud83d\udcb0 Money Mode (Rich = \ud83e\udd11, Poor = \ud83d\ude22)\"),\n            (\"energy\", \"\u26a1 Energy Level (High = \ud83d\ude80, Low = \ud83d\udd0b)\"),\n            (\"love\", \"\u2764\ufe0f Love Meter (High = \ud83d\ude0d, Low = \ud83d\udc94)\"),\n            (\"chaos\", \"\ud83c\udfb2 Pure Chaos (Random emojis!)\"),\n            (\"pizza\", \"\ud83c\udf55 Pizza Scale (Everything becomes pizza)\")\n        ],\n        default=\"performance\"\n    )\n\n    threshold_value: NumericInput = NumericInput(\n        label=\"Mood Threshold\",\n        default=50.0,\n        min_value=0,\n        max_value=100\n    )\n\n    emoji_column_name: TextInput = TextInput(\n        label=\"New Emoji Column Name\",\n        default=\"mood_emoji\",\n        placeholder=\"Name your emoji column...\"\n    )\n</code></pre>"},{"location":"for-developers/custom-node-tutorial.html#create-the-second-section","title":"Create the Second Section","text":"<pre><code>class EmojiStyleSection(Section):\n    emoji_intensity: SingleSelect = SingleSelect(\n        label=\"Emoji Intensity\",\n        options=[\n            (\"subtle\", \"\ud83d\ude10 Subtle (One emoji)\"),\n            (\"normal\", \"\ud83d\ude0a Normal (1-2 emojis)\"),\n            (\"extra\", \"\ud83e\udd29 Extra (2-3 emojis)\"),\n            (\"maximum\", \"\ud83e\udd2f\ud83c\udf89\ud83d\ude80 MAXIMUM OVERDRIVE\")\n        ],\n        default=\"normal\"\n    )\n\n    add_random_sparkle: ToggleSwitch = ToggleSwitch(\n        label=\"Add Random Sparkles \u2728\",\n        default=True,\n        description=\"Randomly sprinkle \u2728 for extra pizzazz\"\n    )\n\n    emoji_categories: MultiSelect = MultiSelect(\n        label=\"Allowed Emoji Categories\",\n        options=[\n            (\"faces\", \"\ud83d\ude00 Faces &amp; Emotions\"),\n            (\"animals\", \"\ud83e\udd84 Animals\"),\n            (\"food\", \"\ud83c\udf54 Food &amp; Drink\"),\n            (\"nature\", \"\ud83c\udf08 Nature\"),\n            (\"objects\", \"\ud83c\udfae Objects\"),\n            (\"symbols\", \"\ud83d\udcaf Symbols\"),\n            (\"flags\", \"\ud83c\udff4\u200d\u2620\ufe0f Flags\")\n        ],\n        default=[\"faces\", \"animals\", \"food\"]\n    )\n</code></pre>"},{"location":"for-developers/custom-node-tutorial.html#combine-sections-into-settings","title":"Combine Sections into Settings","text":"<pre><code>class EmojiSettings(NodeSettings):\n    mood_config: EmojiMoodSection = EmojiMoodSection(\n        title=\"Mood Detection \ud83d\ude0a\",\n        description=\"Configure how to detect the vibe of your data\"\n    )\n\n    style_options: EmojiStyleSection = EmojiStyleSection(\n        title=\"Emoji Style \ud83c\udfa8\",\n        description=\"Fine-tune your emoji experience\"\n    )\n</code></pre>"},{"location":"for-developers/custom-node-tutorial.html#step-4-create-the-main-node-class","title":"Step 4: Create the Main Node Class","text":"<pre><code>class EmojiGenerator(CustomNodeBase):\n    # Node metadata - how it appears in Flowfile\n    node_name: str = \"Emoji Generator \ud83c\udf89\"\n    node_category: str = \"Fun Stuff\"  # This creates a new category\n    node_group: str = \"custom\"\n    title: str = \"Emoji Generator\"\n    intro: str = \"Transform boring data into fun emoji-filled datasets! \ud83d\ude80\"\n\n    # I/O configuration\n    number_of_inputs: int = 1\n    number_of_outputs: int = 1\n\n    # Link to our settings schema\n    settings_schema: EmojiSettings = EmojiSettings()\n\n    def process(self, input_df: pl.DataFrame) -&gt; pl.DataFrame:\n        # We'll implement this in the next step\n        pass\n</code></pre>"},{"location":"for-developers/custom-node-tutorial.html#step-5-implement-the-processing-logic","title":"Step 5: Implement the Processing Logic","text":"<p>Now for the fun part - the actual emoji generation logic:</p> <pre><code>def process(self, input_df: pl.DataFrame) -&gt; pl.DataFrame:\n    # Get settings values from the UI\n    column_name = self.settings_schema.mood_config.source_column.value\n    mood_type = self.settings_schema.mood_config.mood_type.value\n    threshold = self.settings_schema.mood_config.threshold_value.value\n    emoji_col_name = self.settings_schema.mood_config.emoji_column_name.value\n    intensity = self.settings_schema.style_options.emoji_intensity.value\n    add_sparkle = self.settings_schema.style_options.add_random_sparkle.value\n\n    # Define emoji sets for different moods\n    emoji_sets = {\n        \"performance\": {\n            \"high\": [\"\ud83d\ude0e\", \"\ud83d\udcaa\", \"\ud83c\udfc6\", \"\ud83d\udc51\", \"\ud83c\udf1f\", \"\ud83d\udcaf\", \"\ud83d\udd25\"],\n            \"low\": [\"\ud83d\ude30\", \"\ud83d\ude13\", \"\ud83d\udcc9\", \"\ud83d\ude22\", \"\ud83d\udc94\", \"\ud83c\udd98\", \"\ud83d\ude35\"]\n        },\n        \"temperature\": {\n            \"high\": [\"\ud83d\udd25\", \"\ud83c\udf0b\", \"\u2600\ufe0f\", \"\ud83e\udd75\", \"\ud83c\udf21\ufe0f\", \"\u2668\ufe0f\", \"\ud83c\udfd6\ufe0f\"],\n            \"low\": [\"\ud83e\uddca\", \"\u2744\ufe0f\", \"\u26c4\", \"\ud83e\udd76\", \"\ud83c\udf28\ufe0f\", \"\ud83c\udfd4\ufe0f\", \"\ud83d\udc27\"]\n        },\n        \"money\": {\n            \"high\": [\"\ud83e\udd11\", \"\ud83d\udcb0\", \"\ud83d\udc8e\", \"\ud83c\udfe6\", \"\ud83d\udcb3\", \"\ud83e\ude99\", \"\ud83d\udcc8\"],\n            \"low\": [\"\ud83d\ude22\", \"\ud83d\udcb8\", \"\ud83d\udcc9\", \"\ud83c\udfda\ufe0f\", \"\ud83d\ude2d\", \"\ud83e\udd7a\", \"\ud83d\udcca\"]\n        },\n        \"energy\": {\n            \"high\": [\"\ud83d\ude80\", \"\u26a1\", \"\ud83d\udca5\", \"\ud83c\udfaf\", \"\ud83c\udfc3\", \"\ud83e\udd38\", \"\ud83c\udfaa\"],\n            \"low\": [\"\ud83d\udd0b\", \"\ud83d\ude34\", \"\ud83d\udecc\", \"\ud83d\udc0c\", \"\ud83e\udd71\", \"\ud83d\ude2a\", \"\ud83d\udca4\"]\n        },\n        \"love\": {\n            \"high\": [\"\ud83d\ude0d\", \"\u2764\ufe0f\", \"\ud83d\udc95\", \"\ud83e\udd70\", \"\ud83d\udc98\", \"\ud83d\udc9d\", \"\ud83d\udc68\u200d\u2764\ufe0f\u200d\ud83d\udc68\"],\n            \"low\": [\"\ud83d\udc94\", \"\ud83d\ude22\", \"\ud83d\ude2d\", \"\ud83e\udd40\", \"\ud83d\ude14\", \"\ud83d\udc80\", \"\ud83d\udda4\"]\n        },\n        \"chaos\": {\n            \"high\": [\"\ud83e\udd96\", \"\ud83c\udfb8\", \"\ud83d\ude81\", \"\ud83c\udfaa\", \"\ud83e\udd9c\", \"\ud83c\udfad\", \"\ud83c\udff4\u200d\u2620\ufe0f\"],\n            \"low\": [\"\ud83e\udd54\", \"\ud83e\udde6\", \"\ud83d\udcce\", \"\ud83e\uddb7\", \"\ud83e\uddf2\", \"\ud83d\udd0c\", \"\ud83e\udea3\"]\n        },\n        \"pizza\": {\n            \"high\": [\"\ud83c\udf55\", \"\ud83c\udf55\ud83c\udf55\", \"\ud83c\udf55\ud83d\udd25\", \"\ud83c\udf55\ud83d\ude0d\", \"\ud83c\udf55\ud83c\udf89\", \"\ud83c\udf55\ud83d\udcaf\", \"\ud83c\udf55\ud83d\udc51\"],\n            \"low\": [\"\ud83c\udf55\", \"\ud83c\udf55\ud83d\ude22\", \"\ud83c\udf55\ud83d\udc94\", \"\ud83c\udf55\ud83d\ude2d\", \"\ud83c\udf55\ud83e\udd7a\", \"\ud83c\udf55\ud83d\ude14\", \"\ud83c\udf55\"]\n        }\n    }\n\n    # Helper function to get emoji based on value\n    def get_emoji(value, mood_type, threshold, intensity):\n        if value is None:\n            return \"\u2753\"\n\n        emoji_list = emoji_sets.get(mood_type, emoji_sets[\"performance\"])\n\n        if mood_type == \"chaos\":\n            # Random emoji from both lists\n            all_emojis = emoji_list[\"high\"] + emoji_list[\"low\"]\n            base_emoji = random.choice(all_emojis)\n        elif mood_type == \"pizza\":\n            # Everything is pizza\n            base_emoji = \"\ud83c\udf55\"\n        else:\n            # Use threshold to determine high/low\n            if value &gt;= threshold:\n                base_emoji = random.choice(emoji_list[\"high\"])\n            else:\n                base_emoji = random.choice(emoji_list[\"low\"])\n\n        # Add intensity\n        if intensity == \"subtle\":\n            result = base_emoji\n        elif intensity == \"normal\":\n            result = base_emoji\n            if random.random() &gt; 0.5:\n                result += random.choice([\"\", \"\u2728\", \"\"])\n        elif intensity == \"extra\":\n            extras = [\"\u2728\", \"\ud83d\udcab\", \"\u2b50\", \"\"]\n            result = base_emoji + random.choice(extras) + random.choice(extras)\n        else:  # maximum\n            chaos_emojis = [\"\ud83c\udf89\", \"\ud83d\ude80\", \"\ud83d\udca5\", \"\ud83c\udf08\", \"\u2728\", \"\ud83d\udd25\", \"\ud83d\udcaf\", \"\u26a1\"]\n            result = base_emoji + \"\".join(random.choices(chaos_emojis, k=3))\n\n        # Add random sparkle\n        if add_sparkle and random.random() &gt; 0.7:\n            result += \"\u2728\"\n\n        return result\n\n    # Create emoji column using map_elements\n    emoji_expr = (\n        pl.col(column_name)\n        .map_elements(\n            lambda x: get_emoji(x, mood_type, threshold, intensity),\n            return_dtype=pl.String\n        )\n        .alias(emoji_col_name)\n    )\n\n    # Add bonus columns based on intensity\n    if intensity == \"maximum\":\n        # Add extra fun columns in maximum mode\n        return input_df.with_columns([\n            emoji_expr,\n            pl.lit(\"\ud83c\udf89 PARTY MODE \ud83c\udf89\").alias(\"vibe_check\"),\n            pl.col(column_name).map_elements(\n                lambda x: \"\ud83d\udd25\" * min(int((x or 0) / 20), 5) if x else \"\ud83d\udca4\",\n                return_dtype=pl.String\n            ).alias(\"fire_meter\")\n        ])\n    else:\n        return input_df.with_columns([emoji_expr])\n</code></pre>"},{"location":"for-developers/custom-node-tutorial.html#step-6-test-your-node","title":"Step 6: Test Your Node","text":"<p>Save your file and test it:</p> <ol> <li> <p>Restart Flowfile completely:    <pre><code># Stop any running Flowfile processes\n# Then start again\nflowfile run ui\n</code></pre></p> </li> <li> <p>Create a test flow:</p> </li> <li>Create a new flow</li> <li> <p>Add a \"Manual Input\" node with some test data:     <pre><code>[\n  {\n    \"name\": \"bob\",\n    \"value\": \"21\"\n  },\n  {\n    \"name\": \"magret\",\n    \"value\": \"62.1\"\n  },\n  {\n    \"name\": \"fish\",\n    \"value\": \"1.2\"\n  },\n  {\n    \"name\": \"dog\",\n    \"value\": \"20\"\n  }\n]\n</code></pre></p> </li> <li> <p>Look for your \"Emoji Generator \ud83c\udf89\" in the \"User defined operations\" category</p> </li> <li>Connect it to your manual input</li> <li>Configure the settings and run!</li> </ol>  Visual overview of the result!  <p></p>"},{"location":"for-developers/custom-node-tutorial.html#performance-tips","title":"Performance Tips","text":"<ol> <li>Use Polars expressions instead of Python loops when possible</li> <li>Avoid collecting DataFrames in the middle of processing</li> <li>Handle large datasets by checking input size and warning users</li> <li>Cache expensive operations like random number generation</li> <li>Use lazy evaluation - let Polars optimize the query plan</li> </ol>"},{"location":"for-developers/custom-node-tutorial.html#complete-working-example","title":"Complete Working Example","text":"<p>Here's the complete, working emoji generator node:</p> <pre><code>import polars as pl\nimport random\nfrom typing import List\n\nfrom flowfile_core.flowfile.node_designer import (\n    CustomNodeBase,\n    Section,\n    NodeSettings,\n    TextInput,\n    NumericInput,\n    SingleSelect,\n    ToggleSwitch,\n    ColumnSelector,\n    MultiSelect,\n    Types\n)\n\n\nclass EmojiMoodSection(Section):\n    source_column: ColumnSelector = ColumnSelector(\n        label=\"Analyze This Column\",\n        multiple=False,\n        required=True,\n        data_types=Types.Numeric  # Only show numeric columns\n    )\n\n    mood_type: SingleSelect = SingleSelect(\n        label=\"Emoji Mood Logic\",\n        options=[\n            (\"performance\", \"\ud83d\udcc8 Performance Based (High = \ud83d\ude0e, Low = \ud83d\ude30)\"),\n            (\"temperature\", \"\ud83c\udf21\ufe0f Temperature (Hot = \ud83d\udd25, Cold = \ud83e\uddca)\"),\n            (\"money\", \"\ud83d\udcb0 Money Mode (Rich = \ud83e\udd11, Poor = \ud83d\ude22)\"),\n            (\"energy\", \"\u26a1 Energy Level (High = \ud83d\ude80, Low = \ud83d\udd0b)\"),\n            (\"love\", \"\u2764\ufe0f Love Meter (High = \ud83d\ude0d, Low = \ud83d\udc94)\"),\n            (\"chaos\", \"\ud83c\udfb2 Pure Chaos (Random emojis!)\"),\n            (\"pizza\", \"\ud83c\udf55 Pizza Scale (Everything becomes pizza)\")\n        ],\n        default=\"performance\"\n    )\n\n    threshold_value: NumericInput = NumericInput(\n        label=\"Mood Threshold\",\n        default=50.0,\n        min_value=0,\n        max_value=100\n    )\n\n    emoji_column_name: TextInput = TextInput(\n        label=\"New Emoji Column Name\",\n        default=\"mood_emoji\",\n        placeholder=\"Name your emoji column...\"\n    )\n\n\nclass EmojiStyleSection(Section):\n    emoji_intensity: SingleSelect = SingleSelect(\n        label=\"Emoji Intensity\",\n        options=[\n            (\"subtle\", \"\ud83d\ude10 Subtle (One emoji)\"),\n            (\"normal\", \"\ud83d\ude0a Normal (1-2 emojis)\"),\n            (\"extra\", \"\ud83e\udd29 Extra (2-3 emojis)\"),\n            (\"maximum\", \"\ud83e\udd2f\ud83c\udf89\ud83d\ude80 MAXIMUM OVERDRIVE\")\n        ],\n        default=\"normal\"\n    )\n\n    add_random_sparkle: ToggleSwitch = ToggleSwitch(\n        label=\"Add Random Sparkles \u2728\",\n        default=True,\n        description=\"Randomly sprinkle \u2728 for extra pizzazz\"\n    )\n\n    emoji_categories: MultiSelect = MultiSelect(\n        label=\"Allowed Emoji Categories\",\n        options=[\n            (\"faces\", \"\ud83d\ude00 Faces &amp; Emotions\"),\n            (\"animals\", \"\ud83e\udd84 Animals\"),\n            (\"food\", \"\ud83c\udf54 Food &amp; Drink\"),\n            (\"nature\", \"\ud83c\udf08 Nature\"),\n            (\"objects\", \"\ud83c\udfae Objects\"),\n            (\"symbols\", \"\ud83d\udcaf Symbols\"),\n            (\"flags\", \"\ud83c\udff4\u200d\u2620\ufe0f Flags\")\n        ],\n        default=[\"faces\", \"animals\", \"food\"]\n    )\n\n\nclass EmojiSettings(NodeSettings):\n    mood_config: EmojiMoodSection = EmojiMoodSection(\n        title=\"Mood Detection \ud83d\ude0a\",\n        description=\"Configure how to detect the vibe of your data\"\n    )\n\n    style_options: EmojiStyleSection = EmojiStyleSection(\n        title=\"Emoji Style \ud83c\udfa8\",\n        description=\"Fine-tune your emoji experience\"\n    )\n\n\nclass EmojiGenerator(CustomNodeBase):\n    # Node metadata - how it appears in Flowfile\n    node_name: str = \"Emoji Generator \ud83c\udf89\"\n    node_category: str = \"Fun Stuff\"  # This creates a new category\n    node_group: str = \"custom\"\n    title: str = \"Emoji Generator\"\n    intro: str = \"Transform boring data into fun emoji-filled datasets! \ud83d\ude80\"\n\n    # I/O configuration\n    number_of_inputs: int = 1\n    number_of_outputs: int = 1\n\n    # Link to our settings schema\n    settings_schema: EmojiSettings = EmojiSettings()\n\n    def process(self, input_df: pl.DataFrame) -&gt; pl.DataFrame:\n        # Get settings values from the UI\n        # Note how we can just access these from the settings that we have defined\n        column_name = self.settings_schema.mood_config.source_column.value\n        mood_type = self.settings_schema.mood_config.mood_type.value\n        threshold = self.settings_schema.mood_config.threshold_value.value\n        emoji_col_name = self.settings_schema.mood_config.emoji_column_name.value\n        intensity = self.settings_schema.style_options.emoji_intensity.value\n        add_sparkle = self.settings_schema.style_options.add_random_sparkle.value\n\n        # Define emoji sets for different moods\n        emoji_sets = {\n            \"performance\": {\n                \"high\": [\"\ud83d\ude0e\", \"\ud83d\udcaa\", \"\ud83c\udfc6\", \"\ud83d\udc51\", \"\ud83c\udf1f\", \"\ud83d\udcaf\", \"\ud83d\udd25\"],\n                \"low\": [\"\ud83d\ude30\", \"\ud83d\ude13\", \"\ud83d\udcc9\", \"\ud83d\ude22\", \"\ud83d\udc94\", \"\ud83c\udd98\", \"\ud83d\ude35\"]\n            },\n            \"temperature\": {\n                \"high\": [\"\ud83d\udd25\", \"\ud83c\udf0b\", \"\u2600\ufe0f\", \"\ud83e\udd75\", \"\ud83c\udf21\ufe0f\", \"\u2668\ufe0f\", \"\ud83c\udfd6\ufe0f\"],\n                \"low\": [\"\ud83e\uddca\", \"\u2744\ufe0f\", \"\u26c4\", \"\ud83e\udd76\", \"\ud83c\udf28\ufe0f\", \"\ud83c\udfd4\ufe0f\", \"\ud83d\udc27\"]\n            },\n            \"money\": {\n                \"high\": [\"\ud83e\udd11\", \"\ud83d\udcb0\", \"\ud83d\udc8e\", \"\ud83c\udfe6\", \"\ud83d\udcb3\", \"\ud83e\ude99\", \"\ud83d\udcc8\"],\n                \"low\": [\"\ud83d\ude22\", \"\ud83d\udcb8\", \"\ud83d\udcc9\", \"\ud83c\udfda\ufe0f\", \"\ud83d\ude2d\", \"\ud83e\udd7a\", \"\ud83d\udcca\"]\n            },\n            \"energy\": {\n                \"high\": [\"\ud83d\ude80\", \"\u26a1\", \"\ud83d\udca5\", \"\ud83c\udfaf\", \"\ud83c\udfc3\", \"\ud83e\udd38\", \"\ud83c\udfaa\"],\n                \"low\": [\"\ud83d\udd0b\", \"\ud83d\ude34\", \"\ud83d\udecc\", \"\ud83d\udc0c\", \"\ud83e\udd71\", \"\ud83d\ude2a\", \"\ud83d\udca4\"]\n            },\n            \"love\": {\n                \"high\": [\"\ud83d\ude0d\", \"\u2764\ufe0f\", \"\ud83d\udc95\", \"\ud83e\udd70\", \"\ud83d\udc98\", \"\ud83d\udc9d\", \"\ud83d\udc68\u200d\u2764\ufe0f\u200d\ud83d\udc68\"],\n                \"low\": [\"\ud83d\udc94\", \"\ud83d\ude22\", \"\ud83d\ude2d\", \"\ud83e\udd40\", \"\ud83d\ude14\", \"\ud83d\udc80\", \"\ud83d\udda4\"]\n            },\n            \"chaos\": {\n                \"high\": [\"\ud83e\udd96\", \"\ud83c\udfb8\", \"\ud83d\ude81\", \"\ud83c\udfaa\", \"\ud83e\udd9c\", \"\ud83c\udfad\", \"\ud83c\udff4\u200d\u2620\ufe0f\"],\n                \"low\": [\"\ud83e\udd54\", \"\ud83e\udde6\", \"\ud83d\udcce\", \"\ud83e\uddb7\", \"\ud83e\uddf2\", \"\ud83d\udd0c\", \"\ud83e\udea3\"]\n            },\n            \"pizza\": {\n                \"high\": [\"\ud83c\udf55\", \"\ud83c\udf55\ud83c\udf55\", \"\ud83c\udf55\ud83d\udd25\", \"\ud83c\udf55\ud83d\ude0d\", \"\ud83c\udf55\ud83c\udf89\", \"\ud83c\udf55\ud83d\udcaf\", \"\ud83c\udf55\ud83d\udc51\"],\n                \"low\": [\"\ud83c\udf55\", \"\ud83c\udf55\ud83d\ude22\", \"\ud83c\udf55\ud83d\udc94\", \"\ud83c\udf55\ud83d\ude2d\", \"\ud83c\udf55\ud83e\udd7a\", \"\ud83c\udf55\ud83d\ude14\", \"\ud83c\udf55\"]\n            }\n        }\n\n        # Helper function to get emoji based on value\n        def get_emoji(value, mood_type, threshold, intensity):\n            if value is None:\n                return \"\u2753\"\n\n            emoji_list = emoji_sets.get(mood_type, emoji_sets[\"performance\"])\n\n            if mood_type == \"chaos\":\n                # Random emoji from both lists\n                all_emojis = emoji_list[\"high\"] + emoji_list[\"low\"]\n                base_emoji = random.choice(all_emojis)\n            elif mood_type == \"pizza\":\n                # Everything is pizza\n                base_emoji = \"\ud83c\udf55\"\n            else:\n                # Use threshold to determine high/low\n                if value &gt;= threshold:\n                    base_emoji = random.choice(emoji_list[\"high\"])\n                else:\n                    base_emoji = random.choice(emoji_list[\"low\"])\n\n            # Add intensity\n            if intensity == \"subtle\":\n                result = base_emoji\n            elif intensity == \"normal\":\n                result = base_emoji\n                if random.random() &gt; 0.5:\n                    result += random.choice([\"\", \"\u2728\", \"\"])\n            elif intensity == \"extra\":\n                extras = [\"\u2728\", \"\ud83d\udcab\", \"\u2b50\", \"\"]\n                result = base_emoji + random.choice(extras) + random.choice(extras)\n            else:  # maximum\n                chaos_emojis = [\"\ud83c\udf89\", \"\ud83d\ude80\", \"\ud83d\udca5\", \"\ud83c\udf08\", \"\u2728\", \"\ud83d\udd25\", \"\ud83d\udcaf\", \"\u26a1\"]\n                result = base_emoji + \"\".join(random.choices(chaos_emojis, k=3))\n\n            # Add random sparkle\n            if add_sparkle and random.random() &gt; 0.7:\n                result += \"\u2728\"\n\n            return result\n\n        # Create emoji column using map_elements\n        emoji_expr = (\n            pl.col(column_name)\n            .map_elements(\n                lambda x: get_emoji(x, mood_type, threshold, intensity),\n                return_dtype=pl.String\n            )\n            .alias(emoji_col_name)\n        )\n\n        # Add bonus columns based on intensity\n        if intensity == \"maximum\":\n            # Add extra fun columns in maximum mode\n            return input_df.with_columns([\n                emoji_expr,\n                pl.lit(\"\ud83c\udf89 PARTY MODE \ud83c\udf89\").alias(\"vibe_check\"),\n                pl.col(column_name).map_elements(\n                    lambda x: \"\ud83d\udd25\" * min(int((x or 0) / 20), 5) if x else \"\ud83d\udca4\",\n                    return_dtype=pl.String\n                ).alias(\"fire_meter\")\n            ])\n        else:\n            return input_df.with_columns([emoji_expr])\n</code></pre> <p>Save this as <code>~/.flowfile/user_defined_nodes/emoji_generator.py</code>, restart Flowfile, and enjoy your new emoji-powered data transformations!</p>"},{"location":"for-developers/custom-node-tutorial.html#congratulations","title":"Congratulations! \ud83c\udf89","text":"<p>You've successfully created a fully functional custom node </p> <ul> <li>\u2705 Multi-section UI with 6 different component types</li> <li>\u2705 Complex processing logic with multiple mood themes</li> <li>\u2705 Advanced features like intensity control and random effects</li> <li>\u2705 Professional documentation and structure</li> </ul>"},{"location":"for-developers/design-philosophy.html","title":"The Architecture Story: How Flowfile Bridges Code and Visual Worlds","text":"<p>\ud83d\udccb TL;DR - Key Takeaways</p> <p>Key Points</p> <ul> <li>Two ways to build pipelines: Write Python code or use drag-and-drop UI - both create the same thing</li> <li>Settings-based design: Every transformation is just a configuration object (Pydantic model)</li> <li>Clear separation: Graph structure, settings, and execution are handled separately</li> <li>Happy accident: Started as a UI project, ended up with an architecture that works great for both UI and code</li> </ul> <p>Navigation</p> <ul> <li>This page: Architecture overview and design decisions</li> <li>Core Developer Guide: Technical implementation details</li> <li>Python API: How to use Flowfile in your projects</li> </ul> <p>\ud83d\udc65 Who Should Read This?</p> <p>Target Audience</p> <ul> <li>Contributors who want to understand the codebase</li> <li>Users curious about how things work internally</li> <li>Developers building similar dual-interface tools</li> <li>Anyone interested in bridging UI and code approaches</li> </ul>"},{"location":"for-developers/design-philosophy.html#the-problem-we-solved","title":"The Problem We Solved","text":"<p>Most data tools force you to choose: either use a visual interface (easy but limited) or write code (powerful but complex). We wanted both in the same tool.</p> <p>The challenge: How do you make a visual drag-and-drop interface that creates the exact same pipelines as writing code?</p> <p>The platform started with a clean, settings-based backend where every transformation is a declarative configuration object. This design is perfect for a UI. But developers don't think in configuration objects\u2014they think in code:</p> <pre><code># How developers want to write data code\ndf.filter(col(\"price\") &gt; 100).group_by(\"region\").sum()\n</code></pre> <p>The breakthrough came from realizing that the Polars API would be able to convert to our settings object and therefore creating the same settings object that the UI creates. Both interfaces become different ways to build the same underlying configuration, giving developers the expressiveness they want while maintaining the structured settings the UI needs.</p>"},{"location":"for-developers/design-philosophy.html#the-result","title":"The result","text":"<p>How It Actually Happened</p> <p>This wasn't some grand plan. I started building a drag-and-drop UI and needed a clean way to configure nodes. Settings objects made sense for the UI. But the development of Flowfile has never been a planned approach, it was just about building what sounded fun. Later, when looking at other projects, I realized I could just have the API methods create the same settings objects, well that is fun. Suddenly there were two equivalent interfaces almost by accident,. Since Polars does the actual data processing, our settings just configure what Polars should do. This turned out to be an easy abstraction layer that showed it's potential from the start.</p> <p>The result is a Python API that constructs the exact same configuration objects as the visual editor:</p> <ul> <li>The Python API <code>df.filter(...)</code>  translates directly to a <code>NodeFilter</code> object</li> <li>The Visual Editor creates an identical <code>NodeFilter</code> object through clicks and drags</li> </ul> <p>Both interfaces are different ways to build the same Directed Acyclic Graph (DAG), providing the experience of a code-native API combined with the accessibility of a visual editor.</p>"},{"location":"for-developers/design-philosophy.html#one-pipeline-two-ways","title":"One Pipeline, Two Ways","text":"<p>Let's build the same pipeline using both approaches to see how they produce identical results.</p>"},{"location":"for-developers/design-philosophy.html#sample-data","title":"Sample Data","text":"<pre><code>import flowfile as ff\n\nraw_data = [\n    {\"id\": 1, \"region\": \"North\", \"quantity\": 10, \"price\": 150},\n    {\"id\": 2, \"region\": \"South\", \"quantity\": 5, \"price\": 300},\n    {\"id\": 3, \"region\": \"East\", \"quantity\": 8, \"price\": 200},\n    {\"id\": 4, \"region\": \"West\", \"quantity\": 12, \"price\": 100},\n    {\"id\": 5, \"region\": \"North\", \"quantity\": 20, \"price\": 250},\n    {\"id\": 6, \"region\": \"South\", \"quantity\": 15, \"price\": 400},\n    {\"id\": 7, \"region\": \"East\", \"quantity\": 18, \"price\": 350},\n    {\"id\": 8, \"region\": \"West\", \"quantity\": 25, \"price\": 500},\n]\n</code></pre>"},{"location":"for-developers/design-philosophy.html#method-1-the-flowfile-api-developer-experience","title":"Method 1: The Flowfile API (Developer Experience)","text":"<p>Code: <pre><code>import flowfile as ff\nfrom flowfile_core.flowfile.flow_graph import FlowGraph\n\ngraph: FlowGraph = ff.create_flow_graph()\n\n# Create pipeline with fluent API\ndf_1 = ff.FlowFrame(raw_data, flow_graph=graph)\n\ndf_2 = df_1.with_columns(\n    flowfile_formulas=['[quantity] * [price]'], \n    output_column_names=[\"total\"]\n)\n\ndf_3 = df_2.filter(flowfile_formula=\"[total]&gt;1500\")\n\ndf_4 = df_3.group_by(['region']).agg([\n    ff.col(\"total\").sum().alias(\"total_revenue\"),\n    ff.col(\"total\").mean().alias(\"avg_transaction\"),\n])\n</code></pre></p> Inspecting the graph <p>Graph Introspection: <pre><code># Access all nodes that were created in the graph\nprint(graph._node_db)\n# {1: Node id: 1 (manual_input), \n#  3: Node id: 3 (formula), \n#  4: Node id: 4 (filter), \n#  5: Node id: 5 (group_by)}\n\n# Find the starting node(s) of the graph\nprint(graph._flow_starts)\n# [Node id: 1 (manual_input)]\n\n# From every node, access the next node that depends on it\nprint(graph.get_node(1).leads_to_nodes)\n# [Node id: 3 (formula)]\n\n# The other way around works too\nprint(graph.get_node(3).node_inputs)\n# NodeStepInputs(Left Input: None, Right Input: None, \n#                Main Inputs: [Node id: 1 (manual_input)])\n\n# Access the settings and type of any node\nprint(graph.get_node(4).setting_input)\nprint(graph.get_node(4).node_type)\n</code></pre></p>"},{"location":"for-developers/design-philosophy.html#method-2-direct-graph-construction-what-happens-internally","title":"Method 2: Direct Graph Construction (What Happens Internally)","text":"<p>Code: <pre><code>from flowfile_core.schemas import node_interface, transformation_settings, RawData\nfrom flowfile_core.flowfile.flow_graph import add_connection\n\nflow = ff.create_flow_graph()\n\n# Node 1: Manual input\nnode_manual_input = node_interface.NodeManualInput(\n    flow_id=flow.flow_id, \n    node_id=1,\n    raw_data_format=RawData.from_pylist(raw_data)\n)\nflow.add_manual_input(node_manual_input)\n\n# Node 2: Add formula for total\nformula_node = node_interface.NodeFormula(\n    flow_id=1,\n    node_id=2,\n    function=transformation_settings.FunctionInput(\n        field=transformation_settings.FieldInput(\n            name=\"total\", \n            data_type=\"Double\"\n        ),\n        function=\"[quantity] * [price]\"\n    )\n)\nflow.add_formula(formula_node)\nadd_connection(flow, \n    node_interface.NodeConnection.create_from_simple_input(1, 2))\n\n# Node 3: Filter high value transactions\nfilter_node = node_interface.NodeFilter(\n    flow_id=1,\n    node_id=3,\n    filter_input=transformation_settings.FilterInput(\n        filter_type=\"advanced\",\n        advanced_filter=\"[total]&gt;1500\"\n    )\n)\nflow.add_filter(filter_node)\nadd_connection(flow, \n    node_interface.NodeConnection.create_from_simple_input(2, 3))\n\n# Node 4: Group by region\ngroup_by_node = node_interface.NodeGroupBy(\n    flow_id=1,\n    node_id=4,\n    groupby_input=transformation_settings.GroupByInput(\n        agg_cols=[\n            transformation_settings.AggColl(\"region\", \"groupby\"),\n            transformation_settings.AggColl(\"total\", \"sum\", \"total_revenue\"),\n            transformation_settings.AggColl(\"total\", \"mean\", \"avg_transaction\")\n        ]\n    )\n)\nflow.add_group_by(group_by_node)\nadd_connection(flow, \n    node_interface.NodeConnection.create_from_simple_input(3, 4))\n</code></pre></p> <p>Schema Inspection: <pre><code># Check the schema at any node\nprint([s.get_minimal_field_info() for s in flow.get_node(4).schema])\n# [MinimalFieldInfo(name='region', data_type='String'), \n#  MinimalFieldInfo(name='total_revenue', data_type='Float64'), \n#  MinimalFieldInfo(name='avg_transaction', data_type='Float64')]\n</code></pre></p> Both methods produce the exact same Polars execution plan: <p>This is the polars query plan generated by both methods:</p> <pre><code>```\nAGGREGATE[maintain_order: false]\n  [col(\"total\").sum().alias(\"total_revenue\"), \n   col(\"total\").mean().alias(\"avg_transaction\")] BY [col(\"region\")]\n  FROM\n  FILTER [(col(\"total\")) &gt; (1500)]\n  FROM\n  WITH_COLUMNS:\n  [[(col(\"quantity\")) * (col(\"price\"))].alias(\"total\")]\n  DF [\"id\", \"region\", \"quantity\", \"price\"]; PROJECT 3/4 COLUMNS\n```\n</code></pre>"},{"location":"for-developers/design-philosophy.html#core-architecture","title":"Core Architecture","text":""},{"location":"for-developers/design-philosophy.html#three-fundamental-concepts","title":"Three Fundamental Concepts","text":""},{"location":"for-developers/design-philosophy.html#1-the-dag-is-everything","title":"1. The DAG is Everything","text":"<p>Every Flowfile pipeline is a Directed Acyclic Graph where. This is captured in the  FlowGraph</p> <ul> <li>Nodes are transformations (filter, join, group_by, etc.)</li> <li>Edges represent data flow between nodes</li> <li>Settings are Pydantic models configuring each transformation</li> </ul>"},{"location":"for-developers/design-philosophy.html#2-settings-drive-everything","title":"2. Settings Drive Everything","text":"<p>Every node is composed of two parts: the Node class (a Pydantic BaseModel) that holds metadata and the Settings (often dataclasses) that configure the transformation:</p> <p>Read more about Nodes and the transformations</p> <pre><code># The Node: metadata and graph position\nclass NodeGroupBy(NodeSingleInput):\n    groupby_input: transform_schema.GroupByInput = None\n\nclass NodeSingleInput(NodeBase):\n    depending_on_id: Optional[int] = -1  # Parent node reference\n\nclass NodeBase(BaseModel):\n    flow_id: int\n    node_id: int\n    cache_results: Optional[bool] = False\n    pos_x: Optional[float] = 0\n    pos_y: Optional[float] = 0\n    description: Optional[str] = None\n    # ... graph metadata ...\n\n# The Settings: transformation configuration (dataclass)\n@dataclass\nclass GroupByInput:\n    \"\"\"Defines how to perform the group by operation\"\"\"\n    agg_cols: List[AggColl]\n\n@dataclass\nclass AggColl:\n    \"\"\"Single aggregation operation\"\"\"\n    old_name: str      # Column to aggregate\n    agg: str          # Aggregation function ('sum', 'mean', etc.)\n    new_name: Optional[str]  # Output column name\n    output_type: Optional[str] = None\n</code></pre> <p>Settings Power The Backend</p> <p>This dual structure\u2014Nodes for graph metadata, Settings for transformation logic\u2014drives the backend:</p> <ul> <li>\ud83d\udd27 Code generation (method signatures match settings)</li> <li>\ud83d\udcbe Serialization (graphs can be saved/loaded)</li> <li>\ud83d\udd2e Schema prediction (output types are inferred from AggColl)</li> <li>\ud83c\udfa8 UI structure (defines what the frontend needs to collect, though forms are manually built)</li> </ul>"},{"location":"for-developers/design-philosophy.html#3-execution-is-everything","title":"3. Execution is Everything","text":"<p>The <code>FlowDataEngine</code> orchestrates everything about execution. While the DAG defines structure and settings define configuration, FlowDataEngine is the runtime brain that makes it all happen.</p> <p>FlowDataEngine handles: - Compute location (worker service vs local execution) - Caching strategy (when to materialize, where to store) - Schema caching (avoiding redundant schema calculations) - Lazy vs eager evaluation (performance vs debugging modes) - Data movement (passing LazyFrames between transformations)</p> <p>This separation is powerful: the DAG remains a pure specification, settings stay declarative, and FlowDataEngine owns all execution concerns. It wraps a Polars LazyFrame/DataFrame but is really the execution orchestrator\u2014deciding where, when, and how transformations run.</p>"},{"location":"for-developers/design-philosophy.html#understanding-flownode","title":"Understanding FlowNode","text":"<p>The <code>FlowNode</code> class is the heart of each transformation in the graph. Each node encapsulates everything needed for a single transformation step:</p> <p>Core FlowNode Components</p> <p>Essential State:</p> <ul> <li><code>_function</code>: The closure containing the transformation logic</li> <li><code>leads_to_nodes</code>: List of downstream nodes that depend on this one</li> <li><code>node_information</code>: Metadata (id, type, position, connections)</li> <li><code>_hash</code>: Unique identifier based on settings and parent hashes</li> </ul> <p>Runtime State:</p> <ul> <li><code>results</code>: Holds the resulting data, errors, and example data paths</li> <li><code>node_stats</code>: Tracks execution status (has_run, is_canceled, etc.)</li> <li><code>node_settings</code>: Runtime settings (cache_results, streamable, etc.)</li> <li><code>state_needs_reset</code>: Flag indicating if the node needs recalculation</li> </ul> <p>Schema Information:</p> <ul> <li><code>node_schema</code>: Input/output columns and predicted schemas</li> <li><code>schema_callback</code>: Function to calculate schema without execution</li> </ul> <p>The beauty is that FlowNode doesn't know about specific transformations\u2014it just orchestrates the execution of its <code>_function</code> closure with the right inputs and manages the resulting state.</p>"},{"location":"for-developers/design-philosophy.html#flowfile-the-use-of-closures","title":"Flowfile: The Use of Closures","text":"<p>When a method like <code>.filter()</code> is called, no data is actually filtered. Instead, a <code>FlowNode</code> is created containing a function\u2014a closure that remembers its settings.</p> <p>Visual: How Closures Build the Execution Chain <pre><code>graph LR\n    subgraph \"Node 1: manual_input\"\n        direction TB\n        settings1(\"&lt;b&gt;Settings&lt;/b&gt;&lt;br/&gt;raw_data = [...]\")\n        func1(\"&lt;b&gt;_func()&lt;/b&gt;&lt;br/&gt;&lt;i&gt;closure&lt;/i&gt;\")\n        settings1 -.-&gt; |remembered by| func1\n    end\n\n    subgraph \"Node 2: with_columns&lt;br/&gt;(formula)\"\n        direction TB\n        settings2(\"&lt;b&gt;Settings&lt;/b&gt;&lt;br/&gt;formula = '[q] * [p]'\")\n        func2(\"&lt;b&gt;_func(fl)&lt;/b&gt;&lt;br/&gt;&lt;i&gt;closure&lt;/i&gt;\")\n        settings2 -.-&gt; |remembered by| func2\n    end\n\n    subgraph \"Node 3: filter\"\n        direction TB\n        settings3(\"&lt;b&gt;Settings&lt;/b&gt;&lt;br/&gt;filter = '[total] &gt; 1500'\")\n        func3(\"&lt;b&gt;_func(fl)&lt;/b&gt;&lt;br/&gt;&lt;i&gt;closure&lt;/i&gt;\")\n        settings3 -.-&gt; |remembered by| func3\n    end\n\n    subgraph \"Node 4: group_by\"\n        direction TB\n        settings4(\"&lt;b&gt;Settings&lt;/b&gt;&lt;br/&gt;agg = sum(total)\")\n        func4(\"&lt;b&gt;_func(fl)&lt;/b&gt;&lt;br/&gt;&lt;i&gt;closure&lt;/i&gt;\")\n        settings4 -.-&gt; |remembered by| func4\n    end\n\n    Result([Schema / Data])\n\n    func1 ==&gt; |FlowDataEngine| func2\n    func2 ==&gt; |FlowDataEngine| func3\n    func3 ==&gt; |FlowDataEngine| func4\n    func4 ==&gt; |Final FlowDataEngine&lt;br/&gt;with full LazyFrame plan| Result</code></pre></p> <p>Each <code>_func</code> is a closure that wraps around the previous one, building up a chain. The beauty is that Polars can track the schema through this entire chain without executing any data transformations\u2014it just builds the query plan!</p>"},{"location":"for-developers/design-philosophy.html#the-closure-pattern-in-practice","title":"The Closure Pattern in Practice","text":"<p>Here's how closures are actually created in FlowGraph:</p> <pre><code># From the FlowGraph implementation\ndef add_group_by(self, group_by_settings: input_schema.NodeGroupBy):\n    # The closure: captures group_by_settings\n    def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n        return fl.do_group_by(group_by_settings.groupby_input, False)\n\n    self.add_node_step(\n        node_id=group_by_settings.node_id,\n        function=_func,  # This closure remembers group_by_settings!\n        node_type='group_by',\n        setting_input=group_by_settings,\n        input_node_ids=[group_by_settings.depending_on_id]\n    )\n\ndef add_union(self, union_settings: input_schema.NodeUnion):\n    # Another closure: captures union_settings\n    def _func(*flowfile_tables: FlowDataEngine):\n        dfs = [flt.data_frame for flt in flowfile_tables]\n        return FlowDataEngine(pl.concat(dfs, how='diagonal_relaxed'))\n\n    self.add_node_step(\n        node_id=union_settings.node_id,\n        function=_func,  # This closure has everything it needs\n        node_type='union',\n        setting_input=union_settings,\n        input_node_ids=union_settings.depending_on_ids\n    )\n</code></pre> <p>Each <code>_func</code> is a closure that captures its specific settings. When these functions are composed during execution, they form a chain:</p> <pre><code># Conceptual composition of the closures\nresult = group_by._func(\n    filter._func(\n        formula._func(\n            manual_input._func()\n        )\n    )\n)\n\n# Result is a FlowDataEngine with a LazyFrame that knows its schema\nprint(result.data_frame.collect_schema())\n# Schema([('region', String), ('total_revenue', Float64), ('avg_transaction', Float64)])\n</code></pre> <p>Why This Works</p> <ol> <li>Each <code>_func</code> is a closure containing the node's settings</li> <li>Functions only need FlowDataEngine as input (or multiple for joins/unions)</li> <li>LazyFrame tracks schema changes through the entire chain</li> <li>No data is processed\u2014Polars just builds the query plan</li> </ol> <p>The result: instant schema feedback without running expensive computations!</p>"},{"location":"for-developers/design-philosophy.html#fallback-schema-callbacks","title":"Fallback: Schema Callbacks","text":"<p>For nodes that can't infer schemas automatically (external data sources), each FlowNode can have a <code>schema_callback</code>:</p> <pre><code>def schema_callback(settings, input_schema):\n    \"\"\"Pure function: settings + input schema \u2192 output schema\"\"\"\n    # Calculate output schema without data\n    return new_schema\n</code></pre>"},{"location":"for-developers/design-philosophy.html#execution-methods","title":"Execution Methods","text":"<p>Flowfile offers flexible execution strategies depending on your needs:</p>"},{"location":"for-developers/design-philosophy.html#available-execution-methods","title":"\ud83d\ude80 Available Execution Methods","text":""},{"location":"for-developers/design-philosophy.html#performance-mode","title":"Performance Mode","text":"<p>When to use: Production pipelines, large datasets</p> <pre><code># Get the final result efficiently\nresult = flow.get_node(final_node_id).get_resulting_data()\n</code></pre> <p>Characteristics:</p> <ul> <li>\u26a1 Pull-based execution from the final node</li> <li>\ud83c\udfaf Polars optimizes the entire pipeline</li> <li>\ud83d\udca8 Data flows once through optimized plan</li> <li>\ud83d\udeab No intermediate materialization</li> </ul>"},{"location":"for-developers/design-philosophy.html#development-mode","title":"Development Mode","text":"<p>When to use: Debugging, inspection, incremental development</p> <pre><code># Execute with caching enabled\nimport flowfile as ff\n\nflow = ff.create_flow_graph()\nflow.flow_settings.execution_mode = \"Development\"\n\n# Add transformations here\nflow.run_graph()\n\n# Inspect intermediate results\nnode_3_result = flow.get_node(3).results.get_example_data()\n\nflow.get_node(3).needs_run(performance_mode=False) # False\n</code></pre> <p>Characteristics: - \ud83d\udcdd Push-based execution in topological order - \ud83d\udcbe Each node's output written to disk - \ud83d\udd0d Inspect any intermediate result - \ud83d\udd04 When re-running the flow, only the steps that have changed(directly and indirectly) will run</p>"},{"location":"for-developers/design-philosophy.html#explain-plan","title":"Explain Plan","text":"<p>When to use: Optimization, understanding deeply the execution plan.</p> <p>!!! warning This feature uses directly the Polars implementation, when the full flow cannot be fully converted to Polars, it will show partial executions.</p> <pre><code># See what Polars will actually do\nplan = flow.get_node(node_id).get_resulting_data().data_frame.explain()\nprint(plan)\n</code></pre> <p>Characteristics: - \ud83d\udcca Shows optimized query plan - \ud83d\udd0d Understand Polars optimizations - \ud83d\udcc8 Identify performance bottlenecks - \ud83c\udfaf No actual execution</p>"},{"location":"for-developers/design-philosophy.html#system-architecture","title":"System Architecture","text":""},{"location":"for-developers/design-philosophy.html#service-architecture","title":"Service Architecture","text":"<pre><code>graph LR\n    subgraph \"Frontend\"\n        A[Designer&lt;br/&gt;Vue/Electron]\n    end\n\n    subgraph \"Backend\"\n        B[Core Service&lt;br/&gt;FastAPI]\n        C[Worker Service&lt;br/&gt;FastAPI]\n    end\n\n    subgraph \"Storage\"\n        D[Arrow IPC&lt;br/&gt;Cache]\n    end\n\n    A &lt;--&gt;|Settings/Schema| B\n    B &lt;--&gt;|Execution| C\n    C &lt;--&gt;|Data| D</code></pre> <p>Service Responsibilities</p> <p>Designer: - Visual graph building interface - Node configuration forms (manually implemented) - Real-time schema feedback</p> <p>Core: - DAG management - Execution orchestration - Schema prediction</p> <p>Worker: - Polars transformations - Data caching (Arrow IPC) - Isolated from Core for scalability</p>"},{"location":"for-developers/design-philosophy.html#project-structure","title":"Project Structure","text":"<pre><code>flowfile/\n\u251c\u2500\u2500 flowfile_core/\n\u2502   \u251c\u2500\u2500 nodes/              # Node implementations\n\u2502   \u251c\u2500\u2500 schemas/            # Pydantic models\n\u2502   \u2514\u2500\u2500 flowfile/          # Graph management\n\u251c\u2500\u2500 flowfile_worker/\n\u2502   \u251c\u2500\u2500 execution/         # Polars execution\n\u2502   \u2514\u2500\u2500 cache/            # Arrow IPC caching\n\u2514\u2500\u2500 flowfile_frontend/\n    \u251c\u2500\u2500 components/       # Vue components\n    \u2514\u2500\u2500 electron/        # Desktop app\n</code></pre>"},{"location":"for-developers/design-philosophy.html#contributing","title":"Contributing","text":"<p>Current State of Node Development</p> <p>While the backend architecture elegantly uses settings-driven nodes, adding new nodes requires work across multiple layers. The frontend currently requires manual implementation for each node type\u2014the visual editor doesn't automatically generate forms from Pydantic schemas yet.</p> <p>However, there are also opportunities for more focused contributions! Integration with databases and cloud services is needed\u2014these are smaller, more targeted tasks since the core structure is already in place. There's a lot of active development happening, so it's an exciting time to contribute!</p>"},{"location":"for-developers/design-philosophy.html#adding-a-new-node-the-full-picture","title":"Adding a New Node: The Full Picture","text":"<p>Adding a node isn't as simple as defining settings and a function. Here's what's actually required:</p>"},{"location":"for-developers/design-philosophy.html#backend-requirements","title":"Backend Requirements","text":"<ol> <li>Define the Pydantic settings model in <code>schemas/</code></li> <li>Implement the transformation method on <code>FlowDataEngine</code></li> <li>Add the node method to <code>FlowGraph</code> (e.g., <code>add_custom_transform()</code>)</li> <li>Create the closure function that captures settings</li> <li>Define schema callbacks for predicting output schemas</li> <li>Register the node in the node registry</li> </ol> <p>Example of what's really needed in FlowGraph:</p> <pre><code>def add_custom_transform(self, transform_settings: input_schema.NodeCustomTransform):\n    # Create the closure that captures settings\n    def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n        return fl.do_custom_transform(transform_settings.transform_input)\n\n    # Register with the graph\n    self.add_node_step(\n        node_id=transform_settings.node_id,\n        function=_func,\n        node_type='custom_transform',\n        setting_input=transform_settings,\n        input_node_ids=[transform_settings.depending_on_id]\n    )\n\n    # Don't forget schema prediction!\n    node = self.get_node(transform_settings.node_id)\n    # ... schema callback setup ...\n</code></pre>"},{"location":"for-developers/design-philosophy.html#frontend-requirements","title":"Frontend Requirements","text":"<p>Currently, you'll need to:</p> <ol> <li>Create a new Vue component for the node's configuration form</li> <li>Handle the visual representation in the graph editor</li> <li>Map the UI inputs to the backend settings structure</li> <li>Add the node type to the visual editor's palette</li> </ol> <p>This manual process ensures full control over the UI/UX but requires significant development effort.</p>"},{"location":"for-developers/design-philosophy.html#future-vision","title":"Future Vision","text":"<p>The goal is to eventually auto-generate UI from Pydantic schemas, which would complete the settings-driven architecture. This would make adding new nodes closer to just defining the backend settings and transformation logic, with the UI automatically following.</p> <p>The beauty of Flowfile's architecture\u2014discovered through the organic evolution from a UI-first approach\u2014is that even though adding nodes requires work across multiple layers today, the settings-based design provides a clear contract between visual and code interfaces. </p> <p>I hope you enjoyed learning about Flowfile's architecture and found the dual-interface approach as exciting as I do! If you have questions, ideas, or want to contribute, ] feel free to reach out via GitHub or check our Core Developer Guide. Happy building!</p>"},{"location":"for-developers/flowfile-core.html","title":"Flowfile Core: A Developer's Guide","text":"<p>Welcome! This guide is for developers who want to understand, use, and contribute to <code>flowfile-core</code>. We'll dive into the architecture, see how data flows, and learn how to build powerful data pipelines.</p> <p>Looking for the API docs?</p> <ul> <li>Python API Reference for users: If you want to USE Flowfile</li> <li>Design Philosophy: If you want to understand WHY Flowfile works this way</li> <li>This page: If you want to understand HOW Flowfile works internally</li> </ul> <p>New to Flowfile?</p> <p>If you're looking for the high-level Python API, start with the Python API Overview. This guide dives into the internal architecture.</p> <p>Ready? Let's build something!</p>"},{"location":"for-developers/flowfile-core.html#the-core-architecture","title":"The Core Architecture","text":"<p>At its heart, <code>flowfile-core</code> is composed of three main objects:</p> <ol> <li><code>FlowGraph</code>: The central orchestrator. It holds your pipeline, manages the nodes, and controls the execution flow.</li> <li><code>FlowNode</code>: An individual step in your pipeline. It's a wrapper around your settings and logic, making it an executable part of the graph.</li> <li><code>FlowDataEngine</code>: The data itself, which flows between nodes. It's a smart wrapper around a Polars LazyFrame, carrying both the data and its schema.</li> </ol> <p>Let's see these in action.</p>"},{"location":"for-developers/flowfile-core.html#1-the-flowgraph-your-pipeline-orchestrator","title":"1. The FlowGraph: Your Pipeline Orchestrator","text":"<p>Everything starts with the <code>FlowGraph</code>. Think of it as the canvas for your data pipeline.</p> <p>Let's create one:</p> <pre><code>from flowfile_core.flowfile.flow_graph import FlowGraph\nfrom flowfile_core.schemas.schemas import FlowSettings\n\n# Initialize the graph with some basic settings\ngraph = FlowGraph(\n    flow_settings=FlowSettings(\n        flow_id=1,\n        name=\"My First Pipeline\"\n    )\n)\n\nprint(graph)\n</code></pre> Output of <code>print(graph)</code> <pre><code>FlowGraph(\nNodes: {}                                #&lt;-- An empty dictionary. No nodes yet!\n\nSettings:                                #&lt;-- The FlowSettings object you provided.\n  -flow_id: 1                            #&lt;-- A unique ID for this flow.\n  -description: None                     #&lt;-- An optional description.\n  -save_location: None                   #&lt;-- Where the flow definition is saved.\n  -name: My First Pipeline               #&lt;-- The name of our flow.\n  -path:                                 #&lt;-- Path to the flow file.\n  -execution_mode: Development           #&lt;-- 'Development' for debugging or 'Performance' for speed.\n  -execution_location: local             #&lt;-- Where the flow runs ('local' or 'remote').\n  -auto_save: False                      #&lt;-- Auto-save changes (feature in development).\n  -modified_on: None                     #&lt;-- Last modified timestamp (feature in development).\n  -show_detailed_progress: True          #&lt;-- If True, shows detailed logs in the UI.\n  -is_running: False                     #&lt;-- Is the flow currently running?\n  -is_canceled: False                    #&lt;-- Was a cancellation requested?\n)\n</code></pre> <pre><code>print(graph.run_graph())\n# flow_id=1 start_time=datetime.datetime(...) end_time=datetime.datetime(...) success=True nodes_completed=0 number_of_nodes=0 node_step_result=[]\n</code></pre> <p>It runs successfully but does nothing, as expected. The FlowGraph's job is to:</p> <ul> <li>Contain all the nodes.</li> <li>Manage the connections between them.</li> <li>Calculate the optimal execution order.</li> <li>Orchestrate the entire run lifecycle.</li> </ul> <p>Let's give it a node to manage.</p>"},{"location":"for-developers/flowfile-core.html#2-adding-a-node-where-settings-come-to-life","title":"2. Adding a Node: Where Settings Come to Life","text":"<p>You don't add raw functions or data directly to the graph. Instead, you provide settings objects (which are just Pydantic models). The graph then transforms these settings into executable <code>FlowNodes</code>.</p> <p>Watch this:</p> <p><pre><code>from flowfile_core.schemas import input_schema\n\n# 1. Define your data using a settings object.\n# This is just a Pydantic model holding configuration.\nmanual_input_settings = input_schema.NodeManualInput(\n    flow_id=1,\n    node_id=1,\n    raw_data_format=input_schema.RawData.from_pylist([\n        {\"name\": \"Alice\", \"age\": 30},\n        {\"name\": \"Bob\", \"age\": 25}\n    ])\n)\n\n# 2. Add the settings to the graph.\ngraph.add_manual_input(manual_input_settings)\n</code></pre> So, what did the graph just do? It didn't just store our settings. It created a FlowNode.</p> <pre><code># Let's retrieve the object the graph created\nnode = graph.get_node(1)\n\nprint(type(node))\n# &lt;class 'flowfile_core.flowfile.flow_node.flow_node.FlowNode'&gt;\n</code></pre> <p>The <code>FlowNode</code> is the wrapper that makes your settings operational. It holds your original settings but also adds the machinery needed for execution.</p> Peek inside the <code>FlowNode</code> <pre><code># The FlowNode keeps your original settings\nprint(node.setting_input == manual_input_settings)\n# True\n\n# But it also contains the execution logic...\nprint(f\"Has a function to run: {node._function is not None}\")\n# Has a function to run: True\n\n# ...state tracking...\nprint(f\"Can track its state: {hasattr(node, 'node_stats')}\")\n# Can track its state: True\n\n# ...connections to other nodes...\nprint(f\"Can connect to other nodes: {hasattr(node, 'leads_to_nodes')}\")\n# Can connect to other nodes: True\n\n# ...and a place to store its results.\nprint(f\"Has a place for results: {hasattr(node, 'results')}\")\n# Has a place for results: True\n</code></pre> <p>This separation is key: Settings define what to do, and the FlowNode figures out how to do it within the graph.</p>"},{"location":"for-developers/flowfile-core.html#3-connections-the-key-to-a-flowing-pipeline","title":"3. Connections: The Key to a Flowing Pipeline","text":"<pre><code>from flowfile_core.schemas.transform_schema import FilterInput\n\n# 1. Define settings for a filter node\nfilter_settings = input_schema.NodeFilter(\n    flow_id=1,\n    node_id=2,\n    filter_input=FilterInput(\n        filter_type=\"advanced\",\n        advanced_filter=\"[age] &gt; 28\" # Polars expression syntax\n    )\n)\ngraph.add_filter(filter_settings)\n\n# 2. Run the graph\nresult = graph.run_graph()\nprint(f\"Nodes executed: {result.nodes_completed}/{len(graph.nodes)}\")\n# Nodes executed: 1/2\n</code></pre> <p>Only one node ran! Why? The graph is smart; it knows the filter node has no input, thus will never succeed running.</p> Why the filter node was skipped <pre><code>filter_node = graph.get_node(2)\n\n# The graph checks if a node is a \"start\" node (has no inputs)\nprint(f\"Is filter_node a start node? {filter_node.is_start}\")\n# Is filter_node a start node? False\n\n# It sees that the filter node is missing an input connection\nprint(f\"Does filter_node have an input? {filter_node.has_input}\")\n# Does filter_node have an input? False\n\n# The graph's execution plan only includes nodes it can reach from a start node\nprint(f\"Graph start nodes: {graph._flow_starts}\")\n# Graph start nodes: [Node id: 1 (manual_input)]\n</code></pre> <p>The execution engine works like this:</p> <ol> <li>It identifies all start nodes (like our manual input).</li> <li>It builds an execution plan by following the connections from those start nodes.</li> <li>Any node not connected to this flow is ignored.</li> </ol> <p>Let's fix that by adding a connection.</p> <pre><code>from flowfile_core.flowfile.flow_graph import add_connection\n# Create a connection object\nconnection = input_schema.NodeConnection.create_from_simple_input(\n    from_id=1, # From our manual input node\n    to_id=2,   # To our filter node\n    input_type=\"main\"\n)\n\n# Add it to the graph\nadd_connection(graph, connection)\n\n# Let's check what changed\nprint(f\"Node 1 now leads to: {graph.get_node(1).leads_to_nodes}\")\n# Node 1 now leads to: [Node id: 2 (filter)]\n\nprint(f\"Node 2 now receives from: {graph.get_node(2).node_inputs.main_inputs}\")\n# Node 2 now receives from: [Node id: 1 (manual_input)]\n</code></pre> <p>Now that they are connected, let's run the graph again.</p> <pre><code>result = graph.run_graph()\n\n# The graph determines the correct execution order\nprint(\"Execution Order:\")\nfor node_result in result.node_step_result:\n    print(f\"  - Node {node_result.node_id} ran successfully: {node_result.success}\")\n# Execution Order:\n#   - Node 1 ran successfully: True\n#   - Node 2 ran successfully: True\n</code></pre> <p>Success! Both nodes executed. The connection allowed data to flow from the input to the filter.</p>"},{"location":"for-developers/flowfile-core.html#4-the-flowdataengine-the-data-carrier","title":"4. The FlowDataEngine: The Data Carrier","text":"<p>When data moves from one node to another, it's bundled up in a FlowDataEngine object. This isn't just raw data; it's an enhanced wrapper around a Polars LazyFrame.</p> <pre><code># Let's inspect the data after the run\nnode1 = graph.get_node(1)\nnode2 = graph.get_node(2)\n\n# Get the resulting data from each node\ndata_engine1 = node1.get_resulting_data()\ndata_engine2 = node2.get_resulting_data()\n\nprint(f\"Type of object passed between nodes: {type(data_engine1)}\")\n# Type of object passed between nodes: &lt;class 'flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine'&gt;\n\n# Let's see the transformation\nprint(f\"Rows from Node 1 (Input): {len(data_engine1.collect())}\")\n# Rows from Node 1 (Input): 2\n\nprint(f\"Rows from Node 2 (Filter): {len(data_engine2.collect())}\")\n# Rows from Node 2 (Filter): 1  &lt;-- Success! Bob (age 25) was filtered out.\n</code></pre> <p>The <code>FlowDataEngine</code> is the boundary between <code>flowfile-core</code> and Polars. It: * Carries the data (as a LazyFrame). * Maintains schema information. * Tracks metadata like record counts. * Manages lazy vs. eager execution.</p>"},{"location":"for-developers/flowfile-core.html#5-the-hash-system-smart-change-detection","title":"5. The Hash System: Smart Change Detection","text":"<p>How does the graph know when to re-run a node? Every <code>FlowNode</code> has a unique hash based on its configuration and its inputs.</p> <pre><code>node2 = graph.get_node(2)\noriginal_hash = node2.hash\nprint(f\"Original hash of filter node: {original_hash[:10]}...\")\n# Original hash of filter node: ...\n\n# Now, let's change the filter's settings\nnode2.setting_input.filter_input.advanced_filter = \"[age] &gt; 20\"\n\n# The node instantly knows it's been changed\nprint(f\"Settings changed, needs reset: {node2.needs_reset()}\")\n# Settings changed, needs reset: True\n\n# Resetting recalculates the hash\nnode2.reset()\nnew_hash = node2.hash\nprint(f\"New hash of filter node: {new_hash[:10]}...\")\n# New hash of filter node: ...\n\nprint(f\"Hash changed: {original_hash != new_hash}\")\n# Hash changed: True\n</code></pre> <p>This hash is calculated from:</p> <ul> <li>The node's own settings.</li> <li>The hashes of all its direct parent nodes.</li> </ul> <p>This creates a chain of dependency. If you change a node, <code>flowfile-core</code> knows that it and all downstream nodes need to be re-run, while upstream nodes can use their cached results. This is crucial for efficiency.</p>"},{"location":"for-developers/flowfile-core.html#6-schema-prediction-see-the-future","title":"6. Schema Prediction: See the Future","text":"<p>One of the most powerful features for interactive UI is schema prediction. A node can predict its output schema without processing any data.</p> <p>Let's add a \"formula\" node to create a new column.</p> <pre><code>from flowfile_core.schemas.transform_schema import FunctionInput, FieldInput\n\n# 1. Add a formula node to double the age\nformula_settings = input_schema.NodeFormula(\n    flow_id=1,\n    node_id=3,\n    function=FunctionInput(\n        field=FieldInput(name=\"age_doubled\", data_type=\"Int64\"),\n        function=\"[age] * 2\" # Polars expression\n    )\n)\ngraph.add_formula(formula_settings)\n\n# 2. Connect the filter node to our new formula node\nadd_connection(graph, input_schema.NodeConnection.create_from_simple_input(2, 3))\n\n# 3. Predict the schema\nformula_node = graph.get_node(3)\npredicted_schema = formula_node.get_predicted_schema()\n\nprint(\"Predicted columns for Node 3:\")\nfor col in predicted_schema:\n    print(f\"  - {col.column_name} (Type: {col.data_type})\")\n\n# This works even though the node has not run yet!\nprint(f\"\\nHas the formula node run? {formula_node.node_stats.has_run_with_current_setup}\")\n</code></pre> Output of Schema Prediction <pre><code>Predicted columns for Node 3:\n  - name (Type: String)\n  - age (Type: Int64)\n  - city (Type: String)\n  - age_doubled (Type: Int64)\n</code></pre> <p>How does this work? The node simply:</p> <ol> <li>Asks its parent node(s) for their output schema.</li> <li>Applies its own transformation logic to that schema (not the data).</li> <li>Returns the resulting new schema.</li> </ol> <p>This allows a UI to show you how your data will be transformed in real-time, as you build the pipeline.</p>"},{"location":"for-developers/flowfile-core.html#the-complete-picture-a-summary","title":"The Complete Picture: A Summary","text":"<p>Let's recap the entire lifecycle:</p> <ul> <li>You provide Settings: You define steps using simple Pydantic models (<code>NodeManualInput</code>, <code>NodeFilter</code>, etc.).</li> <li>Graph Creates FlowNodes: The <code>FlowGraph</code> takes your settings and wraps them in <code>FlowNode</code> objects, adding execution logic, state, and connection points.</li> <li>You Connect Nodes: You create <code>NodeConnection</code> objects. This builds the pipeline topology, which the graph uses to determine the execution order.</li> <li>You Run the Graph: When <code>graph.run_graph()</code> is called:<ul> <li>An execution plan is created via topological sort.</li> <li>Execution starts from the \"start nodes\".</li> <li>Each node receives a <code>FlowDataEngine</code> from its parent.</li> <li>It applies its transformation logic.</li> <li>It returns a new <code>FlowDataEngine</code> to its children.</li> </ul> </li> <li>Results Flow Through: The data, wrapped in the <code>FlowDataEngine</code>, moves down the pipeline, getting transformed at each step.</li> </ul> <p>This architecture provides a powerful combination of flexibility, introspection, and performance, bridging the gap between a visual, no-code interface and a powerful, code-driven engine.</p>"},{"location":"for-developers/flowfile-core.html#the-fastapi-service-your-api-layer","title":"The FastAPI Service: Your API Layer","text":"<p>While <code>FlowGraph</code>, <code>FlowNode</code>, and <code>FlowDataEngine</code> power the core pipeline logic, the FastAPI service is what makes it accessible from the outside world.</p> <p>Think of it as the control panel for your pipelines:</p> <ul> <li>HTTP interface \u2013 Wraps the core Python objects in a REST API so UIs (like Flowfile\u2019s) or other systems can create, run, and inspect flows via standard web requests.</li> <li>State management \u2013 Keeps track of all active <code>FlowGraph</code> sessions. When the UI triggers a change, it\u2019s really calling one of these endpoints, which updates the in-memory graph.</li> <li>Security \u2013 Handles authentication and authorization so only the right users can access or modify flows.</li> <li>Data previews \u2013 When you view a node\u2019s output in the UI, the API calls <code>.get_resulting_data()</code> on the corresponding <code>FlowNode</code> and returns a sample to the client.</li> </ul> <p>In short: FastAPI turns the in-memory power of <code>flowfile-core</code> into a secure, interactive web service, enabling rich, real-time applications to be built on top of your pipelines.</p>"},{"location":"for-developers/python-api-reference.html","title":"Flowfile Core API Reference","text":"<p>This section provides a detailed API reference for the core Python objects, data models, and API routes in <code>flowfile-core</code>. The documentation is generated directly from the source code docstrings.</p>"},{"location":"for-developers/python-api-reference.html#core-components","title":"Core Components","text":"<p>This section covers the fundamental classes that manage the state and execution of data pipelines. These are the main \"verbs\" of the library.</p>"},{"location":"for-developers/python-api-reference.html#flowgraph","title":"FlowGraph","text":"<p>The <code>FlowGraph</code> is the central object that orchestrates the execution of data transformations. It is built incrementally as you chain operations. This DAG (Directed Acyclic Graph) represents the entire pipeline.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph","title":"<code>flowfile_core.flowfile.flow_graph.FlowGraph</code>","text":"<p>A class representing a Directed Acyclic Graph (DAG) for data processing pipelines.</p> <p>It manages nodes, connections, and the execution of the entire flow.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes a new FlowGraph instance.</p> <code>__repr__</code> <p>Provides the official string representation of the FlowGraph instance.</p> <code>add_cloud_storage_reader</code> <p>Adds a cloud storage read node to the flow graph.</p> <code>add_cloud_storage_writer</code> <p>Adds a node to write data to a cloud storage provider.</p> <code>add_cross_join</code> <p>Adds a cross join node to the graph.</p> <code>add_database_reader</code> <p>Adds a node to read data from a database.</p> <code>add_database_writer</code> <p>Adds a node to write data to a database.</p> <code>add_datasource</code> <p>Adds a data source node to the graph.</p> <code>add_dependency_on_polars_lazy_frame</code> <p>Adds a special node that directly injects a Polars LazyFrame into the graph.</p> <code>add_explore_data</code> <p>Adds a specialized node for data exploration and visualization.</p> <code>add_external_source</code> <p>Adds a node for a custom external data source.</p> <code>add_filter</code> <p>Adds a filter node to the graph.</p> <code>add_formula</code> <p>Adds a node that applies a formula to create or modify a column.</p> <code>add_fuzzy_match</code> <p>Adds a fuzzy matching node to join data on approximate string matches.</p> <code>add_graph_solver</code> <p>Adds a node that solves graph-like problems within the data.</p> <code>add_group_by</code> <p>Adds a group-by aggregation node to the graph.</p> <code>add_include_cols</code> <p>Adds columns to both the input and output column lists.</p> <code>add_initial_node_analysis</code> <p>Adds a data exploration/analysis node based on a node promise.</p> <code>add_join</code> <p>Adds a join node to combine two data streams based on key columns.</p> <code>add_manual_input</code> <p>Adds a node for manual data entry.</p> <code>add_node_promise</code> <p>Adds a placeholder node to the graph that is not yet fully configured.</p> <code>add_node_step</code> <p>The core method for adding or updating a node in the graph.</p> <code>add_output</code> <p>Adds an output node to write the final data to a destination.</p> <code>add_pivot</code> <p>Adds a pivot node to the graph.</p> <code>add_polars_code</code> <p>Adds a node that executes custom Polars code.</p> <code>add_read</code> <p>Adds a node to read data from a local file (e.g., CSV, Parquet, Excel).</p> <code>add_record_count</code> <p>Adds a filter node to the graph.</p> <code>add_record_id</code> <p>Adds a node to create a new column with a unique ID for each record.</p> <code>add_sample</code> <p>Adds a node to take a random or top-N sample of the data.</p> <code>add_select</code> <p>Adds a node to select, rename, reorder, or drop columns.</p> <code>add_sort</code> <p>Adds a node to sort the data based on one or more columns.</p> <code>add_sql_source</code> <p>Adds a node that reads data from a SQL source.</p> <code>add_text_to_rows</code> <p>Adds a node that splits cell values into multiple rows.</p> <code>add_union</code> <p>Adds a union node to combine multiple data streams.</p> <code>add_unique</code> <p>Adds a node to find and remove duplicate rows.</p> <code>add_unpivot</code> <p>Adds an unpivot node to the graph.</p> <code>apply_layout</code> <p>Calculates and applies a layered layout to all nodes in the graph.</p> <code>cancel</code> <p>Cancels an ongoing graph execution.</p> <code>close_flow</code> <p>Performs cleanup operations, such as clearing node caches.</p> <code>copy_node</code> <p>Creates a copy of an existing node.</p> <code>delete_node</code> <p>Deletes a node from the graph and updates all its connections.</p> <code>generate_code</code> <p>Generates code for the flow graph.</p> <code>get_frontend_data</code> <p>Formats the graph structure into a JSON-like dictionary for a specific legacy frontend.</p> <code>get_implicit_starter_nodes</code> <p>Finds nodes that can act as starting points but are not explicitly defined as such.</p> <code>get_node</code> <p>Retrieves a node from the graph by its ID.</p> <code>get_node_data</code> <p>Retrieves all data needed to render a node in the UI.</p> <code>get_node_storage</code> <p>Serializes the entire graph's state into a storable format.</p> <code>get_nodes_overview</code> <p>Gets a list of dictionary representations for all nodes in the graph.</p> <code>get_run_info</code> <p>Gets a summary of the most recent graph execution.</p> <code>get_vue_flow_input</code> <p>Formats the graph's nodes and edges into a schema suitable for the VueFlow frontend.</p> <code>print_tree</code> <p>Print flow_graph as a visual tree structure, showing the DAG relationships with ASCII art.</p> <code>remove_from_output_cols</code> <p>Removes specified columns from the list of expected output columns.</p> <code>reset</code> <p>Forces a deep reset on all nodes in the graph.</p> <code>run_graph</code> <p>Executes the entire data flow graph from start to finish.</p> <code>save_flow</code> <p>Saves the current state of the flow graph to a file.</p> <code>trigger_fetch_node</code> <p>Executes a specific node in the graph by its ID.</p> <p>Attributes:</p> Name Type Description <code>execution_location</code> <code>ExecutionLocationsLiteral</code> <p>Gets the current execution location.</p> <code>execution_mode</code> <code>ExecutionModeLiteral</code> <p>Gets the current execution mode ('Development' or 'Performance').</p> <code>flow_id</code> <code>int</code> <p>Gets the unique identifier of the flow.</p> <code>graph_has_functions</code> <code>bool</code> <p>Checks if the graph has any nodes.</p> <code>graph_has_input_data</code> <code>bool</code> <p>Checks if the graph has an initial input data source.</p> <code>node_connections</code> <code>List[Tuple[int, int]]</code> <p>Computes and returns a list of all connections in the graph.</p> <code>nodes</code> <code>List[FlowNode]</code> <p>Gets a list of all FlowNode objects in the graph.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>class FlowGraph:\n    \"\"\"A class representing a Directed Acyclic Graph (DAG) for data processing pipelines.\n\n    It manages nodes, connections, and the execution of the entire flow.\n    \"\"\"\n    uuid: str\n    depends_on: Dict[int, Union[ParquetFile, FlowDataEngine, \"FlowGraph\", pl.DataFrame,]]\n    _flow_id: int\n    _input_data: Union[ParquetFile, FlowDataEngine, \"FlowGraph\"]\n    _input_cols: List[str]\n    _output_cols: List[str]\n    _node_db: Dict[Union[str, int], FlowNode]\n    _node_ids: List[Union[str, int]]\n    _results: Optional[FlowDataEngine] = None\n    cache_results: bool = False\n    schema: Optional[List[FlowfileColumn]] = None\n    has_over_row_function: bool = False\n    _flow_starts: List[Union[int, str]] = None\n    latest_run_info: Optional[RunInformation] = None\n    start_datetime: datetime = None\n    end_datetime: datetime = None\n    _flow_settings: schemas.FlowSettings = None\n    flow_logger: FlowLogger\n\n    def __init__(self,\n                 flow_settings: schemas.FlowSettings | schemas.FlowGraphConfig,\n                 name: str = None, input_cols: List[str] = None,\n                 output_cols: List[str] = None,\n                 path_ref: str = None,\n                 input_flow: Union[ParquetFile, FlowDataEngine, \"FlowGraph\"] = None,\n                 cache_results: bool = False):\n        \"\"\"Initializes a new FlowGraph instance.\n\n        Args:\n            flow_settings: The configuration settings for the flow.\n            name: The name of the flow.\n            input_cols: A list of input column names.\n            output_cols: A list of output column names.\n            path_ref: An optional path to an initial data source.\n            input_flow: An optional existing data object to start the flow with.\n            cache_results: A global flag to enable or disable result caching.\n        \"\"\"\n        if isinstance(flow_settings, schemas.FlowGraphConfig):\n            flow_settings = schemas.FlowSettings.from_flow_settings_input(flow_settings)\n\n        self._flow_settings = flow_settings\n        self.uuid = str(uuid1())\n        self.start_datetime = None\n        self.end_datetime = None\n        self.latest_run_info = None\n        self._flow_id = flow_settings.flow_id\n        self.flow_logger = FlowLogger(flow_settings.flow_id)\n        self._flow_starts: List[FlowNode] = []\n        self._results = None\n        self.schema = None\n        self.has_over_row_function = False\n        self._input_cols = [] if input_cols is None else input_cols\n        self._output_cols = [] if output_cols is None else output_cols\n        self._node_ids = []\n        self._node_db = {}\n        self.cache_results = cache_results\n        self.__name__ = name if name else id(self)\n        self.depends_on = {}\n        if path_ref is not None:\n            self.add_datasource(input_schema.NodeDatasource(file_path=path_ref))\n        elif input_flow is not None:\n            self.add_datasource(input_file=input_flow)\n\n    @property\n    def flow_settings(self) -&gt; schemas.FlowSettings:\n        return self._flow_settings\n\n    @flow_settings.setter\n    def flow_settings(self, flow_settings: schemas.FlowSettings):\n        if (\n                (self._flow_settings.execution_location != flow_settings.execution_location) or\n                (self._flow_settings.execution_mode != flow_settings.execution_mode)\n        ):\n            self.reset()\n        self._flow_settings = flow_settings\n\n    def add_node_promise(self, node_promise: input_schema.NodePromise):\n        \"\"\"Adds a placeholder node to the graph that is not yet fully configured.\n\n        Useful for building the graph structure before all settings are available.\n\n        Args:\n            node_promise: A promise object containing basic node information.\n        \"\"\"\n        def placeholder(n: FlowNode = None):\n            if n is None:\n                return FlowDataEngine()\n            return n\n\n        self.add_node_step(node_id=node_promise.node_id, node_type=node_promise.node_type, function=placeholder,\n                           setting_input=node_promise)\n\n    def apply_layout(self, y_spacing: int = 150, x_spacing: int = 200, initial_y: int = 100):\n        \"\"\"Calculates and applies a layered layout to all nodes in the graph.\n\n        This updates their x and y positions for UI rendering.\n\n        Args:\n            y_spacing: The vertical spacing between layers.\n            x_spacing: The horizontal spacing between nodes in the same layer.\n            initial_y: The initial y-position for the first layer.\n        \"\"\"\n        self.flow_logger.info(\"Applying layered layout...\")\n        start_time = time()\n        try:\n            # Calculate new positions for all nodes\n            new_positions = calculate_layered_layout(\n                self, y_spacing=y_spacing, x_spacing=x_spacing, initial_y=initial_y\n            )\n\n            if not new_positions:\n                self.flow_logger.warning(\"Layout calculation returned no positions.\")\n                return\n\n            # Apply the new positions to the setting_input of each node\n            updated_count = 0\n            for node_id, (pos_x, pos_y) in new_positions.items():\n                node = self.get_node(node_id)\n                if node and hasattr(node, 'setting_input'):\n                    setting = node.setting_input\n                    if hasattr(setting, 'pos_x') and hasattr(setting, 'pos_y'):\n                        setting.pos_x = pos_x\n                        setting.pos_y = pos_y\n                        updated_count += 1\n                    else:\n                        self.flow_logger.warning(f\"Node {node_id} setting_input ({type(setting)}) lacks pos_x/pos_y attributes.\")\n                elif node:\n                    self.flow_logger.warning(f\"Node {node_id} lacks setting_input attribute.\")\n                # else: Node not found, already warned by calculate_layered_layout\n\n            end_time = time()\n            self.flow_logger.info(f\"Layout applied to {updated_count}/{len(self.nodes)} nodes in {end_time - start_time:.2f} seconds.\")\n\n        except Exception as e:\n            self.flow_logger.error(f\"Error applying layout: {e}\")\n            raise  # Optional: re-raise the exception\n\n    @property\n    def flow_id(self) -&gt; int:\n        \"\"\"Gets the unique identifier of the flow.\"\"\"\n        return self._flow_id\n\n    @flow_id.setter\n    def flow_id(self, new_id: int):\n        \"\"\"Sets the unique identifier for the flow and updates all child nodes.\n\n        Args:\n            new_id: The new flow ID.\n        \"\"\"\n        self._flow_id = new_id\n        for node in self.nodes:\n            if hasattr(node.setting_input, 'flow_id'):\n                node.setting_input.flow_id = new_id\n        self.flow_settings.flow_id = new_id\n\n    def __repr__(self):\n        \"\"\"Provides the official string representation of the FlowGraph instance.\"\"\"\n        settings_str = \"  -\" + '\\n  -'.join(f\"{k}: {v}\" for k, v in self.flow_settings)\n        return f\"FlowGraph(\\nNodes: {self._node_db}\\n\\nSettings:\\n{settings_str}\"\n\n    def print_tree(self):\n        \"\"\"Print flow_graph as a visual tree structure, showing the DAG relationships with ASCII art.\"\"\"\n        if not self._node_db:\n            self.flow_logger.info(\"Empty flow graph\")\n            return\n\n        # Build node information\n        node_info = build_node_info(self.nodes)\n\n        # Calculate depths for all nodes\n        for node_id in node_info:\n            calculate_depth(node_id, node_info)\n\n        # Group nodes by depth\n        depth_groups, max_depth = group_nodes_by_depth(node_info)\n\n        # Sort nodes within each depth group\n        for depth in depth_groups:\n            depth_groups[depth].sort()\n\n        # Create the main flow visualization\n        lines = [\"=\" * 80, \"Flow Graph Visualization\", \"=\" * 80, \"\"]\n\n        # Track which nodes connect to what\n        merge_points = define_node_connections(node_info)\n\n        # Build the flow paths\n\n        # Find the maximum label length for each depth level\n        max_label_length = {}\n        for depth in range(max_depth + 1):\n            if depth in depth_groups:\n                max_len = max(len(node_info[nid].label) for nid in depth_groups[depth])\n                max_label_length[depth] = max_len\n\n        # Draw the paths\n        drawn_nodes = set()\n        merge_drawn = set()\n\n        # Group paths by their merge points\n        paths_by_merge = {}\n        standalone_paths = []\n\n        # Build flow paths\n        paths = build_flow_paths(node_info, self._flow_starts, merge_points)\n\n        # Define paths to merge and standalone paths\n        for path in paths:\n            if len(path) &gt; 1 and path[-1] in merge_points and len(merge_points[path[-1]]) &gt; 1:\n                merge_id = path[-1]\n                if merge_id not in paths_by_merge:\n                    paths_by_merge[merge_id] = []\n                paths_by_merge[merge_id].append(path)\n            else:\n                standalone_paths.append(path)\n\n        # Draw merged paths\n        draw_merged_paths(node_info, merge_points, paths_by_merge, merge_drawn, drawn_nodes, lines)\n\n        # Draw standlone paths\n        draw_standalone_paths(drawn_nodes, standalone_paths, lines, node_info)\n\n        # Add undrawn nodes\n        add_un_drawn_nodes(drawn_nodes, node_info, lines)\n\n        try:\n            skip_nodes, ordered_nodes = compute_execution_plan(\n                nodes=self.nodes,\n                flow_starts=self._flow_starts+self.get_implicit_starter_nodes())\n            if ordered_nodes:\n                for i, node in enumerate(ordered_nodes, 1):\n                    lines.append(f\"  {i:3d}. {node_info[node.node_id].label}\")\n        except Exception as e:\n            lines.append(f\"  Could not determine execution order: {e}\")\n\n        # Print everything\n        output = \"\\n\".join(lines)\n\n        print(output)\n\n    def get_nodes_overview(self):\n        \"\"\"Gets a list of dictionary representations for all nodes in the graph.\"\"\"\n        output = []\n        for v in self._node_db.values():\n            output.append(v.get_repr())\n        return output\n\n    def remove_from_output_cols(self, columns: List[str]):\n        \"\"\"Removes specified columns from the list of expected output columns.\n\n        Args:\n            columns: A list of column names to remove.\n        \"\"\"\n        cols = set(columns)\n        self._output_cols = [c for c in self._output_cols if c not in cols]\n\n    def get_node(self, node_id: Union[int, str] = None) -&gt; FlowNode | None:\n        \"\"\"Retrieves a node from the graph by its ID.\n\n        Args:\n            node_id: The ID of the node to retrieve. If None, retrieves the last added node.\n\n        Returns:\n            The FlowNode object, or None if not found.\n        \"\"\"\n        if node_id is None:\n            node_id = self._node_ids[-1]\n        node = self._node_db.get(node_id)\n        if node is not None:\n            return node\n\n    def add_user_defined_node(self, *,\n                              custom_node: CustomNodeBase,\n                              user_defined_node_settings: input_schema.UserDefinedNode\n                              ):\n\n        def _func(*fdes: FlowDataEngine) -&gt; FlowDataEngine | None:\n            output = custom_node.process(*(fde.data_frame for fde in fdes))\n            if isinstance(output, pl.LazyFrame | pl.DataFrame):\n                return FlowDataEngine(output)\n            return None\n\n        self.add_node_step(node_id=user_defined_node_settings.node_id,\n                           function=_func,\n                           setting_input=user_defined_node_settings,\n                           input_node_ids=user_defined_node_settings.depending_on_ids,\n                           node_type=custom_node.item,\n                           )\n\n    def add_pivot(self, pivot_settings: input_schema.NodePivot):\n        \"\"\"Adds a pivot node to the graph.\n\n        Args:\n            pivot_settings: The settings for the pivot operation.\n        \"\"\"\n\n        def _func(fl: FlowDataEngine):\n            return fl.do_pivot(pivot_settings.pivot_input, self.flow_logger.get_node_logger(pivot_settings.node_id))\n\n        self.add_node_step(node_id=pivot_settings.node_id,\n                           function=_func,\n                           node_type='pivot',\n                           setting_input=pivot_settings,\n                           input_node_ids=[pivot_settings.depending_on_id])\n\n        node = self.get_node(pivot_settings.node_id)\n\n        def schema_callback():\n            input_data = node.singular_main_input.get_resulting_data()  # get from the previous step the data\n            input_data.lazy = True  # ensure the dataset is lazy\n            input_lf = input_data.data_frame  # get the lazy frame\n            return pre_calculate_pivot_schema(input_data.schema, pivot_settings.pivot_input, input_lf=input_lf)\n        node.schema_callback = schema_callback\n\n    def add_unpivot(self, unpivot_settings: input_schema.NodeUnpivot):\n        \"\"\"Adds an unpivot node to the graph.\n\n        Args:\n            unpivot_settings: The settings for the unpivot operation.\n        \"\"\"\n\n        def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n            return fl.unpivot(unpivot_settings.unpivot_input)\n\n        self.add_node_step(node_id=unpivot_settings.node_id,\n                           function=_func,\n                           node_type='unpivot',\n                           setting_input=unpivot_settings,\n                           input_node_ids=[unpivot_settings.depending_on_id])\n\n    def add_union(self, union_settings: input_schema.NodeUnion):\n        \"\"\"Adds a union node to combine multiple data streams.\n\n        Args:\n            union_settings: The settings for the union operation.\n        \"\"\"\n\n        def _func(*flowfile_tables: FlowDataEngine):\n            dfs: List[pl.LazyFrame] | List[pl.DataFrame] = [flt.data_frame for flt in flowfile_tables]\n            return FlowDataEngine(pl.concat(dfs, how='diagonal_relaxed'))\n\n        self.add_node_step(node_id=union_settings.node_id,\n                           function=_func,\n                           node_type=f'union',\n                           setting_input=union_settings,\n                           input_node_ids=union_settings.depending_on_ids)\n\n    def add_initial_node_analysis(self, node_promise: input_schema.NodePromise):\n        \"\"\"Adds a data exploration/analysis node based on a node promise.\n\n        Args:\n            node_promise: The promise representing the node to be analyzed.\n        \"\"\"\n        node_analysis = create_graphic_walker_node_from_node_promise(node_promise)\n        self.add_explore_data(node_analysis)\n\n    def add_explore_data(self, node_analysis: input_schema.NodeExploreData):\n        \"\"\"Adds a specialized node for data exploration and visualization.\n\n        Args:\n            node_analysis: The settings for the data exploration node.\n        \"\"\"\n        sample_size: int = 10000\n\n        def analysis_preparation(flowfile_table: FlowDataEngine):\n            if flowfile_table.number_of_records &lt;= 0:\n                number_of_records = flowfile_table.get_number_of_records(calculate_in_worker_process=True)\n            else:\n                number_of_records = flowfile_table.number_of_records\n            if number_of_records &gt; sample_size:\n                flowfile_table = flowfile_table.get_sample(sample_size, random=True)\n            external_sampler = ExternalDfFetcher(\n                lf=flowfile_table.data_frame,\n                file_ref=\"__gf_walker\"+node.hash,\n                wait_on_completion=True,\n                node_id=node.node_id,\n                flow_id=self.flow_id,\n            )\n            node.results.analysis_data_generator = get_read_top_n(external_sampler.status.file_ref,\n                                                                  n=min(sample_size, number_of_records))\n            return flowfile_table\n\n        def schema_callback():\n            node = self.get_node(node_analysis.node_id)\n            if len(node.all_inputs) == 1:\n                input_node = node.all_inputs[0]\n                return input_node.schema\n            else:\n                return [FlowfileColumn.from_input('col_1', 'na')]\n\n        self.add_node_step(node_id=node_analysis.node_id, node_type='explore_data',\n                           function=analysis_preparation,\n                           setting_input=node_analysis, schema_callback=schema_callback)\n        node = self.get_node(node_analysis.node_id)\n\n    def add_group_by(self, group_by_settings: input_schema.NodeGroupBy):\n        \"\"\"Adds a group-by aggregation node to the graph.\n\n        Args:\n            group_by_settings: The settings for the group-by operation.\n        \"\"\"\n\n        def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n            return fl.do_group_by(group_by_settings.groupby_input, False)\n\n        self.add_node_step(node_id=group_by_settings.node_id,\n                           function=_func,\n                           node_type=f'group_by',\n                           setting_input=group_by_settings,\n                           input_node_ids=[group_by_settings.depending_on_id])\n\n        node = self.get_node(group_by_settings.node_id)\n\n        def schema_callback():\n\n            output_columns = [(c.old_name, c.new_name, c.output_type) for c in group_by_settings.groupby_input.agg_cols]\n            depends_on = node.node_inputs.main_inputs[0]\n            input_schema_dict: Dict[str, str] = {s.name: s.data_type for s in depends_on.schema}\n            output_schema = []\n            for old_name, new_name, data_type in output_columns:\n                data_type = input_schema_dict[old_name] if data_type is None else data_type\n                output_schema.append(FlowfileColumn.from_input(data_type=data_type, column_name=new_name))\n            return output_schema\n\n        node.schema_callback = schema_callback\n\n    def add_filter(self, filter_settings: input_schema.NodeFilter):\n        \"\"\"Adds a filter node to the graph.\n\n        Args:\n            filter_settings: The settings for the filter operation.\n        \"\"\"\n\n        is_advanced = filter_settings.filter_input.filter_type == 'advanced'\n        if is_advanced:\n            predicate = filter_settings.filter_input.advanced_filter\n        else:\n            _basic_filter = filter_settings.filter_input.basic_filter\n            filter_settings.filter_input.advanced_filter = (f'[{_basic_filter.field}]{_basic_filter.filter_type}\"'\n                                                            f'{_basic_filter.filter_value}\"')\n\n        def _func(fl: FlowDataEngine):\n            is_advanced = filter_settings.filter_input.filter_type == 'advanced'\n            if is_advanced:\n                return fl.do_filter(predicate)\n            else:\n                basic_filter = filter_settings.filter_input.basic_filter\n                if basic_filter.filter_value.isnumeric():\n                    field_data_type = fl.get_schema_column(basic_filter.field).generic_datatype()\n                    if field_data_type == 'str':\n                        _f = f'[{basic_filter.field}]{basic_filter.filter_type}\"{basic_filter.filter_value}\"'\n                    else:\n                        _f = f'[{basic_filter.field}]{basic_filter.filter_type}{basic_filter.filter_value}'\n                else:\n                    _f = f'[{basic_filter.field}]{basic_filter.filter_type}\"{basic_filter.filter_value}\"'\n                filter_settings.filter_input.advanced_filter = _f\n                return fl.do_filter(_f)\n\n        self.add_node_step(filter_settings.node_id, _func,\n                           node_type='filter',\n                           renew_schema=False,\n                           setting_input=filter_settings,\n                           input_node_ids=[filter_settings.depending_on_id]\n                           )\n\n    def add_record_count(self, node_number_of_records: input_schema.NodeRecordCount):\n        \"\"\"Adds a filter node to the graph.\n\n        Args:\n            node_number_of_records: The settings for the record count operation.\n        \"\"\"\n\n        def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n            return fl.get_record_count()\n\n        self.add_node_step(node_id=node_number_of_records.node_id,\n                           function=_func,\n                           node_type='record_count',\n                           setting_input=node_number_of_records,\n                           input_node_ids=[node_number_of_records.depending_on_id])\n\n    def add_polars_code(self, node_polars_code: input_schema.NodePolarsCode):\n        \"\"\"Adds a node that executes custom Polars code.\n\n        Args:\n            node_polars_code: The settings for the Polars code node.\n        \"\"\"\n\n        def _func(*flowfile_tables: FlowDataEngine) -&gt; FlowDataEngine:\n            return execute_polars_code(*flowfile_tables, code=node_polars_code.polars_code_input.polars_code)\n        self.add_node_step(node_id=node_polars_code.node_id,\n                           function=_func,\n                           node_type='polars_code',\n                           setting_input=node_polars_code,\n                           input_node_ids=node_polars_code.depending_on_ids)\n\n        try:\n            polars_code_parser.validate_code(node_polars_code.polars_code_input.polars_code)\n        except Exception as e:\n            node = self.get_node(node_id=node_polars_code.node_id)\n            node.results.errors = str(e)\n\n    def add_dependency_on_polars_lazy_frame(self,\n                                            lazy_frame: pl.LazyFrame,\n                                            node_id: int):\n        \"\"\"Adds a special node that directly injects a Polars LazyFrame into the graph.\n\n        Note: This is intended for backend use and will not work in the UI editor.\n\n        Args:\n            lazy_frame: The Polars LazyFrame to inject.\n            node_id: The ID for the new node.\n        \"\"\"\n        def _func():\n            return FlowDataEngine(lazy_frame)\n        node_promise = input_schema.NodePromise(flow_id=self.flow_id,\n                                                node_id=node_id, node_type=\"polars_lazy_frame\",\n                                                is_setup=True)\n        self.add_node_step(node_id=node_promise.node_id, node_type=node_promise.node_type, function=_func,\n                           setting_input=node_promise)\n\n    def add_unique(self, unique_settings: input_schema.NodeUnique):\n        \"\"\"Adds a node to find and remove duplicate rows.\n\n        Args:\n            unique_settings: The settings for the unique operation.\n        \"\"\"\n\n        def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n            return fl.make_unique(unique_settings.unique_input)\n\n        self.add_node_step(node_id=unique_settings.node_id,\n                           function=_func,\n                           input_columns=[],\n                           node_type='unique',\n                           setting_input=unique_settings,\n                           input_node_ids=[unique_settings.depending_on_id])\n\n    def add_graph_solver(self, graph_solver_settings: input_schema.NodeGraphSolver):\n        \"\"\"Adds a node that solves graph-like problems within the data.\n\n        This node can be used for operations like finding network paths,\n        calculating connected components, or performing other graph algorithms\n        on relational data that represents nodes and edges.\n\n        Args:\n            graph_solver_settings: The settings object defining the graph inputs\n                and the specific algorithm to apply.\n        \"\"\"\n        def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n            return fl.solve_graph(graph_solver_settings.graph_solver_input)\n\n        self.add_node_step(node_id=graph_solver_settings.node_id,\n                           function=_func,\n                           node_type='graph_solver',\n                           setting_input=graph_solver_settings,\n                           input_node_ids=[graph_solver_settings.depending_on_id])\n\n    def add_formula(self, function_settings: input_schema.NodeFormula):\n        \"\"\"Adds a node that applies a formula to create or modify a column.\n\n        Args:\n            function_settings: The settings for the formula operation.\n        \"\"\"\n\n        error = \"\"\n        if function_settings.function.field.data_type not in (None, \"Auto\"):\n            output_type = cast_str_to_polars_type(function_settings.function.field.data_type)\n        else:\n            output_type = None\n        if output_type not in (None, \"Auto\"):\n            new_col = [FlowfileColumn.from_input(column_name=function_settings.function.field.name,\n                                                 data_type=str(output_type))]\n        else:\n            new_col = [FlowfileColumn.from_input(function_settings.function.field.name, 'String')]\n\n        def _func(fl: FlowDataEngine):\n            return fl.apply_sql_formula(func=function_settings.function.function,\n                                        col_name=function_settings.function.field.name,\n                                        output_data_type=output_type)\n\n        self.add_node_step(function_settings.node_id, _func,\n                           output_schema=new_col,\n                           node_type='formula',\n                           renew_schema=False,\n                           setting_input=function_settings,\n                           input_node_ids=[function_settings.depending_on_id]\n                           )\n        if error != \"\":\n            node = self.get_node(function_settings.node_id)\n            node.results.errors = error\n            return False, error\n        else:\n            return True, \"\"\n\n    def add_cross_join(self, cross_join_settings: input_schema.NodeCrossJoin) -&gt; \"FlowGraph\":\n        \"\"\"Adds a cross join node to the graph.\n\n        Args:\n            cross_join_settings: The settings for the cross join operation.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n\n        def _func(main: FlowDataEngine, right: FlowDataEngine) -&gt; FlowDataEngine:\n            for left_select in cross_join_settings.cross_join_input.left_select.renames:\n                left_select.is_available = True if left_select.old_name in main.schema else False\n            for right_select in cross_join_settings.cross_join_input.right_select.renames:\n                right_select.is_available = True if right_select.old_name in right.schema else False\n\n            return main.do_cross_join(cross_join_input=cross_join_settings.cross_join_input,\n                                      auto_generate_selection=cross_join_settings.auto_generate_selection,\n                                      verify_integrity=False,\n                                      other=right)\n\n        self.add_node_step(node_id=cross_join_settings.node_id,\n                           function=_func,\n                           input_columns=[],\n                           node_type='cross_join',\n                           setting_input=cross_join_settings,\n                           input_node_ids=cross_join_settings.depending_on_ids)\n        return self\n\n    def add_join(self, join_settings: input_schema.NodeJoin) -&gt; \"FlowGraph\":\n        \"\"\"Adds a join node to combine two data streams based on key columns.\n\n        Args:\n            join_settings: The settings for the join operation.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n\n        def _func(main: FlowDataEngine, right: FlowDataEngine) -&gt; FlowDataEngine:\n            for left_select in join_settings.join_input.left_select.renames:\n                left_select.is_available = True if left_select.old_name in main.schema else False\n            for right_select in join_settings.join_input.right_select.renames:\n                right_select.is_available = True if right_select.old_name in right.schema else False\n\n            return main.join(join_input=join_settings.join_input,\n                             auto_generate_selection=join_settings.auto_generate_selection,\n                             verify_integrity=False,\n                             other=right)\n\n        self.add_node_step(node_id=join_settings.node_id,\n                           function=_func,\n                           input_columns=[],\n                           node_type='join',\n                           setting_input=join_settings,\n                           input_node_ids=join_settings.depending_on_ids)\n        return self\n\n    def add_fuzzy_match(self, fuzzy_settings: input_schema.NodeFuzzyMatch) -&gt; \"FlowGraph\":\n        \"\"\"Adds a fuzzy matching node to join data on approximate string matches.\n\n        Args:\n            fuzzy_settings: The settings for the fuzzy match operation.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n\n        def _func(main: FlowDataEngine, right: FlowDataEngine) -&gt; FlowDataEngine:\n            node = self.get_node(node_id=fuzzy_settings.node_id)\n            if self.execution_location == \"local\":\n                return main.fuzzy_join(fuzzy_match_input=deepcopy(fuzzy_settings.join_input),\n                                       other=right,\n                                       node_logger=self.flow_logger.get_node_logger(fuzzy_settings.node_id))\n\n            f = main.start_fuzzy_join(fuzzy_match_input=deepcopy(fuzzy_settings.join_input), other=right, file_ref=node.hash,\n                                      flow_id=self.flow_id, node_id=fuzzy_settings.node_id)\n            logger.info(\"Started the fuzzy match action\")\n            node._fetch_cached_df = f  # Add to the node so it can be cancelled and fetch later if needed\n            return FlowDataEngine(f.get_result())\n\n        def schema_callback():\n            fm_input_copy = deepcopy(fuzzy_settings.join_input)  # Deepcopy create an unique object per func\n            node = self.get_node(node_id=fuzzy_settings.node_id)\n            return calculate_fuzzy_match_schema(fm_input_copy,\n                                                left_schema=node.node_inputs.main_inputs[0].schema,\n                                                right_schema=node.node_inputs.right_input.schema\n                                                )\n\n        self.add_node_step(node_id=fuzzy_settings.node_id,\n                           function=_func,\n                           input_columns=[],\n                           node_type='fuzzy_match',\n                           setting_input=fuzzy_settings,\n                           input_node_ids=fuzzy_settings.depending_on_ids,\n                           schema_callback=schema_callback)\n\n        return self\n\n    def add_text_to_rows(self, node_text_to_rows: input_schema.NodeTextToRows) -&gt; \"FlowGraph\":\n        \"\"\"Adds a node that splits cell values into multiple rows.\n\n        This is useful for un-nesting data where a single field contains multiple\n        values separated by a delimiter.\n\n        Args:\n            node_text_to_rows: The settings object that specifies the column to split\n                and the delimiter to use.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n        def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n            return table.split(node_text_to_rows.text_to_rows_input)\n\n        self.add_node_step(node_id=node_text_to_rows.node_id,\n                           function=_func,\n                           node_type='text_to_rows',\n                           setting_input=node_text_to_rows,\n                           input_node_ids=[node_text_to_rows.depending_on_id])\n        return self\n\n    def add_sort(self, sort_settings: input_schema.NodeSort) -&gt; \"FlowGraph\":\n        \"\"\"Adds a node to sort the data based on one or more columns.\n\n        Args:\n            sort_settings: The settings for the sort operation.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n\n        def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n            return table.do_sort(sort_settings.sort_input)\n\n        self.add_node_step(node_id=sort_settings.node_id,\n                           function=_func,\n                           node_type='sort',\n                           setting_input=sort_settings,\n                           input_node_ids=[sort_settings.depending_on_id])\n        return self\n\n    def add_sample(self, sample_settings: input_schema.NodeSample) -&gt; \"FlowGraph\":\n        \"\"\"Adds a node to take a random or top-N sample of the data.\n\n        Args:\n            sample_settings: The settings object specifying the size of the sample.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n        def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n            return table.get_sample(sample_settings.sample_size)\n\n        self.add_node_step(node_id=sample_settings.node_id,\n                           function=_func,\n                           node_type='sample',\n                           setting_input=sample_settings,\n                           input_node_ids=[sample_settings.depending_on_id]\n                           )\n        return self\n\n    def add_record_id(self, record_id_settings: input_schema.NodeRecordId) -&gt; \"FlowGraph\":\n        \"\"\"Adds a node to create a new column with a unique ID for each record.\n\n        Args:\n            record_id_settings: The settings object specifying the name of the\n                new record ID column.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n\n        def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n            return table.add_record_id(record_id_settings.record_id_input)\n\n        self.add_node_step(node_id=record_id_settings.node_id,\n                           function=_func,\n                           node_type='record_id',\n                           setting_input=record_id_settings,\n                           input_node_ids=[record_id_settings.depending_on_id]\n                           )\n        return self\n\n    def add_select(self, select_settings: input_schema.NodeSelect) -&gt; \"FlowGraph\":\n        \"\"\"Adds a node to select, rename, reorder, or drop columns.\n\n        Args:\n            select_settings: The settings for the select operation.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n\n        select_cols = select_settings.select_input\n        drop_cols = tuple(s.old_name for s in select_settings.select_input)\n\n        def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n            input_cols = set(f.name for f in table.schema)\n            ids_to_remove = []\n            for i, select_col in enumerate(select_cols):\n                if select_col.data_type is None:\n                    select_col.data_type = table.get_schema_column(select_col.old_name).data_type\n                if select_col.old_name not in input_cols:\n                    select_col.is_available = False\n                    if not select_col.keep:\n                        ids_to_remove.append(i)\n                else:\n                    select_col.is_available = True\n            ids_to_remove.reverse()\n            for i in ids_to_remove:\n                v = select_cols.pop(i)\n                del v\n            return table.do_select(select_inputs=transform_schema.SelectInputs(select_cols),\n                                   keep_missing=select_settings.keep_missing)\n\n        self.add_node_step(node_id=select_settings.node_id,\n                           function=_func,\n                           input_columns=[],\n                           node_type='select',\n                           drop_columns=list(drop_cols),\n                           setting_input=select_settings,\n                           input_node_ids=[select_settings.depending_on_id])\n        return self\n\n    @property\n    def graph_has_functions(self) -&gt; bool:\n        \"\"\"Checks if the graph has any nodes.\"\"\"\n        return len(self._node_ids) &gt; 0\n\n    def delete_node(self, node_id: Union[int, str]):\n        \"\"\"Deletes a node from the graph and updates all its connections.\n\n        Args:\n            node_id: The ID of the node to delete.\n\n        Raises:\n            Exception: If the node with the given ID does not exist.\n        \"\"\"\n        logger.info(f\"Starting deletion of node with ID: {node_id}\")\n\n        node = self._node_db.get(node_id)\n        if node:\n            logger.info(f\"Found node: {node_id}, processing deletion\")\n\n            lead_to_steps: List[FlowNode] = node.leads_to_nodes\n            logger.debug(f\"Node {node_id} leads to {len(lead_to_steps)} other nodes\")\n\n            if len(lead_to_steps) &gt; 0:\n                for lead_to_step in lead_to_steps:\n                    logger.debug(f\"Deleting input node {node_id} from dependent node {lead_to_step}\")\n                    lead_to_step.delete_input_node(node_id, complete=True)\n\n            if not node.is_start:\n                depends_on: List[FlowNode] = node.node_inputs.get_all_inputs()\n                logger.debug(f\"Node {node_id} depends on {len(depends_on)} other nodes\")\n\n                for depend_on in depends_on:\n                    logger.debug(f\"Removing lead_to reference {node_id} from node {depend_on}\")\n                    depend_on.delete_lead_to_node(node_id)\n\n            self._node_db.pop(node_id)\n            logger.debug(f\"Successfully removed node {node_id} from node_db\")\n            del node\n            logger.info(\"Node object deleted\")\n        else:\n            logger.error(f\"Failed to find node with id {node_id}\")\n            raise Exception(f\"Node with id {node_id} does not exist\")\n\n    @property\n    def graph_has_input_data(self) -&gt; bool:\n        \"\"\"Checks if the graph has an initial input data source.\"\"\"\n        return self._input_data is not None\n\n    def add_node_step(self,\n                      node_id: Union[int, str],\n                      function: Callable,\n                      input_columns: List[str] = None,\n                      output_schema: List[FlowfileColumn] = None,\n                      node_type: str = None,\n                      drop_columns: List[str] = None,\n                      renew_schema: bool = True,\n                      setting_input: Any = None,\n                      cache_results: bool = None,\n                      schema_callback: Callable = None,\n                      input_node_ids: List[int] = None) -&gt; FlowNode:\n        \"\"\"The core method for adding or updating a node in the graph.\n\n        Args:\n            node_id: The unique ID for the node.\n            function: The core processing function for the node.\n            input_columns: A list of input column names required by the function.\n            output_schema: A predefined schema for the node's output.\n            node_type: A string identifying the type of node (e.g., 'filter', 'join').\n            drop_columns: A list of columns to be dropped after the function executes.\n            renew_schema: If True, the schema is recalculated after execution.\n            setting_input: A configuration object containing settings for the node.\n            cache_results: If True, the node's results are cached for future runs.\n            schema_callback: A function that dynamically calculates the output schema.\n            input_node_ids: A list of IDs for the nodes that this node depends on.\n\n        Returns:\n            The created or updated FlowNode object.\n        \"\"\"\n        existing_node = self.get_node(node_id)\n        if existing_node is not None:\n            if existing_node.node_type != node_type:\n                self.delete_node(existing_node.node_id)\n                existing_node = None\n        if existing_node:\n            input_nodes = existing_node.all_inputs\n        elif input_node_ids is not None:\n            input_nodes = [self.get_node(node_id) for node_id in input_node_ids]\n        else:\n            input_nodes = None\n        if isinstance(input_columns, str):\n            input_columns = [input_columns]\n        if (\n                input_nodes is not None or\n                function.__name__ in ('placeholder', 'analysis_preparation') or\n                node_type in (\"cloud_storage_reader\", \"polars_lazy_frame\", \"input_data\")\n        ):\n            if not existing_node:\n                node = FlowNode(node_id=node_id,\n                                function=function,\n                                output_schema=output_schema,\n                                input_columns=input_columns,\n                                drop_columns=drop_columns,\n                                renew_schema=renew_schema,\n                                setting_input=setting_input,\n                                node_type=node_type,\n                                name=function.__name__,\n                                schema_callback=schema_callback,\n                                parent_uuid=self.uuid)\n            else:\n                existing_node.update_node(function=function,\n                                          output_schema=output_schema,\n                                          input_columns=input_columns,\n                                          drop_columns=drop_columns,\n                                          setting_input=setting_input,\n                                          schema_callback=schema_callback)\n                node = existing_node\n        else:\n            raise Exception(\"No data initialized\")\n        self._node_db[node_id] = node\n        self._node_ids.append(node_id)\n        return node\n\n    def add_include_cols(self, include_columns: List[str]):\n        \"\"\"Adds columns to both the input and output column lists.\n\n        Args:\n            include_columns: A list of column names to include.\n        \"\"\"\n        for column in include_columns:\n            if column not in self._input_cols:\n                self._input_cols.append(column)\n            if column not in self._output_cols:\n                self._output_cols.append(column)\n        return self\n\n    def add_output(self, output_file: input_schema.NodeOutput):\n        \"\"\"Adds an output node to write the final data to a destination.\n\n        Args:\n            output_file: The settings for the output file.\n        \"\"\"\n\n        def _func(df: FlowDataEngine):\n            output_file.output_settings.populate_abs_file_path()\n            execute_remote = self.execution_location != 'local'\n            df.output(output_fs=output_file.output_settings, flow_id=self.flow_id, node_id=output_file.node_id,\n                      execute_remote=execute_remote)\n            return df\n\n        def schema_callback():\n            input_node: FlowNode = self.get_node(output_file.node_id).node_inputs.main_inputs[0]\n\n            return input_node.schema\n        input_node_id = getattr(output_file, \"depending_on_id\") if hasattr(output_file, 'depending_on_id') else None\n        self.add_node_step(node_id=output_file.node_id,\n                           function=_func,\n                           input_columns=[],\n                           node_type='output',\n                           setting_input=output_file,\n                           schema_callback=schema_callback,\n                           input_node_ids=[input_node_id])\n\n    def add_database_writer(self, node_database_writer: input_schema.NodeDatabaseWriter):\n        \"\"\"Adds a node to write data to a database.\n\n        Args:\n            node_database_writer: The settings for the database writer node.\n        \"\"\"\n\n        node_type = 'database_writer'\n        database_settings: input_schema.DatabaseWriteSettings = node_database_writer.database_write_settings\n        database_connection: Optional[input_schema.DatabaseConnection | input_schema.FullDatabaseConnection]\n        if database_settings.connection_mode == 'inline':\n            database_connection: input_schema.DatabaseConnection = database_settings.database_connection\n            encrypted_password = get_encrypted_secret(current_user_id=node_database_writer.user_id,\n                                                      secret_name=database_connection.password_ref)\n            if encrypted_password is None:\n                raise HTTPException(status_code=400, detail=\"Password not found\")\n        else:\n            database_reference_settings = get_local_database_connection(database_settings.database_connection_name,\n                                                                        node_database_writer.user_id)\n            encrypted_password = database_reference_settings.password.get_secret_value()\n\n        def _func(df: FlowDataEngine):\n            df.lazy = True\n            database_external_write_settings = (\n                sql_models.DatabaseExternalWriteSettings.create_from_from_node_database_writer(\n                    node_database_writer=node_database_writer,\n                    password=encrypted_password,\n                    table_name=(database_settings.schema_name+'.'+database_settings.table_name\n                                if database_settings.schema_name else database_settings.table_name),\n                    database_reference_settings=(database_reference_settings if database_settings.connection_mode == 'reference'\n                                                 else None),\n                    lf=df.data_frame\n                )\n            )\n            external_database_writer = ExternalDatabaseWriter(database_external_write_settings, wait_on_completion=False)\n            node._fetch_cached_df = external_database_writer\n            external_database_writer.get_result()\n            return df\n\n        def schema_callback():\n            input_node: FlowNode = self.get_node(node_database_writer.node_id).node_inputs.main_inputs[0]\n            return input_node.schema\n\n        self.add_node_step(\n            node_id=node_database_writer.node_id,\n            function=_func,\n            input_columns=[],\n            node_type=node_type,\n            setting_input=node_database_writer,\n            schema_callback=schema_callback,\n        )\n        node = self.get_node(node_database_writer.node_id)\n\n    def add_database_reader(self, node_database_reader: input_schema.NodeDatabaseReader):\n        \"\"\"Adds a node to read data from a database.\n\n        Args:\n            node_database_reader: The settings for the database reader node.\n        \"\"\"\n\n        logger.info(\"Adding database reader\")\n        node_type = 'database_reader'\n        database_settings: input_schema.DatabaseSettings = node_database_reader.database_settings\n        database_connection: Optional[input_schema.DatabaseConnection | input_schema.FullDatabaseConnection]\n        if database_settings.connection_mode == 'inline':\n            database_connection: input_schema.DatabaseConnection = database_settings.database_connection\n            encrypted_password = get_encrypted_secret(current_user_id=node_database_reader.user_id,\n                                                      secret_name=database_connection.password_ref)\n            if encrypted_password is None:\n                raise HTTPException(status_code=400, detail=\"Password not found\")\n        else:\n            database_reference_settings = get_local_database_connection(database_settings.database_connection_name,\n                                                                        node_database_reader.user_id)\n            database_connection = database_reference_settings\n            encrypted_password = database_reference_settings.password.get_secret_value()\n\n        def _func():\n            sql_source = BaseSqlSource(query=None if database_settings.query_mode == 'table' else database_settings.query,\n                                       table_name=database_settings.table_name,\n                                       schema_name=database_settings.schema_name,\n                                       fields=node_database_reader.fields,\n                                       )\n            database_external_read_settings = (\n                sql_models.DatabaseExternalReadSettings.create_from_from_node_database_reader(\n                    node_database_reader=node_database_reader,\n                    password=encrypted_password,\n                    query=sql_source.query,\n                    database_reference_settings=(database_reference_settings if database_settings.connection_mode == 'reference'\n                                                 else None),\n                )\n            )\n\n            external_database_fetcher = ExternalDatabaseFetcher(database_external_read_settings, wait_on_completion=False)\n            node._fetch_cached_df = external_database_fetcher\n            fl = FlowDataEngine(external_database_fetcher.get_result())\n            node_database_reader.fields = [c.get_minimal_field_info() for c in fl.schema]\n            return fl\n\n        def schema_callback():\n            sql_source = SqlSource(connection_string=\n                                   sql_utils.construct_sql_uri(database_type=database_connection.database_type,\n                                                               host=database_connection.host,\n                                                               port=database_connection.port,\n                                                               database=database_connection.database,\n                                                               username=database_connection.username,\n                                                               password=decrypt_secret(encrypted_password)),\n                                   query=None if database_settings.query_mode == 'table' else database_settings.query,\n                                   table_name=database_settings.table_name,\n                                   schema_name=database_settings.schema_name,\n                                   fields=node_database_reader.fields,\n                                   )\n            return sql_source.get_schema()\n\n        node = self.get_node(node_database_reader.node_id)\n        if node:\n            node.node_type = node_type\n            node.name = node_type\n            node.function = _func\n            node.setting_input = node_database_reader\n            node.node_settings.cache_results = node_database_reader.cache_results\n            if node_database_reader.node_id not in set(start_node.node_id for start_node in self._flow_starts):\n                self._flow_starts.append(node)\n            node.schema_callback = schema_callback\n        else:\n            node = FlowNode(node_database_reader.node_id, function=_func,\n                            setting_input=node_database_reader,\n                            name=node_type, node_type=node_type, parent_uuid=self.uuid,\n                            schema_callback=schema_callback)\n            self._node_db[node_database_reader.node_id] = node\n            self._flow_starts.append(node)\n            self._node_ids.append(node_database_reader.node_id)\n\n    def add_sql_source(self, external_source_input: input_schema.NodeExternalSource):\n        \"\"\"Adds a node that reads data from a SQL source.\n\n        This is a convenience alias for `add_external_source`.\n\n        Args:\n            external_source_input: The settings for the external SQL source node.\n        \"\"\"\n        logger.info('Adding sql source')\n        self.add_external_source(external_source_input)\n\n    def add_cloud_storage_writer(self, node_cloud_storage_writer: input_schema.NodeCloudStorageWriter) -&gt; None:\n        \"\"\"Adds a node to write data to a cloud storage provider.\n\n        Args:\n            node_cloud_storage_writer: The settings for the cloud storage writer node.\n        \"\"\"\n\n        node_type = \"cloud_storage_writer\"\n        def _func(df: FlowDataEngine):\n            df.lazy = True\n            execute_remote = self.execution_location != 'local'\n            cloud_connection_settings = get_cloud_connection_settings(\n                connection_name=node_cloud_storage_writer.cloud_storage_settings.connection_name,\n                user_id=node_cloud_storage_writer.user_id,\n                auth_mode=node_cloud_storage_writer.cloud_storage_settings.auth_mode\n            )\n            full_cloud_storage_connection = FullCloudStorageConnection(\n                storage_type=cloud_connection_settings.storage_type,\n                auth_method=cloud_connection_settings.auth_method,\n                aws_allow_unsafe_html=cloud_connection_settings.aws_allow_unsafe_html,\n                **CloudStorageReader.get_storage_options(cloud_connection_settings)\n            )\n            if execute_remote:\n                settings = get_cloud_storage_write_settings_worker_interface(\n                    write_settings=node_cloud_storage_writer.cloud_storage_settings,\n                    connection=full_cloud_storage_connection,\n                    lf=df.data_frame,\n                    flowfile_node_id=node_cloud_storage_writer.node_id,\n                    flowfile_flow_id=self.flow_id)\n                external_database_writer = ExternalCloudWriter(settings, wait_on_completion=False)\n                node._fetch_cached_df = external_database_writer\n                external_database_writer.get_result()\n            else:\n                cloud_storage_write_settings_internal = CloudStorageWriteSettingsInternal(\n                    connection=full_cloud_storage_connection,\n                    write_settings=node_cloud_storage_writer.cloud_storage_settings,\n                )\n                df.to_cloud_storage_obj(cloud_storage_write_settings_internal)\n            return df\n\n        def schema_callback():\n            logger.info(\"Starting to run the schema callback for cloud storage writer\")\n            if self.get_node(node_cloud_storage_writer.node_id).is_correct:\n                return self.get_node(node_cloud_storage_writer.node_id).node_inputs.main_inputs[0].schema\n            else:\n                return [FlowfileColumn.from_input(column_name=\"__error__\", data_type=\"String\")]\n\n        self.add_node_step(\n            node_id=node_cloud_storage_writer.node_id,\n            function=_func,\n            input_columns=[],\n            node_type=node_type,\n            setting_input=node_cloud_storage_writer,\n            schema_callback=schema_callback,\n            input_node_ids=[node_cloud_storage_writer.depending_on_id]\n        )\n\n        node = self.get_node(node_cloud_storage_writer.node_id)\n\n    def add_cloud_storage_reader(self, node_cloud_storage_reader: input_schema.NodeCloudStorageReader) -&gt; None:\n        \"\"\"Adds a cloud storage read node to the flow graph.\n\n        Args:\n            node_cloud_storage_reader: The settings for the cloud storage read node.\n        \"\"\"\n        node_type = \"cloud_storage_reader\"\n        logger.info(\"Adding cloud storage reader\")\n        cloud_storage_read_settings = node_cloud_storage_reader.cloud_storage_settings\n\n        def _func():\n            logger.info(\"Starting to run the schema callback for cloud storage reader\")\n            self.flow_logger.info(\"Starting to run the schema callback for cloud storage reader\")\n            settings = CloudStorageReadSettingsInternal(read_settings=cloud_storage_read_settings,\n                                                        connection=get_cloud_connection_settings(\n                                                            connection_name=cloud_storage_read_settings.connection_name,\n                                                            user_id=node_cloud_storage_reader.user_id,\n                                                            auth_mode=cloud_storage_read_settings.auth_mode\n                                                        ))\n            fl = FlowDataEngine.from_cloud_storage_obj(settings)\n            return fl\n\n        node = self.add_node_step(node_id=node_cloud_storage_reader.node_id,\n                                  function=_func,\n                                  cache_results=node_cloud_storage_reader.cache_results,\n                                  setting_input=node_cloud_storage_reader,\n                                  node_type=node_type,\n                                  )\n        if node_cloud_storage_reader.node_id not in set(start_node.node_id for start_node in self._flow_starts):\n            self._flow_starts.append(node)\n\n    def add_external_source(self,\n                            external_source_input: input_schema.NodeExternalSource):\n        \"\"\"Adds a node for a custom external data source.\n\n        Args:\n            external_source_input: The settings for the external source node.\n        \"\"\"\n\n        node_type = 'external_source'\n        external_source_script = getattr(external_sources.custom_external_sources, external_source_input.identifier)\n        source_settings = (getattr(input_schema, snake_case_to_camel_case(external_source_input.identifier)).\n                           model_validate(external_source_input.source_settings))\n        if hasattr(external_source_script, 'initial_getter'):\n            initial_getter = getattr(external_source_script, 'initial_getter')(source_settings)\n        else:\n            initial_getter = None\n        data_getter = external_source_script.getter(source_settings)\n        external_source = data_source_factory(source_type='custom',\n                                              data_getter=data_getter,\n                                              initial_data_getter=initial_getter,\n                                              orientation=external_source_input.source_settings.orientation,\n                                              schema=None)\n\n        def _func():\n            logger.info('Calling external source')\n            fl = FlowDataEngine.create_from_external_source(external_source=external_source)\n            external_source_input.source_settings.fields = [c.get_minimal_field_info() for c in fl.schema]\n            return fl\n\n        node = self.get_node(external_source_input.node_id)\n        if node:\n            node.node_type = node_type\n            node.name = node_type\n            node.function = _func\n            node.setting_input = external_source_input\n            node.node_settings.cache_results = external_source_input.cache_results\n            if external_source_input.node_id not in set(start_node.node_id for start_node in self._flow_starts):\n                self._flow_starts.append(node)\n        else:\n            node = FlowNode(external_source_input.node_id, function=_func,\n                            setting_input=external_source_input,\n                            name=node_type, node_type=node_type, parent_uuid=self.uuid)\n            self._node_db[external_source_input.node_id] = node\n            self._flow_starts.append(node)\n            self._node_ids.append(external_source_input.node_id)\n        if external_source_input.source_settings.fields and len(external_source_input.source_settings.fields) &gt; 0:\n            logger.info('Using provided schema in the node')\n\n            def schema_callback():\n                return [FlowfileColumn.from_input(f.name, f.data_type) for f in\n                        external_source_input.source_settings.fields]\n\n            node.schema_callback = schema_callback\n        else:\n            logger.warning('Removing schema')\n            node._schema_callback = None\n        self.add_node_step(node_id=external_source_input.node_id,\n                           function=_func,\n                           input_columns=[],\n                           node_type=node_type,\n                           setting_input=external_source_input)\n\n    def add_read(self, input_file: input_schema.NodeRead):\n        \"\"\"Adds a node to read data from a local file (e.g., CSV, Parquet, Excel).\n\n        Args:\n            input_file: The settings for the read operation.\n        \"\"\"\n\n        if input_file.received_file.file_type in ('xlsx', 'excel') and input_file.received_file.sheet_name == '':\n            sheet_name = fastexcel.read_excel(input_file.received_file.path).sheet_names[0]\n            input_file.received_file.sheet_name = sheet_name\n\n        received_file = input_file.received_file\n        input_file.received_file.set_absolute_filepath()\n\n        def _func():\n            input_file.received_file.set_absolute_filepath()\n            if input_file.received_file.file_type == 'parquet':\n                input_data = FlowDataEngine.create_from_path(input_file.received_file)\n            elif input_file.received_file.file_type == 'csv' and 'utf' in input_file.received_file.encoding:\n                input_data = FlowDataEngine.create_from_path(input_file.received_file)\n            else:\n                input_data = FlowDataEngine.create_from_path_worker(input_file.received_file,\n                                                                    node_id=input_file.node_id,\n                                                                    flow_id=self.flow_id)\n            input_data.name = input_file.received_file.name\n            return input_data\n\n        node = self.get_node(input_file.node_id)\n        schema_callback = None\n        if node:\n            start_hash = node.hash\n            node.node_type = 'read'\n            node.name = 'read'\n            node.function = _func\n            node.setting_input = input_file\n            if input_file.node_id not in set(start_node.node_id for start_node in self._flow_starts):\n                self._flow_starts.append(node)\n\n            if start_hash != node.hash:\n                logger.info('Hash changed, updating schema')\n                if len(received_file.fields) &gt; 0:\n                    # If the file has fields defined, we can use them to create the schema\n                    def schema_callback():\n                        return [FlowfileColumn.from_input(f.name, f.data_type) for f in received_file.fields]\n\n                elif input_file.received_file.file_type in ('csv', 'json', 'parquet'):\n                    # everything that can be scanned by polars\n                    def schema_callback():\n                        input_data = FlowDataEngine.create_from_path(input_file.received_file)\n                        return input_data.schema\n\n                elif input_file.received_file.file_type in ('xlsx', 'excel'):\n                    # If the file is an Excel file, we need to use the openpyxl engine to read the schema\n                    schema_callback = get_xlsx_schema_callback(engine='openpyxl',\n                                                               file_path=received_file.file_path,\n                                                               sheet_name=received_file.sheet_name,\n                                                               start_row=received_file.start_row,\n                                                               end_row=received_file.end_row,\n                                                               start_column=received_file.start_column,\n                                                               end_column=received_file.end_column,\n                                                               has_headers=received_file.has_headers)\n                else:\n                    schema_callback = None\n        else:\n            node = FlowNode(input_file.node_id, function=_func,\n                            setting_input=input_file,\n                            name='read', node_type='read', parent_uuid=self.uuid)\n            self._node_db[input_file.node_id] = node\n            self._flow_starts.append(node)\n            self._node_ids.append(input_file.node_id)\n\n        if schema_callback is not None:\n            node.schema_callback = schema_callback\n        return self\n\n    def add_datasource(self, input_file: Union[input_schema.NodeDatasource, input_schema.NodeManualInput]) -&gt; \"FlowGraph\":\n        \"\"\"Adds a data source node to the graph.\n\n        This method serves as a factory for creating starting nodes, handling both\n        file-based sources and direct manual data entry.\n\n        Args:\n            input_file: The configuration object for the data source.\n\n        Returns:\n            The `FlowGraph` instance for method chaining.\n        \"\"\"\n        if isinstance(input_file, input_schema.NodeManualInput):\n            input_data = FlowDataEngine(input_file.raw_data_format)\n            ref = 'manual_input'\n        else:\n            input_data = FlowDataEngine(path_ref=input_file.file_ref)\n            ref = 'datasource'\n        node = self.get_node(input_file.node_id)\n        if node:\n            node.node_type = ref\n            node.name = ref\n            node.function = input_data\n            node.setting_input = input_file\n            if not input_file.node_id in set(start_node.node_id for start_node in self._flow_starts):\n                self._flow_starts.append(node)\n        else:\n            input_data.collect()\n            node = FlowNode(input_file.node_id, function=input_data,\n                            setting_input=input_file,\n                            name=ref, node_type=ref, parent_uuid=self.uuid)\n            self._node_db[input_file.node_id] = node\n            self._flow_starts.append(node)\n            self._node_ids.append(input_file.node_id)\n        return self\n\n    def add_manual_input(self, input_file: input_schema.NodeManualInput):\n        \"\"\"Adds a node for manual data entry.\n\n        This is a convenience alias for `add_datasource`.\n\n        Args:\n            input_file: The settings and data for the manual input node.\n        \"\"\"\n        self.add_datasource(input_file)\n\n    @property\n    def nodes(self) -&gt; List[FlowNode]:\n        \"\"\"Gets a list of all FlowNode objects in the graph.\"\"\"\n\n        return list(self._node_db.values())\n\n    @property\n    def execution_mode(self) -&gt; schemas.ExecutionModeLiteral:\n        \"\"\"Gets the current execution mode ('Development' or 'Performance').\"\"\"\n        return self.flow_settings.execution_mode\n\n    def get_implicit_starter_nodes(self) -&gt; List[FlowNode]:\n        \"\"\"Finds nodes that can act as starting points but are not explicitly defined as such.\n\n        Some nodes, like the Polars Code node, can function without an input. This\n        method identifies such nodes if they have no incoming connections.\n\n        Returns:\n            A list of `FlowNode` objects that are implicit starting nodes.\n        \"\"\"\n        starting_node_ids = [node.node_id for node in self._flow_starts]\n        implicit_starting_nodes = []\n        for node in self.nodes:\n            if node.node_template.can_be_start and not node.has_input and node.node_id not in starting_node_ids:\n                implicit_starting_nodes.append(node)\n        return implicit_starting_nodes\n\n    @execution_mode.setter\n    def execution_mode(self, mode: schemas.ExecutionModeLiteral):\n        \"\"\"Sets the execution mode for the flow.\n\n        Args:\n            mode: The execution mode to set.\n        \"\"\"\n        self.flow_settings.execution_mode = mode\n\n    @property\n    def execution_location(self) -&gt; schemas.ExecutionLocationsLiteral:\n        \"\"\"Gets the current execution location.\"\"\"\n        return self.flow_settings.execution_location\n\n    @execution_location.setter\n    def execution_location(self, execution_location: schemas.ExecutionLocationsLiteral):\n        \"\"\"Sets the execution location for the flow.\n\n        Args:\n            execution_location: The execution location to set.\n        \"\"\"\n        if self.flow_settings.execution_location != execution_location:\n            self.reset()\n        self.flow_settings.execution_location = execution_location\n\n    def validate_if_node_can_be_fetched(self, node_id: int) -&gt; None:\n        flow_node = self._node_db.get(node_id)\n        if not flow_node:\n            raise Exception(\"Node not found found\")\n        skip_nodes, execution_order = compute_execution_plan(\n            nodes=self.nodes, flow_starts=self._flow_starts+self.get_implicit_starter_nodes()\n        )\n        if flow_node.node_id in [skip_node.node_id for skip_node in skip_nodes]:\n            raise Exception(\"Node can not be executed because it does not have it's inputs\")\n\n    def create_initial_run_information(self, number_of_nodes: int,\n                                       run_type: Literal[\"fetch_one\", \"full_run\"]):\n        return RunInformation(\n            flow_id=self.flow_id, start_time=datetime.datetime.now(), end_time=None,\n            success=None, number_of_nodes=number_of_nodes, node_step_result=[],\n            run_type=run_type\n        )\n\n    def trigger_fetch_node(self, node_id: int) -&gt; RunInformation | None:\n        \"\"\"Executes a specific node in the graph by its ID.\"\"\"\n        if self.flow_settings.is_running:\n            raise Exception(\"Flow is already running\")\n        flow_node = self.get_node(node_id)\n        self.flow_settings.is_running = True\n        self.flow_settings.is_canceled = False\n        self.flow_logger.clear_log_file()\n        self.latest_run_info = self.create_initial_run_information(1, \"fetch_one\")\n        node_logger = self.flow_logger.get_node_logger(flow_node.node_id)\n        node_result = NodeResult(node_id=flow_node.node_id, node_name=flow_node.name)\n        logger.info(f'Starting to run: node {flow_node.node_id}, start time: {node_result.start_timestamp}')\n        try:\n            self.latest_run_info.node_step_result.append(node_result)\n            flow_node.execute_node(run_location=self.flow_settings.execution_location,\n                                   performance_mode=False,\n                                   node_logger=node_logger,\n                                   optimize_for_downstream=False,\n                                   reset_cache=True)\n            node_result.error = str(flow_node.results.errors)\n            if self.flow_settings.is_canceled:\n                node_result.success = None\n                node_result.success = None\n                node_result.is_running = False\n            node_result.success = flow_node.results.errors is None\n            node_result.end_timestamp = time()\n            node_result.run_time = int(node_result.end_timestamp - node_result.start_timestamp)\n            node_result.is_running = False\n            self.latest_run_info.nodes_completed += 1\n            self.latest_run_info.end_time = datetime.datetime.now()\n            self.flow_settings.is_running = False\n            return self.get_run_info()\n        except Exception as e:\n            node_result.error = 'Node did not run'\n            node_result.success = False\n            node_result.end_timestamp = time()\n            node_result.run_time = int(node_result.end_timestamp - node_result.start_timestamp)\n            node_result.is_running = False\n            node_logger.error(f'Error in node {flow_node.node_id}: {e}')\n        finally:\n            self.flow_settings.is_running = False\n\n    def run_graph(self) -&gt; RunInformation | None:\n        \"\"\"Executes the entire data flow graph from start to finish.\n\n        It determines the correct execution order, runs each node,\n        collects results, and handles errors and cancellations.\n\n        Returns:\n            A RunInformation object summarizing the execution results.\n\n        Raises:\n            Exception: If the flow is already running.\n        \"\"\"\n        if self.flow_settings.is_running:\n            raise Exception('Flow is already running')\n        try:\n\n            self.flow_settings.is_running = True\n            self.flow_settings.is_canceled = False\n            self.flow_logger.clear_log_file()\n            self.flow_logger.info('Starting to run flowfile flow...')\n\n            skip_nodes, execution_order = compute_execution_plan(\n                nodes=self.nodes,\n                flow_starts=self._flow_starts+self.get_implicit_starter_nodes()\n            )\n\n            self.latest_run_info = self.create_initial_run_information(len(execution_order), \"full_run\")\n\n            skip_node_message(self.flow_logger, skip_nodes)\n            execution_order_message(self.flow_logger, execution_order)\n            performance_mode = self.flow_settings.execution_mode == 'Performance'\n\n            for node in execution_order:\n                node_logger = self.flow_logger.get_node_logger(node.node_id)\n                if self.flow_settings.is_canceled:\n                    self.flow_logger.info('Flow canceled')\n                    break\n                if node in skip_nodes:\n                    node_logger.info(f'Skipping node {node.node_id}')\n                    continue\n                node_result = NodeResult(node_id=node.node_id, node_name=node.name)\n                self.latest_run_info.node_step_result.append(node_result)\n                logger.info(f'Starting to run: node {node.node_id}, start time: {node_result.start_timestamp}')\n                node.execute_node(run_location=self.flow_settings.execution_location,\n                                  performance_mode=performance_mode,\n                                  node_logger=node_logger)\n                try:\n                    node_result.error = str(node.results.errors)\n                    if self.flow_settings.is_canceled:\n                        node_result.success = None\n                        node_result.success = None\n                        node_result.is_running = False\n                        continue\n                    node_result.success = node.results.errors is None\n                    node_result.end_timestamp = time()\n                    node_result.run_time = int(node_result.end_timestamp - node_result.start_timestamp)\n                    node_result.is_running = False\n                except Exception as e:\n                    node_result.error = 'Node did not run'\n                    node_result.success = False\n                    node_result.end_timestamp = time()\n                    node_result.run_time = int(node_result.end_timestamp - node_result.start_timestamp)\n                    node_result.is_running = False\n                    node_logger.error(f'Error in node {node.node_id}: {e}')\n                if not node_result.success:\n                    skip_nodes.extend(list(node.get_all_dependent_nodes()))\n                node_logger.info(f'Completed node with success: {node_result.success}')\n                self.latest_run_info.nodes_completed += 1\n            self.flow_logger.info('Flow completed!')\n            self.end_datetime = datetime.datetime.now()\n            self.flow_settings.is_running = False\n            if self.flow_settings.is_canceled:\n                self.flow_logger.info('Flow canceled')\n            return self.get_run_info()\n        except Exception as e:\n            raise e\n        finally:\n            self.flow_settings.is_running = False\n\n    def get_run_info(self) -&gt; RunInformation | None:\n        \"\"\"Gets a summary of the most recent graph execution.\n\n        Returns:\n            A RunInformation object with details about the last run.\n        \"\"\"\n        is_running = self.flow_settings.is_running\n        if self.latest_run_info is None:\n            return\n\n        elif not is_running and self.latest_run_info.success is not None:\n            return self.latest_run_info\n\n        run_info = self.latest_run_info\n        if not is_running:\n            run_info.success = all(nr.success for nr in run_info.node_step_result)\n        return run_info\n\n    @property\n    def node_connections(self) -&gt; List[Tuple[int, int]]:\n        \"\"\"Computes and returns a list of all connections in the graph.\n\n        Returns:\n            A list of tuples, where each tuple is a (source_id, target_id) pair.\n        \"\"\"\n        connections = set()\n        for node in self.nodes:\n            outgoing_connections = [(node.node_id, ltn.node_id) for ltn in node.leads_to_nodes]\n            incoming_connections = [(don.node_id, node.node_id) for don in node.all_inputs]\n            node_connections = [c for c in outgoing_connections + incoming_connections if (c[0] is not None\n                                                                                           and c[1] is not None)]\n            for node_connection in node_connections:\n                if node_connection not in connections:\n                    connections.add(node_connection)\n        return list(connections)\n\n    def get_node_data(self, node_id: int, include_example: bool = True) -&gt; NodeData:\n        \"\"\"Retrieves all data needed to render a node in the UI.\n\n        Args:\n            node_id: The ID of the node.\n            include_example: Whether to include data samples in the result.\n\n        Returns:\n            A NodeData object, or None if the node is not found.\n        \"\"\"\n        node = self._node_db[node_id]\n        return node.get_node_data(flow_id=self.flow_id, include_example=include_example)\n\n    def get_node_storage(self) -&gt; schemas.FlowInformation:\n        \"\"\"Serializes the entire graph's state into a storable format.\n\n        Returns:\n            A FlowInformation object representing the complete graph.\n        \"\"\"\n        node_information = {node.node_id: node.get_node_information() for\n                            node in self.nodes if node.is_setup and node.is_correct}\n\n        return schemas.FlowInformation(flow_id=self.flow_id,\n                                       flow_name=self.__name__,\n                                       flow_settings=self.flow_settings,\n                                       data=node_information,\n                                       node_starts=[v.node_id for v in self._flow_starts],\n                                       node_connections=self.node_connections\n                                       )\n\n    def cancel(self):\n        \"\"\"Cancels an ongoing graph execution.\"\"\"\n\n        if not self.flow_settings.is_running:\n            return\n        self.flow_settings.is_canceled = True\n        for node in self.nodes:\n            node.cancel()\n\n    def close_flow(self):\n        \"\"\"Performs cleanup operations, such as clearing node caches.\"\"\"\n\n        for node in self.nodes:\n            node.remove_cache()\n\n    def save_flow(self, flow_path: str):\n        \"\"\"Saves the current state of the flow graph to a file.\n\n        Args:\n            flow_path: The path where the flow file will be saved.\n        \"\"\"\n        logger.info(\"Saving flow to %s\", flow_path)\n        os.makedirs(os.path.dirname(flow_path), exist_ok=True)\n        try:\n            with open(flow_path, 'wb') as f:\n                pickle.dump(self.get_node_storage(), f)\n        except Exception as e:\n            logger.error(f\"Error saving flow: {e}\")\n\n        self.flow_settings.path = flow_path\n\n    def get_frontend_data(self) -&gt; dict:\n        \"\"\"Formats the graph structure into a JSON-like dictionary for a specific legacy frontend.\n\n        This method transforms the graph's state into a format compatible with the\n        Drawflow.js library.\n\n        Returns:\n            A dictionary representing the graph in Drawflow format.\n        \"\"\"\n        result = {\n            'Home': {\n                \"data\": {}\n            }\n        }\n        flow_info: schemas.FlowInformation = self.get_node_storage()\n\n        for node_id, node_info in flow_info.data.items():\n            if node_info.is_setup:\n                try:\n                    pos_x = node_info.data.pos_x\n                    pos_y = node_info.data.pos_y\n                    # Basic node structure\n                    result[\"Home\"][\"data\"][str(node_id)] = {\n                        \"id\": node_info.id,\n                        \"name\": node_info.type,\n                        \"data\": {},  # Additional data can go here\n                        \"class\": node_info.type,\n                        \"html\": node_info.type,\n                        \"typenode\": \"vue\",\n                        \"inputs\": {},\n                        \"outputs\": {},\n                        \"pos_x\": pos_x,\n                        \"pos_y\": pos_y\n                    }\n                except Exception as e:\n                    logger.error(e)\n            # Add outputs to the node based on `outputs` in your backend data\n            if node_info.outputs:\n                outputs = {o: 0 for o in node_info.outputs}\n                for o in node_info.outputs:\n                    outputs[o] += 1\n                connections = []\n                for output_node_id, n_connections in outputs.items():\n                    leading_to_node = self.get_node(output_node_id)\n                    input_types = leading_to_node.get_input_type(node_info.id)\n                    for input_type in input_types:\n                        if input_type == 'main':\n                            input_frontend_id = 'input_1'\n                        elif input_type == 'right':\n                            input_frontend_id = 'input_2'\n                        elif input_type == 'left':\n                            input_frontend_id = 'input_3'\n                        else:\n                            input_frontend_id = 'input_1'\n                        connection = {\"node\": str(output_node_id), \"input\": input_frontend_id}\n                        connections.append(connection)\n\n                result[\"Home\"][\"data\"][str(node_id)][\"outputs\"][\"output_1\"] = {\n                    \"connections\": connections}\n            else:\n                result[\"Home\"][\"data\"][str(node_id)][\"outputs\"] = {\"output_1\": {\"connections\": []}}\n\n            # Add input to the node based on `depending_on_id` in your backend data\n            if node_info.left_input_id is not None or node_info.right_input_id is not None or node_info.input_ids is not None:\n                main_inputs = node_info.main_input_ids\n                result[\"Home\"][\"data\"][str(node_id)][\"inputs\"][\"input_1\"] = {\n                    \"connections\": [{\"node\": str(main_node_id), \"input\": \"output_1\"} for main_node_id in main_inputs]\n                }\n                if node_info.right_input_id is not None:\n                    result[\"Home\"][\"data\"][str(node_id)][\"inputs\"][\"input_2\"] = {\n                        \"connections\": [{\"node\": str(node_info.right_input_id), \"input\": \"output_1\"}]\n                    }\n                if node_info.left_input_id is not None:\n                    result[\"Home\"][\"data\"][str(node_id)][\"inputs\"][\"input_3\"] = {\n                        \"connections\": [{\"node\": str(node_info.left_input_id), \"input\": \"output_1\"}]\n                    }\n        return result\n\n    def get_vue_flow_input(self) -&gt; schemas.VueFlowInput:\n        \"\"\"Formats the graph's nodes and edges into a schema suitable for the VueFlow frontend.\n\n        Returns:\n            A VueFlowInput object.\n        \"\"\"\n        edges: List[schemas.NodeEdge] = []\n        nodes: List[schemas.NodeInput] = []\n        for node in self.nodes:\n            nodes.append(node.get_node_input())\n            edges.extend(node.get_edge_input())\n        return schemas.VueFlowInput(node_edges=edges, node_inputs=nodes)\n\n    def reset(self):\n        \"\"\"Forces a deep reset on all nodes in the graph.\"\"\"\n\n        for node in self.nodes:\n            node.reset(True)\n\n    def copy_node(self, new_node_settings: input_schema.NodePromise, existing_setting_input: Any, node_type: str) -&gt; None:\n        \"\"\"Creates a copy of an existing node.\n\n        Args:\n            new_node_settings: The promise containing new settings (like ID and position).\n            existing_setting_input: The settings object from the node being copied.\n            node_type: The type of the node being copied.\n        \"\"\"\n        self.add_node_promise(new_node_settings)\n\n        if isinstance(existing_setting_input, input_schema.NodePromise):\n            return\n\n        combined_settings = combine_existing_settings_and_new_settings(\n            existing_setting_input, new_node_settings\n        )\n        getattr(self, f\"add_{node_type}\")(combined_settings)\n\n    def generate_code(self):\n        \"\"\"Generates code for the flow graph.\n        This method exports the flow graph to a Polars-compatible format.\n        \"\"\"\n        from flowfile_core.flowfile.code_generator.code_generator import export_flow_to_polars\n        print(export_flow_to_polars(self))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.execution_location","title":"<code>execution_location</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the current execution location.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.execution_mode","title":"<code>execution_mode</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the current execution mode ('Development' or 'Performance').</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.flow_id","title":"<code>flow_id</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the unique identifier of the flow.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.graph_has_functions","title":"<code>graph_has_functions</code>  <code>property</code>","text":"<p>Checks if the graph has any nodes.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.graph_has_input_data","title":"<code>graph_has_input_data</code>  <code>property</code>","text":"<p>Checks if the graph has an initial input data source.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.node_connections","title":"<code>node_connections</code>  <code>property</code>","text":"<p>Computes and returns a list of all connections in the graph.</p> <p>Returns:</p> Type Description <code>List[Tuple[int, int]]</code> <p>A list of tuples, where each tuple is a (source_id, target_id) pair.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.nodes","title":"<code>nodes</code>  <code>property</code>","text":"<p>Gets a list of all FlowNode objects in the graph.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.__init__","title":"<code>__init__(flow_settings, name=None, input_cols=None, output_cols=None, path_ref=None, input_flow=None, cache_results=False)</code>","text":"<p>Initializes a new FlowGraph instance.</p> <p>Parameters:</p> Name Type Description Default <code>flow_settings</code> <code>FlowSettings | FlowGraphConfig</code> <p>The configuration settings for the flow.</p> required <code>name</code> <code>str</code> <p>The name of the flow.</p> <code>None</code> <code>input_cols</code> <code>List[str]</code> <p>A list of input column names.</p> <code>None</code> <code>output_cols</code> <code>List[str]</code> <p>A list of output column names.</p> <code>None</code> <code>path_ref</code> <code>str</code> <p>An optional path to an initial data source.</p> <code>None</code> <code>input_flow</code> <code>Union[ParquetFile, FlowDataEngine, FlowGraph]</code> <p>An optional existing data object to start the flow with.</p> <code>None</code> <code>cache_results</code> <code>bool</code> <p>A global flag to enable or disable result caching.</p> <code>False</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def __init__(self,\n             flow_settings: schemas.FlowSettings | schemas.FlowGraphConfig,\n             name: str = None, input_cols: List[str] = None,\n             output_cols: List[str] = None,\n             path_ref: str = None,\n             input_flow: Union[ParquetFile, FlowDataEngine, \"FlowGraph\"] = None,\n             cache_results: bool = False):\n    \"\"\"Initializes a new FlowGraph instance.\n\n    Args:\n        flow_settings: The configuration settings for the flow.\n        name: The name of the flow.\n        input_cols: A list of input column names.\n        output_cols: A list of output column names.\n        path_ref: An optional path to an initial data source.\n        input_flow: An optional existing data object to start the flow with.\n        cache_results: A global flag to enable or disable result caching.\n    \"\"\"\n    if isinstance(flow_settings, schemas.FlowGraphConfig):\n        flow_settings = schemas.FlowSettings.from_flow_settings_input(flow_settings)\n\n    self._flow_settings = flow_settings\n    self.uuid = str(uuid1())\n    self.start_datetime = None\n    self.end_datetime = None\n    self.latest_run_info = None\n    self._flow_id = flow_settings.flow_id\n    self.flow_logger = FlowLogger(flow_settings.flow_id)\n    self._flow_starts: List[FlowNode] = []\n    self._results = None\n    self.schema = None\n    self.has_over_row_function = False\n    self._input_cols = [] if input_cols is None else input_cols\n    self._output_cols = [] if output_cols is None else output_cols\n    self._node_ids = []\n    self._node_db = {}\n    self.cache_results = cache_results\n    self.__name__ = name if name else id(self)\n    self.depends_on = {}\n    if path_ref is not None:\n        self.add_datasource(input_schema.NodeDatasource(file_path=path_ref))\n    elif input_flow is not None:\n        self.add_datasource(input_file=input_flow)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.__repr__","title":"<code>__repr__()</code>","text":"<p>Provides the official string representation of the FlowGraph instance.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def __repr__(self):\n    \"\"\"Provides the official string representation of the FlowGraph instance.\"\"\"\n    settings_str = \"  -\" + '\\n  -'.join(f\"{k}: {v}\" for k, v in self.flow_settings)\n    return f\"FlowGraph(\\nNodes: {self._node_db}\\n\\nSettings:\\n{settings_str}\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_cloud_storage_reader","title":"<code>add_cloud_storage_reader(node_cloud_storage_reader)</code>","text":"<p>Adds a cloud storage read node to the flow graph.</p> <p>Parameters:</p> Name Type Description Default <code>node_cloud_storage_reader</code> <code>NodeCloudStorageReader</code> <p>The settings for the cloud storage read node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_cloud_storage_reader(self, node_cloud_storage_reader: input_schema.NodeCloudStorageReader) -&gt; None:\n    \"\"\"Adds a cloud storage read node to the flow graph.\n\n    Args:\n        node_cloud_storage_reader: The settings for the cloud storage read node.\n    \"\"\"\n    node_type = \"cloud_storage_reader\"\n    logger.info(\"Adding cloud storage reader\")\n    cloud_storage_read_settings = node_cloud_storage_reader.cloud_storage_settings\n\n    def _func():\n        logger.info(\"Starting to run the schema callback for cloud storage reader\")\n        self.flow_logger.info(\"Starting to run the schema callback for cloud storage reader\")\n        settings = CloudStorageReadSettingsInternal(read_settings=cloud_storage_read_settings,\n                                                    connection=get_cloud_connection_settings(\n                                                        connection_name=cloud_storage_read_settings.connection_name,\n                                                        user_id=node_cloud_storage_reader.user_id,\n                                                        auth_mode=cloud_storage_read_settings.auth_mode\n                                                    ))\n        fl = FlowDataEngine.from_cloud_storage_obj(settings)\n        return fl\n\n    node = self.add_node_step(node_id=node_cloud_storage_reader.node_id,\n                              function=_func,\n                              cache_results=node_cloud_storage_reader.cache_results,\n                              setting_input=node_cloud_storage_reader,\n                              node_type=node_type,\n                              )\n    if node_cloud_storage_reader.node_id not in set(start_node.node_id for start_node in self._flow_starts):\n        self._flow_starts.append(node)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_cloud_storage_writer","title":"<code>add_cloud_storage_writer(node_cloud_storage_writer)</code>","text":"<p>Adds a node to write data to a cloud storage provider.</p> <p>Parameters:</p> Name Type Description Default <code>node_cloud_storage_writer</code> <code>NodeCloudStorageWriter</code> <p>The settings for the cloud storage writer node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_cloud_storage_writer(self, node_cloud_storage_writer: input_schema.NodeCloudStorageWriter) -&gt; None:\n    \"\"\"Adds a node to write data to a cloud storage provider.\n\n    Args:\n        node_cloud_storage_writer: The settings for the cloud storage writer node.\n    \"\"\"\n\n    node_type = \"cloud_storage_writer\"\n    def _func(df: FlowDataEngine):\n        df.lazy = True\n        execute_remote = self.execution_location != 'local'\n        cloud_connection_settings = get_cloud_connection_settings(\n            connection_name=node_cloud_storage_writer.cloud_storage_settings.connection_name,\n            user_id=node_cloud_storage_writer.user_id,\n            auth_mode=node_cloud_storage_writer.cloud_storage_settings.auth_mode\n        )\n        full_cloud_storage_connection = FullCloudStorageConnection(\n            storage_type=cloud_connection_settings.storage_type,\n            auth_method=cloud_connection_settings.auth_method,\n            aws_allow_unsafe_html=cloud_connection_settings.aws_allow_unsafe_html,\n            **CloudStorageReader.get_storage_options(cloud_connection_settings)\n        )\n        if execute_remote:\n            settings = get_cloud_storage_write_settings_worker_interface(\n                write_settings=node_cloud_storage_writer.cloud_storage_settings,\n                connection=full_cloud_storage_connection,\n                lf=df.data_frame,\n                flowfile_node_id=node_cloud_storage_writer.node_id,\n                flowfile_flow_id=self.flow_id)\n            external_database_writer = ExternalCloudWriter(settings, wait_on_completion=False)\n            node._fetch_cached_df = external_database_writer\n            external_database_writer.get_result()\n        else:\n            cloud_storage_write_settings_internal = CloudStorageWriteSettingsInternal(\n                connection=full_cloud_storage_connection,\n                write_settings=node_cloud_storage_writer.cloud_storage_settings,\n            )\n            df.to_cloud_storage_obj(cloud_storage_write_settings_internal)\n        return df\n\n    def schema_callback():\n        logger.info(\"Starting to run the schema callback for cloud storage writer\")\n        if self.get_node(node_cloud_storage_writer.node_id).is_correct:\n            return self.get_node(node_cloud_storage_writer.node_id).node_inputs.main_inputs[0].schema\n        else:\n            return [FlowfileColumn.from_input(column_name=\"__error__\", data_type=\"String\")]\n\n    self.add_node_step(\n        node_id=node_cloud_storage_writer.node_id,\n        function=_func,\n        input_columns=[],\n        node_type=node_type,\n        setting_input=node_cloud_storage_writer,\n        schema_callback=schema_callback,\n        input_node_ids=[node_cloud_storage_writer.depending_on_id]\n    )\n\n    node = self.get_node(node_cloud_storage_writer.node_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_cross_join","title":"<code>add_cross_join(cross_join_settings)</code>","text":"<p>Adds a cross join node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>cross_join_settings</code> <code>NodeCrossJoin</code> <p>The settings for the cross join operation.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_cross_join(self, cross_join_settings: input_schema.NodeCrossJoin) -&gt; \"FlowGraph\":\n    \"\"\"Adds a cross join node to the graph.\n\n    Args:\n        cross_join_settings: The settings for the cross join operation.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n\n    def _func(main: FlowDataEngine, right: FlowDataEngine) -&gt; FlowDataEngine:\n        for left_select in cross_join_settings.cross_join_input.left_select.renames:\n            left_select.is_available = True if left_select.old_name in main.schema else False\n        for right_select in cross_join_settings.cross_join_input.right_select.renames:\n            right_select.is_available = True if right_select.old_name in right.schema else False\n\n        return main.do_cross_join(cross_join_input=cross_join_settings.cross_join_input,\n                                  auto_generate_selection=cross_join_settings.auto_generate_selection,\n                                  verify_integrity=False,\n                                  other=right)\n\n    self.add_node_step(node_id=cross_join_settings.node_id,\n                       function=_func,\n                       input_columns=[],\n                       node_type='cross_join',\n                       setting_input=cross_join_settings,\n                       input_node_ids=cross_join_settings.depending_on_ids)\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_database_reader","title":"<code>add_database_reader(node_database_reader)</code>","text":"<p>Adds a node to read data from a database.</p> <p>Parameters:</p> Name Type Description Default <code>node_database_reader</code> <code>NodeDatabaseReader</code> <p>The settings for the database reader node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_database_reader(self, node_database_reader: input_schema.NodeDatabaseReader):\n    \"\"\"Adds a node to read data from a database.\n\n    Args:\n        node_database_reader: The settings for the database reader node.\n    \"\"\"\n\n    logger.info(\"Adding database reader\")\n    node_type = 'database_reader'\n    database_settings: input_schema.DatabaseSettings = node_database_reader.database_settings\n    database_connection: Optional[input_schema.DatabaseConnection | input_schema.FullDatabaseConnection]\n    if database_settings.connection_mode == 'inline':\n        database_connection: input_schema.DatabaseConnection = database_settings.database_connection\n        encrypted_password = get_encrypted_secret(current_user_id=node_database_reader.user_id,\n                                                  secret_name=database_connection.password_ref)\n        if encrypted_password is None:\n            raise HTTPException(status_code=400, detail=\"Password not found\")\n    else:\n        database_reference_settings = get_local_database_connection(database_settings.database_connection_name,\n                                                                    node_database_reader.user_id)\n        database_connection = database_reference_settings\n        encrypted_password = database_reference_settings.password.get_secret_value()\n\n    def _func():\n        sql_source = BaseSqlSource(query=None if database_settings.query_mode == 'table' else database_settings.query,\n                                   table_name=database_settings.table_name,\n                                   schema_name=database_settings.schema_name,\n                                   fields=node_database_reader.fields,\n                                   )\n        database_external_read_settings = (\n            sql_models.DatabaseExternalReadSettings.create_from_from_node_database_reader(\n                node_database_reader=node_database_reader,\n                password=encrypted_password,\n                query=sql_source.query,\n                database_reference_settings=(database_reference_settings if database_settings.connection_mode == 'reference'\n                                             else None),\n            )\n        )\n\n        external_database_fetcher = ExternalDatabaseFetcher(database_external_read_settings, wait_on_completion=False)\n        node._fetch_cached_df = external_database_fetcher\n        fl = FlowDataEngine(external_database_fetcher.get_result())\n        node_database_reader.fields = [c.get_minimal_field_info() for c in fl.schema]\n        return fl\n\n    def schema_callback():\n        sql_source = SqlSource(connection_string=\n                               sql_utils.construct_sql_uri(database_type=database_connection.database_type,\n                                                           host=database_connection.host,\n                                                           port=database_connection.port,\n                                                           database=database_connection.database,\n                                                           username=database_connection.username,\n                                                           password=decrypt_secret(encrypted_password)),\n                               query=None if database_settings.query_mode == 'table' else database_settings.query,\n                               table_name=database_settings.table_name,\n                               schema_name=database_settings.schema_name,\n                               fields=node_database_reader.fields,\n                               )\n        return sql_source.get_schema()\n\n    node = self.get_node(node_database_reader.node_id)\n    if node:\n        node.node_type = node_type\n        node.name = node_type\n        node.function = _func\n        node.setting_input = node_database_reader\n        node.node_settings.cache_results = node_database_reader.cache_results\n        if node_database_reader.node_id not in set(start_node.node_id for start_node in self._flow_starts):\n            self._flow_starts.append(node)\n        node.schema_callback = schema_callback\n    else:\n        node = FlowNode(node_database_reader.node_id, function=_func,\n                        setting_input=node_database_reader,\n                        name=node_type, node_type=node_type, parent_uuid=self.uuid,\n                        schema_callback=schema_callback)\n        self._node_db[node_database_reader.node_id] = node\n        self._flow_starts.append(node)\n        self._node_ids.append(node_database_reader.node_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_database_writer","title":"<code>add_database_writer(node_database_writer)</code>","text":"<p>Adds a node to write data to a database.</p> <p>Parameters:</p> Name Type Description Default <code>node_database_writer</code> <code>NodeDatabaseWriter</code> <p>The settings for the database writer node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_database_writer(self, node_database_writer: input_schema.NodeDatabaseWriter):\n    \"\"\"Adds a node to write data to a database.\n\n    Args:\n        node_database_writer: The settings for the database writer node.\n    \"\"\"\n\n    node_type = 'database_writer'\n    database_settings: input_schema.DatabaseWriteSettings = node_database_writer.database_write_settings\n    database_connection: Optional[input_schema.DatabaseConnection | input_schema.FullDatabaseConnection]\n    if database_settings.connection_mode == 'inline':\n        database_connection: input_schema.DatabaseConnection = database_settings.database_connection\n        encrypted_password = get_encrypted_secret(current_user_id=node_database_writer.user_id,\n                                                  secret_name=database_connection.password_ref)\n        if encrypted_password is None:\n            raise HTTPException(status_code=400, detail=\"Password not found\")\n    else:\n        database_reference_settings = get_local_database_connection(database_settings.database_connection_name,\n                                                                    node_database_writer.user_id)\n        encrypted_password = database_reference_settings.password.get_secret_value()\n\n    def _func(df: FlowDataEngine):\n        df.lazy = True\n        database_external_write_settings = (\n            sql_models.DatabaseExternalWriteSettings.create_from_from_node_database_writer(\n                node_database_writer=node_database_writer,\n                password=encrypted_password,\n                table_name=(database_settings.schema_name+'.'+database_settings.table_name\n                            if database_settings.schema_name else database_settings.table_name),\n                database_reference_settings=(database_reference_settings if database_settings.connection_mode == 'reference'\n                                             else None),\n                lf=df.data_frame\n            )\n        )\n        external_database_writer = ExternalDatabaseWriter(database_external_write_settings, wait_on_completion=False)\n        node._fetch_cached_df = external_database_writer\n        external_database_writer.get_result()\n        return df\n\n    def schema_callback():\n        input_node: FlowNode = self.get_node(node_database_writer.node_id).node_inputs.main_inputs[0]\n        return input_node.schema\n\n    self.add_node_step(\n        node_id=node_database_writer.node_id,\n        function=_func,\n        input_columns=[],\n        node_type=node_type,\n        setting_input=node_database_writer,\n        schema_callback=schema_callback,\n    )\n    node = self.get_node(node_database_writer.node_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_datasource","title":"<code>add_datasource(input_file)</code>","text":"<p>Adds a data source node to the graph.</p> <p>This method serves as a factory for creating starting nodes, handling both file-based sources and direct manual data entry.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>Union[NodeDatasource, NodeManualInput]</code> <p>The configuration object for the data source.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_datasource(self, input_file: Union[input_schema.NodeDatasource, input_schema.NodeManualInput]) -&gt; \"FlowGraph\":\n    \"\"\"Adds a data source node to the graph.\n\n    This method serves as a factory for creating starting nodes, handling both\n    file-based sources and direct manual data entry.\n\n    Args:\n        input_file: The configuration object for the data source.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n    if isinstance(input_file, input_schema.NodeManualInput):\n        input_data = FlowDataEngine(input_file.raw_data_format)\n        ref = 'manual_input'\n    else:\n        input_data = FlowDataEngine(path_ref=input_file.file_ref)\n        ref = 'datasource'\n    node = self.get_node(input_file.node_id)\n    if node:\n        node.node_type = ref\n        node.name = ref\n        node.function = input_data\n        node.setting_input = input_file\n        if not input_file.node_id in set(start_node.node_id for start_node in self._flow_starts):\n            self._flow_starts.append(node)\n    else:\n        input_data.collect()\n        node = FlowNode(input_file.node_id, function=input_data,\n                        setting_input=input_file,\n                        name=ref, node_type=ref, parent_uuid=self.uuid)\n        self._node_db[input_file.node_id] = node\n        self._flow_starts.append(node)\n        self._node_ids.append(input_file.node_id)\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_dependency_on_polars_lazy_frame","title":"<code>add_dependency_on_polars_lazy_frame(lazy_frame, node_id)</code>","text":"<p>Adds a special node that directly injects a Polars LazyFrame into the graph.</p> <p>Note: This is intended for backend use and will not work in the UI editor.</p> <p>Parameters:</p> Name Type Description Default <code>lazy_frame</code> <code>LazyFrame</code> <p>The Polars LazyFrame to inject.</p> required <code>node_id</code> <code>int</code> <p>The ID for the new node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_dependency_on_polars_lazy_frame(self,\n                                        lazy_frame: pl.LazyFrame,\n                                        node_id: int):\n    \"\"\"Adds a special node that directly injects a Polars LazyFrame into the graph.\n\n    Note: This is intended for backend use and will not work in the UI editor.\n\n    Args:\n        lazy_frame: The Polars LazyFrame to inject.\n        node_id: The ID for the new node.\n    \"\"\"\n    def _func():\n        return FlowDataEngine(lazy_frame)\n    node_promise = input_schema.NodePromise(flow_id=self.flow_id,\n                                            node_id=node_id, node_type=\"polars_lazy_frame\",\n                                            is_setup=True)\n    self.add_node_step(node_id=node_promise.node_id, node_type=node_promise.node_type, function=_func,\n                       setting_input=node_promise)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_explore_data","title":"<code>add_explore_data(node_analysis)</code>","text":"<p>Adds a specialized node for data exploration and visualization.</p> <p>Parameters:</p> Name Type Description Default <code>node_analysis</code> <code>NodeExploreData</code> <p>The settings for the data exploration node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_explore_data(self, node_analysis: input_schema.NodeExploreData):\n    \"\"\"Adds a specialized node for data exploration and visualization.\n\n    Args:\n        node_analysis: The settings for the data exploration node.\n    \"\"\"\n    sample_size: int = 10000\n\n    def analysis_preparation(flowfile_table: FlowDataEngine):\n        if flowfile_table.number_of_records &lt;= 0:\n            number_of_records = flowfile_table.get_number_of_records(calculate_in_worker_process=True)\n        else:\n            number_of_records = flowfile_table.number_of_records\n        if number_of_records &gt; sample_size:\n            flowfile_table = flowfile_table.get_sample(sample_size, random=True)\n        external_sampler = ExternalDfFetcher(\n            lf=flowfile_table.data_frame,\n            file_ref=\"__gf_walker\"+node.hash,\n            wait_on_completion=True,\n            node_id=node.node_id,\n            flow_id=self.flow_id,\n        )\n        node.results.analysis_data_generator = get_read_top_n(external_sampler.status.file_ref,\n                                                              n=min(sample_size, number_of_records))\n        return flowfile_table\n\n    def schema_callback():\n        node = self.get_node(node_analysis.node_id)\n        if len(node.all_inputs) == 1:\n            input_node = node.all_inputs[0]\n            return input_node.schema\n        else:\n            return [FlowfileColumn.from_input('col_1', 'na')]\n\n    self.add_node_step(node_id=node_analysis.node_id, node_type='explore_data',\n                       function=analysis_preparation,\n                       setting_input=node_analysis, schema_callback=schema_callback)\n    node = self.get_node(node_analysis.node_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_external_source","title":"<code>add_external_source(external_source_input)</code>","text":"<p>Adds a node for a custom external data source.</p> <p>Parameters:</p> Name Type Description Default <code>external_source_input</code> <code>NodeExternalSource</code> <p>The settings for the external source node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_external_source(self,\n                        external_source_input: input_schema.NodeExternalSource):\n    \"\"\"Adds a node for a custom external data source.\n\n    Args:\n        external_source_input: The settings for the external source node.\n    \"\"\"\n\n    node_type = 'external_source'\n    external_source_script = getattr(external_sources.custom_external_sources, external_source_input.identifier)\n    source_settings = (getattr(input_schema, snake_case_to_camel_case(external_source_input.identifier)).\n                       model_validate(external_source_input.source_settings))\n    if hasattr(external_source_script, 'initial_getter'):\n        initial_getter = getattr(external_source_script, 'initial_getter')(source_settings)\n    else:\n        initial_getter = None\n    data_getter = external_source_script.getter(source_settings)\n    external_source = data_source_factory(source_type='custom',\n                                          data_getter=data_getter,\n                                          initial_data_getter=initial_getter,\n                                          orientation=external_source_input.source_settings.orientation,\n                                          schema=None)\n\n    def _func():\n        logger.info('Calling external source')\n        fl = FlowDataEngine.create_from_external_source(external_source=external_source)\n        external_source_input.source_settings.fields = [c.get_minimal_field_info() for c in fl.schema]\n        return fl\n\n    node = self.get_node(external_source_input.node_id)\n    if node:\n        node.node_type = node_type\n        node.name = node_type\n        node.function = _func\n        node.setting_input = external_source_input\n        node.node_settings.cache_results = external_source_input.cache_results\n        if external_source_input.node_id not in set(start_node.node_id for start_node in self._flow_starts):\n            self._flow_starts.append(node)\n    else:\n        node = FlowNode(external_source_input.node_id, function=_func,\n                        setting_input=external_source_input,\n                        name=node_type, node_type=node_type, parent_uuid=self.uuid)\n        self._node_db[external_source_input.node_id] = node\n        self._flow_starts.append(node)\n        self._node_ids.append(external_source_input.node_id)\n    if external_source_input.source_settings.fields and len(external_source_input.source_settings.fields) &gt; 0:\n        logger.info('Using provided schema in the node')\n\n        def schema_callback():\n            return [FlowfileColumn.from_input(f.name, f.data_type) for f in\n                    external_source_input.source_settings.fields]\n\n        node.schema_callback = schema_callback\n    else:\n        logger.warning('Removing schema')\n        node._schema_callback = None\n    self.add_node_step(node_id=external_source_input.node_id,\n                       function=_func,\n                       input_columns=[],\n                       node_type=node_type,\n                       setting_input=external_source_input)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_filter","title":"<code>add_filter(filter_settings)</code>","text":"<p>Adds a filter node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>filter_settings</code> <code>NodeFilter</code> <p>The settings for the filter operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_filter(self, filter_settings: input_schema.NodeFilter):\n    \"\"\"Adds a filter node to the graph.\n\n    Args:\n        filter_settings: The settings for the filter operation.\n    \"\"\"\n\n    is_advanced = filter_settings.filter_input.filter_type == 'advanced'\n    if is_advanced:\n        predicate = filter_settings.filter_input.advanced_filter\n    else:\n        _basic_filter = filter_settings.filter_input.basic_filter\n        filter_settings.filter_input.advanced_filter = (f'[{_basic_filter.field}]{_basic_filter.filter_type}\"'\n                                                        f'{_basic_filter.filter_value}\"')\n\n    def _func(fl: FlowDataEngine):\n        is_advanced = filter_settings.filter_input.filter_type == 'advanced'\n        if is_advanced:\n            return fl.do_filter(predicate)\n        else:\n            basic_filter = filter_settings.filter_input.basic_filter\n            if basic_filter.filter_value.isnumeric():\n                field_data_type = fl.get_schema_column(basic_filter.field).generic_datatype()\n                if field_data_type == 'str':\n                    _f = f'[{basic_filter.field}]{basic_filter.filter_type}\"{basic_filter.filter_value}\"'\n                else:\n                    _f = f'[{basic_filter.field}]{basic_filter.filter_type}{basic_filter.filter_value}'\n            else:\n                _f = f'[{basic_filter.field}]{basic_filter.filter_type}\"{basic_filter.filter_value}\"'\n            filter_settings.filter_input.advanced_filter = _f\n            return fl.do_filter(_f)\n\n    self.add_node_step(filter_settings.node_id, _func,\n                       node_type='filter',\n                       renew_schema=False,\n                       setting_input=filter_settings,\n                       input_node_ids=[filter_settings.depending_on_id]\n                       )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_formula","title":"<code>add_formula(function_settings)</code>","text":"<p>Adds a node that applies a formula to create or modify a column.</p> <p>Parameters:</p> Name Type Description Default <code>function_settings</code> <code>NodeFormula</code> <p>The settings for the formula operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_formula(self, function_settings: input_schema.NodeFormula):\n    \"\"\"Adds a node that applies a formula to create or modify a column.\n\n    Args:\n        function_settings: The settings for the formula operation.\n    \"\"\"\n\n    error = \"\"\n    if function_settings.function.field.data_type not in (None, \"Auto\"):\n        output_type = cast_str_to_polars_type(function_settings.function.field.data_type)\n    else:\n        output_type = None\n    if output_type not in (None, \"Auto\"):\n        new_col = [FlowfileColumn.from_input(column_name=function_settings.function.field.name,\n                                             data_type=str(output_type))]\n    else:\n        new_col = [FlowfileColumn.from_input(function_settings.function.field.name, 'String')]\n\n    def _func(fl: FlowDataEngine):\n        return fl.apply_sql_formula(func=function_settings.function.function,\n                                    col_name=function_settings.function.field.name,\n                                    output_data_type=output_type)\n\n    self.add_node_step(function_settings.node_id, _func,\n                       output_schema=new_col,\n                       node_type='formula',\n                       renew_schema=False,\n                       setting_input=function_settings,\n                       input_node_ids=[function_settings.depending_on_id]\n                       )\n    if error != \"\":\n        node = self.get_node(function_settings.node_id)\n        node.results.errors = error\n        return False, error\n    else:\n        return True, \"\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_fuzzy_match","title":"<code>add_fuzzy_match(fuzzy_settings)</code>","text":"<p>Adds a fuzzy matching node to join data on approximate string matches.</p> <p>Parameters:</p> Name Type Description Default <code>fuzzy_settings</code> <code>NodeFuzzyMatch</code> <p>The settings for the fuzzy match operation.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_fuzzy_match(self, fuzzy_settings: input_schema.NodeFuzzyMatch) -&gt; \"FlowGraph\":\n    \"\"\"Adds a fuzzy matching node to join data on approximate string matches.\n\n    Args:\n        fuzzy_settings: The settings for the fuzzy match operation.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n\n    def _func(main: FlowDataEngine, right: FlowDataEngine) -&gt; FlowDataEngine:\n        node = self.get_node(node_id=fuzzy_settings.node_id)\n        if self.execution_location == \"local\":\n            return main.fuzzy_join(fuzzy_match_input=deepcopy(fuzzy_settings.join_input),\n                                   other=right,\n                                   node_logger=self.flow_logger.get_node_logger(fuzzy_settings.node_id))\n\n        f = main.start_fuzzy_join(fuzzy_match_input=deepcopy(fuzzy_settings.join_input), other=right, file_ref=node.hash,\n                                  flow_id=self.flow_id, node_id=fuzzy_settings.node_id)\n        logger.info(\"Started the fuzzy match action\")\n        node._fetch_cached_df = f  # Add to the node so it can be cancelled and fetch later if needed\n        return FlowDataEngine(f.get_result())\n\n    def schema_callback():\n        fm_input_copy = deepcopy(fuzzy_settings.join_input)  # Deepcopy create an unique object per func\n        node = self.get_node(node_id=fuzzy_settings.node_id)\n        return calculate_fuzzy_match_schema(fm_input_copy,\n                                            left_schema=node.node_inputs.main_inputs[0].schema,\n                                            right_schema=node.node_inputs.right_input.schema\n                                            )\n\n    self.add_node_step(node_id=fuzzy_settings.node_id,\n                       function=_func,\n                       input_columns=[],\n                       node_type='fuzzy_match',\n                       setting_input=fuzzy_settings,\n                       input_node_ids=fuzzy_settings.depending_on_ids,\n                       schema_callback=schema_callback)\n\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_graph_solver","title":"<code>add_graph_solver(graph_solver_settings)</code>","text":"<p>Adds a node that solves graph-like problems within the data.</p> <p>This node can be used for operations like finding network paths, calculating connected components, or performing other graph algorithms on relational data that represents nodes and edges.</p> <p>Parameters:</p> Name Type Description Default <code>graph_solver_settings</code> <code>NodeGraphSolver</code> <p>The settings object defining the graph inputs and the specific algorithm to apply.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_graph_solver(self, graph_solver_settings: input_schema.NodeGraphSolver):\n    \"\"\"Adds a node that solves graph-like problems within the data.\n\n    This node can be used for operations like finding network paths,\n    calculating connected components, or performing other graph algorithms\n    on relational data that represents nodes and edges.\n\n    Args:\n        graph_solver_settings: The settings object defining the graph inputs\n            and the specific algorithm to apply.\n    \"\"\"\n    def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n        return fl.solve_graph(graph_solver_settings.graph_solver_input)\n\n    self.add_node_step(node_id=graph_solver_settings.node_id,\n                       function=_func,\n                       node_type='graph_solver',\n                       setting_input=graph_solver_settings,\n                       input_node_ids=[graph_solver_settings.depending_on_id])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_group_by","title":"<code>add_group_by(group_by_settings)</code>","text":"<p>Adds a group-by aggregation node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>group_by_settings</code> <code>NodeGroupBy</code> <p>The settings for the group-by operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_group_by(self, group_by_settings: input_schema.NodeGroupBy):\n    \"\"\"Adds a group-by aggregation node to the graph.\n\n    Args:\n        group_by_settings: The settings for the group-by operation.\n    \"\"\"\n\n    def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n        return fl.do_group_by(group_by_settings.groupby_input, False)\n\n    self.add_node_step(node_id=group_by_settings.node_id,\n                       function=_func,\n                       node_type=f'group_by',\n                       setting_input=group_by_settings,\n                       input_node_ids=[group_by_settings.depending_on_id])\n\n    node = self.get_node(group_by_settings.node_id)\n\n    def schema_callback():\n\n        output_columns = [(c.old_name, c.new_name, c.output_type) for c in group_by_settings.groupby_input.agg_cols]\n        depends_on = node.node_inputs.main_inputs[0]\n        input_schema_dict: Dict[str, str] = {s.name: s.data_type for s in depends_on.schema}\n        output_schema = []\n        for old_name, new_name, data_type in output_columns:\n            data_type = input_schema_dict[old_name] if data_type is None else data_type\n            output_schema.append(FlowfileColumn.from_input(data_type=data_type, column_name=new_name))\n        return output_schema\n\n    node.schema_callback = schema_callback\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_include_cols","title":"<code>add_include_cols(include_columns)</code>","text":"<p>Adds columns to both the input and output column lists.</p> <p>Parameters:</p> Name Type Description Default <code>include_columns</code> <code>List[str]</code> <p>A list of column names to include.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_include_cols(self, include_columns: List[str]):\n    \"\"\"Adds columns to both the input and output column lists.\n\n    Args:\n        include_columns: A list of column names to include.\n    \"\"\"\n    for column in include_columns:\n        if column not in self._input_cols:\n            self._input_cols.append(column)\n        if column not in self._output_cols:\n            self._output_cols.append(column)\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_initial_node_analysis","title":"<code>add_initial_node_analysis(node_promise)</code>","text":"<p>Adds a data exploration/analysis node based on a node promise.</p> <p>Parameters:</p> Name Type Description Default <code>node_promise</code> <code>NodePromise</code> <p>The promise representing the node to be analyzed.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_initial_node_analysis(self, node_promise: input_schema.NodePromise):\n    \"\"\"Adds a data exploration/analysis node based on a node promise.\n\n    Args:\n        node_promise: The promise representing the node to be analyzed.\n    \"\"\"\n    node_analysis = create_graphic_walker_node_from_node_promise(node_promise)\n    self.add_explore_data(node_analysis)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_join","title":"<code>add_join(join_settings)</code>","text":"<p>Adds a join node to combine two data streams based on key columns.</p> <p>Parameters:</p> Name Type Description Default <code>join_settings</code> <code>NodeJoin</code> <p>The settings for the join operation.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_join(self, join_settings: input_schema.NodeJoin) -&gt; \"FlowGraph\":\n    \"\"\"Adds a join node to combine two data streams based on key columns.\n\n    Args:\n        join_settings: The settings for the join operation.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n\n    def _func(main: FlowDataEngine, right: FlowDataEngine) -&gt; FlowDataEngine:\n        for left_select in join_settings.join_input.left_select.renames:\n            left_select.is_available = True if left_select.old_name in main.schema else False\n        for right_select in join_settings.join_input.right_select.renames:\n            right_select.is_available = True if right_select.old_name in right.schema else False\n\n        return main.join(join_input=join_settings.join_input,\n                         auto_generate_selection=join_settings.auto_generate_selection,\n                         verify_integrity=False,\n                         other=right)\n\n    self.add_node_step(node_id=join_settings.node_id,\n                       function=_func,\n                       input_columns=[],\n                       node_type='join',\n                       setting_input=join_settings,\n                       input_node_ids=join_settings.depending_on_ids)\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_manual_input","title":"<code>add_manual_input(input_file)</code>","text":"<p>Adds a node for manual data entry.</p> <p>This is a convenience alias for <code>add_datasource</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>NodeManualInput</code> <p>The settings and data for the manual input node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_manual_input(self, input_file: input_schema.NodeManualInput):\n    \"\"\"Adds a node for manual data entry.\n\n    This is a convenience alias for `add_datasource`.\n\n    Args:\n        input_file: The settings and data for the manual input node.\n    \"\"\"\n    self.add_datasource(input_file)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_node_promise","title":"<code>add_node_promise(node_promise)</code>","text":"<p>Adds a placeholder node to the graph that is not yet fully configured.</p> <p>Useful for building the graph structure before all settings are available.</p> <p>Parameters:</p> Name Type Description Default <code>node_promise</code> <code>NodePromise</code> <p>A promise object containing basic node information.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_node_promise(self, node_promise: input_schema.NodePromise):\n    \"\"\"Adds a placeholder node to the graph that is not yet fully configured.\n\n    Useful for building the graph structure before all settings are available.\n\n    Args:\n        node_promise: A promise object containing basic node information.\n    \"\"\"\n    def placeholder(n: FlowNode = None):\n        if n is None:\n            return FlowDataEngine()\n        return n\n\n    self.add_node_step(node_id=node_promise.node_id, node_type=node_promise.node_type, function=placeholder,\n                       setting_input=node_promise)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_node_step","title":"<code>add_node_step(node_id, function, input_columns=None, output_schema=None, node_type=None, drop_columns=None, renew_schema=True, setting_input=None, cache_results=None, schema_callback=None, input_node_ids=None)</code>","text":"<p>The core method for adding or updating a node in the graph.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>Union[int, str]</code> <p>The unique ID for the node.</p> required <code>function</code> <code>Callable</code> <p>The core processing function for the node.</p> required <code>input_columns</code> <code>List[str]</code> <p>A list of input column names required by the function.</p> <code>None</code> <code>output_schema</code> <code>List[FlowfileColumn]</code> <p>A predefined schema for the node's output.</p> <code>None</code> <code>node_type</code> <code>str</code> <p>A string identifying the type of node (e.g., 'filter', 'join').</p> <code>None</code> <code>drop_columns</code> <code>List[str]</code> <p>A list of columns to be dropped after the function executes.</p> <code>None</code> <code>renew_schema</code> <code>bool</code> <p>If True, the schema is recalculated after execution.</p> <code>True</code> <code>setting_input</code> <code>Any</code> <p>A configuration object containing settings for the node.</p> <code>None</code> <code>cache_results</code> <code>bool</code> <p>If True, the node's results are cached for future runs.</p> <code>None</code> <code>schema_callback</code> <code>Callable</code> <p>A function that dynamically calculates the output schema.</p> <code>None</code> <code>input_node_ids</code> <code>List[int]</code> <p>A list of IDs for the nodes that this node depends on.</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowNode</code> <p>The created or updated FlowNode object.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_node_step(self,\n                  node_id: Union[int, str],\n                  function: Callable,\n                  input_columns: List[str] = None,\n                  output_schema: List[FlowfileColumn] = None,\n                  node_type: str = None,\n                  drop_columns: List[str] = None,\n                  renew_schema: bool = True,\n                  setting_input: Any = None,\n                  cache_results: bool = None,\n                  schema_callback: Callable = None,\n                  input_node_ids: List[int] = None) -&gt; FlowNode:\n    \"\"\"The core method for adding or updating a node in the graph.\n\n    Args:\n        node_id: The unique ID for the node.\n        function: The core processing function for the node.\n        input_columns: A list of input column names required by the function.\n        output_schema: A predefined schema for the node's output.\n        node_type: A string identifying the type of node (e.g., 'filter', 'join').\n        drop_columns: A list of columns to be dropped after the function executes.\n        renew_schema: If True, the schema is recalculated after execution.\n        setting_input: A configuration object containing settings for the node.\n        cache_results: If True, the node's results are cached for future runs.\n        schema_callback: A function that dynamically calculates the output schema.\n        input_node_ids: A list of IDs for the nodes that this node depends on.\n\n    Returns:\n        The created or updated FlowNode object.\n    \"\"\"\n    existing_node = self.get_node(node_id)\n    if existing_node is not None:\n        if existing_node.node_type != node_type:\n            self.delete_node(existing_node.node_id)\n            existing_node = None\n    if existing_node:\n        input_nodes = existing_node.all_inputs\n    elif input_node_ids is not None:\n        input_nodes = [self.get_node(node_id) for node_id in input_node_ids]\n    else:\n        input_nodes = None\n    if isinstance(input_columns, str):\n        input_columns = [input_columns]\n    if (\n            input_nodes is not None or\n            function.__name__ in ('placeholder', 'analysis_preparation') or\n            node_type in (\"cloud_storage_reader\", \"polars_lazy_frame\", \"input_data\")\n    ):\n        if not existing_node:\n            node = FlowNode(node_id=node_id,\n                            function=function,\n                            output_schema=output_schema,\n                            input_columns=input_columns,\n                            drop_columns=drop_columns,\n                            renew_schema=renew_schema,\n                            setting_input=setting_input,\n                            node_type=node_type,\n                            name=function.__name__,\n                            schema_callback=schema_callback,\n                            parent_uuid=self.uuid)\n        else:\n            existing_node.update_node(function=function,\n                                      output_schema=output_schema,\n                                      input_columns=input_columns,\n                                      drop_columns=drop_columns,\n                                      setting_input=setting_input,\n                                      schema_callback=schema_callback)\n            node = existing_node\n    else:\n        raise Exception(\"No data initialized\")\n    self._node_db[node_id] = node\n    self._node_ids.append(node_id)\n    return node\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_output","title":"<code>add_output(output_file)</code>","text":"<p>Adds an output node to write the final data to a destination.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>NodeOutput</code> <p>The settings for the output file.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_output(self, output_file: input_schema.NodeOutput):\n    \"\"\"Adds an output node to write the final data to a destination.\n\n    Args:\n        output_file: The settings for the output file.\n    \"\"\"\n\n    def _func(df: FlowDataEngine):\n        output_file.output_settings.populate_abs_file_path()\n        execute_remote = self.execution_location != 'local'\n        df.output(output_fs=output_file.output_settings, flow_id=self.flow_id, node_id=output_file.node_id,\n                  execute_remote=execute_remote)\n        return df\n\n    def schema_callback():\n        input_node: FlowNode = self.get_node(output_file.node_id).node_inputs.main_inputs[0]\n\n        return input_node.schema\n    input_node_id = getattr(output_file, \"depending_on_id\") if hasattr(output_file, 'depending_on_id') else None\n    self.add_node_step(node_id=output_file.node_id,\n                       function=_func,\n                       input_columns=[],\n                       node_type='output',\n                       setting_input=output_file,\n                       schema_callback=schema_callback,\n                       input_node_ids=[input_node_id])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_pivot","title":"<code>add_pivot(pivot_settings)</code>","text":"<p>Adds a pivot node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>pivot_settings</code> <code>NodePivot</code> <p>The settings for the pivot operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_pivot(self, pivot_settings: input_schema.NodePivot):\n    \"\"\"Adds a pivot node to the graph.\n\n    Args:\n        pivot_settings: The settings for the pivot operation.\n    \"\"\"\n\n    def _func(fl: FlowDataEngine):\n        return fl.do_pivot(pivot_settings.pivot_input, self.flow_logger.get_node_logger(pivot_settings.node_id))\n\n    self.add_node_step(node_id=pivot_settings.node_id,\n                       function=_func,\n                       node_type='pivot',\n                       setting_input=pivot_settings,\n                       input_node_ids=[pivot_settings.depending_on_id])\n\n    node = self.get_node(pivot_settings.node_id)\n\n    def schema_callback():\n        input_data = node.singular_main_input.get_resulting_data()  # get from the previous step the data\n        input_data.lazy = True  # ensure the dataset is lazy\n        input_lf = input_data.data_frame  # get the lazy frame\n        return pre_calculate_pivot_schema(input_data.schema, pivot_settings.pivot_input, input_lf=input_lf)\n    node.schema_callback = schema_callback\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_polars_code","title":"<code>add_polars_code(node_polars_code)</code>","text":"<p>Adds a node that executes custom Polars code.</p> <p>Parameters:</p> Name Type Description Default <code>node_polars_code</code> <code>NodePolarsCode</code> <p>The settings for the Polars code node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_polars_code(self, node_polars_code: input_schema.NodePolarsCode):\n    \"\"\"Adds a node that executes custom Polars code.\n\n    Args:\n        node_polars_code: The settings for the Polars code node.\n    \"\"\"\n\n    def _func(*flowfile_tables: FlowDataEngine) -&gt; FlowDataEngine:\n        return execute_polars_code(*flowfile_tables, code=node_polars_code.polars_code_input.polars_code)\n    self.add_node_step(node_id=node_polars_code.node_id,\n                       function=_func,\n                       node_type='polars_code',\n                       setting_input=node_polars_code,\n                       input_node_ids=node_polars_code.depending_on_ids)\n\n    try:\n        polars_code_parser.validate_code(node_polars_code.polars_code_input.polars_code)\n    except Exception as e:\n        node = self.get_node(node_id=node_polars_code.node_id)\n        node.results.errors = str(e)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_read","title":"<code>add_read(input_file)</code>","text":"<p>Adds a node to read data from a local file (e.g., CSV, Parquet, Excel).</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>NodeRead</code> <p>The settings for the read operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_read(self, input_file: input_schema.NodeRead):\n    \"\"\"Adds a node to read data from a local file (e.g., CSV, Parquet, Excel).\n\n    Args:\n        input_file: The settings for the read operation.\n    \"\"\"\n\n    if input_file.received_file.file_type in ('xlsx', 'excel') and input_file.received_file.sheet_name == '':\n        sheet_name = fastexcel.read_excel(input_file.received_file.path).sheet_names[0]\n        input_file.received_file.sheet_name = sheet_name\n\n    received_file = input_file.received_file\n    input_file.received_file.set_absolute_filepath()\n\n    def _func():\n        input_file.received_file.set_absolute_filepath()\n        if input_file.received_file.file_type == 'parquet':\n            input_data = FlowDataEngine.create_from_path(input_file.received_file)\n        elif input_file.received_file.file_type == 'csv' and 'utf' in input_file.received_file.encoding:\n            input_data = FlowDataEngine.create_from_path(input_file.received_file)\n        else:\n            input_data = FlowDataEngine.create_from_path_worker(input_file.received_file,\n                                                                node_id=input_file.node_id,\n                                                                flow_id=self.flow_id)\n        input_data.name = input_file.received_file.name\n        return input_data\n\n    node = self.get_node(input_file.node_id)\n    schema_callback = None\n    if node:\n        start_hash = node.hash\n        node.node_type = 'read'\n        node.name = 'read'\n        node.function = _func\n        node.setting_input = input_file\n        if input_file.node_id not in set(start_node.node_id for start_node in self._flow_starts):\n            self._flow_starts.append(node)\n\n        if start_hash != node.hash:\n            logger.info('Hash changed, updating schema')\n            if len(received_file.fields) &gt; 0:\n                # If the file has fields defined, we can use them to create the schema\n                def schema_callback():\n                    return [FlowfileColumn.from_input(f.name, f.data_type) for f in received_file.fields]\n\n            elif input_file.received_file.file_type in ('csv', 'json', 'parquet'):\n                # everything that can be scanned by polars\n                def schema_callback():\n                    input_data = FlowDataEngine.create_from_path(input_file.received_file)\n                    return input_data.schema\n\n            elif input_file.received_file.file_type in ('xlsx', 'excel'):\n                # If the file is an Excel file, we need to use the openpyxl engine to read the schema\n                schema_callback = get_xlsx_schema_callback(engine='openpyxl',\n                                                           file_path=received_file.file_path,\n                                                           sheet_name=received_file.sheet_name,\n                                                           start_row=received_file.start_row,\n                                                           end_row=received_file.end_row,\n                                                           start_column=received_file.start_column,\n                                                           end_column=received_file.end_column,\n                                                           has_headers=received_file.has_headers)\n            else:\n                schema_callback = None\n    else:\n        node = FlowNode(input_file.node_id, function=_func,\n                        setting_input=input_file,\n                        name='read', node_type='read', parent_uuid=self.uuid)\n        self._node_db[input_file.node_id] = node\n        self._flow_starts.append(node)\n        self._node_ids.append(input_file.node_id)\n\n    if schema_callback is not None:\n        node.schema_callback = schema_callback\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_record_count","title":"<code>add_record_count(node_number_of_records)</code>","text":"<p>Adds a filter node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>node_number_of_records</code> <code>NodeRecordCount</code> <p>The settings for the record count operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_record_count(self, node_number_of_records: input_schema.NodeRecordCount):\n    \"\"\"Adds a filter node to the graph.\n\n    Args:\n        node_number_of_records: The settings for the record count operation.\n    \"\"\"\n\n    def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n        return fl.get_record_count()\n\n    self.add_node_step(node_id=node_number_of_records.node_id,\n                       function=_func,\n                       node_type='record_count',\n                       setting_input=node_number_of_records,\n                       input_node_ids=[node_number_of_records.depending_on_id])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_record_id","title":"<code>add_record_id(record_id_settings)</code>","text":"<p>Adds a node to create a new column with a unique ID for each record.</p> <p>Parameters:</p> Name Type Description Default <code>record_id_settings</code> <code>NodeRecordId</code> <p>The settings object specifying the name of the new record ID column.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_record_id(self, record_id_settings: input_schema.NodeRecordId) -&gt; \"FlowGraph\":\n    \"\"\"Adds a node to create a new column with a unique ID for each record.\n\n    Args:\n        record_id_settings: The settings object specifying the name of the\n            new record ID column.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n\n    def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n        return table.add_record_id(record_id_settings.record_id_input)\n\n    self.add_node_step(node_id=record_id_settings.node_id,\n                       function=_func,\n                       node_type='record_id',\n                       setting_input=record_id_settings,\n                       input_node_ids=[record_id_settings.depending_on_id]\n                       )\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_sample","title":"<code>add_sample(sample_settings)</code>","text":"<p>Adds a node to take a random or top-N sample of the data.</p> <p>Parameters:</p> Name Type Description Default <code>sample_settings</code> <code>NodeSample</code> <p>The settings object specifying the size of the sample.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_sample(self, sample_settings: input_schema.NodeSample) -&gt; \"FlowGraph\":\n    \"\"\"Adds a node to take a random or top-N sample of the data.\n\n    Args:\n        sample_settings: The settings object specifying the size of the sample.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n    def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n        return table.get_sample(sample_settings.sample_size)\n\n    self.add_node_step(node_id=sample_settings.node_id,\n                       function=_func,\n                       node_type='sample',\n                       setting_input=sample_settings,\n                       input_node_ids=[sample_settings.depending_on_id]\n                       )\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_select","title":"<code>add_select(select_settings)</code>","text":"<p>Adds a node to select, rename, reorder, or drop columns.</p> <p>Parameters:</p> Name Type Description Default <code>select_settings</code> <code>NodeSelect</code> <p>The settings for the select operation.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_select(self, select_settings: input_schema.NodeSelect) -&gt; \"FlowGraph\":\n    \"\"\"Adds a node to select, rename, reorder, or drop columns.\n\n    Args:\n        select_settings: The settings for the select operation.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n\n    select_cols = select_settings.select_input\n    drop_cols = tuple(s.old_name for s in select_settings.select_input)\n\n    def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n        input_cols = set(f.name for f in table.schema)\n        ids_to_remove = []\n        for i, select_col in enumerate(select_cols):\n            if select_col.data_type is None:\n                select_col.data_type = table.get_schema_column(select_col.old_name).data_type\n            if select_col.old_name not in input_cols:\n                select_col.is_available = False\n                if not select_col.keep:\n                    ids_to_remove.append(i)\n            else:\n                select_col.is_available = True\n        ids_to_remove.reverse()\n        for i in ids_to_remove:\n            v = select_cols.pop(i)\n            del v\n        return table.do_select(select_inputs=transform_schema.SelectInputs(select_cols),\n                               keep_missing=select_settings.keep_missing)\n\n    self.add_node_step(node_id=select_settings.node_id,\n                       function=_func,\n                       input_columns=[],\n                       node_type='select',\n                       drop_columns=list(drop_cols),\n                       setting_input=select_settings,\n                       input_node_ids=[select_settings.depending_on_id])\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_sort","title":"<code>add_sort(sort_settings)</code>","text":"<p>Adds a node to sort the data based on one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>sort_settings</code> <code>NodeSort</code> <p>The settings for the sort operation.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_sort(self, sort_settings: input_schema.NodeSort) -&gt; \"FlowGraph\":\n    \"\"\"Adds a node to sort the data based on one or more columns.\n\n    Args:\n        sort_settings: The settings for the sort operation.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n\n    def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n        return table.do_sort(sort_settings.sort_input)\n\n    self.add_node_step(node_id=sort_settings.node_id,\n                       function=_func,\n                       node_type='sort',\n                       setting_input=sort_settings,\n                       input_node_ids=[sort_settings.depending_on_id])\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_sql_source","title":"<code>add_sql_source(external_source_input)</code>","text":"<p>Adds a node that reads data from a SQL source.</p> <p>This is a convenience alias for <code>add_external_source</code>.</p> <p>Parameters:</p> Name Type Description Default <code>external_source_input</code> <code>NodeExternalSource</code> <p>The settings for the external SQL source node.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_sql_source(self, external_source_input: input_schema.NodeExternalSource):\n    \"\"\"Adds a node that reads data from a SQL source.\n\n    This is a convenience alias for `add_external_source`.\n\n    Args:\n        external_source_input: The settings for the external SQL source node.\n    \"\"\"\n    logger.info('Adding sql source')\n    self.add_external_source(external_source_input)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_text_to_rows","title":"<code>add_text_to_rows(node_text_to_rows)</code>","text":"<p>Adds a node that splits cell values into multiple rows.</p> <p>This is useful for un-nesting data where a single field contains multiple values separated by a delimiter.</p> <p>Parameters:</p> Name Type Description Default <code>node_text_to_rows</code> <code>NodeTextToRows</code> <p>The settings object that specifies the column to split and the delimiter to use.</p> required <p>Returns:</p> Type Description <code>FlowGraph</code> <p>The <code>FlowGraph</code> instance for method chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_text_to_rows(self, node_text_to_rows: input_schema.NodeTextToRows) -&gt; \"FlowGraph\":\n    \"\"\"Adds a node that splits cell values into multiple rows.\n\n    This is useful for un-nesting data where a single field contains multiple\n    values separated by a delimiter.\n\n    Args:\n        node_text_to_rows: The settings object that specifies the column to split\n            and the delimiter to use.\n\n    Returns:\n        The `FlowGraph` instance for method chaining.\n    \"\"\"\n    def _func(table: FlowDataEngine) -&gt; FlowDataEngine:\n        return table.split(node_text_to_rows.text_to_rows_input)\n\n    self.add_node_step(node_id=node_text_to_rows.node_id,\n                       function=_func,\n                       node_type='text_to_rows',\n                       setting_input=node_text_to_rows,\n                       input_node_ids=[node_text_to_rows.depending_on_id])\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_union","title":"<code>add_union(union_settings)</code>","text":"<p>Adds a union node to combine multiple data streams.</p> <p>Parameters:</p> Name Type Description Default <code>union_settings</code> <code>NodeUnion</code> <p>The settings for the union operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_union(self, union_settings: input_schema.NodeUnion):\n    \"\"\"Adds a union node to combine multiple data streams.\n\n    Args:\n        union_settings: The settings for the union operation.\n    \"\"\"\n\n    def _func(*flowfile_tables: FlowDataEngine):\n        dfs: List[pl.LazyFrame] | List[pl.DataFrame] = [flt.data_frame for flt in flowfile_tables]\n        return FlowDataEngine(pl.concat(dfs, how='diagonal_relaxed'))\n\n    self.add_node_step(node_id=union_settings.node_id,\n                       function=_func,\n                       node_type=f'union',\n                       setting_input=union_settings,\n                       input_node_ids=union_settings.depending_on_ids)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_unique","title":"<code>add_unique(unique_settings)</code>","text":"<p>Adds a node to find and remove duplicate rows.</p> <p>Parameters:</p> Name Type Description Default <code>unique_settings</code> <code>NodeUnique</code> <p>The settings for the unique operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_unique(self, unique_settings: input_schema.NodeUnique):\n    \"\"\"Adds a node to find and remove duplicate rows.\n\n    Args:\n        unique_settings: The settings for the unique operation.\n    \"\"\"\n\n    def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n        return fl.make_unique(unique_settings.unique_input)\n\n    self.add_node_step(node_id=unique_settings.node_id,\n                       function=_func,\n                       input_columns=[],\n                       node_type='unique',\n                       setting_input=unique_settings,\n                       input_node_ids=[unique_settings.depending_on_id])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.add_unpivot","title":"<code>add_unpivot(unpivot_settings)</code>","text":"<p>Adds an unpivot node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>unpivot_settings</code> <code>NodeUnpivot</code> <p>The settings for the unpivot operation.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def add_unpivot(self, unpivot_settings: input_schema.NodeUnpivot):\n    \"\"\"Adds an unpivot node to the graph.\n\n    Args:\n        unpivot_settings: The settings for the unpivot operation.\n    \"\"\"\n\n    def _func(fl: FlowDataEngine) -&gt; FlowDataEngine:\n        return fl.unpivot(unpivot_settings.unpivot_input)\n\n    self.add_node_step(node_id=unpivot_settings.node_id,\n                       function=_func,\n                       node_type='unpivot',\n                       setting_input=unpivot_settings,\n                       input_node_ids=[unpivot_settings.depending_on_id])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.apply_layout","title":"<code>apply_layout(y_spacing=150, x_spacing=200, initial_y=100)</code>","text":"<p>Calculates and applies a layered layout to all nodes in the graph.</p> <p>This updates their x and y positions for UI rendering.</p> <p>Parameters:</p> Name Type Description Default <code>y_spacing</code> <code>int</code> <p>The vertical spacing between layers.</p> <code>150</code> <code>x_spacing</code> <code>int</code> <p>The horizontal spacing between nodes in the same layer.</p> <code>200</code> <code>initial_y</code> <code>int</code> <p>The initial y-position for the first layer.</p> <code>100</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def apply_layout(self, y_spacing: int = 150, x_spacing: int = 200, initial_y: int = 100):\n    \"\"\"Calculates and applies a layered layout to all nodes in the graph.\n\n    This updates their x and y positions for UI rendering.\n\n    Args:\n        y_spacing: The vertical spacing between layers.\n        x_spacing: The horizontal spacing between nodes in the same layer.\n        initial_y: The initial y-position for the first layer.\n    \"\"\"\n    self.flow_logger.info(\"Applying layered layout...\")\n    start_time = time()\n    try:\n        # Calculate new positions for all nodes\n        new_positions = calculate_layered_layout(\n            self, y_spacing=y_spacing, x_spacing=x_spacing, initial_y=initial_y\n        )\n\n        if not new_positions:\n            self.flow_logger.warning(\"Layout calculation returned no positions.\")\n            return\n\n        # Apply the new positions to the setting_input of each node\n        updated_count = 0\n        for node_id, (pos_x, pos_y) in new_positions.items():\n            node = self.get_node(node_id)\n            if node and hasattr(node, 'setting_input'):\n                setting = node.setting_input\n                if hasattr(setting, 'pos_x') and hasattr(setting, 'pos_y'):\n                    setting.pos_x = pos_x\n                    setting.pos_y = pos_y\n                    updated_count += 1\n                else:\n                    self.flow_logger.warning(f\"Node {node_id} setting_input ({type(setting)}) lacks pos_x/pos_y attributes.\")\n            elif node:\n                self.flow_logger.warning(f\"Node {node_id} lacks setting_input attribute.\")\n            # else: Node not found, already warned by calculate_layered_layout\n\n        end_time = time()\n        self.flow_logger.info(f\"Layout applied to {updated_count}/{len(self.nodes)} nodes in {end_time - start_time:.2f} seconds.\")\n\n    except Exception as e:\n        self.flow_logger.error(f\"Error applying layout: {e}\")\n        raise  # Optional: re-raise the exception\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.cancel","title":"<code>cancel()</code>","text":"<p>Cancels an ongoing graph execution.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def cancel(self):\n    \"\"\"Cancels an ongoing graph execution.\"\"\"\n\n    if not self.flow_settings.is_running:\n        return\n    self.flow_settings.is_canceled = True\n    for node in self.nodes:\n        node.cancel()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.close_flow","title":"<code>close_flow()</code>","text":"<p>Performs cleanup operations, such as clearing node caches.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def close_flow(self):\n    \"\"\"Performs cleanup operations, such as clearing node caches.\"\"\"\n\n    for node in self.nodes:\n        node.remove_cache()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.copy_node","title":"<code>copy_node(new_node_settings, existing_setting_input, node_type)</code>","text":"<p>Creates a copy of an existing node.</p> <p>Parameters:</p> Name Type Description Default <code>new_node_settings</code> <code>NodePromise</code> <p>The promise containing new settings (like ID and position).</p> required <code>existing_setting_input</code> <code>Any</code> <p>The settings object from the node being copied.</p> required <code>node_type</code> <code>str</code> <p>The type of the node being copied.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def copy_node(self, new_node_settings: input_schema.NodePromise, existing_setting_input: Any, node_type: str) -&gt; None:\n    \"\"\"Creates a copy of an existing node.\n\n    Args:\n        new_node_settings: The promise containing new settings (like ID and position).\n        existing_setting_input: The settings object from the node being copied.\n        node_type: The type of the node being copied.\n    \"\"\"\n    self.add_node_promise(new_node_settings)\n\n    if isinstance(existing_setting_input, input_schema.NodePromise):\n        return\n\n    combined_settings = combine_existing_settings_and_new_settings(\n        existing_setting_input, new_node_settings\n    )\n    getattr(self, f\"add_{node_type}\")(combined_settings)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.delete_node","title":"<code>delete_node(node_id)</code>","text":"<p>Deletes a node from the graph and updates all its connections.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>Union[int, str]</code> <p>The ID of the node to delete.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If the node with the given ID does not exist.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def delete_node(self, node_id: Union[int, str]):\n    \"\"\"Deletes a node from the graph and updates all its connections.\n\n    Args:\n        node_id: The ID of the node to delete.\n\n    Raises:\n        Exception: If the node with the given ID does not exist.\n    \"\"\"\n    logger.info(f\"Starting deletion of node with ID: {node_id}\")\n\n    node = self._node_db.get(node_id)\n    if node:\n        logger.info(f\"Found node: {node_id}, processing deletion\")\n\n        lead_to_steps: List[FlowNode] = node.leads_to_nodes\n        logger.debug(f\"Node {node_id} leads to {len(lead_to_steps)} other nodes\")\n\n        if len(lead_to_steps) &gt; 0:\n            for lead_to_step in lead_to_steps:\n                logger.debug(f\"Deleting input node {node_id} from dependent node {lead_to_step}\")\n                lead_to_step.delete_input_node(node_id, complete=True)\n\n        if not node.is_start:\n            depends_on: List[FlowNode] = node.node_inputs.get_all_inputs()\n            logger.debug(f\"Node {node_id} depends on {len(depends_on)} other nodes\")\n\n            for depend_on in depends_on:\n                logger.debug(f\"Removing lead_to reference {node_id} from node {depend_on}\")\n                depend_on.delete_lead_to_node(node_id)\n\n        self._node_db.pop(node_id)\n        logger.debug(f\"Successfully removed node {node_id} from node_db\")\n        del node\n        logger.info(\"Node object deleted\")\n    else:\n        logger.error(f\"Failed to find node with id {node_id}\")\n        raise Exception(f\"Node with id {node_id} does not exist\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.generate_code","title":"<code>generate_code()</code>","text":"<p>Generates code for the flow graph. This method exports the flow graph to a Polars-compatible format.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def generate_code(self):\n    \"\"\"Generates code for the flow graph.\n    This method exports the flow graph to a Polars-compatible format.\n    \"\"\"\n    from flowfile_core.flowfile.code_generator.code_generator import export_flow_to_polars\n    print(export_flow_to_polars(self))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_frontend_data","title":"<code>get_frontend_data()</code>","text":"<p>Formats the graph structure into a JSON-like dictionary for a specific legacy frontend.</p> <p>This method transforms the graph's state into a format compatible with the Drawflow.js library.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary representing the graph in Drawflow format.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_frontend_data(self) -&gt; dict:\n    \"\"\"Formats the graph structure into a JSON-like dictionary for a specific legacy frontend.\n\n    This method transforms the graph's state into a format compatible with the\n    Drawflow.js library.\n\n    Returns:\n        A dictionary representing the graph in Drawflow format.\n    \"\"\"\n    result = {\n        'Home': {\n            \"data\": {}\n        }\n    }\n    flow_info: schemas.FlowInformation = self.get_node_storage()\n\n    for node_id, node_info in flow_info.data.items():\n        if node_info.is_setup:\n            try:\n                pos_x = node_info.data.pos_x\n                pos_y = node_info.data.pos_y\n                # Basic node structure\n                result[\"Home\"][\"data\"][str(node_id)] = {\n                    \"id\": node_info.id,\n                    \"name\": node_info.type,\n                    \"data\": {},  # Additional data can go here\n                    \"class\": node_info.type,\n                    \"html\": node_info.type,\n                    \"typenode\": \"vue\",\n                    \"inputs\": {},\n                    \"outputs\": {},\n                    \"pos_x\": pos_x,\n                    \"pos_y\": pos_y\n                }\n            except Exception as e:\n                logger.error(e)\n        # Add outputs to the node based on `outputs` in your backend data\n        if node_info.outputs:\n            outputs = {o: 0 for o in node_info.outputs}\n            for o in node_info.outputs:\n                outputs[o] += 1\n            connections = []\n            for output_node_id, n_connections in outputs.items():\n                leading_to_node = self.get_node(output_node_id)\n                input_types = leading_to_node.get_input_type(node_info.id)\n                for input_type in input_types:\n                    if input_type == 'main':\n                        input_frontend_id = 'input_1'\n                    elif input_type == 'right':\n                        input_frontend_id = 'input_2'\n                    elif input_type == 'left':\n                        input_frontend_id = 'input_3'\n                    else:\n                        input_frontend_id = 'input_1'\n                    connection = {\"node\": str(output_node_id), \"input\": input_frontend_id}\n                    connections.append(connection)\n\n            result[\"Home\"][\"data\"][str(node_id)][\"outputs\"][\"output_1\"] = {\n                \"connections\": connections}\n        else:\n            result[\"Home\"][\"data\"][str(node_id)][\"outputs\"] = {\"output_1\": {\"connections\": []}}\n\n        # Add input to the node based on `depending_on_id` in your backend data\n        if node_info.left_input_id is not None or node_info.right_input_id is not None or node_info.input_ids is not None:\n            main_inputs = node_info.main_input_ids\n            result[\"Home\"][\"data\"][str(node_id)][\"inputs\"][\"input_1\"] = {\n                \"connections\": [{\"node\": str(main_node_id), \"input\": \"output_1\"} for main_node_id in main_inputs]\n            }\n            if node_info.right_input_id is not None:\n                result[\"Home\"][\"data\"][str(node_id)][\"inputs\"][\"input_2\"] = {\n                    \"connections\": [{\"node\": str(node_info.right_input_id), \"input\": \"output_1\"}]\n                }\n            if node_info.left_input_id is not None:\n                result[\"Home\"][\"data\"][str(node_id)][\"inputs\"][\"input_3\"] = {\n                    \"connections\": [{\"node\": str(node_info.left_input_id), \"input\": \"output_1\"}]\n                }\n    return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_implicit_starter_nodes","title":"<code>get_implicit_starter_nodes()</code>","text":"<p>Finds nodes that can act as starting points but are not explicitly defined as such.</p> <p>Some nodes, like the Polars Code node, can function without an input. This method identifies such nodes if they have no incoming connections.</p> <p>Returns:</p> Type Description <code>List[FlowNode]</code> <p>A list of <code>FlowNode</code> objects that are implicit starting nodes.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_implicit_starter_nodes(self) -&gt; List[FlowNode]:\n    \"\"\"Finds nodes that can act as starting points but are not explicitly defined as such.\n\n    Some nodes, like the Polars Code node, can function without an input. This\n    method identifies such nodes if they have no incoming connections.\n\n    Returns:\n        A list of `FlowNode` objects that are implicit starting nodes.\n    \"\"\"\n    starting_node_ids = [node.node_id for node in self._flow_starts]\n    implicit_starting_nodes = []\n    for node in self.nodes:\n        if node.node_template.can_be_start and not node.has_input and node.node_id not in starting_node_ids:\n            implicit_starting_nodes.append(node)\n    return implicit_starting_nodes\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_node","title":"<code>get_node(node_id=None)</code>","text":"<p>Retrieves a node from the graph by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>Union[int, str]</code> <p>The ID of the node to retrieve. If None, retrieves the last added node.</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowNode | None</code> <p>The FlowNode object, or None if not found.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_node(self, node_id: Union[int, str] = None) -&gt; FlowNode | None:\n    \"\"\"Retrieves a node from the graph by its ID.\n\n    Args:\n        node_id: The ID of the node to retrieve. If None, retrieves the last added node.\n\n    Returns:\n        The FlowNode object, or None if not found.\n    \"\"\"\n    if node_id is None:\n        node_id = self._node_ids[-1]\n    node = self._node_db.get(node_id)\n    if node is not None:\n        return node\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_node_data","title":"<code>get_node_data(node_id, include_example=True)</code>","text":"<p>Retrieves all data needed to render a node in the UI.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>int</code> <p>The ID of the node.</p> required <code>include_example</code> <code>bool</code> <p>Whether to include data samples in the result.</p> <code>True</code> <p>Returns:</p> Type Description <code>NodeData</code> <p>A NodeData object, or None if the node is not found.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_node_data(self, node_id: int, include_example: bool = True) -&gt; NodeData:\n    \"\"\"Retrieves all data needed to render a node in the UI.\n\n    Args:\n        node_id: The ID of the node.\n        include_example: Whether to include data samples in the result.\n\n    Returns:\n        A NodeData object, or None if the node is not found.\n    \"\"\"\n    node = self._node_db[node_id]\n    return node.get_node_data(flow_id=self.flow_id, include_example=include_example)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_node_storage","title":"<code>get_node_storage()</code>","text":"<p>Serializes the entire graph's state into a storable format.</p> <p>Returns:</p> Type Description <code>FlowInformation</code> <p>A FlowInformation object representing the complete graph.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_node_storage(self) -&gt; schemas.FlowInformation:\n    \"\"\"Serializes the entire graph's state into a storable format.\n\n    Returns:\n        A FlowInformation object representing the complete graph.\n    \"\"\"\n    node_information = {node.node_id: node.get_node_information() for\n                        node in self.nodes if node.is_setup and node.is_correct}\n\n    return schemas.FlowInformation(flow_id=self.flow_id,\n                                   flow_name=self.__name__,\n                                   flow_settings=self.flow_settings,\n                                   data=node_information,\n                                   node_starts=[v.node_id for v in self._flow_starts],\n                                   node_connections=self.node_connections\n                                   )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_nodes_overview","title":"<code>get_nodes_overview()</code>","text":"<p>Gets a list of dictionary representations for all nodes in the graph.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_nodes_overview(self):\n    \"\"\"Gets a list of dictionary representations for all nodes in the graph.\"\"\"\n    output = []\n    for v in self._node_db.values():\n        output.append(v.get_repr())\n    return output\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_run_info","title":"<code>get_run_info()</code>","text":"<p>Gets a summary of the most recent graph execution.</p> <p>Returns:</p> Type Description <code>RunInformation | None</code> <p>A RunInformation object with details about the last run.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_run_info(self) -&gt; RunInformation | None:\n    \"\"\"Gets a summary of the most recent graph execution.\n\n    Returns:\n        A RunInformation object with details about the last run.\n    \"\"\"\n    is_running = self.flow_settings.is_running\n    if self.latest_run_info is None:\n        return\n\n    elif not is_running and self.latest_run_info.success is not None:\n        return self.latest_run_info\n\n    run_info = self.latest_run_info\n    if not is_running:\n        run_info.success = all(nr.success for nr in run_info.node_step_result)\n    return run_info\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.get_vue_flow_input","title":"<code>get_vue_flow_input()</code>","text":"<p>Formats the graph's nodes and edges into a schema suitable for the VueFlow frontend.</p> <p>Returns:</p> Type Description <code>VueFlowInput</code> <p>A VueFlowInput object.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def get_vue_flow_input(self) -&gt; schemas.VueFlowInput:\n    \"\"\"Formats the graph's nodes and edges into a schema suitable for the VueFlow frontend.\n\n    Returns:\n        A VueFlowInput object.\n    \"\"\"\n    edges: List[schemas.NodeEdge] = []\n    nodes: List[schemas.NodeInput] = []\n    for node in self.nodes:\n        nodes.append(node.get_node_input())\n        edges.extend(node.get_edge_input())\n    return schemas.VueFlowInput(node_edges=edges, node_inputs=nodes)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.print_tree","title":"<code>print_tree()</code>","text":"<p>Print flow_graph as a visual tree structure, showing the DAG relationships with ASCII art.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def print_tree(self):\n    \"\"\"Print flow_graph as a visual tree structure, showing the DAG relationships with ASCII art.\"\"\"\n    if not self._node_db:\n        self.flow_logger.info(\"Empty flow graph\")\n        return\n\n    # Build node information\n    node_info = build_node_info(self.nodes)\n\n    # Calculate depths for all nodes\n    for node_id in node_info:\n        calculate_depth(node_id, node_info)\n\n    # Group nodes by depth\n    depth_groups, max_depth = group_nodes_by_depth(node_info)\n\n    # Sort nodes within each depth group\n    for depth in depth_groups:\n        depth_groups[depth].sort()\n\n    # Create the main flow visualization\n    lines = [\"=\" * 80, \"Flow Graph Visualization\", \"=\" * 80, \"\"]\n\n    # Track which nodes connect to what\n    merge_points = define_node_connections(node_info)\n\n    # Build the flow paths\n\n    # Find the maximum label length for each depth level\n    max_label_length = {}\n    for depth in range(max_depth + 1):\n        if depth in depth_groups:\n            max_len = max(len(node_info[nid].label) for nid in depth_groups[depth])\n            max_label_length[depth] = max_len\n\n    # Draw the paths\n    drawn_nodes = set()\n    merge_drawn = set()\n\n    # Group paths by their merge points\n    paths_by_merge = {}\n    standalone_paths = []\n\n    # Build flow paths\n    paths = build_flow_paths(node_info, self._flow_starts, merge_points)\n\n    # Define paths to merge and standalone paths\n    for path in paths:\n        if len(path) &gt; 1 and path[-1] in merge_points and len(merge_points[path[-1]]) &gt; 1:\n            merge_id = path[-1]\n            if merge_id not in paths_by_merge:\n                paths_by_merge[merge_id] = []\n            paths_by_merge[merge_id].append(path)\n        else:\n            standalone_paths.append(path)\n\n    # Draw merged paths\n    draw_merged_paths(node_info, merge_points, paths_by_merge, merge_drawn, drawn_nodes, lines)\n\n    # Draw standlone paths\n    draw_standalone_paths(drawn_nodes, standalone_paths, lines, node_info)\n\n    # Add undrawn nodes\n    add_un_drawn_nodes(drawn_nodes, node_info, lines)\n\n    try:\n        skip_nodes, ordered_nodes = compute_execution_plan(\n            nodes=self.nodes,\n            flow_starts=self._flow_starts+self.get_implicit_starter_nodes())\n        if ordered_nodes:\n            for i, node in enumerate(ordered_nodes, 1):\n                lines.append(f\"  {i:3d}. {node_info[node.node_id].label}\")\n    except Exception as e:\n        lines.append(f\"  Could not determine execution order: {e}\")\n\n    # Print everything\n    output = \"\\n\".join(lines)\n\n    print(output)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.remove_from_output_cols","title":"<code>remove_from_output_cols(columns)</code>","text":"<p>Removes specified columns from the list of expected output columns.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>List[str]</code> <p>A list of column names to remove.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def remove_from_output_cols(self, columns: List[str]):\n    \"\"\"Removes specified columns from the list of expected output columns.\n\n    Args:\n        columns: A list of column names to remove.\n    \"\"\"\n    cols = set(columns)\n    self._output_cols = [c for c in self._output_cols if c not in cols]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.reset","title":"<code>reset()</code>","text":"<p>Forces a deep reset on all nodes in the graph.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def reset(self):\n    \"\"\"Forces a deep reset on all nodes in the graph.\"\"\"\n\n    for node in self.nodes:\n        node.reset(True)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.run_graph","title":"<code>run_graph()</code>","text":"<p>Executes the entire data flow graph from start to finish.</p> <p>It determines the correct execution order, runs each node, collects results, and handles errors and cancellations.</p> <p>Returns:</p> Type Description <code>RunInformation | None</code> <p>A RunInformation object summarizing the execution results.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the flow is already running.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def run_graph(self) -&gt; RunInformation | None:\n    \"\"\"Executes the entire data flow graph from start to finish.\n\n    It determines the correct execution order, runs each node,\n    collects results, and handles errors and cancellations.\n\n    Returns:\n        A RunInformation object summarizing the execution results.\n\n    Raises:\n        Exception: If the flow is already running.\n    \"\"\"\n    if self.flow_settings.is_running:\n        raise Exception('Flow is already running')\n    try:\n\n        self.flow_settings.is_running = True\n        self.flow_settings.is_canceled = False\n        self.flow_logger.clear_log_file()\n        self.flow_logger.info('Starting to run flowfile flow...')\n\n        skip_nodes, execution_order = compute_execution_plan(\n            nodes=self.nodes,\n            flow_starts=self._flow_starts+self.get_implicit_starter_nodes()\n        )\n\n        self.latest_run_info = self.create_initial_run_information(len(execution_order), \"full_run\")\n\n        skip_node_message(self.flow_logger, skip_nodes)\n        execution_order_message(self.flow_logger, execution_order)\n        performance_mode = self.flow_settings.execution_mode == 'Performance'\n\n        for node in execution_order:\n            node_logger = self.flow_logger.get_node_logger(node.node_id)\n            if self.flow_settings.is_canceled:\n                self.flow_logger.info('Flow canceled')\n                break\n            if node in skip_nodes:\n                node_logger.info(f'Skipping node {node.node_id}')\n                continue\n            node_result = NodeResult(node_id=node.node_id, node_name=node.name)\n            self.latest_run_info.node_step_result.append(node_result)\n            logger.info(f'Starting to run: node {node.node_id}, start time: {node_result.start_timestamp}')\n            node.execute_node(run_location=self.flow_settings.execution_location,\n                              performance_mode=performance_mode,\n                              node_logger=node_logger)\n            try:\n                node_result.error = str(node.results.errors)\n                if self.flow_settings.is_canceled:\n                    node_result.success = None\n                    node_result.success = None\n                    node_result.is_running = False\n                    continue\n                node_result.success = node.results.errors is None\n                node_result.end_timestamp = time()\n                node_result.run_time = int(node_result.end_timestamp - node_result.start_timestamp)\n                node_result.is_running = False\n            except Exception as e:\n                node_result.error = 'Node did not run'\n                node_result.success = False\n                node_result.end_timestamp = time()\n                node_result.run_time = int(node_result.end_timestamp - node_result.start_timestamp)\n                node_result.is_running = False\n                node_logger.error(f'Error in node {node.node_id}: {e}')\n            if not node_result.success:\n                skip_nodes.extend(list(node.get_all_dependent_nodes()))\n            node_logger.info(f'Completed node with success: {node_result.success}')\n            self.latest_run_info.nodes_completed += 1\n        self.flow_logger.info('Flow completed!')\n        self.end_datetime = datetime.datetime.now()\n        self.flow_settings.is_running = False\n        if self.flow_settings.is_canceled:\n            self.flow_logger.info('Flow canceled')\n        return self.get_run_info()\n    except Exception as e:\n        raise e\n    finally:\n        self.flow_settings.is_running = False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.save_flow","title":"<code>save_flow(flow_path)</code>","text":"<p>Saves the current state of the flow graph to a file.</p> <p>Parameters:</p> Name Type Description Default <code>flow_path</code> <code>str</code> <p>The path where the flow file will be saved.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def save_flow(self, flow_path: str):\n    \"\"\"Saves the current state of the flow graph to a file.\n\n    Args:\n        flow_path: The path where the flow file will be saved.\n    \"\"\"\n    logger.info(\"Saving flow to %s\", flow_path)\n    os.makedirs(os.path.dirname(flow_path), exist_ok=True)\n    try:\n        with open(flow_path, 'wb') as f:\n            pickle.dump(self.get_node_storage(), f)\n    except Exception as e:\n        logger.error(f\"Error saving flow: {e}\")\n\n    self.flow_settings.path = flow_path\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_graph.FlowGraph.trigger_fetch_node","title":"<code>trigger_fetch_node(node_id)</code>","text":"<p>Executes a specific node in the graph by its ID.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_graph.py</code> <pre><code>def trigger_fetch_node(self, node_id: int) -&gt; RunInformation | None:\n    \"\"\"Executes a specific node in the graph by its ID.\"\"\"\n    if self.flow_settings.is_running:\n        raise Exception(\"Flow is already running\")\n    flow_node = self.get_node(node_id)\n    self.flow_settings.is_running = True\n    self.flow_settings.is_canceled = False\n    self.flow_logger.clear_log_file()\n    self.latest_run_info = self.create_initial_run_information(1, \"fetch_one\")\n    node_logger = self.flow_logger.get_node_logger(flow_node.node_id)\n    node_result = NodeResult(node_id=flow_node.node_id, node_name=flow_node.name)\n    logger.info(f'Starting to run: node {flow_node.node_id}, start time: {node_result.start_timestamp}')\n    try:\n        self.latest_run_info.node_step_result.append(node_result)\n        flow_node.execute_node(run_location=self.flow_settings.execution_location,\n                               performance_mode=False,\n                               node_logger=node_logger,\n                               optimize_for_downstream=False,\n                               reset_cache=True)\n        node_result.error = str(flow_node.results.errors)\n        if self.flow_settings.is_canceled:\n            node_result.success = None\n            node_result.success = None\n            node_result.is_running = False\n        node_result.success = flow_node.results.errors is None\n        node_result.end_timestamp = time()\n        node_result.run_time = int(node_result.end_timestamp - node_result.start_timestamp)\n        node_result.is_running = False\n        self.latest_run_info.nodes_completed += 1\n        self.latest_run_info.end_time = datetime.datetime.now()\n        self.flow_settings.is_running = False\n        return self.get_run_info()\n    except Exception as e:\n        node_result.error = 'Node did not run'\n        node_result.success = False\n        node_result.end_timestamp = time()\n        node_result.run_time = int(node_result.end_timestamp - node_result.start_timestamp)\n        node_result.is_running = False\n        node_logger.error(f'Error in node {flow_node.node_id}: {e}')\n    finally:\n        self.flow_settings.is_running = False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flownode","title":"FlowNode","text":"<p>The <code>FlowNode</code> represents a single operation in the <code>FlowGraph</code>. Each node corresponds to a specific transformation or action, such as filtering or grouping data.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode","title":"<code>flowfile_core.flowfile.flow_node.flow_node.FlowNode</code>","text":"<p>Represents a single node in a data flow graph.</p> <p>This class manages the node's state, its data processing function, and its connections to other nodes within the graph.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Makes the node instance callable, acting as an alias for execute_node.</p> <code>__init__</code> <p>Initializes a FlowNode instance.</p> <code>__repr__</code> <p>Provides a string representation of the FlowNode instance.</p> <code>add_lead_to_in_depend_source</code> <p>Ensures this node is registered in the <code>leads_to_nodes</code> list of its inputs.</p> <code>add_node_connection</code> <p>Adds a connection from a source node to this node.</p> <code>calculate_hash</code> <p>Calculates a hash based on settings and input node hashes.</p> <code>cancel</code> <p>Cancels an ongoing external process if one is running.</p> <code>clear_table_example</code> <p>Clear the table example in the results so that it clears the existing results</p> <code>create_schema_callback_from_function</code> <p>Wraps a node's function to create a schema callback that extracts the schema.</p> <code>delete_input_node</code> <p>Removes a connection from a specific input node.</p> <code>delete_lead_to_node</code> <p>Removes a connection to a specific downstream node.</p> <code>evaluate_nodes</code> <p>Triggers a state reset for all directly connected downstream nodes.</p> <code>execute_full_local</code> <p>Executes the node's logic locally, including example data generation.</p> <code>execute_local</code> <p>Executes the node's logic locally.</p> <code>execute_node</code> <p>Orchestrates the execution, handling location, caching, and retries.</p> <code>execute_remote</code> <p>Executes the node's logic remotely or handles cached results.</p> <code>get_all_dependent_node_ids</code> <p>Yields the IDs of all downstream nodes recursively.</p> <code>get_all_dependent_nodes</code> <p>Yields all downstream nodes recursively.</p> <code>get_edge_input</code> <p>Generates <code>NodeEdge</code> objects for all input connections to this node.</p> <code>get_flow_file_column_schema</code> <p>Retrieves the schema for a specific column from the output schema.</p> <code>get_input_type</code> <p>Gets the type of connection ('main', 'left', 'right') for a given input node ID.</p> <code>get_node_data</code> <p>Gathers all necessary data for representing the node in the UI.</p> <code>get_node_information</code> <p>Updates and returns the node's information object.</p> <code>get_node_input</code> <p>Creates a <code>NodeInput</code> schema object for representing this node in the UI.</p> <code>get_output_data</code> <p>Gets the full output data sample for this node.</p> <code>get_predicted_resulting_data</code> <p>Creates a <code>FlowDataEngine</code> instance based on the predicted schema.</p> <code>get_predicted_schema</code> <p>Predicts the output schema of the node without full execution.</p> <code>get_repr</code> <p>Gets a detailed dictionary representation of the node's state.</p> <code>get_resulting_data</code> <p>Executes the node's function to produce the actual output data.</p> <code>get_table_example</code> <p>Generates a <code>TableExample</code> model summarizing the node's output.</p> <code>needs_reset</code> <p>Checks if the node's hash has changed, indicating an outdated state.</p> <code>needs_run</code> <p>Determines if the node needs to be executed.</p> <code>post_init</code> <p>Initializes or resets the node's attributes to their default states.</p> <code>prepare_before_run</code> <p>Resets results and errors before a new execution.</p> <code>print</code> <p>Helper method to log messages with node context.</p> <code>remove_cache</code> <p>Removes cached results for this node.</p> <code>reset</code> <p>Resets the node's execution state and schema information.</p> <code>set_node_information</code> <p>Populates the <code>node_information</code> attribute with the current state.</p> <code>store_example_data_generator</code> <p>Stores a generator function for fetching a sample of the result data.</p> <code>update_node</code> <p>Updates the properties of the node.</p> <p>Attributes:</p> Name Type Description <code>all_inputs</code> <code>List[FlowNode]</code> <p>Gets a list of all nodes connected to any input port.</p> <code>function</code> <code>Callable</code> <p>Gets the core processing function of the node.</p> <code>has_input</code> <code>bool</code> <p>Checks if this node has any input connections.</p> <code>has_next_step</code> <code>bool</code> <p>Checks if this node has any downstream connections.</p> <code>hash</code> <code>str</code> <p>Gets the cached hash for the node, calculating it if it doesn't exist.</p> <code>is_correct</code> <code>bool</code> <p>Checks if the node's input connections satisfy its template requirements.</p> <code>is_setup</code> <code>bool</code> <p>Checks if the node has been properly configured and is ready for execution.</p> <code>is_start</code> <code>bool</code> <p>Determines if the node is a starting node in the flow.</p> <code>left_input</code> <code>Optional[FlowNode]</code> <p>Gets the node connected to the left input port.</p> <code>main_input</code> <code>List[FlowNode]</code> <p>Gets the list of nodes connected to the main input port(s).</p> <code>name</code> <code>str</code> <p>Gets the name of the node.</p> <code>node_id</code> <code>Union[str, int]</code> <p>Gets the unique identifier of the node.</p> <code>number_of_leads_to_nodes</code> <code>int | None</code> <p>Counts the number of downstream node connections.</p> <code>right_input</code> <code>Optional[FlowNode]</code> <p>Gets the node connected to the right input port.</p> <code>schema</code> <code>List[FlowfileColumn]</code> <p>Gets the definitive output schema of the node.</p> <code>schema_callback</code> <code>SingleExecutionFuture</code> <p>Gets the schema callback function, creating one if it doesn't exist.</p> <code>setting_input</code> <code>Any</code> <p>Gets the node's specific configuration settings.</p> <code>singular_input</code> <code>bool</code> <p>Checks if the node template specifies exactly one input.</p> <code>singular_main_input</code> <code>FlowNode</code> <p>Gets the input node, assuming it is a single-input type.</p> <code>state_needs_reset</code> <code>bool</code> <p>Checks if the node's state needs to be reset.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>class FlowNode:\n    \"\"\"Represents a single node in a data flow graph.\n\n    This class manages the node's state, its data processing function,\n    and its connections to other nodes within the graph.\n    \"\"\"\n    parent_uuid: str\n    node_type: str\n    node_template: node_store.NodeTemplate\n    node_default: schemas.NodeDefault\n    node_schema: NodeSchemaInformation\n    node_inputs: NodeStepInputs\n    node_stats: NodeStepStats\n    node_settings: NodeStepSettings\n    results: NodeResults\n    node_information: Optional[schemas.NodeInformation] = None\n    leads_to_nodes: List[\"FlowNode\"] = []  # list with target flows, after execution the step will trigger those step(s)\n    user_provided_schema_callback: Optional[Callable] = None  # user provided callback function for schema calculation\n    _setting_input: Any = None\n    _hash: Optional[str] = None  # host this for caching results\n    _function: Callable = None  # the function that needs to be executed when triggered\n    _name: str = None  # name of the node, used for display\n    _schema_callback: Optional[SingleExecutionFuture] = None  # Function that calculates the schema without executing\n    _state_needs_reset: bool = False\n    _fetch_cached_df: Optional[ExternalDfFetcher | ExternalDatabaseFetcher | ExternalDatabaseWriter | ExternalCloudWriter] = None\n    _cache_progress: Optional[ExternalDfFetcher | ExternalDatabaseFetcher | ExternalDatabaseWriter | ExternalCloudWriter] = None\n\n    def __init__(self, node_id: Union[str, int], function: Callable,\n                 parent_uuid: str,\n                 setting_input: Any,\n                 name: str,\n                 node_type: str,\n                 input_columns: List[str] = None,\n                 output_schema: List[FlowfileColumn] = None,\n                 drop_columns: List[str] = None,\n                 renew_schema: bool = True,\n                 pos_x: float = 0,\n                 pos_y: float = 0,\n                 schema_callback: Callable = None,\n                 ):\n        \"\"\"Initializes a FlowNode instance.\n\n        Args:\n            node_id: Unique identifier for the node.\n            function: The core data processing function for the node.\n            parent_uuid: The UUID of the parent flow.\n            setting_input: The configuration/settings object for the node.\n            name: The name of the node.\n            node_type: The type identifier of the node (e.g., 'join', 'filter').\n            input_columns: List of column names expected as input.\n            output_schema: The schema of the columns to be added.\n            drop_columns: List of column names to be dropped.\n            renew_schema: Flag to indicate if the schema should be renewed.\n            pos_x: The x-coordinate on the canvas.\n            pos_y: The y-coordinate on the canvas.\n            schema_callback: A custom function to calculate the output schema.\n        \"\"\"\n        self._name = None\n        self.parent_uuid = parent_uuid\n        self.post_init()\n        self.active = True\n        self.node_information.id = node_id\n        self.node_type = node_type\n        self.node_settings.renew_schema = renew_schema\n        self.update_node(function=function,\n                         input_columns=input_columns,\n                         output_schema=output_schema,\n                         drop_columns=drop_columns,\n                         setting_input=setting_input,\n                         name=name,\n                         pos_x=pos_x,\n                         pos_y=pos_y,\n                         schema_callback=schema_callback,\n                         )\n\n    def post_init(self):\n        \"\"\"Initializes or resets the node's attributes to their default states.\"\"\"\n        self.node_inputs = NodeStepInputs()\n        self.node_stats = NodeStepStats()\n        self.node_settings = NodeStepSettings()\n        self.node_schema = NodeSchemaInformation()\n        self.results = NodeResults()\n        self.node_information = schemas.NodeInformation()\n        self.leads_to_nodes = []\n        self._setting_input = None\n        self._cache_progress = None\n        self._schema_callback = None\n        self._state_needs_reset = False\n\n    @property\n    def state_needs_reset(self) -&gt; bool:\n        \"\"\"Checks if the node's state needs to be reset.\n\n        Returns:\n            True if a reset is required, False otherwise.\n        \"\"\"\n        return self._state_needs_reset\n\n    @state_needs_reset.setter\n    def state_needs_reset(self, v: bool):\n        \"\"\"Sets the flag indicating that the node's state needs to be reset.\n\n        Args:\n            v: The boolean value to set.\n        \"\"\"\n        self._state_needs_reset = v\n\n    @staticmethod\n    def create_schema_callback_from_function(f: Callable) -&gt; Callable[[], List[FlowfileColumn]]:\n        \"\"\"Wraps a node's function to create a schema callback that extracts the schema.\n\n        Args:\n            f: The node's core function that returns a FlowDataEngine instance.\n\n        Returns:\n            A callable that, when executed, returns the output schema.\n        \"\"\"\n        def schema_callback() -&gt; List[FlowfileColumn]:\n            try:\n                logger.info('Executing the schema callback function based on the node function')\n                return f().schema\n            except Exception as e:\n                logger.warning(f'Error with the schema callback: {e}')\n                return []\n        return schema_callback\n\n    @property\n    def schema_callback(self) -&gt; SingleExecutionFuture:\n        \"\"\"Gets the schema callback function, creating one if it doesn't exist.\n\n        The callback is used for predicting the output schema without full execution.\n\n        Returns:\n            A SingleExecutionFuture instance wrapping the schema function.\n        \"\"\"\n        if self._schema_callback is None:\n            if self.user_provided_schema_callback is not None:\n                self.schema_callback = self.user_provided_schema_callback\n            elif self.is_start:\n                self.schema_callback = self.create_schema_callback_from_function(self._function)\n        return self._schema_callback\n\n    @schema_callback.setter\n    def schema_callback(self, f: Callable):\n        \"\"\"Sets the schema callback function for the node.\n\n        Args:\n            f: The function to be used for schema calculation.\n        \"\"\"\n        if f is None:\n            return\n\n        def error_callback(e: Exception) -&gt; List:\n            logger.warning(e)\n\n            self.node_settings.setup_errors = True\n            return []\n\n        self._schema_callback = SingleExecutionFuture(f, error_callback)\n\n    @property\n    def is_start(self) -&gt; bool:\n        \"\"\"Determines if the node is a starting node in the flow.\n\n        A starting node requires no inputs.\n\n        Returns:\n            True if the node is a start node, False otherwise.\n        \"\"\"\n        return not self.has_input and self.node_template.input == 0\n\n    def get_input_type(self, node_id: int) -&gt; List:\n        \"\"\"Gets the type of connection ('main', 'left', 'right') for a given input node ID.\n\n        Args:\n            node_id: The ID of the input node.\n\n        Returns:\n            A list of connection types for that node ID.\n        \"\"\"\n        relation_type = []\n        if node_id in [n.node_id for n in self.node_inputs.main_inputs]:\n            relation_type.append('main')\n        if self.node_inputs.left_input is not None and node_id == self.node_inputs.left_input.node_id:\n            relation_type.append('left')\n        if self.node_inputs.right_input is not None and node_id == self.node_inputs.right_input.node_id:\n            relation_type.append('right')\n        return list(set(relation_type))\n\n    def update_node(self,\n                    function: Callable,\n                    input_columns: List[str] = None,\n                    output_schema: List[FlowfileColumn] = None,\n                    drop_columns: List[str] = None,\n                    name: str = None,\n                    setting_input: Any = None,\n                    pos_x: float = 0,\n                    pos_y: float = 0,\n                    schema_callback: Callable = None,\n                    ):\n        \"\"\"Updates the properties of the node.\n\n        This is called during initialization and when settings are changed.\n\n        Args:\n            function: The new core data processing function.\n            input_columns: The new list of input columns.\n            output_schema: The new schema of added columns.\n            drop_columns: The new list of dropped columns.\n            name: The new name for the node.\n            setting_input: The new settings object.\n            pos_x: The new x-coordinate.\n            pos_y: The new y-coordinate.\n            schema_callback: The new custom schema callback function.\n        \"\"\"\n        self.user_provided_schema_callback = schema_callback\n        self.node_information.y_position = int(pos_y)\n        self.node_information.x_position = int(pos_x)\n        self.node_information.setting_input = setting_input\n        self.name = self.node_type if name is None else name\n        self._function = function\n\n        self.node_schema.input_columns = [] if input_columns is None else input_columns\n        self.node_schema.output_columns = [] if output_schema is None else output_schema\n        self.node_schema.drop_columns = [] if drop_columns is None else drop_columns\n        self.node_settings.renew_schema = True\n        if hasattr(setting_input, 'cache_results'):\n            self.node_settings.cache_results = setting_input.cache_results\n\n        self.results.errors = None\n        self.add_lead_to_in_depend_source()\n        _ = self.hash\n        self.node_template = node_store.node_dict.get(self.node_type)\n        if self.node_template is None:\n            raise Exception(f'Node template {self.node_type} not found')\n        self.node_default = node_store.node_defaults.get(self.node_type)\n        self.setting_input = setting_input  # wait until the end so that the hash is calculated correctly\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Gets the name of the node.\n\n        Returns:\n            The node's name.\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, name: str):\n        \"\"\"Sets the name of the node.\n\n        Args:\n            name: The new name.\n        \"\"\"\n        self._name = name\n        self.__name__ = name\n\n    @property\n    def setting_input(self) -&gt; Any:\n        \"\"\"Gets the node's specific configuration settings.\n\n        Returns:\n            The settings object.\n        \"\"\"\n        return self._setting_input\n\n    @setting_input.setter\n    def setting_input(self, setting_input: Any):\n        \"\"\"Sets the node's configuration and triggers a reset if necessary.\n\n        Args:\n            setting_input: The new settings object.\n        \"\"\"\n        is_manual_input = (self.node_type == 'manual_input' and\n                           isinstance(setting_input, input_schema.NodeManualInput) and\n                           isinstance(self._setting_input, input_schema.NodeManualInput)\n                           )\n        if is_manual_input:\n            _ = self.hash\n        self._setting_input = setting_input\n        self.set_node_information()\n        if is_manual_input:\n            if self.hash != self.calculate_hash(setting_input) or not self.node_stats.has_run_with_current_setup:\n                self.function = FlowDataEngine(setting_input.raw_data_format)\n                self.reset()\n                self.get_predicted_schema()\n        elif self._setting_input is not None:\n            self.reset()\n\n    @property\n    def node_id(self) -&gt; Union[str, int]:\n        \"\"\"Gets the unique identifier of the node.\n\n        Returns:\n            The node's ID.\n        \"\"\"\n        return self.node_information.id\n\n    @property\n    def left_input(self) -&gt; Optional[\"FlowNode\"]:\n        \"\"\"Gets the node connected to the left input port.\n\n        Returns:\n            The left input FlowNode, or None.\n        \"\"\"\n        return self.node_inputs.left_input\n\n    @property\n    def right_input(self) -&gt; Optional[\"FlowNode\"]:\n        \"\"\"Gets the node connected to the right input port.\n\n        Returns:\n            The right input FlowNode, or None.\n        \"\"\"\n        return self.node_inputs.right_input\n\n    @property\n    def main_input(self) -&gt; List[\"FlowNode\"]:\n        \"\"\"Gets the list of nodes connected to the main input port(s).\n\n        Returns:\n            A list of main input FlowNodes.\n        \"\"\"\n        return self.node_inputs.main_inputs\n\n    @property\n    def is_correct(self) -&gt; bool:\n        \"\"\"Checks if the node's input connections satisfy its template requirements.\n\n        Returns:\n            True if connections are valid, False otherwise.\n        \"\"\"\n        if isinstance(self.setting_input, input_schema.NodePromise):\n            return False\n        return (self.node_template.input == len(self.node_inputs.get_all_inputs()) or\n                (self.node_template.multi and len(self.node_inputs.get_all_inputs()) &gt; 0) or\n                (self.node_template.multi and self.node_template.can_be_start))\n\n    def set_node_information(self):\n        \"\"\"Populates the `node_information` attribute with the current state.\n\n        This includes the node's connections, settings, and position.\n        \"\"\"\n        logger.info('setting node information')\n        node_information = self.node_information\n        node_information.left_input_id = self.node_inputs.left_input.node_id if self.left_input else None\n        node_information.right_input_id = self.node_inputs.right_input.node_id if self.right_input else None\n        node_information.input_ids = [mi.node_id for mi in\n                                      self.node_inputs.main_inputs] if self.node_inputs.main_inputs is not None else None\n        node_information.setting_input = self.setting_input\n        node_information.outputs = [n.node_id for n in self.leads_to_nodes]\n        node_information.is_setup = self.is_setup\n        node_information.x_position = self.setting_input.pos_x\n        node_information.y_position = self.setting_input.pos_y\n        node_information.type = self.node_type\n\n    def get_node_information(self) -&gt; schemas.NodeInformation:\n        \"\"\"Updates and returns the node's information object.\n\n        Returns:\n            The `NodeInformation` object for this node.\n        \"\"\"\n        self.set_node_information()\n        return self.node_information\n\n    @property\n    def function(self) -&gt; Callable:\n        \"\"\"Gets the core processing function of the node.\n\n        Returns:\n            The callable function.\n        \"\"\"\n        return self._function\n\n    @function.setter\n    def function(self, function: Callable):\n        \"\"\"Sets the core processing function of the node.\n\n        Args:\n            function: The new callable function.\n        \"\"\"\n        self._function = function\n\n    @property\n    def all_inputs(self) -&gt; List[\"FlowNode\"]:\n        \"\"\"Gets a list of all nodes connected to any input port.\n\n        Returns:\n            A list of all input FlowNodes.\n        \"\"\"\n        return self.node_inputs.get_all_inputs()\n\n    def calculate_hash(self, setting_input: Any) -&gt; str:\n        \"\"\"Calculates a hash based on settings and input node hashes.\n\n        Args:\n            setting_input: The node's settings object to be included in the hash.\n\n        Returns:\n            A string hash value.\n        \"\"\"\n        depends_on_hashes = [_node.hash for _node in self.all_inputs]\n        node_data_hash = get_hash(setting_input)\n        return get_hash(depends_on_hashes + [node_data_hash, self.parent_uuid])\n\n    @property\n    def hash(self) -&gt; str:\n        \"\"\"Gets the cached hash for the node, calculating it if it doesn't exist.\n\n        Returns:\n            The string hash value.\n        \"\"\"\n        if not self._hash:\n            self._hash = self.calculate_hash(self.setting_input)\n        return self._hash\n\n    def add_node_connection(self, from_node: \"FlowNode\",\n                            insert_type: Literal['main', 'left', 'right'] = 'main') -&gt; None:\n        \"\"\"Adds a connection from a source node to this node.\n\n        Args:\n            from_node: The node to connect from.\n            insert_type: The type of input to connect to ('main', 'left', 'right').\n\n        Raises:\n            Exception: If the insert_type is invalid.\n        \"\"\"\n        from_node.leads_to_nodes.append(self)\n        if insert_type == 'main':\n            if self.node_template.input &lt;= 2 or self.node_inputs.main_inputs is None:\n                self.node_inputs.main_inputs = [from_node]\n            else:\n                self.node_inputs.main_inputs.append(from_node)\n        elif insert_type == 'right':\n            self.node_inputs.right_input = from_node\n        elif insert_type == 'left':\n            self.node_inputs.left_input = from_node\n        else:\n            raise Exception('Cannot find the connection')\n        if self.setting_input.is_setup:\n            if hasattr(self.setting_input, 'depending_on_id') and insert_type == 'main':\n                self.setting_input.depending_on_id = from_node.node_id\n        self.reset()\n        from_node.reset()\n\n    def evaluate_nodes(self, deep: bool = False) -&gt; None:\n        \"\"\"Triggers a state reset for all directly connected downstream nodes.\n\n        Args:\n            deep: If True, the reset propagates recursively through the entire downstream graph.\n        \"\"\"\n        for node in self.leads_to_nodes:\n            self.print(f'resetting node: {node.node_id}')\n            node.reset(deep)\n\n    def get_flow_file_column_schema(self, col_name: str) -&gt; FlowfileColumn | None:\n        \"\"\"Retrieves the schema for a specific column from the output schema.\n\n        Args:\n            col_name: The name of the column.\n\n        Returns:\n            The FlowfileColumn object for that column, or None if not found.\n        \"\"\"\n        for s in self.schema:\n            if s.column_name == col_name:\n                return s\n\n    def get_predicted_schema(self, force: bool = False) -&gt; List[FlowfileColumn] | None:\n        \"\"\"Predicts the output schema of the node without full execution.\n\n        It uses the schema_callback or infers from predicted data.\n\n        Args:\n            force: If True, forces recalculation even if a predicted schema exists.\n\n        Returns:\n            A list of FlowfileColumn objects representing the predicted schema.\n        \"\"\"\n        if self.node_schema.predicted_schema and not force:\n            return self.node_schema.predicted_schema\n        if self.schema_callback is not None and (self.node_schema.predicted_schema is None or force):\n            self.print('Getting the data from a schema callback')\n            if force:\n                # Force the schema callback to reset, so that it will be executed again\n                self.schema_callback.reset()\n            schema = self.schema_callback()\n            if schema is not None and len(schema) &gt; 0:\n                self.print('Calculating the schema based on the schema callback')\n                self.node_schema.predicted_schema = schema\n                return self.node_schema.predicted_schema\n        predicted_data = self._predicted_data_getter()\n        if predicted_data is not None and predicted_data.schema is not None:\n            self.print('Calculating the schema based on the predicted resulting data')\n            self.node_schema.predicted_schema = self._predicted_data_getter().schema\n        return self.node_schema.predicted_schema\n\n    @property\n    def is_setup(self) -&gt; bool:\n        \"\"\"Checks if the node has been properly configured and is ready for execution.\n\n        Returns:\n            True if the node is set up, False otherwise.\n        \"\"\"\n        if not self.node_information.is_setup:\n            if self.function.__name__ != 'placeholder':\n                self.node_information.is_setup = True\n                self.setting_input.is_setup = True\n        return self.node_information.is_setup\n\n    def print(self, v: Any):\n        \"\"\"Helper method to log messages with node context.\n\n        Args:\n            v: The message or value to log.\n        \"\"\"\n        logger.info(f'{self.node_type}, node_id: {self.node_id}: {v}')\n\n    def get_resulting_data(self) -&gt; FlowDataEngine | None:\n        \"\"\"Executes the node's function to produce the actual output data.\n\n        Handles both regular functions and external data sources.\n\n        Returns:\n            A FlowDataEngine instance containing the result, or None on error.\n\n        Raises:\n            Exception: Propagates exceptions from the node's function execution.\n        \"\"\"\n        if self.is_setup:\n            if self.results.resulting_data is None and self.results.errors is None:\n                self.print('getting resulting data')\n                try:\n                    if isinstance(self.function, FlowDataEngine):\n                        fl: FlowDataEngine = self.function\n                    elif self.node_type == 'external_source':\n                        fl: FlowDataEngine = self.function()\n                        fl.collect_external()\n                        self.node_settings.streamable = False\n                    else:\n                        try:\n                            fl = self._function(*[v.get_resulting_data() for v in self.all_inputs])\n                        except Exception as e:\n                            raise e\n                    fl.set_streamable(self.node_settings.streamable)\n                    self.results.resulting_data = fl\n                    self.node_schema.result_schema = fl.schema\n                except Exception as e:\n                    self.results.resulting_data = FlowDataEngine()\n                    self.results.errors = str(e)\n                    self.node_stats.has_run_with_current_setup = False\n                    self.node_stats.has_completed_last_run = False\n                    raise e\n            return self.results.resulting_data\n\n    def _predicted_data_getter(self) -&gt; FlowDataEngine | None:\n        \"\"\"Internal helper to get a predicted data result.\n\n        This calls the function with predicted data from input nodes.\n\n        Returns:\n            A FlowDataEngine instance with predicted data, or an empty one on error.\n        \"\"\"\n        try:\n            fl = self._function(*[v.get_predicted_resulting_data() for v in self.all_inputs])\n            return fl\n        except ValueError as e:\n            if str(e) == \"generator already executing\":\n                logger.info('Generator already executing, waiting for the result')\n                sleep(1)\n                return self._predicted_data_getter()\n            fl = FlowDataEngine()\n            return fl\n\n        except Exception as e:\n            logger.warning('there was an issue with the function, returning an empty Flowfile')\n            logger.warning(e)\n\n    def get_predicted_resulting_data(self) -&gt; FlowDataEngine:\n        \"\"\"Creates a `FlowDataEngine` instance based on the predicted schema.\n\n        This avoids executing the node's full logic.\n\n        Returns:\n            A FlowDataEngine instance with a schema but no data.\n        \"\"\"\n        if self.needs_run(False) and self.schema_callback is not None or self.node_schema.result_schema is not None:\n            self.print('Getting data based on the schema')\n\n            _s = self.schema_callback() if self.node_schema.result_schema is None else self.node_schema.result_schema\n            return FlowDataEngine.create_from_schema(_s)\n        else:\n            if isinstance(self.function, FlowDataEngine):\n                fl = self.function\n            else:\n                fl = FlowDataEngine.create_from_schema(self.get_predicted_schema())\n            return fl\n\n    def add_lead_to_in_depend_source(self):\n        \"\"\"Ensures this node is registered in the `leads_to_nodes` list of its inputs.\"\"\"\n        for input_node in self.all_inputs:\n            if self.node_id not in [n.node_id for n in input_node.leads_to_nodes]:\n                input_node.leads_to_nodes.append(self)\n\n    def get_all_dependent_nodes(self) -&gt; Generator[\"FlowNode\", None, None]:\n        \"\"\"Yields all downstream nodes recursively.\n\n        Returns:\n            A generator of all dependent FlowNode objects.\n        \"\"\"\n        for node in self.leads_to_nodes:\n            yield node\n            for n in node.get_all_dependent_nodes():\n                yield n\n\n    def get_all_dependent_node_ids(self) -&gt; Generator[int, None, None]:\n        \"\"\"Yields the IDs of all downstream nodes recursively.\n\n        Returns:\n            A generator of all dependent node IDs.\n        \"\"\"\n        for node in self.leads_to_nodes:\n            yield node.node_id\n            for n in node.get_all_dependent_node_ids():\n                yield n\n\n    @property\n    def schema(self) -&gt; List[FlowfileColumn]:\n        \"\"\"Gets the definitive output schema of the node.\n\n        If not already run, it falls back to the predicted schema.\n\n        Returns:\n            A list of FlowfileColumn objects.\n        \"\"\"\n        try:\n            if self.is_setup and self.results.errors is None:\n                if self.node_schema.result_schema is not None and len(self.node_schema.result_schema) &gt; 0:\n                    return self.node_schema.result_schema\n                elif self.node_type == 'output':\n                    if len(self.node_inputs.main_inputs) &gt; 0:\n                        self.node_schema.result_schema = self.node_inputs.main_inputs[0].schema\n                else:\n                    self.node_schema.result_schema = self.get_predicted_schema()\n                return self.node_schema.result_schema\n            else:\n                return []\n        except Exception as e:\n            logger.error(e)\n            return []\n\n    def remove_cache(self):\n        \"\"\"Removes cached results for this node.\n\n        Note: Currently not fully implemented.\n        \"\"\"\n\n        if results_exists(self.hash):\n            logger.warning('Not implemented')\n            clear_task_from_worker(self.hash)\n\n    def needs_run(self, performance_mode: bool, node_logger: NodeLogger = None,\n                  execution_location: schemas.ExecutionLocationsLiteral = \"remote\") -&gt; bool:\n        \"\"\"Determines if the node needs to be executed.\n\n        The decision is based on its run state, caching settings, and execution mode.\n\n        Args:\n            performance_mode: True if the flow is in performance mode.\n            node_logger: The logger instance for this node.\n            execution_location: The target execution location.\n\n        Returns:\n            True if the node should be run, False otherwise.\n        \"\"\"\n        if execution_location == \"local\":\n            return False\n\n        flow_logger = logger if node_logger is None else node_logger\n        cache_result_exists = results_exists(self.hash)\n        if not self.node_stats.has_run_with_current_setup:\n            flow_logger.info('Node has not run, needs to run')\n            return True\n        if self.node_settings.cache_results and cache_result_exists:\n            return False\n        elif self.node_settings.cache_results and not cache_result_exists:\n            return True\n        elif not performance_mode and cache_result_exists:\n            return False\n        else:\n            return True\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"Makes the node instance callable, acting as an alias for execute_node.\"\"\"\n        self.execute_node(*args, **kwargs)\n\n    def execute_full_local(self, performance_mode: bool = False) -&gt; None:\n        \"\"\"Executes the node's logic locally, including example data generation.\n\n        Args:\n            performance_mode: If True, skips generating example data.\n\n        Raises:\n            Exception: Propagates exceptions from the execution.\n        \"\"\"\n        self.clear_table_example()\n\n        def example_data_generator():\n            example_data = None\n\n            def get_example_data():\n                nonlocal example_data\n                if example_data is None:\n                    example_data = resulting_data.get_sample(100).to_arrow()\n                return example_data\n            return get_example_data\n        resulting_data = self.get_resulting_data()\n\n        if not performance_mode:\n            self.node_stats.has_run_with_current_setup = True\n            self.results.example_data_generator = example_data_generator()\n            self.node_schema.result_schema = self.results.resulting_data.schema\n            self.node_stats.has_completed_last_run = True\n\n    def execute_local(self, flow_id: int, performance_mode: bool = False):\n        \"\"\"Executes the node's logic locally.\n\n        Args:\n            flow_id: The ID of the parent flow.\n            performance_mode: If True, skips generating example data.\n\n        Raises:\n            Exception: Propagates exceptions from the execution.\n        \"\"\"\n        try:\n            resulting_data = self.get_resulting_data()\n            if not performance_mode:\n                external_sampler = ExternalSampler(lf=resulting_data.data_frame, file_ref=self.hash,\n                                                   wait_on_completion=True, node_id=self.node_id, flow_id=flow_id)\n                self.store_example_data_generator(external_sampler)\n                if self.results.errors is None and not self.node_stats.is_canceled:\n                    self.node_stats.has_run_with_current_setup = True\n            self.node_schema.result_schema = resulting_data.schema\n\n        except Exception as e:\n            logger.warning(f\"Error with step {self.__name__}\")\n            logger.error(str(e))\n            self.results.errors = str(e)\n            self.node_stats.has_run_with_current_setup = False\n            self.node_stats.has_completed_last_run = False\n            raise e\n\n        if self.node_stats.has_run_with_current_setup:\n            for step in self.leads_to_nodes:\n                if not self.node_settings.streamable:\n                    step.node_settings.streamable = self.node_settings.streamable\n\n    def execute_remote(self, performance_mode: bool = False, node_logger: NodeLogger = None):\n        \"\"\"Executes the node's logic remotely or handles cached results.\n\n        Args:\n            performance_mode: If True, skips generating example data.\n            node_logger: The logger for this node execution.\n\n        Raises:\n            Exception: If the node_logger is not provided or if execution fails.\n        \"\"\"\n        if node_logger is None:\n            raise Exception('Node logger is not defined')\n        if self.node_settings.cache_results and results_exists(self.hash):\n            try:\n                self.results.resulting_data = get_external_df_result(self.hash)\n                self._cache_progress = None\n                return\n            except Exception as e:\n                node_logger.warning('Failed to read the cache, rerunning the code')\n        if self.node_type == 'output':\n            self.results.resulting_data = self.get_resulting_data()\n            self.node_stats.has_run_with_current_setup = True\n            return\n        try:\n            self.get_resulting_data()\n        except Exception as e:\n            self.results.errors = 'Error with creating the lazy frame, most likely due to invalid graph'\n            raise e\n        if not performance_mode:\n            external_df_fetcher = ExternalDfFetcher(lf=self.get_resulting_data().data_frame,\n                                                    file_ref=self.hash, wait_on_completion=False,\n                                                    flow_id=node_logger.flow_id,\n                                                    node_id=self.node_id)\n            self._fetch_cached_df = external_df_fetcher\n            try:\n                lf = external_df_fetcher.get_result()\n                self.results.resulting_data = FlowDataEngine(\n                    lf, number_of_records=ExternalDfFetcher(lf=lf, operation_type='calculate_number_of_records',\n                                                            flow_id=node_logger.flow_id, node_id=self.node_id).result\n                )\n                if not performance_mode:\n                    self.store_example_data_generator(external_df_fetcher)\n                    self.node_stats.has_run_with_current_setup = True\n\n            except Exception as e:\n                node_logger.error('Error with external process')\n                if external_df_fetcher.error_code == -1:\n                    try:\n                        self.results.resulting_data = self.get_resulting_data()\n                        self.results.warnings = ('Error with external process (unknown error), '\n                                                 'likely the process was killed by the server because of memory constraints, '\n                                                 'continue with the process. '\n                                                 'We cannot display example data...')\n                    except Exception as e:\n                        self.results.errors = str(e)\n                        raise e\n                elif external_df_fetcher.error_description is None:\n                    self.results.errors = str(e)\n                    raise e\n                else:\n                    self.results.errors = external_df_fetcher.error_description\n                    raise Exception(external_df_fetcher.error_description)\n            finally:\n                self._fetch_cached_df = None\n\n    def prepare_before_run(self):\n        \"\"\"Resets results and errors before a new execution.\"\"\"\n\n        self.results.errors = None\n        self.results.resulting_data = None\n        self.results.example_data = None\n\n    def cancel(self):\n        \"\"\"Cancels an ongoing external process if one is running.\"\"\"\n\n        if self._fetch_cached_df is not None:\n            self._fetch_cached_df.cancel()\n            self.node_stats.is_canceled = True\n        else:\n            logger.warning('No external process to cancel')\n        self.node_stats.is_canceled = True\n\n    def execute_node(self, run_location: schemas.ExecutionLocationsLiteral,\n                     reset_cache: bool = False,\n                     performance_mode: bool = False,\n                     retry: bool = True,\n                     node_logger: NodeLogger = None,\n                     optimize_for_downstream: bool = True):\n        \"\"\"Orchestrates the execution, handling location, caching, and retries.\n\n        Args:\n            run_location: The location for execution ('local', 'remote').\n            reset_cache: If True, forces removal of any existing cache.\n            performance_mode: If True, optimizes for speed over diagnostics.\n            retry: If True, allows retrying execution on recoverable errors.\n            node_logger: The logger for this node execution.\n            optimize_for_downstream: If true, operations that shuffle the order of rows are fully cached and provided as\n            input to downstream steps\n\n        Raises:\n            Exception: If the node_logger is not defined.\n        \"\"\"\n        if node_logger is None:\n            raise Exception('Flow logger is not defined')\n        #  TODO: Simplify which route is being picked there are many duplicate checks\n\n        if reset_cache:\n            self.remove_cache()\n            self.node_stats.has_run_with_current_setup = False\n            self.node_stats.has_completed_last_run = False\n\n        if self.is_setup:\n            node_logger.info(f'Starting to run {self.__name__}')\n            if (self.needs_run(performance_mode, node_logger, run_location) or self.node_template.node_group == \"output\"\n                    and not (run_location == 'local')):\n                self.clear_table_example()\n                self.prepare_before_run()\n                self.reset()\n                try:\n                    if (((run_location == 'remote' or\n                         (self.node_default.transform_type == 'wide' and optimize_for_downstream) and\n                         not run_location == 'local'))\n                            or self.node_settings.cache_results):\n                        node_logger.info('Running the node remotely')\n                        if self.node_settings.cache_results:\n                            performance_mode = False\n                        self.execute_remote(performance_mode=(performance_mode if not self.node_settings.cache_results\n                                                              else False),\n                                            node_logger=node_logger\n                                            )\n                    else:\n                        node_logger.info('Running the node locally')\n                        self.execute_local(performance_mode=performance_mode, flow_id=node_logger.flow_id)\n                except Exception as e:\n                    if 'No such file or directory (os error' in str(e) and retry:\n                        logger.warning('Error with the input node, starting to rerun the input node...')\n                        all_inputs: List[FlowNode] = self.node_inputs.get_all_inputs()\n                        for node_input in all_inputs:\n                            node_input.execute_node(run_location=run_location,\n                                                    performance_mode=performance_mode, retry=True,\n                                                    reset_cache=True,\n                                                    node_logger=node_logger)\n                        self.execute_node(run_location=run_location,\n                                          performance_mode=performance_mode, retry=False,\n                                          node_logger=node_logger)\n                    else:\n                        self.results.errors = str(e)\n                        if \"Connection refused\" in str(e) and \"/submit_query/\" in str(e):\n                            node_logger.warning(\"There was an issue connecting to the remote worker, \"\n                                                \"ensure the worker process is running, \"\n                                                \"or change the settings to, so it executes locally\")\n                            node_logger.error(\"Could not execute in the remote worker. (Re)start the worker service, or change settings to local settings.\")\n                        else:\n                            node_logger.error(f'Error with running the node: {e}')\n            elif ((run_location == 'local') and\n                  (not self.node_stats.has_run_with_current_setup or self.node_template.node_group == \"output\")):\n                try:\n                    node_logger.info('Executing fully locally')\n                    self.execute_full_local(performance_mode)\n                except Exception as e:\n                    self.results.errors = str(e)\n                    node_logger.error(f'Error with running the node: {e}')\n                    self.node_stats.error = str(e)\n                    self.node_stats.has_completed_last_run = False\n\n            else:\n                node_logger.info('Node has already run, not running the node')\n        else:\n            node_logger.warning(f'Node {self.__name__} is not setup, cannot run the node')\n\n    def store_example_data_generator(self, external_df_fetcher: ExternalDfFetcher | ExternalSampler):\n        \"\"\"Stores a generator function for fetching a sample of the result data.\n\n        Args:\n            external_df_fetcher: The process that generated the sample data.\n        \"\"\"\n        if external_df_fetcher.status is not None:\n            file_ref = external_df_fetcher.status.file_ref\n            self.results.example_data_path = file_ref\n            self.results.example_data_generator = get_read_top_n(file_path=file_ref, n=100)\n        else:\n            logger.error('Could not get the sample data, the external process is not ready')\n\n    def needs_reset(self) -&gt; bool:\n        \"\"\"Checks if the node's hash has changed, indicating an outdated state.\n\n        Returns:\n            True if the calculated hash differs from the stored hash.\n        \"\"\"\n        return self._hash != self.calculate_hash(self.setting_input)\n\n    def reset(self, deep: bool = False):\n        \"\"\"Resets the node's execution state and schema information.\n\n        This also triggers a reset on all downstream nodes.\n\n        Args:\n            deep: If True, forces a reset even if the hash hasn't changed.\n        \"\"\"\n        needs_reset = self.needs_reset() or deep\n        if needs_reset:\n            logger.info(f'{self.node_id}: Node needs reset')\n            self.node_stats.has_run_with_current_setup = False\n            self.results.reset()\n            self.node_schema.result_schema = None\n            self.node_schema.predicted_schema = None\n            self._hash = None\n            self.node_information.is_setup = None\n            self.results.errors = None\n            if self.is_correct:\n                self._schema_callback = None  # Ensure the schema callback is reset\n                if self.schema_callback:\n                    logger.info(f'{self.node_id}: Resetting the schema callback')\n                    self.schema_callback.start()\n            self.evaluate_nodes()\n            _ = self.hash  # Recalculate the hash after reset\n\n    def delete_lead_to_node(self, node_id: int) -&gt; bool:\n        \"\"\"Removes a connection to a specific downstream node.\n\n        Args:\n            node_id: The ID of the downstream node to disconnect.\n\n        Returns:\n            True if the connection was found and removed, False otherwise.\n        \"\"\"\n        logger.info(f'Deleting lead to node: {node_id}')\n        for i, lead_to_node in enumerate(self.leads_to_nodes):\n            logger.info(f'Checking lead to node: {lead_to_node.node_id}')\n            if lead_to_node.node_id == node_id:\n                logger.info(f'Found the node to delete: {node_id}')\n                self.leads_to_nodes.pop(i)\n                return True\n        return False\n\n    def delete_input_node(self, node_id: int, connection_type: input_schema.InputConnectionClass = 'input-0',\n                          complete: bool = False) -&gt; bool:\n        \"\"\"Removes a connection from a specific input node.\n\n        Args:\n            node_id: The ID of the input node to disconnect.\n            connection_type: The specific input handle (e.g., 'input-0', 'input-1').\n            complete: If True, tries to delete from all input types.\n\n        Returns:\n            True if a connection was found and removed, False otherwise.\n        \"\"\"\n        deleted: bool = False\n        if connection_type == 'input-0':\n            for i, node in enumerate(self.node_inputs.main_inputs):\n                if node.node_id == node_id:\n                    self.node_inputs.main_inputs.pop(i)\n                    deleted = True\n                    if not complete:\n                        continue\n        elif connection_type == 'input-1' or complete:\n            if self.node_inputs.right_input is not None and self.node_inputs.right_input.node_id == node_id:\n                self.node_inputs.right_input = None\n                deleted = True\n        elif connection_type == 'input-2' or complete:\n            if self.node_inputs.left_input is not None and self.node_inputs.right_input.node_id == node_id:\n                self.node_inputs.left_input = None\n                deleted = True\n        else:\n            logger.warning('Could not find the connection to delete...')\n        if deleted:\n            self.reset()\n        return deleted\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Provides a string representation of the FlowNode instance.\n\n        Returns:\n            A string showing the node's ID and type.\n        \"\"\"\n        return f\"Node id: {self.node_id} ({self.node_type})\"\n\n    def _get_readable_schema(self) -&gt; List[dict] | None:\n        \"\"\"Helper to get a simplified, dictionary representation of the output schema.\n\n        Returns:\n            A list of dictionaries, each with 'column_name' and 'data_type'.\n        \"\"\"\n        if self.is_setup:\n            output = []\n            for s in self.schema:\n                output.append(dict(column_name=s.column_name, data_type=s.data_type))\n            return output\n\n    def get_repr(self) -&gt; dict:\n        \"\"\"Gets a detailed dictionary representation of the node's state.\n\n        Returns:\n            A dictionary containing key information about the node.\n        \"\"\"\n        return dict(FlowNode=\n                    dict(node_id=self.node_id,\n                         step_name=self.__name__,\n                         output_columns=self.node_schema.output_columns,\n                         output_schema=self._get_readable_schema()))\n\n    @property\n    def number_of_leads_to_nodes(self) -&gt; int | None:\n        \"\"\"Counts the number of downstream node connections.\n\n        Returns:\n            The number of nodes this node leads to.\n        \"\"\"\n        if self.is_setup:\n            return len(self.leads_to_nodes)\n\n    @property\n    def has_next_step(self) -&gt; bool:\n        \"\"\"Checks if this node has any downstream connections.\n\n        Returns:\n            True if it has at least one downstream node.\n        \"\"\"\n        return len(self.leads_to_nodes) &gt; 0\n\n    @property\n    def has_input(self) -&gt; bool:\n        \"\"\"Checks if this node has any input connections.\n\n        Returns:\n            True if it has at least one input node.\n        \"\"\"\n        return len(self.all_inputs) &gt; 0\n\n    @property\n    def singular_input(self) -&gt; bool:\n        \"\"\"Checks if the node template specifies exactly one input.\n\n        Returns:\n            True if the node is a single-input type.\n        \"\"\"\n        return self.node_template.input == 1\n\n    @property\n    def singular_main_input(self) -&gt; \"FlowNode\":\n        \"\"\"Gets the input node, assuming it is a single-input type.\n\n        Returns:\n            The single input FlowNode, or None.\n        \"\"\"\n        if self.singular_input:\n            return self.all_inputs[0]\n\n    def clear_table_example(self) -&gt; None:\n        \"\"\"\n        Clear the table example in the results so that it clears the existing results\n        Returns:\n            None\n        \"\"\"\n\n        self.results.example_data = None\n        self.results.example_data_generator = None\n        self.results.example_data_path = None\n\n    def get_table_example(self, include_data: bool = False) -&gt; TableExample | None:\n        \"\"\"Generates a `TableExample` model summarizing the node's output.\n\n        This can optionally include a sample of the data.\n\n        Args:\n            include_data: If True, includes a data sample in the result.\n\n        Returns:\n            A `TableExample` object, or None if the node is not set up.\n        \"\"\"\n        self.print('Getting a table example')\n        if self.is_setup and include_data and self.node_stats.has_completed_last_run:\n\n            if self.node_template.node_group == 'output':\n                self.print('getting the table example')\n                return self.main_input[0].get_table_example(include_data)\n\n            logger.info('getting the table example since the node has run')\n            example_data_getter = self.results.example_data_generator\n            if example_data_getter is not None:\n                data = example_data_getter().to_pylist()\n                if data is None:\n                    data = []\n            else:\n                data = []\n            schema = [FileColumn.model_validate(c.get_column_repr()) for c in self.schema]\n            fl = self.get_resulting_data()\n            has_example_data = self.results.example_data_generator is not None\n\n            return TableExample(node_id=self.node_id,\n                                name=str(self.node_id), number_of_records=999,\n                                number_of_columns=fl.number_of_fields,\n                                table_schema=schema, columns=fl.columns, data=data,\n                                has_example_data=has_example_data,\n                                has_run_with_current_setup=self.node_stats.has_run_with_current_setup\n                                )\n        else:\n            logger.warning('getting the table example but the node has not run')\n            try:\n                schema = [FileColumn.model_validate(c.get_column_repr()) for c in self.schema]\n            except Exception as e:\n                logger.warning(e)\n                schema = []\n            columns = [s.name for s in schema]\n            return TableExample(node_id=self.node_id,\n                                name=str(self.node_id), number_of_records=0,\n                                number_of_columns=len(columns),\n                                table_schema=schema, columns=columns,\n                                data=[])\n\n    def get_node_data(self, flow_id: int, include_example: bool = False) -&gt; NodeData:\n        \"\"\"Gathers all necessary data for representing the node in the UI.\n\n        Args:\n            flow_id: The ID of the parent flow.\n            include_example: If True, includes data samples.\n\n        Returns:\n            A `NodeData` object.\n        \"\"\"\n        node = NodeData(flow_id=flow_id,\n                        node_id=self.node_id,\n                        has_run=self.node_stats.has_run_with_current_setup,\n                        setting_input=self.setting_input,\n                        flow_type=self.node_type)\n        if self.main_input:\n            node.main_input = self.main_input[0].get_table_example()\n        if self.left_input:\n            node.left_input = self.left_input.get_table_example()\n        if self.right_input:\n            node.right_input = self.right_input.get_table_example()\n        if self.is_setup:\n            node.main_output = self.get_table_example(include_example)\n        node = setting_generator.get_setting_generator(self.node_type)(node)\n\n        node = setting_updator.get_setting_updator(self.node_type)(node)\n        return node\n\n    def get_output_data(self) -&gt; TableExample:\n        \"\"\"Gets the full output data sample for this node.\n\n        Returns:\n            A `TableExample` object with data.\n        \"\"\"\n        return self.get_table_example(True)\n\n    def get_node_input(self) -&gt; schemas.NodeInput:\n        \"\"\"Creates a `NodeInput` schema object for representing this node in the UI.\n\n        Returns:\n            A `NodeInput` object.\n        \"\"\"\n        return schemas.NodeInput(pos_y=self.setting_input.pos_y,\n                                 pos_x=self.setting_input.pos_x,\n                                 id=self.node_id,\n                                 **self.node_template.__dict__)\n\n    def get_edge_input(self) -&gt; List[schemas.NodeEdge]:\n        \"\"\"Generates `NodeEdge` objects for all input connections to this node.\n\n        Returns:\n            A list of `NodeEdge` objects.\n        \"\"\"\n        edges = []\n        if self.node_inputs.main_inputs is not None:\n            for i, main_input in enumerate(self.node_inputs.main_inputs):\n                edges.append(schemas.NodeEdge(id=f'{main_input.node_id}-{self.node_id}-{i}',\n                                              source=main_input.node_id,\n                                              target=self.node_id,\n                                              sourceHandle='output-0',\n                                              targetHandle='input-0',\n                                              ))\n        if self.node_inputs.left_input is not None:\n            edges.append(schemas.NodeEdge(id=f'{self.node_inputs.left_input.node_id}-{self.node_id}-right',\n                                          source=self.node_inputs.left_input.node_id,\n                                          target=self.node_id,\n                                          sourceHandle='output-0',\n                                          targetHandle='input-2',\n                                          ))\n        if self.node_inputs.right_input is not None:\n            edges.append(schemas.NodeEdge(id=f'{self.node_inputs.right_input.node_id}-{self.node_id}-left',\n                                          source=self.node_inputs.right_input.node_id,\n                                          target=self.node_id,\n                                          sourceHandle='output-0',\n                                          targetHandle='input-1',\n                                          ))\n        return edges\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.all_inputs","title":"<code>all_inputs</code>  <code>property</code>","text":"<p>Gets a list of all nodes connected to any input port.</p> <p>Returns:</p> Type Description <code>List[FlowNode]</code> <p>A list of all input FlowNodes.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.function","title":"<code>function</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the core processing function of the node.</p> <p>Returns:</p> Type Description <code>Callable</code> <p>The callable function.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.has_input","title":"<code>has_input</code>  <code>property</code>","text":"<p>Checks if this node has any input connections.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if it has at least one input node.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.has_next_step","title":"<code>has_next_step</code>  <code>property</code>","text":"<p>Checks if this node has any downstream connections.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if it has at least one downstream node.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.hash","title":"<code>hash</code>  <code>property</code>","text":"<p>Gets the cached hash for the node, calculating it if it doesn't exist.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string hash value.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.is_correct","title":"<code>is_correct</code>  <code>property</code>","text":"<p>Checks if the node's input connections satisfy its template requirements.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if connections are valid, False otherwise.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.is_setup","title":"<code>is_setup</code>  <code>property</code>","text":"<p>Checks if the node has been properly configured and is ready for execution.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the node is set up, False otherwise.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.is_start","title":"<code>is_start</code>  <code>property</code>","text":"<p>Determines if the node is a starting node in the flow.</p> <p>A starting node requires no inputs.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the node is a start node, False otherwise.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.left_input","title":"<code>left_input</code>  <code>property</code>","text":"<p>Gets the node connected to the left input port.</p> <p>Returns:</p> Type Description <code>Optional[FlowNode]</code> <p>The left input FlowNode, or None.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.main_input","title":"<code>main_input</code>  <code>property</code>","text":"<p>Gets the list of nodes connected to the main input port(s).</p> <p>Returns:</p> Type Description <code>List[FlowNode]</code> <p>A list of main input FlowNodes.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.name","title":"<code>name</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the name of the node.</p> <p>Returns:</p> Type Description <code>str</code> <p>The node's name.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.node_id","title":"<code>node_id</code>  <code>property</code>","text":"<p>Gets the unique identifier of the node.</p> <p>Returns:</p> Type Description <code>Union[str, int]</code> <p>The node's ID.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.number_of_leads_to_nodes","title":"<code>number_of_leads_to_nodes</code>  <code>property</code>","text":"<p>Counts the number of downstream node connections.</p> <p>Returns:</p> Type Description <code>int | None</code> <p>The number of nodes this node leads to.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.right_input","title":"<code>right_input</code>  <code>property</code>","text":"<p>Gets the node connected to the right input port.</p> <p>Returns:</p> Type Description <code>Optional[FlowNode]</code> <p>The right input FlowNode, or None.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.schema","title":"<code>schema</code>  <code>property</code>","text":"<p>Gets the definitive output schema of the node.</p> <p>If not already run, it falls back to the predicted schema.</p> <p>Returns:</p> Type Description <code>List[FlowfileColumn]</code> <p>A list of FlowfileColumn objects.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.schema_callback","title":"<code>schema_callback</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the schema callback function, creating one if it doesn't exist.</p> <p>The callback is used for predicting the output schema without full execution.</p> <p>Returns:</p> Type Description <code>SingleExecutionFuture</code> <p>A SingleExecutionFuture instance wrapping the schema function.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.setting_input","title":"<code>setting_input</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the node's specific configuration settings.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The settings object.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.singular_input","title":"<code>singular_input</code>  <code>property</code>","text":"<p>Checks if the node template specifies exactly one input.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the node is a single-input type.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.singular_main_input","title":"<code>singular_main_input</code>  <code>property</code>","text":"<p>Gets the input node, assuming it is a single-input type.</p> <p>Returns:</p> Type Description <code>FlowNode</code> <p>The single input FlowNode, or None.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.state_needs_reset","title":"<code>state_needs_reset</code>  <code>property</code> <code>writable</code>","text":"<p>Checks if the node's state needs to be reset.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if a reset is required, False otherwise.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Makes the node instance callable, acting as an alias for execute_node.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def __call__(self, *args, **kwargs):\n    \"\"\"Makes the node instance callable, acting as an alias for execute_node.\"\"\"\n    self.execute_node(*args, **kwargs)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.__init__","title":"<code>__init__(node_id, function, parent_uuid, setting_input, name, node_type, input_columns=None, output_schema=None, drop_columns=None, renew_schema=True, pos_x=0, pos_y=0, schema_callback=None)</code>","text":"<p>Initializes a FlowNode instance.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>Union[str, int]</code> <p>Unique identifier for the node.</p> required <code>function</code> <code>Callable</code> <p>The core data processing function for the node.</p> required <code>parent_uuid</code> <code>str</code> <p>The UUID of the parent flow.</p> required <code>setting_input</code> <code>Any</code> <p>The configuration/settings object for the node.</p> required <code>name</code> <code>str</code> <p>The name of the node.</p> required <code>node_type</code> <code>str</code> <p>The type identifier of the node (e.g., 'join', 'filter').</p> required <code>input_columns</code> <code>List[str]</code> <p>List of column names expected as input.</p> <code>None</code> <code>output_schema</code> <code>List[FlowfileColumn]</code> <p>The schema of the columns to be added.</p> <code>None</code> <code>drop_columns</code> <code>List[str]</code> <p>List of column names to be dropped.</p> <code>None</code> <code>renew_schema</code> <code>bool</code> <p>Flag to indicate if the schema should be renewed.</p> <code>True</code> <code>pos_x</code> <code>float</code> <p>The x-coordinate on the canvas.</p> <code>0</code> <code>pos_y</code> <code>float</code> <p>The y-coordinate on the canvas.</p> <code>0</code> <code>schema_callback</code> <code>Callable</code> <p>A custom function to calculate the output schema.</p> <code>None</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def __init__(self, node_id: Union[str, int], function: Callable,\n             parent_uuid: str,\n             setting_input: Any,\n             name: str,\n             node_type: str,\n             input_columns: List[str] = None,\n             output_schema: List[FlowfileColumn] = None,\n             drop_columns: List[str] = None,\n             renew_schema: bool = True,\n             pos_x: float = 0,\n             pos_y: float = 0,\n             schema_callback: Callable = None,\n             ):\n    \"\"\"Initializes a FlowNode instance.\n\n    Args:\n        node_id: Unique identifier for the node.\n        function: The core data processing function for the node.\n        parent_uuid: The UUID of the parent flow.\n        setting_input: The configuration/settings object for the node.\n        name: The name of the node.\n        node_type: The type identifier of the node (e.g., 'join', 'filter').\n        input_columns: List of column names expected as input.\n        output_schema: The schema of the columns to be added.\n        drop_columns: List of column names to be dropped.\n        renew_schema: Flag to indicate if the schema should be renewed.\n        pos_x: The x-coordinate on the canvas.\n        pos_y: The y-coordinate on the canvas.\n        schema_callback: A custom function to calculate the output schema.\n    \"\"\"\n    self._name = None\n    self.parent_uuid = parent_uuid\n    self.post_init()\n    self.active = True\n    self.node_information.id = node_id\n    self.node_type = node_type\n    self.node_settings.renew_schema = renew_schema\n    self.update_node(function=function,\n                     input_columns=input_columns,\n                     output_schema=output_schema,\n                     drop_columns=drop_columns,\n                     setting_input=setting_input,\n                     name=name,\n                     pos_x=pos_x,\n                     pos_y=pos_y,\n                     schema_callback=schema_callback,\n                     )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.__repr__","title":"<code>__repr__()</code>","text":"<p>Provides a string representation of the FlowNode instance.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string showing the node's ID and type.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Provides a string representation of the FlowNode instance.\n\n    Returns:\n        A string showing the node's ID and type.\n    \"\"\"\n    return f\"Node id: {self.node_id} ({self.node_type})\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.add_lead_to_in_depend_source","title":"<code>add_lead_to_in_depend_source()</code>","text":"<p>Ensures this node is registered in the <code>leads_to_nodes</code> list of its inputs.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def add_lead_to_in_depend_source(self):\n    \"\"\"Ensures this node is registered in the `leads_to_nodes` list of its inputs.\"\"\"\n    for input_node in self.all_inputs:\n        if self.node_id not in [n.node_id for n in input_node.leads_to_nodes]:\n            input_node.leads_to_nodes.append(self)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.add_node_connection","title":"<code>add_node_connection(from_node, insert_type='main')</code>","text":"<p>Adds a connection from a source node to this node.</p> <p>Parameters:</p> Name Type Description Default <code>from_node</code> <code>FlowNode</code> <p>The node to connect from.</p> required <code>insert_type</code> <code>Literal['main', 'left', 'right']</code> <p>The type of input to connect to ('main', 'left', 'right').</p> <code>'main'</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the insert_type is invalid.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def add_node_connection(self, from_node: \"FlowNode\",\n                        insert_type: Literal['main', 'left', 'right'] = 'main') -&gt; None:\n    \"\"\"Adds a connection from a source node to this node.\n\n    Args:\n        from_node: The node to connect from.\n        insert_type: The type of input to connect to ('main', 'left', 'right').\n\n    Raises:\n        Exception: If the insert_type is invalid.\n    \"\"\"\n    from_node.leads_to_nodes.append(self)\n    if insert_type == 'main':\n        if self.node_template.input &lt;= 2 or self.node_inputs.main_inputs is None:\n            self.node_inputs.main_inputs = [from_node]\n        else:\n            self.node_inputs.main_inputs.append(from_node)\n    elif insert_type == 'right':\n        self.node_inputs.right_input = from_node\n    elif insert_type == 'left':\n        self.node_inputs.left_input = from_node\n    else:\n        raise Exception('Cannot find the connection')\n    if self.setting_input.is_setup:\n        if hasattr(self.setting_input, 'depending_on_id') and insert_type == 'main':\n            self.setting_input.depending_on_id = from_node.node_id\n    self.reset()\n    from_node.reset()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.calculate_hash","title":"<code>calculate_hash(setting_input)</code>","text":"<p>Calculates a hash based on settings and input node hashes.</p> <p>Parameters:</p> Name Type Description Default <code>setting_input</code> <code>Any</code> <p>The node's settings object to be included in the hash.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string hash value.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def calculate_hash(self, setting_input: Any) -&gt; str:\n    \"\"\"Calculates a hash based on settings and input node hashes.\n\n    Args:\n        setting_input: The node's settings object to be included in the hash.\n\n    Returns:\n        A string hash value.\n    \"\"\"\n    depends_on_hashes = [_node.hash for _node in self.all_inputs]\n    node_data_hash = get_hash(setting_input)\n    return get_hash(depends_on_hashes + [node_data_hash, self.parent_uuid])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.cancel","title":"<code>cancel()</code>","text":"<p>Cancels an ongoing external process if one is running.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def cancel(self):\n    \"\"\"Cancels an ongoing external process if one is running.\"\"\"\n\n    if self._fetch_cached_df is not None:\n        self._fetch_cached_df.cancel()\n        self.node_stats.is_canceled = True\n    else:\n        logger.warning('No external process to cancel')\n    self.node_stats.is_canceled = True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.clear_table_example","title":"<code>clear_table_example()</code>","text":"<p>Clear the table example in the results so that it clears the existing results Returns:     None</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def clear_table_example(self) -&gt; None:\n    \"\"\"\n    Clear the table example in the results so that it clears the existing results\n    Returns:\n        None\n    \"\"\"\n\n    self.results.example_data = None\n    self.results.example_data_generator = None\n    self.results.example_data_path = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.create_schema_callback_from_function","title":"<code>create_schema_callback_from_function(f)</code>  <code>staticmethod</code>","text":"<p>Wraps a node's function to create a schema callback that extracts the schema.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>The node's core function that returns a FlowDataEngine instance.</p> required <p>Returns:</p> Type Description <code>Callable[[], List[FlowfileColumn]]</code> <p>A callable that, when executed, returns the output schema.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>@staticmethod\ndef create_schema_callback_from_function(f: Callable) -&gt; Callable[[], List[FlowfileColumn]]:\n    \"\"\"Wraps a node's function to create a schema callback that extracts the schema.\n\n    Args:\n        f: The node's core function that returns a FlowDataEngine instance.\n\n    Returns:\n        A callable that, when executed, returns the output schema.\n    \"\"\"\n    def schema_callback() -&gt; List[FlowfileColumn]:\n        try:\n            logger.info('Executing the schema callback function based on the node function')\n            return f().schema\n        except Exception as e:\n            logger.warning(f'Error with the schema callback: {e}')\n            return []\n    return schema_callback\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.delete_input_node","title":"<code>delete_input_node(node_id, connection_type='input-0', complete=False)</code>","text":"<p>Removes a connection from a specific input node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>int</code> <p>The ID of the input node to disconnect.</p> required <code>connection_type</code> <code>InputConnectionClass</code> <p>The specific input handle (e.g., 'input-0', 'input-1').</p> <code>'input-0'</code> <code>complete</code> <code>bool</code> <p>If True, tries to delete from all input types.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if a connection was found and removed, False otherwise.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def delete_input_node(self, node_id: int, connection_type: input_schema.InputConnectionClass = 'input-0',\n                      complete: bool = False) -&gt; bool:\n    \"\"\"Removes a connection from a specific input node.\n\n    Args:\n        node_id: The ID of the input node to disconnect.\n        connection_type: The specific input handle (e.g., 'input-0', 'input-1').\n        complete: If True, tries to delete from all input types.\n\n    Returns:\n        True if a connection was found and removed, False otherwise.\n    \"\"\"\n    deleted: bool = False\n    if connection_type == 'input-0':\n        for i, node in enumerate(self.node_inputs.main_inputs):\n            if node.node_id == node_id:\n                self.node_inputs.main_inputs.pop(i)\n                deleted = True\n                if not complete:\n                    continue\n    elif connection_type == 'input-1' or complete:\n        if self.node_inputs.right_input is not None and self.node_inputs.right_input.node_id == node_id:\n            self.node_inputs.right_input = None\n            deleted = True\n    elif connection_type == 'input-2' or complete:\n        if self.node_inputs.left_input is not None and self.node_inputs.right_input.node_id == node_id:\n            self.node_inputs.left_input = None\n            deleted = True\n    else:\n        logger.warning('Could not find the connection to delete...')\n    if deleted:\n        self.reset()\n    return deleted\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.delete_lead_to_node","title":"<code>delete_lead_to_node(node_id)</code>","text":"<p>Removes a connection to a specific downstream node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>int</code> <p>The ID of the downstream node to disconnect.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the connection was found and removed, False otherwise.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def delete_lead_to_node(self, node_id: int) -&gt; bool:\n    \"\"\"Removes a connection to a specific downstream node.\n\n    Args:\n        node_id: The ID of the downstream node to disconnect.\n\n    Returns:\n        True if the connection was found and removed, False otherwise.\n    \"\"\"\n    logger.info(f'Deleting lead to node: {node_id}')\n    for i, lead_to_node in enumerate(self.leads_to_nodes):\n        logger.info(f'Checking lead to node: {lead_to_node.node_id}')\n        if lead_to_node.node_id == node_id:\n            logger.info(f'Found the node to delete: {node_id}')\n            self.leads_to_nodes.pop(i)\n            return True\n    return False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.evaluate_nodes","title":"<code>evaluate_nodes(deep=False)</code>","text":"<p>Triggers a state reset for all directly connected downstream nodes.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, the reset propagates recursively through the entire downstream graph.</p> <code>False</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def evaluate_nodes(self, deep: bool = False) -&gt; None:\n    \"\"\"Triggers a state reset for all directly connected downstream nodes.\n\n    Args:\n        deep: If True, the reset propagates recursively through the entire downstream graph.\n    \"\"\"\n    for node in self.leads_to_nodes:\n        self.print(f'resetting node: {node.node_id}')\n        node.reset(deep)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.execute_full_local","title":"<code>execute_full_local(performance_mode=False)</code>","text":"<p>Executes the node's logic locally, including example data generation.</p> <p>Parameters:</p> Name Type Description Default <code>performance_mode</code> <code>bool</code> <p>If True, skips generating example data.</p> <code>False</code> <p>Raises:</p> Type Description <code>Exception</code> <p>Propagates exceptions from the execution.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def execute_full_local(self, performance_mode: bool = False) -&gt; None:\n    \"\"\"Executes the node's logic locally, including example data generation.\n\n    Args:\n        performance_mode: If True, skips generating example data.\n\n    Raises:\n        Exception: Propagates exceptions from the execution.\n    \"\"\"\n    self.clear_table_example()\n\n    def example_data_generator():\n        example_data = None\n\n        def get_example_data():\n            nonlocal example_data\n            if example_data is None:\n                example_data = resulting_data.get_sample(100).to_arrow()\n            return example_data\n        return get_example_data\n    resulting_data = self.get_resulting_data()\n\n    if not performance_mode:\n        self.node_stats.has_run_with_current_setup = True\n        self.results.example_data_generator = example_data_generator()\n        self.node_schema.result_schema = self.results.resulting_data.schema\n        self.node_stats.has_completed_last_run = True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.execute_local","title":"<code>execute_local(flow_id, performance_mode=False)</code>","text":"<p>Executes the node's logic locally.</p> <p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>int</code> <p>The ID of the parent flow.</p> required <code>performance_mode</code> <code>bool</code> <p>If True, skips generating example data.</p> <code>False</code> <p>Raises:</p> Type Description <code>Exception</code> <p>Propagates exceptions from the execution.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def execute_local(self, flow_id: int, performance_mode: bool = False):\n    \"\"\"Executes the node's logic locally.\n\n    Args:\n        flow_id: The ID of the parent flow.\n        performance_mode: If True, skips generating example data.\n\n    Raises:\n        Exception: Propagates exceptions from the execution.\n    \"\"\"\n    try:\n        resulting_data = self.get_resulting_data()\n        if not performance_mode:\n            external_sampler = ExternalSampler(lf=resulting_data.data_frame, file_ref=self.hash,\n                                               wait_on_completion=True, node_id=self.node_id, flow_id=flow_id)\n            self.store_example_data_generator(external_sampler)\n            if self.results.errors is None and not self.node_stats.is_canceled:\n                self.node_stats.has_run_with_current_setup = True\n        self.node_schema.result_schema = resulting_data.schema\n\n    except Exception as e:\n        logger.warning(f\"Error with step {self.__name__}\")\n        logger.error(str(e))\n        self.results.errors = str(e)\n        self.node_stats.has_run_with_current_setup = False\n        self.node_stats.has_completed_last_run = False\n        raise e\n\n    if self.node_stats.has_run_with_current_setup:\n        for step in self.leads_to_nodes:\n            if not self.node_settings.streamable:\n                step.node_settings.streamable = self.node_settings.streamable\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.execute_node","title":"<code>execute_node(run_location, reset_cache=False, performance_mode=False, retry=True, node_logger=None, optimize_for_downstream=True)</code>","text":"<p>Orchestrates the execution, handling location, caching, and retries.</p> <p>Parameters:</p> Name Type Description Default <code>run_location</code> <code>ExecutionLocationsLiteral</code> <p>The location for execution ('local', 'remote').</p> required <code>reset_cache</code> <code>bool</code> <p>If True, forces removal of any existing cache.</p> <code>False</code> <code>performance_mode</code> <code>bool</code> <p>If True, optimizes for speed over diagnostics.</p> <code>False</code> <code>retry</code> <code>bool</code> <p>If True, allows retrying execution on recoverable errors.</p> <code>True</code> <code>node_logger</code> <code>NodeLogger</code> <p>The logger for this node execution.</p> <code>None</code> <code>optimize_for_downstream</code> <code>bool</code> <p>If true, operations that shuffle the order of rows are fully cached and provided as</p> <code>True</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the node_logger is not defined.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def execute_node(self, run_location: schemas.ExecutionLocationsLiteral,\n                 reset_cache: bool = False,\n                 performance_mode: bool = False,\n                 retry: bool = True,\n                 node_logger: NodeLogger = None,\n                 optimize_for_downstream: bool = True):\n    \"\"\"Orchestrates the execution, handling location, caching, and retries.\n\n    Args:\n        run_location: The location for execution ('local', 'remote').\n        reset_cache: If True, forces removal of any existing cache.\n        performance_mode: If True, optimizes for speed over diagnostics.\n        retry: If True, allows retrying execution on recoverable errors.\n        node_logger: The logger for this node execution.\n        optimize_for_downstream: If true, operations that shuffle the order of rows are fully cached and provided as\n        input to downstream steps\n\n    Raises:\n        Exception: If the node_logger is not defined.\n    \"\"\"\n    if node_logger is None:\n        raise Exception('Flow logger is not defined')\n    #  TODO: Simplify which route is being picked there are many duplicate checks\n\n    if reset_cache:\n        self.remove_cache()\n        self.node_stats.has_run_with_current_setup = False\n        self.node_stats.has_completed_last_run = False\n\n    if self.is_setup:\n        node_logger.info(f'Starting to run {self.__name__}')\n        if (self.needs_run(performance_mode, node_logger, run_location) or self.node_template.node_group == \"output\"\n                and not (run_location == 'local')):\n            self.clear_table_example()\n            self.prepare_before_run()\n            self.reset()\n            try:\n                if (((run_location == 'remote' or\n                     (self.node_default.transform_type == 'wide' and optimize_for_downstream) and\n                     not run_location == 'local'))\n                        or self.node_settings.cache_results):\n                    node_logger.info('Running the node remotely')\n                    if self.node_settings.cache_results:\n                        performance_mode = False\n                    self.execute_remote(performance_mode=(performance_mode if not self.node_settings.cache_results\n                                                          else False),\n                                        node_logger=node_logger\n                                        )\n                else:\n                    node_logger.info('Running the node locally')\n                    self.execute_local(performance_mode=performance_mode, flow_id=node_logger.flow_id)\n            except Exception as e:\n                if 'No such file or directory (os error' in str(e) and retry:\n                    logger.warning('Error with the input node, starting to rerun the input node...')\n                    all_inputs: List[FlowNode] = self.node_inputs.get_all_inputs()\n                    for node_input in all_inputs:\n                        node_input.execute_node(run_location=run_location,\n                                                performance_mode=performance_mode, retry=True,\n                                                reset_cache=True,\n                                                node_logger=node_logger)\n                    self.execute_node(run_location=run_location,\n                                      performance_mode=performance_mode, retry=False,\n                                      node_logger=node_logger)\n                else:\n                    self.results.errors = str(e)\n                    if \"Connection refused\" in str(e) and \"/submit_query/\" in str(e):\n                        node_logger.warning(\"There was an issue connecting to the remote worker, \"\n                                            \"ensure the worker process is running, \"\n                                            \"or change the settings to, so it executes locally\")\n                        node_logger.error(\"Could not execute in the remote worker. (Re)start the worker service, or change settings to local settings.\")\n                    else:\n                        node_logger.error(f'Error with running the node: {e}')\n        elif ((run_location == 'local') and\n              (not self.node_stats.has_run_with_current_setup or self.node_template.node_group == \"output\")):\n            try:\n                node_logger.info('Executing fully locally')\n                self.execute_full_local(performance_mode)\n            except Exception as e:\n                self.results.errors = str(e)\n                node_logger.error(f'Error with running the node: {e}')\n                self.node_stats.error = str(e)\n                self.node_stats.has_completed_last_run = False\n\n        else:\n            node_logger.info('Node has already run, not running the node')\n    else:\n        node_logger.warning(f'Node {self.__name__} is not setup, cannot run the node')\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.execute_remote","title":"<code>execute_remote(performance_mode=False, node_logger=None)</code>","text":"<p>Executes the node's logic remotely or handles cached results.</p> <p>Parameters:</p> Name Type Description Default <code>performance_mode</code> <code>bool</code> <p>If True, skips generating example data.</p> <code>False</code> <code>node_logger</code> <code>NodeLogger</code> <p>The logger for this node execution.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the node_logger is not provided or if execution fails.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def execute_remote(self, performance_mode: bool = False, node_logger: NodeLogger = None):\n    \"\"\"Executes the node's logic remotely or handles cached results.\n\n    Args:\n        performance_mode: If True, skips generating example data.\n        node_logger: The logger for this node execution.\n\n    Raises:\n        Exception: If the node_logger is not provided or if execution fails.\n    \"\"\"\n    if node_logger is None:\n        raise Exception('Node logger is not defined')\n    if self.node_settings.cache_results and results_exists(self.hash):\n        try:\n            self.results.resulting_data = get_external_df_result(self.hash)\n            self._cache_progress = None\n            return\n        except Exception as e:\n            node_logger.warning('Failed to read the cache, rerunning the code')\n    if self.node_type == 'output':\n        self.results.resulting_data = self.get_resulting_data()\n        self.node_stats.has_run_with_current_setup = True\n        return\n    try:\n        self.get_resulting_data()\n    except Exception as e:\n        self.results.errors = 'Error with creating the lazy frame, most likely due to invalid graph'\n        raise e\n    if not performance_mode:\n        external_df_fetcher = ExternalDfFetcher(lf=self.get_resulting_data().data_frame,\n                                                file_ref=self.hash, wait_on_completion=False,\n                                                flow_id=node_logger.flow_id,\n                                                node_id=self.node_id)\n        self._fetch_cached_df = external_df_fetcher\n        try:\n            lf = external_df_fetcher.get_result()\n            self.results.resulting_data = FlowDataEngine(\n                lf, number_of_records=ExternalDfFetcher(lf=lf, operation_type='calculate_number_of_records',\n                                                        flow_id=node_logger.flow_id, node_id=self.node_id).result\n            )\n            if not performance_mode:\n                self.store_example_data_generator(external_df_fetcher)\n                self.node_stats.has_run_with_current_setup = True\n\n        except Exception as e:\n            node_logger.error('Error with external process')\n            if external_df_fetcher.error_code == -1:\n                try:\n                    self.results.resulting_data = self.get_resulting_data()\n                    self.results.warnings = ('Error with external process (unknown error), '\n                                             'likely the process was killed by the server because of memory constraints, '\n                                             'continue with the process. '\n                                             'We cannot display example data...')\n                except Exception as e:\n                    self.results.errors = str(e)\n                    raise e\n            elif external_df_fetcher.error_description is None:\n                self.results.errors = str(e)\n                raise e\n            else:\n                self.results.errors = external_df_fetcher.error_description\n                raise Exception(external_df_fetcher.error_description)\n        finally:\n            self._fetch_cached_df = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_all_dependent_node_ids","title":"<code>get_all_dependent_node_ids()</code>","text":"<p>Yields the IDs of all downstream nodes recursively.</p> <p>Returns:</p> Type Description <code>None</code> <p>A generator of all dependent node IDs.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_all_dependent_node_ids(self) -&gt; Generator[int, None, None]:\n    \"\"\"Yields the IDs of all downstream nodes recursively.\n\n    Returns:\n        A generator of all dependent node IDs.\n    \"\"\"\n    for node in self.leads_to_nodes:\n        yield node.node_id\n        for n in node.get_all_dependent_node_ids():\n            yield n\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_all_dependent_nodes","title":"<code>get_all_dependent_nodes()</code>","text":"<p>Yields all downstream nodes recursively.</p> <p>Returns:</p> Type Description <code>None</code> <p>A generator of all dependent FlowNode objects.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_all_dependent_nodes(self) -&gt; Generator[\"FlowNode\", None, None]:\n    \"\"\"Yields all downstream nodes recursively.\n\n    Returns:\n        A generator of all dependent FlowNode objects.\n    \"\"\"\n    for node in self.leads_to_nodes:\n        yield node\n        for n in node.get_all_dependent_nodes():\n            yield n\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_edge_input","title":"<code>get_edge_input()</code>","text":"<p>Generates <code>NodeEdge</code> objects for all input connections to this node.</p> <p>Returns:</p> Type Description <code>List[NodeEdge]</code> <p>A list of <code>NodeEdge</code> objects.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_edge_input(self) -&gt; List[schemas.NodeEdge]:\n    \"\"\"Generates `NodeEdge` objects for all input connections to this node.\n\n    Returns:\n        A list of `NodeEdge` objects.\n    \"\"\"\n    edges = []\n    if self.node_inputs.main_inputs is not None:\n        for i, main_input in enumerate(self.node_inputs.main_inputs):\n            edges.append(schemas.NodeEdge(id=f'{main_input.node_id}-{self.node_id}-{i}',\n                                          source=main_input.node_id,\n                                          target=self.node_id,\n                                          sourceHandle='output-0',\n                                          targetHandle='input-0',\n                                          ))\n    if self.node_inputs.left_input is not None:\n        edges.append(schemas.NodeEdge(id=f'{self.node_inputs.left_input.node_id}-{self.node_id}-right',\n                                      source=self.node_inputs.left_input.node_id,\n                                      target=self.node_id,\n                                      sourceHandle='output-0',\n                                      targetHandle='input-2',\n                                      ))\n    if self.node_inputs.right_input is not None:\n        edges.append(schemas.NodeEdge(id=f'{self.node_inputs.right_input.node_id}-{self.node_id}-left',\n                                      source=self.node_inputs.right_input.node_id,\n                                      target=self.node_id,\n                                      sourceHandle='output-0',\n                                      targetHandle='input-1',\n                                      ))\n    return edges\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_flow_file_column_schema","title":"<code>get_flow_file_column_schema(col_name)</code>","text":"<p>Retrieves the schema for a specific column from the output schema.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column.</p> required <p>Returns:</p> Type Description <code>FlowfileColumn | None</code> <p>The FlowfileColumn object for that column, or None if not found.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_flow_file_column_schema(self, col_name: str) -&gt; FlowfileColumn | None:\n    \"\"\"Retrieves the schema for a specific column from the output schema.\n\n    Args:\n        col_name: The name of the column.\n\n    Returns:\n        The FlowfileColumn object for that column, or None if not found.\n    \"\"\"\n    for s in self.schema:\n        if s.column_name == col_name:\n            return s\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_input_type","title":"<code>get_input_type(node_id)</code>","text":"<p>Gets the type of connection ('main', 'left', 'right') for a given input node ID.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>int</code> <p>The ID of the input node.</p> required <p>Returns:</p> Type Description <code>List</code> <p>A list of connection types for that node ID.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_input_type(self, node_id: int) -&gt; List:\n    \"\"\"Gets the type of connection ('main', 'left', 'right') for a given input node ID.\n\n    Args:\n        node_id: The ID of the input node.\n\n    Returns:\n        A list of connection types for that node ID.\n    \"\"\"\n    relation_type = []\n    if node_id in [n.node_id for n in self.node_inputs.main_inputs]:\n        relation_type.append('main')\n    if self.node_inputs.left_input is not None and node_id == self.node_inputs.left_input.node_id:\n        relation_type.append('left')\n    if self.node_inputs.right_input is not None and node_id == self.node_inputs.right_input.node_id:\n        relation_type.append('right')\n    return list(set(relation_type))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_node_data","title":"<code>get_node_data(flow_id, include_example=False)</code>","text":"<p>Gathers all necessary data for representing the node in the UI.</p> <p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>int</code> <p>The ID of the parent flow.</p> required <code>include_example</code> <code>bool</code> <p>If True, includes data samples.</p> <code>False</code> <p>Returns:</p> Type Description <code>NodeData</code> <p>A <code>NodeData</code> object.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_node_data(self, flow_id: int, include_example: bool = False) -&gt; NodeData:\n    \"\"\"Gathers all necessary data for representing the node in the UI.\n\n    Args:\n        flow_id: The ID of the parent flow.\n        include_example: If True, includes data samples.\n\n    Returns:\n        A `NodeData` object.\n    \"\"\"\n    node = NodeData(flow_id=flow_id,\n                    node_id=self.node_id,\n                    has_run=self.node_stats.has_run_with_current_setup,\n                    setting_input=self.setting_input,\n                    flow_type=self.node_type)\n    if self.main_input:\n        node.main_input = self.main_input[0].get_table_example()\n    if self.left_input:\n        node.left_input = self.left_input.get_table_example()\n    if self.right_input:\n        node.right_input = self.right_input.get_table_example()\n    if self.is_setup:\n        node.main_output = self.get_table_example(include_example)\n    node = setting_generator.get_setting_generator(self.node_type)(node)\n\n    node = setting_updator.get_setting_updator(self.node_type)(node)\n    return node\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_node_information","title":"<code>get_node_information()</code>","text":"<p>Updates and returns the node's information object.</p> <p>Returns:</p> Type Description <code>NodeInformation</code> <p>The <code>NodeInformation</code> object for this node.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_node_information(self) -&gt; schemas.NodeInformation:\n    \"\"\"Updates and returns the node's information object.\n\n    Returns:\n        The `NodeInformation` object for this node.\n    \"\"\"\n    self.set_node_information()\n    return self.node_information\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_node_input","title":"<code>get_node_input()</code>","text":"<p>Creates a <code>NodeInput</code> schema object for representing this node in the UI.</p> <p>Returns:</p> Type Description <code>NodeInput</code> <p>A <code>NodeInput</code> object.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_node_input(self) -&gt; schemas.NodeInput:\n    \"\"\"Creates a `NodeInput` schema object for representing this node in the UI.\n\n    Returns:\n        A `NodeInput` object.\n    \"\"\"\n    return schemas.NodeInput(pos_y=self.setting_input.pos_y,\n                             pos_x=self.setting_input.pos_x,\n                             id=self.node_id,\n                             **self.node_template.__dict__)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_output_data","title":"<code>get_output_data()</code>","text":"<p>Gets the full output data sample for this node.</p> <p>Returns:</p> Type Description <code>TableExample</code> <p>A <code>TableExample</code> object with data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_output_data(self) -&gt; TableExample:\n    \"\"\"Gets the full output data sample for this node.\n\n    Returns:\n        A `TableExample` object with data.\n    \"\"\"\n    return self.get_table_example(True)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_predicted_resulting_data","title":"<code>get_predicted_resulting_data()</code>","text":"<p>Creates a <code>FlowDataEngine</code> instance based on the predicted schema.</p> <p>This avoids executing the node's full logic.</p> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A FlowDataEngine instance with a schema but no data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_predicted_resulting_data(self) -&gt; FlowDataEngine:\n    \"\"\"Creates a `FlowDataEngine` instance based on the predicted schema.\n\n    This avoids executing the node's full logic.\n\n    Returns:\n        A FlowDataEngine instance with a schema but no data.\n    \"\"\"\n    if self.needs_run(False) and self.schema_callback is not None or self.node_schema.result_schema is not None:\n        self.print('Getting data based on the schema')\n\n        _s = self.schema_callback() if self.node_schema.result_schema is None else self.node_schema.result_schema\n        return FlowDataEngine.create_from_schema(_s)\n    else:\n        if isinstance(self.function, FlowDataEngine):\n            fl = self.function\n        else:\n            fl = FlowDataEngine.create_from_schema(self.get_predicted_schema())\n        return fl\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_predicted_schema","title":"<code>get_predicted_schema(force=False)</code>","text":"<p>Predicts the output schema of the node without full execution.</p> <p>It uses the schema_callback or infers from predicted data.</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>If True, forces recalculation even if a predicted schema exists.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[FlowfileColumn] | None</code> <p>A list of FlowfileColumn objects representing the predicted schema.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_predicted_schema(self, force: bool = False) -&gt; List[FlowfileColumn] | None:\n    \"\"\"Predicts the output schema of the node without full execution.\n\n    It uses the schema_callback or infers from predicted data.\n\n    Args:\n        force: If True, forces recalculation even if a predicted schema exists.\n\n    Returns:\n        A list of FlowfileColumn objects representing the predicted schema.\n    \"\"\"\n    if self.node_schema.predicted_schema and not force:\n        return self.node_schema.predicted_schema\n    if self.schema_callback is not None and (self.node_schema.predicted_schema is None or force):\n        self.print('Getting the data from a schema callback')\n        if force:\n            # Force the schema callback to reset, so that it will be executed again\n            self.schema_callback.reset()\n        schema = self.schema_callback()\n        if schema is not None and len(schema) &gt; 0:\n            self.print('Calculating the schema based on the schema callback')\n            self.node_schema.predicted_schema = schema\n            return self.node_schema.predicted_schema\n    predicted_data = self._predicted_data_getter()\n    if predicted_data is not None and predicted_data.schema is not None:\n        self.print('Calculating the schema based on the predicted resulting data')\n        self.node_schema.predicted_schema = self._predicted_data_getter().schema\n    return self.node_schema.predicted_schema\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_repr","title":"<code>get_repr()</code>","text":"<p>Gets a detailed dictionary representation of the node's state.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing key information about the node.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_repr(self) -&gt; dict:\n    \"\"\"Gets a detailed dictionary representation of the node's state.\n\n    Returns:\n        A dictionary containing key information about the node.\n    \"\"\"\n    return dict(FlowNode=\n                dict(node_id=self.node_id,\n                     step_name=self.__name__,\n                     output_columns=self.node_schema.output_columns,\n                     output_schema=self._get_readable_schema()))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_resulting_data","title":"<code>get_resulting_data()</code>","text":"<p>Executes the node's function to produce the actual output data.</p> <p>Handles both regular functions and external data sources.</p> <p>Returns:</p> Type Description <code>FlowDataEngine | None</code> <p>A FlowDataEngine instance containing the result, or None on error.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Propagates exceptions from the node's function execution.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_resulting_data(self) -&gt; FlowDataEngine | None:\n    \"\"\"Executes the node's function to produce the actual output data.\n\n    Handles both regular functions and external data sources.\n\n    Returns:\n        A FlowDataEngine instance containing the result, or None on error.\n\n    Raises:\n        Exception: Propagates exceptions from the node's function execution.\n    \"\"\"\n    if self.is_setup:\n        if self.results.resulting_data is None and self.results.errors is None:\n            self.print('getting resulting data')\n            try:\n                if isinstance(self.function, FlowDataEngine):\n                    fl: FlowDataEngine = self.function\n                elif self.node_type == 'external_source':\n                    fl: FlowDataEngine = self.function()\n                    fl.collect_external()\n                    self.node_settings.streamable = False\n                else:\n                    try:\n                        fl = self._function(*[v.get_resulting_data() for v in self.all_inputs])\n                    except Exception as e:\n                        raise e\n                fl.set_streamable(self.node_settings.streamable)\n                self.results.resulting_data = fl\n                self.node_schema.result_schema = fl.schema\n            except Exception as e:\n                self.results.resulting_data = FlowDataEngine()\n                self.results.errors = str(e)\n                self.node_stats.has_run_with_current_setup = False\n                self.node_stats.has_completed_last_run = False\n                raise e\n        return self.results.resulting_data\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.get_table_example","title":"<code>get_table_example(include_data=False)</code>","text":"<p>Generates a <code>TableExample</code> model summarizing the node's output.</p> <p>This can optionally include a sample of the data.</p> <p>Parameters:</p> Name Type Description Default <code>include_data</code> <code>bool</code> <p>If True, includes a data sample in the result.</p> <code>False</code> <p>Returns:</p> Type Description <code>TableExample | None</code> <p>A <code>TableExample</code> object, or None if the node is not set up.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def get_table_example(self, include_data: bool = False) -&gt; TableExample | None:\n    \"\"\"Generates a `TableExample` model summarizing the node's output.\n\n    This can optionally include a sample of the data.\n\n    Args:\n        include_data: If True, includes a data sample in the result.\n\n    Returns:\n        A `TableExample` object, or None if the node is not set up.\n    \"\"\"\n    self.print('Getting a table example')\n    if self.is_setup and include_data and self.node_stats.has_completed_last_run:\n\n        if self.node_template.node_group == 'output':\n            self.print('getting the table example')\n            return self.main_input[0].get_table_example(include_data)\n\n        logger.info('getting the table example since the node has run')\n        example_data_getter = self.results.example_data_generator\n        if example_data_getter is not None:\n            data = example_data_getter().to_pylist()\n            if data is None:\n                data = []\n        else:\n            data = []\n        schema = [FileColumn.model_validate(c.get_column_repr()) for c in self.schema]\n        fl = self.get_resulting_data()\n        has_example_data = self.results.example_data_generator is not None\n\n        return TableExample(node_id=self.node_id,\n                            name=str(self.node_id), number_of_records=999,\n                            number_of_columns=fl.number_of_fields,\n                            table_schema=schema, columns=fl.columns, data=data,\n                            has_example_data=has_example_data,\n                            has_run_with_current_setup=self.node_stats.has_run_with_current_setup\n                            )\n    else:\n        logger.warning('getting the table example but the node has not run')\n        try:\n            schema = [FileColumn.model_validate(c.get_column_repr()) for c in self.schema]\n        except Exception as e:\n            logger.warning(e)\n            schema = []\n        columns = [s.name for s in schema]\n        return TableExample(node_id=self.node_id,\n                            name=str(self.node_id), number_of_records=0,\n                            number_of_columns=len(columns),\n                            table_schema=schema, columns=columns,\n                            data=[])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.needs_reset","title":"<code>needs_reset()</code>","text":"<p>Checks if the node's hash has changed, indicating an outdated state.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the calculated hash differs from the stored hash.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def needs_reset(self) -&gt; bool:\n    \"\"\"Checks if the node's hash has changed, indicating an outdated state.\n\n    Returns:\n        True if the calculated hash differs from the stored hash.\n    \"\"\"\n    return self._hash != self.calculate_hash(self.setting_input)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.needs_run","title":"<code>needs_run(performance_mode, node_logger=None, execution_location='remote')</code>","text":"<p>Determines if the node needs to be executed.</p> <p>The decision is based on its run state, caching settings, and execution mode.</p> <p>Parameters:</p> Name Type Description Default <code>performance_mode</code> <code>bool</code> <p>True if the flow is in performance mode.</p> required <code>node_logger</code> <code>NodeLogger</code> <p>The logger instance for this node.</p> <code>None</code> <code>execution_location</code> <code>ExecutionLocationsLiteral</code> <p>The target execution location.</p> <code>'remote'</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the node should be run, False otherwise.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def needs_run(self, performance_mode: bool, node_logger: NodeLogger = None,\n              execution_location: schemas.ExecutionLocationsLiteral = \"remote\") -&gt; bool:\n    \"\"\"Determines if the node needs to be executed.\n\n    The decision is based on its run state, caching settings, and execution mode.\n\n    Args:\n        performance_mode: True if the flow is in performance mode.\n        node_logger: The logger instance for this node.\n        execution_location: The target execution location.\n\n    Returns:\n        True if the node should be run, False otherwise.\n    \"\"\"\n    if execution_location == \"local\":\n        return False\n\n    flow_logger = logger if node_logger is None else node_logger\n    cache_result_exists = results_exists(self.hash)\n    if not self.node_stats.has_run_with_current_setup:\n        flow_logger.info('Node has not run, needs to run')\n        return True\n    if self.node_settings.cache_results and cache_result_exists:\n        return False\n    elif self.node_settings.cache_results and not cache_result_exists:\n        return True\n    elif not performance_mode and cache_result_exists:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.post_init","title":"<code>post_init()</code>","text":"<p>Initializes or resets the node's attributes to their default states.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def post_init(self):\n    \"\"\"Initializes or resets the node's attributes to their default states.\"\"\"\n    self.node_inputs = NodeStepInputs()\n    self.node_stats = NodeStepStats()\n    self.node_settings = NodeStepSettings()\n    self.node_schema = NodeSchemaInformation()\n    self.results = NodeResults()\n    self.node_information = schemas.NodeInformation()\n    self.leads_to_nodes = []\n    self._setting_input = None\n    self._cache_progress = None\n    self._schema_callback = None\n    self._state_needs_reset = False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.prepare_before_run","title":"<code>prepare_before_run()</code>","text":"<p>Resets results and errors before a new execution.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def prepare_before_run(self):\n    \"\"\"Resets results and errors before a new execution.\"\"\"\n\n    self.results.errors = None\n    self.results.resulting_data = None\n    self.results.example_data = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.print","title":"<code>print(v)</code>","text":"<p>Helper method to log messages with node context.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Any</code> <p>The message or value to log.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def print(self, v: Any):\n    \"\"\"Helper method to log messages with node context.\n\n    Args:\n        v: The message or value to log.\n    \"\"\"\n    logger.info(f'{self.node_type}, node_id: {self.node_id}: {v}')\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.remove_cache","title":"<code>remove_cache()</code>","text":"<p>Removes cached results for this node.</p> <p>Note: Currently not fully implemented.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def remove_cache(self):\n    \"\"\"Removes cached results for this node.\n\n    Note: Currently not fully implemented.\n    \"\"\"\n\n    if results_exists(self.hash):\n        logger.warning('Not implemented')\n        clear_task_from_worker(self.hash)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.reset","title":"<code>reset(deep=False)</code>","text":"<p>Resets the node's execution state and schema information.</p> <p>This also triggers a reset on all downstream nodes.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, forces a reset even if the hash hasn't changed.</p> <code>False</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def reset(self, deep: bool = False):\n    \"\"\"Resets the node's execution state and schema information.\n\n    This also triggers a reset on all downstream nodes.\n\n    Args:\n        deep: If True, forces a reset even if the hash hasn't changed.\n    \"\"\"\n    needs_reset = self.needs_reset() or deep\n    if needs_reset:\n        logger.info(f'{self.node_id}: Node needs reset')\n        self.node_stats.has_run_with_current_setup = False\n        self.results.reset()\n        self.node_schema.result_schema = None\n        self.node_schema.predicted_schema = None\n        self._hash = None\n        self.node_information.is_setup = None\n        self.results.errors = None\n        if self.is_correct:\n            self._schema_callback = None  # Ensure the schema callback is reset\n            if self.schema_callback:\n                logger.info(f'{self.node_id}: Resetting the schema callback')\n                self.schema_callback.start()\n        self.evaluate_nodes()\n        _ = self.hash  # Recalculate the hash after reset\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.set_node_information","title":"<code>set_node_information()</code>","text":"<p>Populates the <code>node_information</code> attribute with the current state.</p> <p>This includes the node's connections, settings, and position.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def set_node_information(self):\n    \"\"\"Populates the `node_information` attribute with the current state.\n\n    This includes the node's connections, settings, and position.\n    \"\"\"\n    logger.info('setting node information')\n    node_information = self.node_information\n    node_information.left_input_id = self.node_inputs.left_input.node_id if self.left_input else None\n    node_information.right_input_id = self.node_inputs.right_input.node_id if self.right_input else None\n    node_information.input_ids = [mi.node_id for mi in\n                                  self.node_inputs.main_inputs] if self.node_inputs.main_inputs is not None else None\n    node_information.setting_input = self.setting_input\n    node_information.outputs = [n.node_id for n in self.leads_to_nodes]\n    node_information.is_setup = self.is_setup\n    node_information.x_position = self.setting_input.pos_x\n    node_information.y_position = self.setting_input.pos_y\n    node_information.type = self.node_type\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.store_example_data_generator","title":"<code>store_example_data_generator(external_df_fetcher)</code>","text":"<p>Stores a generator function for fetching a sample of the result data.</p> <p>Parameters:</p> Name Type Description Default <code>external_df_fetcher</code> <code>ExternalDfFetcher | ExternalSampler</code> <p>The process that generated the sample data.</p> required Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def store_example_data_generator(self, external_df_fetcher: ExternalDfFetcher | ExternalSampler):\n    \"\"\"Stores a generator function for fetching a sample of the result data.\n\n    Args:\n        external_df_fetcher: The process that generated the sample data.\n    \"\"\"\n    if external_df_fetcher.status is not None:\n        file_ref = external_df_fetcher.status.file_ref\n        self.results.example_data_path = file_ref\n        self.results.example_data_generator = get_read_top_n(file_path=file_ref, n=100)\n    else:\n        logger.error('Could not get the sample data, the external process is not ready')\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_node.flow_node.FlowNode.update_node","title":"<code>update_node(function, input_columns=None, output_schema=None, drop_columns=None, name=None, setting_input=None, pos_x=0, pos_y=0, schema_callback=None)</code>","text":"<p>Updates the properties of the node.</p> <p>This is called during initialization and when settings are changed.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>The new core data processing function.</p> required <code>input_columns</code> <code>List[str]</code> <p>The new list of input columns.</p> <code>None</code> <code>output_schema</code> <code>List[FlowfileColumn]</code> <p>The new schema of added columns.</p> <code>None</code> <code>drop_columns</code> <code>List[str]</code> <p>The new list of dropped columns.</p> <code>None</code> <code>name</code> <code>str</code> <p>The new name for the node.</p> <code>None</code> <code>setting_input</code> <code>Any</code> <p>The new settings object.</p> <code>None</code> <code>pos_x</code> <code>float</code> <p>The new x-coordinate.</p> <code>0</code> <code>pos_y</code> <code>float</code> <p>The new y-coordinate.</p> <code>0</code> <code>schema_callback</code> <code>Callable</code> <p>The new custom schema callback function.</p> <code>None</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_node/flow_node.py</code> <pre><code>def update_node(self,\n                function: Callable,\n                input_columns: List[str] = None,\n                output_schema: List[FlowfileColumn] = None,\n                drop_columns: List[str] = None,\n                name: str = None,\n                setting_input: Any = None,\n                pos_x: float = 0,\n                pos_y: float = 0,\n                schema_callback: Callable = None,\n                ):\n    \"\"\"Updates the properties of the node.\n\n    This is called during initialization and when settings are changed.\n\n    Args:\n        function: The new core data processing function.\n        input_columns: The new list of input columns.\n        output_schema: The new schema of added columns.\n        drop_columns: The new list of dropped columns.\n        name: The new name for the node.\n        setting_input: The new settings object.\n        pos_x: The new x-coordinate.\n        pos_y: The new y-coordinate.\n        schema_callback: The new custom schema callback function.\n    \"\"\"\n    self.user_provided_schema_callback = schema_callback\n    self.node_information.y_position = int(pos_y)\n    self.node_information.x_position = int(pos_x)\n    self.node_information.setting_input = setting_input\n    self.name = self.node_type if name is None else name\n    self._function = function\n\n    self.node_schema.input_columns = [] if input_columns is None else input_columns\n    self.node_schema.output_columns = [] if output_schema is None else output_schema\n    self.node_schema.drop_columns = [] if drop_columns is None else drop_columns\n    self.node_settings.renew_schema = True\n    if hasattr(setting_input, 'cache_results'):\n        self.node_settings.cache_results = setting_input.cache_results\n\n    self.results.errors = None\n    self.add_lead_to_in_depend_source()\n    _ = self.hash\n    self.node_template = node_store.node_dict.get(self.node_type)\n    if self.node_template is None:\n        raise Exception(f'Node template {self.node_type} not found')\n    self.node_default = node_store.node_defaults.get(self.node_type)\n    self.setting_input = setting_input  # wait until the end so that the hash is calculated correctly\n</code></pre>"},{"location":"for-developers/python-api-reference.html#the-flowdataengine","title":"The FlowDataEngine","text":"<p>The <code>FlowDataEngine</code> is the primary engine of the library, providing a rich API for data manipulation, I/O, and transformation. Its methods are grouped below by functionality.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine","title":"<code>flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine</code>  <code>dataclass</code>","text":"<p>The core data handling engine for Flowfile.</p> <p>This class acts as a high-level wrapper around a Polars DataFrame or LazyFrame, providing a unified API for data ingestion, transformation, and output. It manages data state (lazy vs. eager), schema information, and execution logic.</p> <p>Attributes:</p> Name Type Description <code>_data_frame</code> <code>Union[DataFrame, LazyFrame]</code> <p>The underlying Polars DataFrame or LazyFrame.</p> <code>columns</code> <code>List[Any]</code> <p>A list of column names in the current data frame.</p> <code>name</code> <code>str</code> <p>An optional name for the data engine instance.</p> <code>number_of_records</code> <code>int</code> <p>The number of records. Can be -1 for lazy frames.</p> <code>errors</code> <code>List</code> <p>A list of errors encountered during operations.</p> <code>_schema</code> <code>Optional[List[FlowfileColumn]]</code> <p>A cached list of <code>FlowfileColumn</code> objects representing the schema.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Makes the class instance callable, returning itself.</p> <code>__get_sample__</code> <p>Internal method to get a sample of the data.</p> <code>__getitem__</code> <p>Accesses a specific column or item from the DataFrame.</p> <code>__init__</code> <p>Initializes the FlowDataEngine from various data sources.</p> <code>__len__</code> <p>Returns the number of records in the table.</p> <code>__repr__</code> <p>Returns a string representation of the FlowDataEngine.</p> <code>add_new_values</code> <p>Adds a new column with the provided values.</p> <code>add_record_id</code> <p>Adds a record ID (row number) column to the DataFrame.</p> <code>apply_flowfile_formula</code> <p>Applies a formula to create a new column or transform an existing one.</p> <code>apply_sql_formula</code> <p>Applies an SQL-style formula using <code>pl.sql_expr</code>.</p> <code>assert_equal</code> <p>Asserts that this DataFrame is equal to another.</p> <code>cache</code> <p>Caches the current DataFrame to disk and updates the internal reference.</p> <code>calculate_schema</code> <p>Calculates and returns the schema.</p> <code>change_column_types</code> <p>Changes the data type of one or more columns.</p> <code>collect</code> <p>Collects the data and returns it as a Polars DataFrame.</p> <code>collect_external</code> <p>Materializes data from a tracked external source.</p> <code>concat</code> <p>Concatenates this DataFrame with one or more other DataFrames.</p> <code>count</code> <p>Gets the total number of records.</p> <code>create_from_external_source</code> <p>Creates a FlowDataEngine from an external data source.</p> <code>create_from_path</code> <p>Creates a FlowDataEngine from a local file path.</p> <code>create_from_path_worker</code> <p>Creates a FlowDataEngine from a path in a worker process.</p> <code>create_from_schema</code> <p>Creates an empty FlowDataEngine from a schema definition.</p> <code>create_from_sql</code> <p>Creates a FlowDataEngine by executing a SQL query.</p> <code>create_random</code> <p>Creates a FlowDataEngine with randomly generated data.</p> <code>do_cross_join</code> <p>Performs a cross join with another DataFrame.</p> <code>do_filter</code> <p>Filters rows based on a predicate expression.</p> <code>do_group_by</code> <p>Performs a group-by operation on the DataFrame.</p> <code>do_pivot</code> <p>Converts the DataFrame from a long to a wide format, aggregating values.</p> <code>do_select</code> <p>Performs a complex column selection, renaming, and reordering operation.</p> <code>do_sort</code> <p>Sorts the DataFrame by one or more columns.</p> <code>drop_columns</code> <p>Drops specified columns from the DataFrame.</p> <code>from_cloud_storage_obj</code> <p>Creates a FlowDataEngine from an object in cloud storage.</p> <code>generate_enumerator</code> <p>Generates a FlowDataEngine with a single column containing a sequence of integers.</p> <code>get_estimated_file_size</code> <p>Estimates the file size in bytes if the data originated from a local file.</p> <code>get_number_of_records</code> <p>Gets the total number of records in the DataFrame.</p> <code>get_number_of_records_in_process</code> <p>Get the number of records in the DataFrame in the local process.</p> <code>get_output_sample</code> <p>Gets a sample of the data as a list of dictionaries.</p> <code>get_record_count</code> <p>Returns a new FlowDataEngine with a single column 'number_of_records'</p> <code>get_sample</code> <p>Gets a sample of rows from the DataFrame.</p> <code>get_schema_column</code> <p>Retrieves the schema information for a single column by its name.</p> <code>get_select_inputs</code> <p>Gets <code>SelectInput</code> specifications for all columns in the current schema.</p> <code>get_subset</code> <p>Gets the first <code>n_rows</code> from the DataFrame.</p> <code>initialize_empty_fl</code> <p>Initializes an empty LazyFrame.</p> <code>iter_batches</code> <p>Iterates over the DataFrame in batches.</p> <code>join</code> <p>Performs a standard SQL-style join with another DataFrame.</p> <code>make_unique</code> <p>Gets the unique rows from the DataFrame.</p> <code>output</code> <p>Writes the DataFrame to an output file.</p> <code>reorganize_order</code> <p>Reorganizes columns into a specified order.</p> <code>save</code> <p>Saves the DataFrame to a file in a separate thread.</p> <code>select_columns</code> <p>Selects a subset of columns from the DataFrame.</p> <code>set_streamable</code> <p>Sets whether DataFrame operations should be streamable.</p> <code>solve_graph</code> <p>Solves a graph problem represented by 'from' and 'to' columns.</p> <code>split</code> <p>Splits a column's text values into multiple rows based on a delimiter.</p> <code>start_fuzzy_join</code> <p>Starts a fuzzy join operation in a background process.</p> <code>to_arrow</code> <p>Converts the DataFrame to a PyArrow Table.</p> <code>to_cloud_storage_obj</code> <p>Writes the DataFrame to an object in cloud storage.</p> <code>to_dict</code> <p>Converts the DataFrame to a Python dictionary of columns.</p> <code>to_pylist</code> <p>Converts the DataFrame to a list of Python dictionaries.</p> <code>to_raw_data</code> <p>Converts the DataFrame to a <code>RawData</code> schema object.</p> <code>unpivot</code> <p>Converts the DataFrame from a wide to a long format.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@dataclass\nclass FlowDataEngine:\n    \"\"\"The core data handling engine for Flowfile.\n\n    This class acts as a high-level wrapper around a Polars DataFrame or\n    LazyFrame, providing a unified API for data ingestion, transformation,\n    and output. It manages data state (lazy vs. eager), schema information,\n    and execution logic.\n\n    Attributes:\n        _data_frame: The underlying Polars DataFrame or LazyFrame.\n        columns: A list of column names in the current data frame.\n        name: An optional name for the data engine instance.\n        number_of_records: The number of records. Can be -1 for lazy frames.\n        errors: A list of errors encountered during operations.\n        _schema: A cached list of `FlowfileColumn` objects representing the schema.\n    \"\"\"\n    # Core attributes\n    _data_frame: Union[pl.DataFrame, pl.LazyFrame]\n    columns: List[Any]\n\n    # Metadata attributes\n    name: str = None\n    number_of_records: int = None\n    errors: List = None\n    _schema: Optional[List['FlowfileColumn']] = None\n\n    # Configuration attributes\n    _optimize_memory: bool = False\n    _lazy: bool = None\n    _streamable: bool = True\n    _calculate_schema_stats: bool = False\n\n    # Cache and optimization attributes\n    __col_name_idx_map: Dict = None\n    __data_map: Dict = None\n    __optimized_columns: List = None\n    __sample__: str = None\n    __number_of_fields: int = None\n    _col_idx: Dict[str, int] = None\n\n    # Source tracking\n    _org_path: Optional[str] = None\n    _external_source: Optional[ExternalDataSource] = None\n\n    # State tracking\n    sorted_by: int = None\n    is_future: bool = False\n    is_collected: bool = True\n    ind_schema_calculated: bool = False\n\n    # Callbacks\n    _future: Future = None\n    _number_of_records_callback: Callable = None\n    _data_callback: Callable = None\n\n\n    def __init__(self,\n                 raw_data: Union[List[Dict], List[Any], Dict[str, Any], 'ParquetFile', pl.DataFrame, pl.LazyFrame, input_schema.RawData] = None,\n                 path_ref: str = None,\n                 name: str = None,\n                 optimize_memory: bool = True,\n                 schema: List['FlowfileColumn'] | List[str] | pl.Schema = None,\n                 number_of_records: int = None,\n                 calculate_schema_stats: bool = False,\n                 streamable: bool = True,\n                 number_of_records_callback: Callable = None,\n                 data_callback: Callable = None):\n        \"\"\"Initializes the FlowDataEngine from various data sources.\n\n        Args:\n            raw_data: The input data. Can be a list of dicts, a Polars DataFrame/LazyFrame,\n                or a `RawData` schema object.\n            path_ref: A string path to a Parquet file.\n            name: An optional name for the data engine instance.\n            optimize_memory: If True, prefers lazy operations to conserve memory.\n            schema: An optional schema definition. Can be a list of `FlowfileColumn` objects,\n                a list of column names, or a Polars `Schema`.\n            number_of_records: The number of records, if known.\n            calculate_schema_stats: If True, computes detailed statistics for each column.\n            streamable: If True, allows for streaming operations when possible.\n            number_of_records_callback: A callback function to retrieve the number of records.\n            data_callback: A callback function to retrieve the data.\n        \"\"\"\n        self._initialize_attributes(number_of_records_callback, data_callback, streamable)\n\n        if raw_data is not None:\n            self._handle_raw_data(raw_data, number_of_records, optimize_memory)\n        elif path_ref:\n            self._handle_path_ref(path_ref, optimize_memory)\n        else:\n            self.initialize_empty_fl()\n        self._finalize_initialization(name, optimize_memory, schema, calculate_schema_stats)\n\n    def _initialize_attributes(self, number_of_records_callback, data_callback, streamable):\n        \"\"\"(Internal) Sets the initial default attributes for a new instance.\n\n        This helper is called first during initialization to ensure all state-tracking\n        and configuration attributes have a clean default value before data is processed.\n        \"\"\"\n        self._external_source = None\n        self._number_of_records_callback = number_of_records_callback\n        self._data_callback = data_callback\n        self.ind_schema_calculated = False\n        self._streamable = streamable\n        self._org_path = None\n        self._lazy = False\n        self.errors = []\n        self._calculate_schema_stats = False\n        self.is_collected = True\n        self.is_future = False\n\n    def _handle_raw_data(self, raw_data, number_of_records, optimize_memory):\n        \"\"\"(Internal) Dispatches raw data to the appropriate handler based on its type.\n\n        This acts as a router during initialization, inspecting the type of `raw_data`\n        and calling the corresponding specialized `_handle_*` method to process it.\n        \"\"\"\n        if isinstance(raw_data, input_schema.RawData):\n            self._handle_raw_data_format(raw_data)\n        elif isinstance(raw_data, pl.DataFrame):\n            self._handle_polars_dataframe(raw_data, number_of_records)\n        elif isinstance(raw_data, pl.LazyFrame):\n            self._handle_polars_lazy_frame(raw_data, number_of_records, optimize_memory)\n        elif isinstance(raw_data, (list, dict)):\n            self._handle_python_data(raw_data)\n\n    def _handle_polars_dataframe(self, df: pl.DataFrame, number_of_records: Optional[int]):\n        \"\"\"(Internal) Initializes the engine from an eager Polars DataFrame.\"\"\"\n        self.data_frame = df\n        self.number_of_records = number_of_records or df.select(pl.len())[0, 0]\n\n    def _handle_polars_lazy_frame(self, lf: pl.LazyFrame, number_of_records: Optional[int], optimize_memory: bool):\n        \"\"\"(Internal) Initializes the engine from a Polars LazyFrame.\"\"\"\n        self.data_frame = lf\n        self._lazy = True\n        if number_of_records is not None:\n            self.number_of_records = number_of_records\n        elif optimize_memory:\n            self.number_of_records = -1\n        else:\n            self.number_of_records = lf.select(pl.len()).collect()[0, 0]\n\n    def _handle_python_data(self, data: Union[List, Dict]):\n        \"\"\"(Internal) Dispatches Python collections to the correct handler.\"\"\"\n        if isinstance(data, dict):\n            self._handle_dict_input(data)\n        else:\n            self._handle_list_input(data)\n\n    def _handle_dict_input(self, data: Dict):\n        \"\"\"(Internal) Initializes the engine from a Python dictionary.\"\"\"\n        if len(data) == 0:\n            self.initialize_empty_fl()\n        lengths = [len(v) if isinstance(v, (list, tuple)) else 1 for v in data.values()]\n\n        if len(set(lengths)) == 1 and lengths[0] &gt; 1:\n            self.number_of_records = lengths[0]\n            self.data_frame = pl.DataFrame(data)\n        else:\n            self.number_of_records = 1\n            self.data_frame = pl.DataFrame([data])\n        self.lazy = True\n\n    def _handle_raw_data_format(self, raw_data: input_schema.RawData):\n        \"\"\"(Internal) Initializes the engine from a `RawData` schema object.\n\n        This method uses the schema provided in the `RawData` object to correctly\n        infer data types when creating the Polars DataFrame.\n\n        Args:\n            raw_data: An instance of `RawData` containing the data and schema.\n        \"\"\"\n        flowfile_schema = list(FlowfileColumn.create_from_minimal_field_info(c) for c in raw_data.columns)\n        polars_schema = pl.Schema([(flowfile_column.column_name, flowfile_column.get_polars_type().pl_datatype)\n                                   for flowfile_column in flowfile_schema])\n        try:\n            df = pl.DataFrame(raw_data.data, polars_schema, strict=False)\n        except TypeError as e:\n            logger.warning(f\"Could not parse the data with the schema:\\n{e}\")\n            df = pl.DataFrame(raw_data.data)\n        self.number_of_records = len(df)\n        self.data_frame = df.lazy()\n        self.lazy = True\n\n    def _handle_list_input(self, data: List):\n        \"\"\"(Internal) Initializes the engine from a list of records.\"\"\"\n        number_of_records = len(data)\n        if number_of_records &gt; 0:\n            processed_data = self._process_list_data(data)\n            self.number_of_records = number_of_records\n            self.data_frame = pl.DataFrame(processed_data)\n            self.lazy = True\n        else:\n            self.initialize_empty_fl()\n            self.number_of_records = 0\n\n    @staticmethod\n    def _process_list_data(data: List) -&gt; List[Dict]:\n        \"\"\"(Internal) Normalizes list data into a list of dictionaries.\n\n        Ensures that a list of objects or non-dict items is converted into a\n        uniform list of dictionaries suitable for Polars DataFrame creation.\n        \"\"\"\n        if not (isinstance(data[0], dict) or hasattr(data[0], '__dict__')):\n            try:\n                return pl.DataFrame(data).to_dicts()\n            except TypeError:\n                raise Exception('Value must be able to be converted to dictionary')\n            except Exception as e:\n                raise Exception(f'Value must be able to be converted to dictionary: {e}')\n\n        if not isinstance(data[0], dict):\n            data = [row.__dict__ for row in data]\n\n        return ensure_similarity_dicts(data)\n\n    def to_cloud_storage_obj(self, settings: cloud_storage_schemas.CloudStorageWriteSettingsInternal):\n        \"\"\"Writes the DataFrame to an object in cloud storage.\n\n        This method supports writing to various cloud storage providers like AWS S3,\n        Azure Data Lake Storage, and Google Cloud Storage.\n\n        Args:\n            settings: A `CloudStorageWriteSettingsInternal` object containing connection\n                details, file format, and write options.\n\n        Raises:\n            ValueError: If the specified file format is not supported for writing.\n            NotImplementedError: If the 'append' write mode is used with an unsupported format.\n            Exception: If the write operation to cloud storage fails for any reason.\n        \"\"\"\n        connection = settings.connection\n        write_settings = settings.write_settings\n\n        logger.info(f\"Writing to {connection.storage_type} storage: {write_settings.resource_path}\")\n\n        if write_settings.write_mode == 'append' and write_settings.file_format != \"delta\":\n            raise NotImplementedError(\"The 'append' write mode is not yet supported for this destination.\")\n        storage_options = CloudStorageReader.get_storage_options(connection)\n        credential_provider = CloudStorageReader.get_credential_provider(connection)\n        # Dispatch to the correct writer based on file format\n        if write_settings.file_format == \"parquet\":\n            self._write_parquet_to_cloud(\n                write_settings.resource_path,\n                storage_options,\n                credential_provider,\n                write_settings\n            )\n        elif write_settings.file_format == \"delta\":\n            self._write_delta_to_cloud(\n                write_settings.resource_path,\n                storage_options,\n                credential_provider,\n                write_settings\n            )\n        elif write_settings.file_format == \"csv\":\n            self._write_csv_to_cloud(\n                write_settings.resource_path,\n                storage_options,\n                credential_provider,\n                write_settings\n            )\n        elif write_settings.file_format == \"json\":\n            self._write_json_to_cloud(\n                write_settings.resource_path,\n                storage_options,\n                credential_provider,\n                write_settings\n            )\n        else:\n            raise ValueError(f\"Unsupported file format for writing: {write_settings.file_format}\")\n\n        logger.info(f\"Successfully wrote data to {write_settings.resource_path}\")\n\n    def _write_parquet_to_cloud(self,\n                                resource_path: str,\n                                storage_options: Dict[str, Any],\n                                credential_provider: Optional[Callable],\n                                write_settings: cloud_storage_schemas.CloudStorageWriteSettings):\n        \"\"\"(Internal) Writes the DataFrame to a Parquet file in cloud storage.\n\n        Uses `sink_parquet` for efficient streaming writes. Falls back to a\n        collect-then-write pattern if sinking fails.\n        \"\"\"\n        try:\n            sink_kwargs = {\n                \"path\": resource_path,\n                \"compression\": write_settings.parquet_compression,\n            }\n            if storage_options:\n                sink_kwargs[\"storage_options\"] = storage_options\n            if credential_provider:\n                sink_kwargs[\"credential_provider\"] = credential_provider\n            try:\n                self.data_frame.sink_parquet(**sink_kwargs)\n            except Exception as e:\n                logger.warning(f\"Failed to sink the data, falling back to collecing and writing. \\n {e}\")\n                pl_df = self.collect()\n                sink_kwargs['file'] = sink_kwargs.pop(\"path\")\n                pl_df.write_parquet(**sink_kwargs)\n\n        except Exception as e:\n            logger.error(f\"Failed to write Parquet to {resource_path}: {str(e)}\")\n            raise Exception(f\"Failed to write Parquet to cloud storage: {str(e)}\")\n\n    def _write_delta_to_cloud(self,\n                              resource_path: str,\n                              storage_options: Dict[str, Any],\n                              credential_provider: Optional[Callable],\n                              write_settings: cloud_storage_schemas.CloudStorageWriteSettings):\n        \"\"\"(Internal) Writes the DataFrame to a Delta Lake table in cloud storage.\n\n        This operation requires collecting the data first, as `write_delta` operates\n        on an eager DataFrame.\n        \"\"\"\n        sink_kwargs = {\n            \"target\": resource_path,\n            \"mode\": write_settings.write_mode,\n        }\n        if storage_options:\n            sink_kwargs[\"storage_options\"] = storage_options\n        if credential_provider:\n            sink_kwargs[\"credential_provider\"] = credential_provider\n        self.collect().write_delta(**sink_kwargs)\n\n    def _write_csv_to_cloud(self,\n                            resource_path: str,\n                            storage_options: Dict[str, Any],\n                            credential_provider: Optional[Callable],\n                            write_settings: cloud_storage_schemas.CloudStorageWriteSettings):\n        \"\"\"(Internal) Writes the DataFrame to a CSV file in cloud storage.\n\n        Uses `sink_csv` for efficient, streaming writes of the data.\n        \"\"\"\n        try:\n            sink_kwargs = {\n                \"path\": resource_path,\n                \"separator\": write_settings.csv_delimiter,\n            }\n            if storage_options:\n                sink_kwargs[\"storage_options\"] = storage_options\n            if credential_provider:\n                sink_kwargs[\"credential_provider\"] = credential_provider\n\n            # sink_csv executes the lazy query and writes the result\n            self.data_frame.sink_csv(**sink_kwargs)\n\n        except Exception as e:\n            logger.error(f\"Failed to write CSV to {resource_path}: {str(e)}\")\n            raise Exception(f\"Failed to write CSV to cloud storage: {str(e)}\")\n\n    def _write_json_to_cloud(self,\n                             resource_path: str,\n                             storage_options: Dict[str, Any],\n                             credential_provider: Optional[Callable],\n                             write_settings: cloud_storage_schemas.CloudStorageWriteSettings):\n        \"\"\"(Internal) Writes the DataFrame to a line-delimited JSON (NDJSON) file.\n\n        Uses `sink_ndjson` for efficient, streaming writes.\n        \"\"\"\n        try:\n            sink_kwargs = {\"path\": resource_path}\n            if storage_options:\n                sink_kwargs[\"storage_options\"] = storage_options\n            if credential_provider:\n                sink_kwargs[\"credential_provider\"] = credential_provider\n            self.data_frame.sink_ndjson(**sink_kwargs)\n\n        except Exception as e:\n            logger.error(f\"Failed to write JSON to {resource_path}: {str(e)}\")\n            raise Exception(f\"Failed to write JSON to cloud storage: {str(e)}\")\n\n    @classmethod\n    def from_cloud_storage_obj(cls, settings: cloud_storage_schemas.CloudStorageReadSettingsInternal) -&gt; \"FlowDataEngine\":\n        \"\"\"Creates a FlowDataEngine from an object in cloud storage.\n\n        This method supports reading from various cloud storage providers like AWS S3,\n        Azure Data Lake Storage, and Google Cloud Storage, with support for\n        various authentication methods.\n\n        Args:\n            settings: A `CloudStorageReadSettingsInternal` object containing connection\n                details, file format, and read options.\n\n        Returns:\n            A new `FlowDataEngine` instance containing the data from cloud storage.\n\n        Raises:\n            ValueError: If the storage type or file format is not supported.\n            NotImplementedError: If a requested file format like \"delta\" or \"iceberg\"\n                is not yet implemented.\n            Exception: If reading from cloud storage fails.\n        \"\"\"\n        connection = settings.connection\n        read_settings = settings.read_settings\n\n        logger.info(f\"Reading from {connection.storage_type} storage: {read_settings.resource_path}\")\n        # Get storage options based on connection type\n        storage_options = CloudStorageReader.get_storage_options(connection)\n        # Get credential provider if needed\n        credential_provider = CloudStorageReader.get_credential_provider(connection)\n        if read_settings.file_format == \"parquet\":\n            return cls._read_parquet_from_cloud(\n                read_settings.resource_path,\n                storage_options,\n                credential_provider,\n                read_settings.scan_mode == \"directory\",\n            )\n        elif read_settings.file_format == \"delta\":\n            return cls._read_delta_from_cloud(\n                read_settings.resource_path,\n                storage_options,\n                credential_provider,\n                read_settings\n            )\n        elif read_settings.file_format == \"csv\":\n            return cls._read_csv_from_cloud(\n                read_settings.resource_path,\n                storage_options,\n                credential_provider,\n                read_settings\n            )\n        elif read_settings.file_format == \"json\":\n            return cls._read_json_from_cloud(\n                read_settings.resource_path,\n                storage_options,\n                credential_provider,\n                read_settings.scan_mode == \"directory\"\n            )\n        elif read_settings.file_format == \"iceberg\":\n            return cls._read_iceberg_from_cloud(\n                read_settings.resource_path,\n                storage_options,\n                credential_provider,\n                read_settings\n            )\n\n        elif read_settings.file_format in [\"delta\", \"iceberg\"]:\n            # These would require additional libraries\n            raise NotImplementedError(f\"File format {read_settings.file_format} not yet implemented\")\n        else:\n            raise ValueError(f\"Unsupported file format: {read_settings.file_format}\")\n\n    @staticmethod\n    def _get_schema_from_first_file_in_dir(source: str, storage_options: Dict[str, Any],\n                                           file_format: Literal[\"csv\", \"parquet\", \"json\", \"delta\"]) -&gt; List[FlowfileColumn] | None:\n        \"\"\"Infers the schema by scanning the first file in a cloud directory.\"\"\"\n        try:\n            scan_func = getattr(pl, \"scan_\" + file_format)\n            first_file_ref = get_first_file_from_s3_dir(source, storage_options=storage_options)\n            return convert_stats_to_column_info(FlowDataEngine._create_schema_stats_from_pl_schema(\n                scan_func(first_file_ref, storage_options=storage_options).collect_schema()))\n        except Exception as e:\n            logger.warning(f\"Could not read schema from first file in directory, using default schema: {e}\")\n\n\n    @classmethod\n    def _read_iceberg_from_cloud(cls,\n                                 resource_path: str,\n                                 storage_options: Dict[str, Any],\n                                 credential_provider: Optional[Callable],\n                                 read_settings: cloud_storage_schemas.CloudStorageReadSettings) -&gt; \"FlowDataEngine\":\n        \"\"\"Reads Iceberg table(s) from cloud storage.\"\"\"\n        raise NotImplementedError(f\"Failed to read Iceberg table from cloud storage: Not yet implemented\")\n\n    @classmethod\n    def _read_parquet_from_cloud(cls,\n                                 resource_path: str,\n                                 storage_options: Dict[str, Any],\n                                 credential_provider: Optional[Callable],\n                                 is_directory: bool) -&gt; \"FlowDataEngine\":\n        \"\"\"Reads Parquet file(s) from cloud storage.\"\"\"\n        try:\n            # Use scan_parquet for lazy evaluation\n            if is_directory:\n                resource_path = ensure_path_has_wildcard_pattern(resource_path=resource_path, file_format=\"parquet\")\n            scan_kwargs = {\"source\": resource_path}\n\n            if storage_options:\n                scan_kwargs[\"storage_options\"] = storage_options\n\n            if credential_provider:\n                scan_kwargs[\"credential_provider\"] = credential_provider\n            if storage_options and is_directory:\n                schema = cls._get_schema_from_first_file_in_dir(resource_path, storage_options, \"parquet\")\n            else:\n                schema = None\n            lf = pl.scan_parquet(**scan_kwargs)\n\n            return cls(\n                lf,\n                number_of_records=6_666_666,  # Set so the provider is not accessed for this stat\n                optimize_memory=True,\n                streamable=True,\n                schema=schema\n            )\n\n        except Exception as e:\n            logger.error(f\"Failed to read Parquet from {resource_path}: {str(e)}\")\n            raise Exception(f\"Failed to read Parquet from cloud storage: {str(e)}\")\n\n    @classmethod\n    def _read_delta_from_cloud(cls,\n                               resource_path: str,\n                               storage_options: Dict[str, Any],\n                               credential_provider: Optional[Callable],\n                               read_settings: cloud_storage_schemas.CloudStorageReadSettings) -&gt; \"FlowDataEngine\":\n        \"\"\"Reads a Delta Lake table from cloud storage.\"\"\"\n        try:\n            logger.info(\"Reading Delta file from cloud storage...\")\n            logger.info(f\"read_settings: {read_settings}\")\n            scan_kwargs = {\"source\": resource_path}\n            if read_settings.delta_version:\n                scan_kwargs['version'] = read_settings.delta_version\n            if storage_options:\n                scan_kwargs[\"storage_options\"] = storage_options\n            if credential_provider:\n                scan_kwargs[\"credential_provider\"] = credential_provider\n            lf = pl.scan_delta(**scan_kwargs)\n\n            return cls(\n                lf,\n                number_of_records=6_666_666,  # Set so the provider is not accessed for this stat\n                optimize_memory=True,\n                streamable=True\n            )\n        except Exception as e:\n            logger.error(f\"Failed to read Delta file from {resource_path}: {str(e)}\")\n            raise Exception(f\"Failed to read Delta file from cloud storage: {str(e)}\")\n\n    @classmethod\n    def _read_csv_from_cloud(cls,\n                             resource_path: str,\n                             storage_options: Dict[str, Any],\n                             credential_provider: Optional[Callable],\n                             read_settings: cloud_storage_schemas.CloudStorageReadSettings) -&gt; \"FlowDataEngine\":\n        \"\"\"Reads CSV file(s) from cloud storage.\"\"\"\n        try:\n            scan_kwargs = {\n                \"source\": resource_path,\n                \"has_header\": read_settings.csv_has_header,\n                \"separator\": read_settings.csv_delimiter,\n                \"encoding\": read_settings.csv_encoding,\n            }\n            if storage_options:\n                scan_kwargs[\"storage_options\"] = storage_options\n            if credential_provider:\n                scan_kwargs[\"credential_provider\"] = credential_provider\n\n            if read_settings.scan_mode == \"directory\":\n                resource_path = ensure_path_has_wildcard_pattern(resource_path=resource_path, file_format=\"csv\")\n                scan_kwargs[\"source\"] = resource_path\n            if storage_options and read_settings.scan_mode == \"directory\":\n                schema = cls._get_schema_from_first_file_in_dir(resource_path, storage_options, \"csv\")\n            else:\n                schema = None\n\n            lf = pl.scan_csv(**scan_kwargs)\n\n            return cls(\n                lf,\n                number_of_records=6_666_666,  # Will be calculated lazily\n                optimize_memory=True,\n                streamable=True,\n                schema=schema\n            )\n\n        except Exception as e:\n            logger.error(f\"Failed to read CSV from {resource_path}: {str(e)}\")\n            raise Exception(f\"Failed to read CSV from cloud storage: {str(e)}\")\n\n    @classmethod\n    def _read_json_from_cloud(cls,\n                              resource_path: str,\n                              storage_options: Dict[str, Any],\n                              credential_provider: Optional[Callable],\n                              is_directory: bool) -&gt; \"FlowDataEngine\":\n        \"\"\"Reads JSON file(s) from cloud storage.\"\"\"\n        try:\n            if is_directory:\n                resource_path = ensure_path_has_wildcard_pattern(resource_path, \"json\")\n            scan_kwargs = {\"source\": resource_path}\n\n            if storage_options:\n                scan_kwargs[\"storage_options\"] = storage_options\n            if credential_provider:\n                scan_kwargs[\"credential_provider\"] = credential_provider\n\n            lf = pl.scan_ndjson(**scan_kwargs)  # Using NDJSON for line-delimited JSON\n\n            return cls(\n                lf,\n                number_of_records=-1,\n                optimize_memory=True,\n                streamable=True,\n            )\n\n        except Exception as e:\n            logger.error(f\"Failed to read JSON from {resource_path}: {str(e)}\")\n            raise Exception(f\"Failed to read JSON from cloud storage: {str(e)}\")\n\n    def _handle_path_ref(self, path_ref: str, optimize_memory: bool):\n        \"\"\"Handles file path reference input.\"\"\"\n        try:\n            pf = ParquetFile(path_ref)\n        except Exception as e:\n            logger.error(e)\n            raise Exception(\"Provided ref is not a parquet file\")\n\n        self.number_of_records = pf.metadata.num_rows\n        if optimize_memory:\n            self._lazy = True\n            self.data_frame = pl.scan_parquet(path_ref)\n        else:\n            self.data_frame = pl.read_parquet(path_ref)\n\n    def _finalize_initialization(self, name: str, optimize_memory: bool, schema: Optional[Any],\n                                 calculate_schema_stats: bool):\n        \"\"\"Finalizes initialization by setting remaining attributes.\"\"\"\n        _ = calculate_schema_stats\n        self.name = name\n        self._optimize_memory = optimize_memory\n        if assert_if_flowfile_schema(schema):\n            self._schema = schema\n            self.columns = [c.column_name for c in self._schema]\n        else:\n            pl_schema = self.data_frame.collect_schema()\n            self._schema = self._handle_schema(schema, pl_schema)\n            self.columns = [c.column_name for c in self._schema] if self._schema else pl_schema.names()\n\n    def __getitem__(self, item):\n        \"\"\"Accesses a specific column or item from the DataFrame.\"\"\"\n        return self.data_frame.select([item])\n\n    @property\n    def data_frame(self) -&gt; pl.LazyFrame | pl.DataFrame | None:\n        \"\"\"The underlying Polars DataFrame or LazyFrame.\n\n        This property provides access to the Polars object that backs the\n        FlowDataEngine. It handles lazy-loading from external sources if necessary.\n\n        Returns:\n            The active Polars `DataFrame` or `LazyFrame`.\n        \"\"\"\n        if self._data_frame is not None and not self.is_future:\n            return self._data_frame\n        elif self.is_future:\n            return self._data_frame\n        elif self._external_source is not None and self.lazy:\n            return self._data_frame\n        elif self._external_source is not None and not self.lazy:\n            if self._external_source.get_pl_df() is None:\n                data_frame = list(self._external_source.get_iter())\n                if len(data_frame) &gt; 0:\n                    self.data_frame = pl.DataFrame(data_frame)\n            else:\n                self.data_frame = self._external_source.get_pl_df()\n            self.calculate_schema()\n            return self._data_frame\n\n    @data_frame.setter\n    def data_frame(self, df: pl.LazyFrame | pl.DataFrame):\n        \"\"\"Sets the underlying Polars DataFrame or LazyFrame.\"\"\"\n        if self.lazy and isinstance(df, pl.DataFrame):\n            raise Exception('Cannot set a non-lazy dataframe to a lazy flowfile')\n        self._data_frame = df\n\n    @staticmethod\n    def _create_schema_stats_from_pl_schema(pl_schema: pl.Schema) -&gt; List[Dict]:\n        \"\"\"Converts a Polars Schema into a list of schema statistics dictionaries.\"\"\"\n        return [\n            dict(column_name=k, pl_datatype=v, col_index=i)\n            for i, (k, v) in enumerate(pl_schema.items())\n        ]\n\n    def _add_schema_from_schema_stats(self, schema_stats: List[Dict]):\n        \"\"\"Populates the schema from a list of schema statistics dictionaries.\"\"\"\n        self._schema = convert_stats_to_column_info(schema_stats)\n\n    @property\n    def schema(self) -&gt; List[FlowfileColumn]:\n        \"\"\"The schema of the DataFrame as a list of `FlowfileColumn` objects.\n\n        This property lazily calculates the schema if it hasn't been determined yet.\n\n        Returns:\n            A list of `FlowfileColumn` objects describing the schema.\n        \"\"\"\n        if self.number_of_fields == 0:\n            return []\n        if self._schema is None or (self._calculate_schema_stats and not self.ind_schema_calculated):\n            if self._calculate_schema_stats and not self.ind_schema_calculated:\n                schema_stats = self._calculate_schema()\n                self.ind_schema_calculated = True\n            else:\n                schema_stats = self._create_schema_stats_from_pl_schema(self.data_frame.collect_schema())\n            self._add_schema_from_schema_stats(schema_stats)\n        return self._schema\n\n    @property\n    def number_of_fields(self) -&gt; int:\n        \"\"\"The number of columns (fields) in the DataFrame.\n\n        Returns:\n            The integer count of columns.\n        \"\"\"\n        if self.__number_of_fields is None:\n            self.__number_of_fields = len(self.columns)\n        return self.__number_of_fields\n\n    def collect(self, n_records: int = None) -&gt; pl.DataFrame:\n        \"\"\"Collects the data and returns it as a Polars DataFrame.\n\n        This method triggers the execution of the lazy query plan (if applicable)\n        and returns the result. It supports streaming to optimize memory usage\n        for large datasets.\n\n        Args:\n            n_records: The maximum number of records to collect. If None, all\n                records are collected.\n\n        Returns:\n            A Polars `DataFrame` containing the collected data.\n        \"\"\"\n        if n_records is None:\n            logger.info(f'Fetching all data for Table object \"{id(self)}\". Settings: streaming={self._streamable}')\n        else:\n            logger.info(f'Fetching {n_records} record(s) for Table object \"{id(self)}\". '\n                        f'Settings: streaming={self._streamable}')\n\n        if not self.lazy:\n            return self.data_frame\n\n        try:\n            return self._collect_data(n_records)\n        except Exception as e:\n            self.errors = [e]\n            return self._handle_collection_error(n_records)\n\n    def _collect_data(self, n_records: int = None) -&gt; pl.DataFrame:\n        \"\"\"Internal method to handle data collection logic.\"\"\"\n        if n_records is None:\n\n            self.collect_external()\n            if self._streamable:\n                try:\n                    logger.info('Collecting data in streaming mode')\n                    return self.data_frame.collect(engine=\"streaming\")\n                except PanicException:\n                    self._streamable = False\n\n            logger.info('Collecting data in non-streaming mode')\n            return self.data_frame.collect()\n\n        if self.external_source is not None:\n            return self._collect_from_external_source(n_records)\n\n        if self._streamable:\n            return self.data_frame.head(n_records).collect(engine=\"streaming\")\n        return self.data_frame.head(n_records).collect()\n\n    def _collect_from_external_source(self, n_records: int) -&gt; pl.DataFrame:\n        \"\"\"Handles collection from an external source.\"\"\"\n        if self.external_source.get_pl_df() is not None:\n            all_data = self.external_source.get_pl_df().head(n_records)\n            self.data_frame = all_data\n        else:\n            all_data = self.external_source.get_sample(n_records)\n            self.data_frame = pl.LazyFrame(all_data)\n        return self.data_frame\n\n    def _handle_collection_error(self, n_records: int) -&gt; pl.DataFrame:\n        \"\"\"Handles errors during collection by attempting partial collection.\"\"\"\n        n_records = 100000000 if n_records is None else n_records\n        ok_cols, error_cols = self._identify_valid_columns(n_records)\n\n        if len(ok_cols) &gt; 0:\n            return self._create_partial_dataframe(ok_cols, error_cols, n_records)\n        return self._create_empty_dataframe(n_records)\n\n    def _identify_valid_columns(self, n_records: int) -&gt; Tuple[List[str], List[Tuple[str, Any]]]:\n        \"\"\"Identifies which columns can be collected successfully.\"\"\"\n        ok_cols = []\n        error_cols = []\n        for c in self.columns:\n            try:\n                _ = self.data_frame.select(c).head(n_records).collect()\n                ok_cols.append(c)\n            except:\n                error_cols.append((c, self.data_frame.schema[c]))\n        return ok_cols, error_cols\n\n    def _create_partial_dataframe(self, ok_cols: List[str], error_cols: List[Tuple[str, Any]],\n                                  n_records: int) -&gt; pl.DataFrame:\n        \"\"\"Creates a DataFrame with partial data for columns that could be collected.\"\"\"\n        df = self.data_frame.select(ok_cols)\n        df = df.with_columns([\n            pl.lit(None).alias(column_name).cast(data_type)\n            for column_name, data_type in error_cols\n        ])\n        return df.select(self.columns).head(n_records).collect()\n\n    def _create_empty_dataframe(self, n_records: int) -&gt; pl.DataFrame:\n        \"\"\"Creates an empty DataFrame with the correct schema.\"\"\"\n        if self.number_of_records &gt; 0:\n            return pl.DataFrame({\n                column_name: pl.Series(\n                    name=column_name,\n                    values=[None] * min(self.number_of_records, n_records)\n                ).cast(data_type)\n                for column_name, data_type in self.data_frame.schema.items()\n            })\n        return pl.DataFrame(schema=self.data_frame.schema)\n\n    def do_group_by(self, group_by_input: transform_schemas.GroupByInput,\n                    calculate_schema_stats: bool = True) -&gt; \"FlowDataEngine\":\n        \"\"\"Performs a group-by operation on the DataFrame.\n\n        Args:\n            group_by_input: A `GroupByInput` object defining the grouping columns\n                and aggregations.\n            calculate_schema_stats: If True, calculates schema statistics for the\n                resulting DataFrame.\n\n        Returns:\n            A new `FlowDataEngine` instance with the grouped and aggregated data.\n        \"\"\"\n        aggregations = [c for c in group_by_input.agg_cols if c.agg != 'groupby']\n        group_columns = [c for c in group_by_input.agg_cols if c.agg == 'groupby']\n\n        if len(group_columns) == 0:\n            return FlowDataEngine(\n                self.data_frame.select(\n                    ac.agg_func(ac.old_name).alias(ac.new_name) for ac in aggregations\n                ),\n                calculate_schema_stats=calculate_schema_stats\n            )\n\n        df = self.data_frame.rename({c.old_name: c.new_name for c in group_columns})\n        group_by_columns = [n_c.new_name for n_c in group_columns]\n        return FlowDataEngine(\n            df.group_by(*group_by_columns).agg(\n                ac.agg_func(ac.old_name).alias(ac.new_name) for ac in aggregations\n            ),\n            calculate_schema_stats=calculate_schema_stats\n        )\n\n    def do_sort(self, sorts: List[transform_schemas.SortByInput]) -&gt; \"FlowDataEngine\":\n        \"\"\"Sorts the DataFrame by one or more columns.\n\n        Args:\n            sorts: A list of `SortByInput` objects, each specifying a column\n                and sort direction ('asc' or 'desc').\n\n        Returns:\n            A new `FlowDataEngine` instance with the sorted data.\n        \"\"\"\n        if not sorts:\n            return self\n\n        descending = [s.how == 'desc' or s.how.lower() == 'descending' for s in sorts]\n        df = self.data_frame.sort([sort_by.column for sort_by in sorts], descending=descending)\n        return FlowDataEngine(df, number_of_records=self.number_of_records, schema=self.schema)\n\n    def change_column_types(self, transforms: List[transform_schemas.SelectInput],\n                            calculate_schema: bool = False) -&gt; \"FlowDataEngine\":\n        \"\"\"Changes the data type of one or more columns.\n\n        Args:\n            transforms: A list of `SelectInput` objects, where each object specifies\n                the column and its new `polars_type`.\n            calculate_schema: If True, recalculates the schema after the type change.\n\n        Returns:\n            A new `FlowDataEngine` instance with the updated column types.\n        \"\"\"\n        dtypes = [dtype.base_type() for dtype in self.data_frame.collect_schema().dtypes()]\n        idx_mapping = list(\n            (transform.old_name, self.cols_idx.get(transform.old_name), getattr(pl, transform.polars_type))\n            for transform in transforms if transform.data_type is not None\n        )\n\n        actual_transforms = [c for c in idx_mapping if c[2] != dtypes[c[1]]]\n        transformations = [\n            utils.define_pl_col_transformation(col_name=transform[0], col_type=transform[2])\n            for transform in actual_transforms\n        ]\n\n        df = self.data_frame.with_columns(transformations)\n        return FlowDataEngine(\n            df,\n            number_of_records=self.number_of_records,\n            calculate_schema_stats=calculate_schema,\n            streamable=self._streamable\n        )\n\n    def save(self, path: str, data_type: str = 'parquet') -&gt; Future:\n        \"\"\"Saves the DataFrame to a file in a separate thread.\n\n        Args:\n            path: The file path to save to.\n            data_type: The format to save in (e.g., 'parquet', 'csv').\n\n        Returns:\n            A `loky.Future` object representing the asynchronous save operation.\n        \"\"\"\n        estimated_size = deepcopy(self.get_estimated_file_size() * 4)\n        df = deepcopy(self.data_frame)\n        return write_threaded(_df=df, path=path, data_type=data_type, estimated_size=estimated_size)\n\n    def to_pylist(self) -&gt; List[Dict]:\n        \"\"\"Converts the DataFrame to a list of Python dictionaries.\n\n        Returns:\n            A list where each item is a dictionary representing a row.\n        \"\"\"\n        if self.lazy:\n            return self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\").to_dicts()\n        return self.data_frame.to_dicts()\n\n    def to_arrow(self) -&gt; PaTable:\n        \"\"\"Converts the DataFrame to a PyArrow Table.\n\n        This method triggers a `.collect()` call if the data is lazy,\n        then converts the resulting eager DataFrame into a `pyarrow.Table`.\n\n        Returns:\n            A `pyarrow.Table` instance representing the data.\n        \"\"\"\n        if self.lazy:\n            return self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\").to_arrow()\n        else:\n            return self.data_frame.to_arrow()\n\n    def to_raw_data(self) -&gt; input_schema.RawData:\n        \"\"\"Converts the DataFrame to a `RawData` schema object.\n\n        Returns:\n            An `input_schema.RawData` object containing the schema and data.\n        \"\"\"\n        columns = [c.get_minimal_field_info() for c in self.schema]\n        data = list(self.to_dict().values())\n        return input_schema.RawData(columns=columns, data=data)\n\n    def to_dict(self) -&gt; Dict[str, List]:\n        \"\"\"Converts the DataFrame to a Python dictionary of columns.\n\n         Each key in the dictionary is a column name, and the corresponding value\n         is a list of the data in that column.\n\n         Returns:\n             A dictionary mapping column names to lists of their values.\n         \"\"\"\n        if self.lazy:\n            return self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\").to_dict(as_series=False)\n        else:\n            return self.data_frame.to_dict(as_series=False)\n\n    @classmethod\n    def create_from_external_source(cls, external_source: ExternalDataSource) -&gt; \"FlowDataEngine\":\n        \"\"\"Creates a FlowDataEngine from an external data source.\n\n        Args:\n            external_source: An object that conforms to the `ExternalDataSource`\n                interface.\n\n        Returns:\n            A new `FlowDataEngine` instance.\n        \"\"\"\n        if external_source.schema is not None:\n            ff = cls.create_from_schema(external_source.schema)\n        elif external_source.initial_data_getter is not None:\n            ff = cls(raw_data=external_source.initial_data_getter())\n        else:\n            ff = cls()\n        ff._external_source = external_source\n        return ff\n\n    @classmethod\n    def create_from_sql(cls, sql: str, conn: Any) -&gt; \"FlowDataEngine\":\n        \"\"\"Creates a FlowDataEngine by executing a SQL query.\n\n        Args:\n            sql: The SQL query string to execute.\n            conn: A database connection object or connection URI string.\n\n        Returns:\n            A new `FlowDataEngine` instance with the query result.\n        \"\"\"\n        return cls(pl.read_sql(sql, conn))\n\n    @classmethod\n    def create_from_schema(cls, schema: List[FlowfileColumn]) -&gt; \"FlowDataEngine\":\n        \"\"\"Creates an empty FlowDataEngine from a schema definition.\n\n        Args:\n            schema: A list of `FlowfileColumn` objects defining the schema.\n\n        Returns:\n            A new, empty `FlowDataEngine` instance with the specified schema.\n        \"\"\"\n        pl_schema = []\n        for i, flow_file_column in enumerate(schema):\n            pl_schema.append((flow_file_column.name, cast_str_to_polars_type(flow_file_column.data_type)))\n            schema[i].col_index = i\n        df = pl.LazyFrame(schema=pl_schema)\n        return cls(df, schema=schema, calculate_schema_stats=False, number_of_records=0)\n\n    @classmethod\n    def create_from_path(cls, received_table: input_schema.ReceivedTableBase) -&gt; \"FlowDataEngine\":\n        \"\"\"Creates a FlowDataEngine from a local file path.\n\n        Supports various file types like CSV, Parquet, and Excel.\n\n        Args:\n            received_table: A `ReceivedTableBase` object containing the file path\n                and format details.\n\n        Returns:\n            A new `FlowDataEngine` instance with data from the file.\n        \"\"\"\n        received_table.set_absolute_filepath()\n        file_type_handlers = {\n            'csv': create_funcs.create_from_path_csv,\n            'parquet': create_funcs.create_from_path_parquet,\n            'excel': create_funcs.create_from_path_excel\n        }\n\n        handler = file_type_handlers.get(received_table.file_type)\n        if not handler:\n            raise Exception(f'Cannot create from {received_table.file_type}')\n\n        flow_file = cls(handler(received_table))\n        flow_file._org_path = received_table.abs_file_path\n        return flow_file\n\n    @classmethod\n    def create_random(cls, number_of_records: int = 1000) -&gt; \"FlowDataEngine\":\n        \"\"\"Creates a FlowDataEngine with randomly generated data.\n\n        Useful for testing and examples.\n\n        Args:\n            number_of_records: The number of random records to generate.\n\n        Returns:\n            A new `FlowDataEngine` instance with fake data.\n        \"\"\"\n        return cls(create_fake_data(number_of_records))\n\n    @classmethod\n    def generate_enumerator(cls, length: int = 1000, output_name: str = 'output_column') -&gt; \"FlowDataEngine\":\n        \"\"\"Generates a FlowDataEngine with a single column containing a sequence of integers.\n\n        Args:\n            length: The number of integers to generate in the sequence.\n            output_name: The name of the output column.\n\n        Returns:\n            A new `FlowDataEngine` instance.\n        \"\"\"\n        if length &gt; 10_000_000:\n            length = 10_000_000\n        return cls(pl.LazyFrame().select((pl.int_range(0, length, dtype=pl.UInt32)).alias(output_name)))\n\n    def _handle_schema(self, schema: List[FlowfileColumn] | List[str] | pl.Schema | None,\n                       pl_schema: pl.Schema) -&gt; List[FlowfileColumn] | None:\n        \"\"\"Handles schema processing and validation during initialization.\"\"\"\n        if schema is None and pl_schema is not None:\n            return convert_stats_to_column_info(self._create_schema_stats_from_pl_schema(pl_schema))\n        elif schema is None and pl_schema is None:\n            return None\n        elif assert_if_flowfile_schema(schema) and pl_schema is None:\n            return schema\n        elif pl_schema is not None and schema is not None:\n            if schema.__len__() != pl_schema.__len__():\n                raise Exception(\n                    f'Schema does not match the data got {schema.__len__()} columns expected {pl_schema.__len__()}')\n            if isinstance(schema, pl.Schema):\n                return self._handle_polars_schema(schema, pl_schema)\n            elif isinstance(schema, list) and len(schema) == 0:\n                return []\n            elif isinstance(schema[0], str):\n                return self._handle_string_schema(schema, pl_schema)\n            return schema\n\n    def _handle_polars_schema(self, schema: pl.Schema, pl_schema: pl.Schema) -&gt; List[FlowfileColumn]:\n        \"\"\"Handles Polars schema conversion.\"\"\"\n        flow_file_columns = [\n            FlowfileColumn.create_from_polars_dtype(column_name=col_name, data_type=dtype)\n            for col_name, dtype in zip(schema.names(), schema.dtypes())\n        ]\n\n        select_arg = [\n            pl.col(o).alias(n).cast(schema_dtype)\n            for o, n, schema_dtype in zip(pl_schema.names(), schema.names(), schema.dtypes())\n        ]\n\n        self.data_frame = self.data_frame.select(select_arg)\n        return flow_file_columns\n\n    def _handle_string_schema(self, schema: List[str], pl_schema: pl.Schema) -&gt; List[FlowfileColumn]:\n        \"\"\"Handles string-based schema conversion.\"\"\"\n        flow_file_columns = [\n            FlowfileColumn.create_from_polars_dtype(column_name=col_name, data_type=dtype)\n            for col_name, dtype in zip(schema, pl_schema.dtypes())\n        ]\n\n        self.data_frame = self.data_frame.rename({\n            o: n for o, n in zip(pl_schema.names(), schema)\n        })\n\n        return flow_file_columns\n\n    def split(self, split_input: transform_schemas.TextToRowsInput) -&gt; \"FlowDataEngine\":\n        \"\"\"Splits a column's text values into multiple rows based on a delimiter.\n\n        This operation is often referred to as \"exploding\" the DataFrame, as it\n        increases the number of rows.\n\n        Args:\n            split_input: A `TextToRowsInput` object specifying the column to split,\n                the delimiter, and the output column name.\n\n        Returns:\n            A new `FlowDataEngine` instance with the exploded rows.\n        \"\"\"\n        output_column_name = (\n            split_input.output_column_name\n            if split_input.output_column_name\n            else split_input.column_to_split\n        )\n\n        split_value = (\n            split_input.split_fixed_value\n            if split_input.split_by_fixed_value\n            else pl.col(split_input.split_by_column)\n        )\n\n        df = (\n            self.data_frame.with_columns(\n                pl.col(split_input.column_to_split)\n                .str.split(by=split_value)\n                .alias(output_column_name)\n            )\n            .explode(output_column_name)\n        )\n\n        return FlowDataEngine(df)\n\n    def unpivot(self, unpivot_input: transform_schemas.UnpivotInput) -&gt; \"FlowDataEngine\":\n        \"\"\"Converts the DataFrame from a wide to a long format.\n\n        This is the inverse of a pivot operation, taking columns and transforming\n        them into `variable` and `value` rows.\n\n        Args:\n            unpivot_input: An `UnpivotInput` object specifying which columns to\n                unpivot and which to keep as index columns.\n\n        Returns:\n            A new, unpivoted `FlowDataEngine` instance.\n        \"\"\"\n        lf = self.data_frame\n\n        if unpivot_input.data_type_selector_expr is not None:\n            result = lf.unpivot(\n                on=unpivot_input.data_type_selector_expr(),\n                index=unpivot_input.index_columns\n            )\n        elif unpivot_input.value_columns is not None:\n            result = lf.unpivot(\n                on=unpivot_input.value_columns,\n                index=unpivot_input.index_columns\n            )\n        else:\n            result = lf.unpivot()\n\n        return FlowDataEngine(result)\n\n    def do_pivot(self, pivot_input: transform_schemas.PivotInput, node_logger: NodeLogger = None) -&gt; \"FlowDataEngine\":\n        \"\"\"Converts the DataFrame from a long to a wide format, aggregating values.\n\n        Args:\n            pivot_input: A `PivotInput` object defining the index, pivot, and value\n                columns, along with the aggregation logic.\n            node_logger: An optional logger for reporting warnings, e.g., if the\n                pivot column has too many unique values.\n\n        Returns:\n            A new, pivoted `FlowDataEngine` instance.\n        \"\"\"\n        # Get unique values for pivot columns\n        max_unique_vals = 200\n        new_cols_unique = fetch_unique_values(self.data_frame.select(pivot_input.pivot_column)\n                                              .unique()\n                                              .sort(pivot_input.pivot_column)\n                                              .limit(max_unique_vals).cast(pl.String))\n        if len(new_cols_unique) &gt;= max_unique_vals:\n            if node_logger:\n                node_logger.warning('Pivot column has too many unique values. Please consider using a different column.'\n                                    f' Max unique values: {max_unique_vals}')\n\n        if len(pivot_input.index_columns) == 0:\n            no_index_cols = True\n            pivot_input.index_columns = ['__temp__']\n            ff = self.apply_flowfile_formula('1', col_name='__temp__')\n        else:\n            no_index_cols = False\n            ff = self\n\n        # Perform pivot operations\n        index_columns = pivot_input.get_index_columns()\n        grouped_ff = ff.do_group_by(pivot_input.get_group_by_input(), False)\n        pivot_column = pivot_input.get_pivot_column()\n\n        input_df = grouped_ff.data_frame.with_columns(\n            pivot_column.cast(pl.String).alias(pivot_input.pivot_column)\n        )\n        number_of_aggregations = len(pivot_input.aggregations)\n        df = (\n            input_df.select(\n                *index_columns,\n                pivot_column,\n                pivot_input.get_values_expr()\n            )\n            .group_by(*index_columns)\n            .agg([\n                (pl.col('vals').filter(pivot_column == new_col_value))\n                .first()\n                .alias(new_col_value)\n                for new_col_value in new_cols_unique\n            ])\n            .select(\n                *index_columns,\n                *[\n                    pl.col(new_col).struct.field(agg).alias(f'{new_col + \"_\" + agg if number_of_aggregations &gt; 1 else new_col }')\n                    for new_col in new_cols_unique\n                    for agg in pivot_input.aggregations\n                ]\n            )\n        )\n\n        # Clean up temporary columns if needed\n        if no_index_cols:\n            df = df.drop('__temp__')\n            pivot_input.index_columns = []\n\n        return FlowDataEngine(df, calculate_schema_stats=False)\n\n    def do_filter(self, predicate: str) -&gt; \"FlowDataEngine\":\n        \"\"\"Filters rows based on a predicate expression.\n\n        Args:\n            predicate: A string containing a Polars expression that evaluates to\n                a boolean value.\n\n        Returns:\n            A new `FlowDataEngine` instance containing only the rows that match\n            the predicate.\n        \"\"\"\n        try:\n            f = to_expr(predicate)\n        except Exception as e:\n            logger.warning(f'Error in filter expression: {e}')\n            f = to_expr(\"False\")\n        df = self.data_frame.filter(f)\n        return FlowDataEngine(df, schema=self.schema, streamable=self._streamable)\n\n    def add_record_id(self, record_id_settings: transform_schemas.RecordIdInput) -&gt; \"FlowDataEngine\":\n        \"\"\"Adds a record ID (row number) column to the DataFrame.\n\n        Can generate a simple sequential ID or a grouped ID that resets for\n        each group.\n\n        Args:\n            record_id_settings: A `RecordIdInput` object specifying the output\n                column name, offset, and optional grouping columns.\n\n        Returns:\n            A new `FlowDataEngine` instance with the added record ID column.\n        \"\"\"\n        if record_id_settings.group_by and len(record_id_settings.group_by_columns) &gt; 0:\n            return self._add_grouped_record_id(record_id_settings)\n        return self._add_simple_record_id(record_id_settings)\n\n    def _add_grouped_record_id(self, record_id_settings: transform_schemas.RecordIdInput) -&gt; \"FlowDataEngine\":\n        \"\"\"Adds a record ID column with grouping.\"\"\"\n        select_cols = [pl.col(record_id_settings.output_column_name)] + [pl.col(c) for c in self.columns]\n\n        df = (\n            self.data_frame\n            .with_columns(pl.lit(1).alias(record_id_settings.output_column_name))\n            .with_columns(\n                (pl.cum_count(record_id_settings.output_column_name)\n                 .over(record_id_settings.group_by_columns) + record_id_settings.offset - 1)\n                .alias(record_id_settings.output_column_name)\n            )\n            .select(select_cols)\n        )\n\n        output_schema = [FlowfileColumn.from_input(record_id_settings.output_column_name, 'UInt64')]\n        output_schema.extend(self.schema)\n\n        return FlowDataEngine(df, schema=output_schema)\n\n    def _add_simple_record_id(self, record_id_settings: transform_schemas.RecordIdInput) -&gt; \"FlowDataEngine\":\n        \"\"\"Adds a simple sequential record ID column.\"\"\"\n        df = self.data_frame.with_row_index(\n            record_id_settings.output_column_name,\n            record_id_settings.offset\n        )\n\n        output_schema = [FlowfileColumn.from_input(record_id_settings.output_column_name, 'UInt64')]\n        output_schema.extend(self.schema)\n\n        return FlowDataEngine(df, schema=output_schema)\n\n    def get_schema_column(self, col_name: str) -&gt; FlowfileColumn:\n        \"\"\"Retrieves the schema information for a single column by its name.\n\n        Args:\n            col_name: The name of the column to retrieve.\n\n        Returns:\n            A `FlowfileColumn` object for the specified column, or `None` if not found.\n        \"\"\"\n        for s in self.schema:\n            if s.name == col_name:\n                return s\n\n    def get_estimated_file_size(self) -&gt; int:\n        \"\"\"Estimates the file size in bytes if the data originated from a local file.\n\n        This relies on the original path being tracked during file ingestion.\n\n        Returns:\n            The file size in bytes, or 0 if the original path is unknown.\n        \"\"\"\n        if self._org_path is not None:\n            return os.path.getsize(self._org_path)\n        return 0\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Returns a string representation of the FlowDataEngine.\"\"\"\n        return f'flow data engine\\n{self.data_frame.__repr__()}'\n\n    def __call__(self) -&gt; \"FlowDataEngine\":\n        \"\"\"Makes the class instance callable, returning itself.\"\"\"\n        return self\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of records in the table.\"\"\"\n        return self.number_of_records if self.number_of_records &gt;= 0 else self.get_number_of_records()\n\n    def cache(self) -&gt; \"FlowDataEngine\":\n        \"\"\"Caches the current DataFrame to disk and updates the internal reference.\n\n        This triggers a background process to write the current LazyFrame's result\n        to a temporary file. Subsequent operations on this `FlowDataEngine` instance\n        will read from the cached file, which can speed up downstream computations.\n\n        Returns:\n            The same `FlowDataEngine` instance, now backed by the cached data.\n        \"\"\"\n        edf = ExternalDfFetcher(lf=self.data_frame, file_ref=str(id(self)), wait_on_completion=False,\n                                flow_id=-1,\n                                node_id=-1)\n        logger.info('Caching data in background')\n        result = edf.get_result()\n        if isinstance(result, pl.LazyFrame):\n            logger.info('Data cached')\n            del self._data_frame\n            self.data_frame = result\n            logger.info('Data loaded from cache')\n        return self\n\n    def collect_external(self):\n        \"\"\"Materializes data from a tracked external source.\n\n        If the `FlowDataEngine` was created from an `ExternalDataSource`, this\n        method will trigger the data retrieval, update the internal `_data_frame`\n        to a `LazyFrame` of the collected data, and reset the schema to be\n        re-evaluated.\n        \"\"\"\n        if self._external_source is not None:\n            logger.info('Collecting external source')\n            if self.external_source.get_pl_df() is not None:\n                self.data_frame = self.external_source.get_pl_df().lazy()\n            else:\n                self.data_frame = pl.LazyFrame(list(self.external_source.get_iter()))\n            self._schema = None  # enforce reset schema\n\n    def get_output_sample(self, n_rows: int = 10) -&gt; List[Dict]:\n        \"\"\"Gets a sample of the data as a list of dictionaries.\n\n        This is typically used to display a preview of the data in a UI.\n\n        Args:\n            n_rows: The number of rows to sample.\n\n        Returns:\n            A list of dictionaries, where each dictionary represents a row.\n        \"\"\"\n        if self.number_of_records &gt; n_rows or self.number_of_records &lt; 0:\n            df = self.collect(n_rows)\n        else:\n            df = self.collect()\n        return df.to_dicts()\n\n    def __get_sample__(self, n_rows: int = 100, streamable: bool = True) -&gt; \"FlowDataEngine\":\n        \"\"\"Internal method to get a sample of the data.\"\"\"\n        if not self.lazy:\n            df = self.data_frame.lazy()\n        else:\n            df = self.data_frame\n\n        if streamable:\n            try:\n                df = df.head(n_rows).collect()\n            except Exception as e:\n                logger.warning(f'Error in getting sample: {e}')\n                df = df.head(n_rows).collect(engine=\"auto\")\n        else:\n            df = self.collect()\n        return FlowDataEngine(df, number_of_records=len(df), schema=self.schema)\n\n    def get_sample(self, n_rows: int = 100, random: bool = False, shuffle: bool = False,\n                   seed: int = None, execution_location: Optional[ExecutionLocationsLiteral] = None) -&gt; \"FlowDataEngine\":\n        \"\"\"Gets a sample of rows from the DataFrame.\n\n        Args:\n            n_rows: The number of rows to sample.\n            random: If True, performs random sampling. If False, takes the first n_rows.\n            shuffle: If True (and `random` is True), shuffles the data before sampling.\n            seed: A random seed for reproducibility.\n            execution_location: Location which is used to calculate the size of the dataframe\n        Returns:\n            A new `FlowDataEngine` instance containing the sampled data.\n        \"\"\"\n        logging.info(f'Getting sample of {n_rows} rows')\n\n        if random:\n            if self.lazy and self.external_source is not None:\n                self.collect_external()\n\n            if self.lazy and shuffle:\n                sample_df = (self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\")\n                             .sample(n_rows, seed=seed, shuffle=shuffle))\n            elif shuffle:\n                sample_df = self.data_frame.sample(n_rows, seed=seed, shuffle=shuffle)\n            else:\n                if execution_location is None:\n                    execution_location = get_global_execution_location()\n                n_rows = min(n_rows, self.get_number_of_records(\n                    calculate_in_worker_process=execution_location == \"remote\")\n                             )\n\n                every_n_records = ceil(self.number_of_records / n_rows)\n                sample_df = self.data_frame.gather_every(every_n_records)\n        else:\n            if self.external_source:\n                self.collect(n_rows)\n            sample_df = self.data_frame.head(n_rows)\n\n        return FlowDataEngine(sample_df, schema=self.schema)\n\n    def get_subset(self, n_rows: int = 100) -&gt; \"FlowDataEngine\":\n        \"\"\"Gets the first `n_rows` from the DataFrame.\n\n        Args:\n            n_rows: The number of rows to include in the subset.\n\n        Returns:\n            A new `FlowDataEngine` instance containing the subset of data.\n        \"\"\"\n        if not self.lazy:\n            return FlowDataEngine(self.data_frame.head(n_rows), calculate_schema_stats=True)\n        else:\n            return FlowDataEngine(self.data_frame.head(n_rows), calculate_schema_stats=True)\n\n    def iter_batches(self, batch_size: int = 1000,\n                     columns: Union[List, Tuple, str] = None) -&gt; Generator[\"FlowDataEngine\", None, None]:\n        \"\"\"Iterates over the DataFrame in batches.\n\n        Args:\n            batch_size: The size of each batch.\n            columns: A list of column names to include in the batches. If None,\n                all columns are included.\n\n        Yields:\n            A `FlowDataEngine` instance for each batch.\n        \"\"\"\n        if columns:\n            self.data_frame = self.data_frame.select(columns)\n        self.lazy = False\n        batches = self.data_frame.iter_slices(batch_size)\n        for batch in batches:\n            yield FlowDataEngine(batch)\n\n    def start_fuzzy_join(self, fuzzy_match_input: transform_schemas.FuzzyMatchInput,\n                         other: \"FlowDataEngine\", file_ref: str, flow_id: int = -1,\n                         node_id: int | str = -1) -&gt; ExternalFuzzyMatchFetcher:\n        \"\"\"Starts a fuzzy join operation in a background process.\n\n        This method prepares the data and initiates the fuzzy matching in a\n        separate process, returning a tracker object immediately.\n\n        Args:\n            fuzzy_match_input: A `FuzzyMatchInput` object with the matching parameters.\n            other: The right `FlowDataEngine` to join with.\n            file_ref: A reference string for temporary files.\n            flow_id: The flow ID for tracking.\n            node_id: The node ID for tracking.\n\n        Returns:\n            An `ExternalFuzzyMatchFetcher` object that can be used to track the\n            progress and retrieve the result of the fuzzy join.\n        \"\"\"\n        left_df, right_df = prepare_for_fuzzy_match(left=self, right=other, fuzzy_match_input=fuzzy_match_input)\n        return ExternalFuzzyMatchFetcher(left_df, right_df,\n                                         fuzzy_maps=fuzzy_match_input.fuzzy_maps,\n                                         file_ref=file_ref + '_fm',\n                                         wait_on_completion=False,\n                                         flow_id=flow_id,\n                                         node_id=node_id)\n\n    def fuzzy_join_external(self,\n                            fuzzy_match_input: transform_schemas.FuzzyMatchInput,\n                            other: \"FlowDataEngine\",\n                            file_ref: str = None,\n                            flow_id: int = -1,\n                            node_id: int = -1\n                            ):\n        if file_ref is None:\n            file_ref = str(id(self)) + '_' + str(id(other))\n\n        left_df, right_df = prepare_for_fuzzy_match(left=self, right=other, fuzzy_match_input=fuzzy_match_input)\n        external_tracker = ExternalFuzzyMatchFetcher(left_df, right_df,\n                                                     fuzzy_maps=fuzzy_match_input.fuzzy_maps,\n                                                     file_ref=file_ref + '_fm',\n                                                     wait_on_completion=False,\n                                                     flow_id=flow_id,\n                                                     node_id=node_id)\n        return FlowDataEngine(external_tracker.get_result())\n\n    def fuzzy_join(self, fuzzy_match_input: transform_schemas.FuzzyMatchInput,\n                   other: \"FlowDataEngine\",\n                   node_logger: NodeLogger = None) -&gt; \"FlowDataEngine\":\n        left_df, right_df = prepare_for_fuzzy_match(left=self, right=other, fuzzy_match_input=fuzzy_match_input)\n        fuzzy_mappings = [FuzzyMapping(**fm.__dict__) for fm in fuzzy_match_input.fuzzy_maps]\n        return FlowDataEngine(fuzzy_match_dfs(left_df, right_df, fuzzy_maps=fuzzy_mappings,\n                                              logger=node_logger.logger if node_logger else logger)\n                              .lazy())\n\n    def do_cross_join(self, cross_join_input: transform_schemas.CrossJoinInput,\n                      auto_generate_selection: bool, verify_integrity: bool,\n                      other: \"FlowDataEngine\") -&gt; \"FlowDataEngine\":\n        \"\"\"Performs a cross join with another DataFrame.\n\n        A cross join produces the Cartesian product of the two DataFrames.\n\n        Args:\n            cross_join_input: A `CrossJoinInput` object specifying column selections.\n            auto_generate_selection: If True, automatically renames columns to avoid conflicts.\n            verify_integrity: If True, checks if the resulting join would be too large.\n            other: The right `FlowDataEngine` to join with.\n\n        Returns:\n            A new `FlowDataEngine` with the result of the cross join.\n\n        Raises:\n            Exception: If `verify_integrity` is True and the join would result in\n                an excessively large number of records.\n        \"\"\"\n\n        self.lazy = True\n\n        other.lazy = True\n\n        verify_join_select_integrity(cross_join_input, left_columns=self.columns, right_columns=other.columns)\n        right_select = [v.old_name for v in cross_join_input.right_select.renames\n                        if (v.keep or v.join_key) and v.is_available]\n        left_select = [v.old_name for v in cross_join_input.left_select.renames\n                       if (v.keep or v.join_key) and v.is_available]\n\n        left = self.data_frame.select(left_select).rename(cross_join_input.left_select.rename_table)\n        right = other.data_frame.select(right_select).rename(cross_join_input.right_select.rename_table)\n\n        joined_df = left.join(right, how='cross')\n\n        cols_to_delete_after = [col.new_name for col in\n                                cross_join_input.left_select.renames + cross_join_input.left_select.renames\n                                if col.join_key and not col.keep and col.is_available]\n\n        fl = FlowDataEngine(joined_df.drop(cols_to_delete_after), calculate_schema_stats=False, streamable=False)\n        return fl\n\n    def join(self, join_input: transform_schemas.JoinInput, auto_generate_selection: bool,\n             verify_integrity: bool, other: \"FlowDataEngine\") -&gt; \"FlowDataEngine\":\n        \"\"\"Performs a standard SQL-style join with another DataFrame.\n\n        Supports various join types like 'inner', 'left', 'right', 'outer', 'semi', and 'anti'.\n\n        Args:\n            join_input: A `JoinInput` object defining the join keys, join type,\n                and column selections.\n            auto_generate_selection: If True, automatically handles column renaming.\n            verify_integrity: If True, performs checks to prevent excessively large joins.\n            other: The right `FlowDataEngine` to join with.\n\n        Returns:\n            A new `FlowDataEngine` with the joined data.\n\n        Raises:\n            Exception: If the join configuration is invalid or if `verify_integrity`\n                is True and the join is predicted to be too large.\n        \"\"\"\n        ensure_right_unselect_for_semi_and_anti_joins(join_input)\n        verify_join_select_integrity(join_input, left_columns=self.columns, right_columns=other.columns)\n        if not verify_join_map_integrity(join_input, left_columns=self.schema, right_columns=other.schema):\n            raise Exception('Join is not valid by the data fields')\n        if auto_generate_selection:\n            join_input.auto_rename()\n        left = self.data_frame.select(get_select_columns(join_input.left_select.renames)).rename(join_input.left_select.rename_table)\n        right = other.data_frame.select(get_select_columns(join_input.right_select.renames)).rename(join_input.right_select.rename_table)\n        if verify_integrity and join_input.how != 'right':\n            n_records = get_join_count(left, right, left_on_keys=join_input.left_join_keys,\n                                       right_on_keys=join_input.right_join_keys, how=join_input.how)\n            if n_records &gt; 1_000_000_000:\n                raise Exception(\"Join will result in too many records, ending process\")\n        else:\n            n_records = -1\n        left, right, reverse_join_key_mapping = _handle_duplication_join_keys(left, right, join_input)\n        left, right = rename_df_table_for_join(left, right, join_input.get_join_key_renames())\n        if join_input.how == 'right':\n            joined_df = right.join(\n                other=left,\n                left_on=join_input.right_join_keys,\n                right_on=join_input.left_join_keys,\n                how=\"left\",\n                suffix=\"\").rename(reverse_join_key_mapping)\n        else:\n            joined_df = left.join(\n                other=right,\n                left_on=join_input.left_join_keys,\n                right_on=join_input.right_join_keys,\n                how=join_input.how,\n                suffix=\"\").rename(reverse_join_key_mapping)\n        left_cols_to_delete_after = [get_col_name_to_delete(col, 'left') for col in join_input.left_select.renames\n                                     if not col.keep\n                                     and col.is_available and col.join_key\n                                     ]\n        right_cols_to_delete_after = [get_col_name_to_delete(col, 'right') for col in join_input.right_select.renames\n                                      if not col.keep\n                                      and col.is_available and col.join_key\n                                      and join_input.how in (\"left\", \"right\", \"inner\", \"cross\", \"outer\")\n                                      ]\n        if len(right_cols_to_delete_after + left_cols_to_delete_after) &gt; 0:\n            joined_df = joined_df.drop(left_cols_to_delete_after + right_cols_to_delete_after)\n        undo_join_key_remapping = get_undo_rename_mapping_join(join_input)\n        joined_df = joined_df.rename(undo_join_key_remapping)\n\n        if verify_integrity:\n            return FlowDataEngine(joined_df, calculate_schema_stats=True,\n                                  number_of_records=n_records, streamable=False)\n        else:\n            fl = FlowDataEngine(joined_df, calculate_schema_stats=False,\n                                number_of_records=0, streamable=False)\n            return fl\n\n    def solve_graph(self, graph_solver_input: transform_schemas.GraphSolverInput) -&gt; \"FlowDataEngine\":\n        \"\"\"Solves a graph problem represented by 'from' and 'to' columns.\n\n        This is used for operations like finding connected components in a graph.\n\n        Args:\n            graph_solver_input: A `GraphSolverInput` object defining the source,\n                destination, and output column names.\n\n        Returns:\n            A new `FlowDataEngine` instance with the solved graph data.\n        \"\"\"\n        lf = self.data_frame.with_columns(\n            graph_solver(graph_solver_input.col_from, graph_solver_input.col_to)\n            .alias(graph_solver_input.output_column_name)\n        )\n        return FlowDataEngine(lf)\n\n    def add_new_values(self, values: Iterable, col_name: str = None) -&gt; \"FlowDataEngine\":\n        \"\"\"Adds a new column with the provided values.\n\n        Args:\n            values: An iterable (e.g., list, tuple) of values to add as a new column.\n            col_name: The name for the new column. Defaults to 'new_values'.\n\n        Returns:\n            A new `FlowDataEngine` instance with the added column.\n        \"\"\"\n        if col_name is None:\n            col_name = 'new_values'\n        return FlowDataEngine(self.data_frame.with_columns(pl.Series(values).alias(col_name)))\n\n    def get_record_count(self) -&gt; \"FlowDataEngine\":\n        \"\"\"Returns a new FlowDataEngine with a single column 'number_of_records'\n        containing the total number of records.\n\n        Returns:\n            A new `FlowDataEngine` instance.\n        \"\"\"\n        return FlowDataEngine(self.data_frame.select(pl.len().alias('number_of_records')))\n\n    def assert_equal(self, other: \"FlowDataEngine\", ordered: bool = True, strict_schema: bool = False):\n        \"\"\"Asserts that this DataFrame is equal to another.\n\n        Useful for testing.\n\n        Args:\n            other: The other `FlowDataEngine` to compare with.\n            ordered: If True, the row order must be identical.\n            strict_schema: If True, the data types of the schemas must be identical.\n\n        Raises:\n            Exception: If the DataFrames are not equal based on the specified criteria.\n        \"\"\"\n        org_laziness = self.lazy, other.lazy\n        self.lazy = False\n        other.lazy = False\n        self.number_of_records = -1\n        other.number_of_records = -1\n        other = other.select_columns(self.columns)\n\n        if self.get_number_of_records_in_process() != other.get_number_of_records_in_process():\n            raise Exception('Number of records is not equal')\n\n        if self.columns != other.columns:\n            raise Exception('Schema is not equal')\n\n        if strict_schema:\n            assert self.data_frame.schema == other.data_frame.schema, 'Data types do not match'\n\n        if ordered:\n            self_lf = self.data_frame.sort(by=self.columns)\n            other_lf = other.data_frame.sort(by=other.columns)\n        else:\n            self_lf = self.data_frame\n            other_lf = other.data_frame\n\n        self.lazy, other.lazy = org_laziness\n        assert self_lf.equals(other_lf), 'Data is not equal'\n\n    def initialize_empty_fl(self):\n        \"\"\"Initializes an empty LazyFrame.\"\"\"\n        self.data_frame = pl.LazyFrame()\n        self.number_of_records = 0\n        self._lazy = True\n\n    def _calculate_number_of_records_in_worker(self) -&gt; int:\n        \"\"\"Calculates the number of records in a worker process.\"\"\"\n        number_of_records = ExternalDfFetcher(\n            lf=self.data_frame,\n            operation_type=\"calculate_number_of_records\",\n            flow_id=-1,\n            node_id=-1,\n            wait_on_completion=True\n        ).result\n        return number_of_records\n\n    def get_number_of_records_in_process(self, force_calculate: bool = False):\n        \"\"\"\n        Get the number of records in the DataFrame in the local process.\n\n        args:\n            force_calculate: If True, forces recalculation even if a value is cached.\n\n        Returns:\n            The total number of records.\n        \"\"\"\n        return self.get_number_of_records(force_calculate=force_calculate)\n\n    def get_number_of_records(self, warn: bool = False, force_calculate: bool = False,\n                              calculate_in_worker_process: bool = False) -&gt; int:\n        \"\"\"Gets the total number of records in the DataFrame.\n\n        For lazy frames, this may trigger a full data scan, which can be expensive.\n\n        Args:\n            warn: If True, logs a warning if a potentially expensive calculation is triggered.\n            force_calculate: If True, forces recalculation even if a value is cached.\n            calculate_in_worker_process: If True, offloads the calculation to a worker process.\n\n        Returns:\n            The total number of records.\n\n        Raises:\n            ValueError: If the number of records could not be determined.\n        \"\"\"\n        if self.is_future and not self.is_collected:\n            return -1\n        if self.number_of_records is None or self.number_of_records &lt; 0 or force_calculate:\n            if self._number_of_records_callback is not None:\n                self._number_of_records_callback(self)\n\n            if self.lazy:\n                if calculate_in_worker_process:\n                    try:\n                        self.number_of_records = self._calculate_number_of_records_in_worker()\n                        return self.number_of_records\n                    except Exception as e:\n                        logger.error(f\"Error: {e}\")\n                if warn:\n                    logger.warning('Calculating the number of records this can be expensive on a lazy frame')\n                try:\n                    self.number_of_records = self.data_frame.select(pl.len()).collect(\n                        engine=\"streaming\" if self._streamable else \"auto\")[0, 0]\n                except Exception:\n                    raise ValueError('Could not get number of records')\n            else:\n                self.number_of_records = self.data_frame.__len__()\n        return self.number_of_records\n\n    @property\n    def has_errors(self) -&gt; bool:\n        \"\"\"Checks if there are any errors.\"\"\"\n        return len(self.errors) &gt; 0\n\n    @property\n    def lazy(self) -&gt; bool:\n        \"\"\"Indicates if the DataFrame is in lazy mode.\"\"\"\n        return self._lazy\n\n    @lazy.setter\n    def lazy(self, exec_lazy: bool = False):\n        \"\"\"Sets the laziness of the DataFrame.\n\n        Args:\n            exec_lazy: If True, converts the DataFrame to a LazyFrame. If False,\n                collects the data and converts it to an eager DataFrame.\n        \"\"\"\n        if exec_lazy != self._lazy:\n            if exec_lazy:\n                self.data_frame = self.data_frame.lazy()\n            else:\n                self._lazy = exec_lazy\n                if self.external_source is not None:\n                    df = self.collect()\n                    self.data_frame = df\n                else:\n                    self.data_frame = self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\")\n            self._lazy = exec_lazy\n\n    @property\n    def external_source(self) -&gt; ExternalDataSource:\n        \"\"\"The external data source, if any.\"\"\"\n        return self._external_source\n\n    @property\n    def cols_idx(self) -&gt; Dict[str, int]:\n        \"\"\"A dictionary mapping column names to their integer index.\"\"\"\n        if self._col_idx is None:\n            self._col_idx = {c: i for i, c in enumerate(self.columns)}\n        return self._col_idx\n\n    @property\n    def __name__(self) -&gt; str:\n        \"\"\"The name of the table.\"\"\"\n        return self.name\n\n    def get_select_inputs(self) -&gt; transform_schemas.SelectInputs:\n        \"\"\"Gets `SelectInput` specifications for all columns in the current schema.\n\n        Returns:\n            A `SelectInputs` object that can be used to configure selection or\n            transformation operations.\n        \"\"\"\n        return transform_schemas.SelectInputs(\n            [transform_schemas.SelectInput(old_name=c.name, data_type=c.data_type) for c in self.schema]\n        )\n\n    def select_columns(self, list_select: Union[List[str], Tuple[str], str]) -&gt; \"FlowDataEngine\":\n        \"\"\"Selects a subset of columns from the DataFrame.\n\n        Args:\n            list_select: A list, tuple, or single string of column names to select.\n\n        Returns:\n            A new `FlowDataEngine` instance containing only the selected columns.\n        \"\"\"\n        if isinstance(list_select, str):\n            list_select = [list_select]\n\n        idx_to_keep = [self.cols_idx.get(c) for c in list_select]\n        selects = [ls for ls, id_to_keep in zip(list_select, idx_to_keep) if id_to_keep is not None]\n        new_schema = [self.schema[i] for i in idx_to_keep if i is not None]\n\n        return FlowDataEngine(\n            self.data_frame.select(selects),\n            number_of_records=self.number_of_records,\n            schema=new_schema,\n            streamable=self._streamable\n        )\n\n    def drop_columns(self, columns: List[str]) -&gt; \"FlowDataEngine\":\n        \"\"\"Drops specified columns from the DataFrame.\n\n        Args:\n            columns: A list of column names to drop.\n\n        Returns:\n            A new `FlowDataEngine` instance without the dropped columns.\n        \"\"\"\n        cols_for_select = tuple(set(self.columns) - set(columns))\n        idx_to_keep = [self.cols_idx.get(c) for c in cols_for_select]\n        new_schema = [self.schema[i] for i in idx_to_keep]\n\n        return FlowDataEngine(\n            self.data_frame.select(cols_for_select),\n            number_of_records=self.number_of_records,\n            schema=new_schema\n        )\n\n    def reorganize_order(self, column_order: List[str]) -&gt; \"FlowDataEngine\":\n        \"\"\"Reorganizes columns into a specified order.\n\n        Args:\n            column_order: A list of column names in the desired order.\n\n        Returns:\n            A new `FlowDataEngine` instance with the columns reordered.\n        \"\"\"\n        df = self.data_frame.select(column_order)\n        schema = sorted(self.schema, key=lambda x: column_order.index(x.column_name))\n        return FlowDataEngine(df, schema=schema, number_of_records=self.number_of_records)\n\n    def apply_flowfile_formula(self, func: str, col_name: str,\n                               output_data_type: pl.DataType = None) -&gt; \"FlowDataEngine\":\n        \"\"\"Applies a formula to create a new column or transform an existing one.\n\n        Args:\n            func: A string containing a Polars expression formula.\n            col_name: The name of the new or transformed column.\n            output_data_type: The desired Polars data type for the output column.\n\n        Returns:\n            A new `FlowDataEngine` instance with the applied formula.\n        \"\"\"\n        parsed_func = to_expr(func)\n        if output_data_type is not None:\n            df2 = self.data_frame.with_columns(parsed_func.cast(output_data_type).alias(col_name))\n        else:\n            df2 = self.data_frame.with_columns(parsed_func.alias(col_name))\n\n        return FlowDataEngine(df2, number_of_records=self.number_of_records)\n\n    def apply_sql_formula(self, func: str, col_name: str,\n                          output_data_type: pl.DataType = None) -&gt; \"FlowDataEngine\":\n        \"\"\"Applies an SQL-style formula using `pl.sql_expr`.\n\n        Args:\n            func: A string containing an SQL expression.\n            col_name: The name of the new or transformed column.\n            output_data_type: The desired Polars data type for the output column.\n\n        Returns:\n            A new `FlowDataEngine` instance with the applied formula.\n        \"\"\"\n        expr = to_expr(func)\n        if output_data_type not in (None, \"Auto\"):\n            df = self.data_frame.with_columns(expr.cast(output_data_type).alias(col_name))\n        else:\n            df = self.data_frame.with_columns(expr.alias(col_name))\n\n        return FlowDataEngine(df, number_of_records=self.number_of_records)\n\n    def output(self, output_fs: input_schema.OutputSettings, flow_id: int, node_id: int | str,\n               execute_remote: bool = True) -&gt; \"FlowDataEngine\":\n        \"\"\"Writes the DataFrame to an output file.\n\n        Can execute the write operation locally or in a remote worker process.\n\n        Args:\n            output_fs: An `OutputSettings` object with details about the output file.\n            flow_id: The flow ID for tracking.\n            node_id: The node ID for tracking.\n            execute_remote: If True, executes the write in a worker process.\n\n        Returns:\n            The same `FlowDataEngine` instance for chaining.\n        \"\"\"\n        logger.info('Starting to write output')\n        if execute_remote:\n            status = utils.write_output(\n                self.data_frame,\n                data_type=output_fs.file_type,\n                path=output_fs.abs_file_path,\n                write_mode=output_fs.write_mode,\n                sheet_name=output_fs.output_excel_table.sheet_name,\n                delimiter=output_fs.output_csv_table.delimiter,\n                flow_id=flow_id,\n                node_id=node_id\n            )\n            tracker = ExternalExecutorTracker(status)\n            tracker.get_result()\n            logger.info('Finished writing output')\n        else:\n            logger.info(\"Starting to write results locally\")\n            utils.local_write_output(\n                self.data_frame,\n                data_type=output_fs.file_type,\n                path=output_fs.abs_file_path,\n                write_mode=output_fs.write_mode,\n                sheet_name=output_fs.output_excel_table.sheet_name,\n                delimiter=output_fs.output_csv_table.delimiter,\n                flow_id=flow_id,\n                node_id=node_id,\n            )\n            logger.info(\"Finished writing output\")\n        return self\n\n    def make_unique(self, unique_input: transform_schemas.UniqueInput = None) -&gt; \"FlowDataEngine\":\n        \"\"\"Gets the unique rows from the DataFrame.\n\n        Args:\n            unique_input: A `UniqueInput` object specifying a subset of columns\n                to consider for uniqueness and a strategy for keeping rows.\n\n        Returns:\n            A new `FlowDataEngine` instance with unique rows.\n        \"\"\"\n        if unique_input is None or unique_input.columns is None:\n            return FlowDataEngine(self.data_frame.unique())\n        return FlowDataEngine(self.data_frame.unique(unique_input.columns, keep=unique_input.strategy))\n\n    def concat(self, other: Iterable[\"FlowDataEngine\"] | \"FlowDataEngine\") -&gt; \"FlowDataEngine\":\n        \"\"\"Concatenates this DataFrame with one or more other DataFrames.\n\n        Args:\n            other: A single `FlowDataEngine` or an iterable of them.\n\n        Returns:\n            A new `FlowDataEngine` containing the concatenated data.\n        \"\"\"\n        if isinstance(other, FlowDataEngine):\n            other = [other]\n\n        dfs: List[pl.LazyFrame] | List[pl.DataFrame] = [self.data_frame] + [flt.data_frame for flt in other]\n        return FlowDataEngine(pl.concat(dfs, how='diagonal_relaxed'))\n\n    def do_select(self, select_inputs: transform_schemas.SelectInputs,\n                  keep_missing: bool = True) -&gt; \"FlowDataEngine\":\n        \"\"\"Performs a complex column selection, renaming, and reordering operation.\n\n        Args:\n            select_inputs: A `SelectInputs` object defining the desired transformations.\n            keep_missing: If True, columns not specified in `select_inputs` are kept.\n                If False, they are dropped.\n\n        Returns:\n            A new `FlowDataEngine` with the transformed selection.\n        \"\"\"\n        new_schema = deepcopy(self.schema)\n        renames = [r for r in select_inputs.renames if r.is_available]\n\n        if not keep_missing:\n            drop_cols = set(self.data_frame.collect_schema().names()) - set(r.old_name for r in renames).union(\n                set(r.old_name for r in renames if not r.keep))\n            keep_cols = []\n        else:\n            keep_cols = list(set(self.data_frame.collect_schema().names()) - set(r.old_name for r in renames))\n            drop_cols = set(r.old_name for r in renames if not r.keep)\n\n        if len(drop_cols) &gt; 0:\n            new_schema = [s for s in new_schema if s.name not in drop_cols]\n        new_schema_mapping = {v.name: v for v in new_schema}\n\n        available_renames = []\n        for rename in renames:\n            if (rename.new_name != rename.old_name or rename.new_name not in new_schema_mapping) and rename.keep:\n                schema_entry = new_schema_mapping.get(rename.old_name)\n                if schema_entry is not None:\n                    available_renames.append(rename)\n                    schema_entry.column_name = rename.new_name\n\n        rename_dict = {r.old_name: r.new_name for r in available_renames}\n        fl = self.select_columns(\n            list_select=[col_to_keep.old_name for col_to_keep in renames if col_to_keep.keep] + keep_cols)\n        fl = fl.change_column_types(transforms=[r for r in renames if r.keep])\n        ndf = fl.data_frame.rename(rename_dict)\n        renames.sort(key=lambda r: 0 if r.position is None else r.position)\n        sorted_cols = utils.match_order(ndf.collect_schema().names(),\n                                        [r.new_name for r in renames] + self.data_frame.collect_schema().names())\n        output_file = FlowDataEngine(ndf, number_of_records=self.number_of_records)\n        return output_file.reorganize_order(sorted_cols)\n\n    def set_streamable(self, streamable: bool = False):\n        \"\"\"Sets whether DataFrame operations should be streamable.\"\"\"\n        self._streamable = streamable\n\n    def _calculate_schema(self) -&gt; List[Dict]:\n        \"\"\"Calculates schema statistics.\"\"\"\n        if self.external_source is not None:\n            self.collect_external()\n        v = utils.calculate_schema(self.data_frame)\n        return v\n\n    def calculate_schema(self):\n        \"\"\"Calculates and returns the schema.\"\"\"\n        self._calculate_schema_stats = True\n        return self.schema\n\n    def count(self) -&gt; int:\n        \"\"\"Gets the total number of records.\"\"\"\n        return self.get_number_of_records()\n\n    @classmethod\n    def create_from_path_worker(cls, received_table: input_schema.ReceivedTable, flow_id: int, node_id: int | str):\n        \"\"\"Creates a FlowDataEngine from a path in a worker process.\"\"\"\n        received_table.set_absolute_filepath()\n        external_fetcher = ExternalCreateFetcher(received_table=received_table,\n                                                 file_type=received_table.file_type, flow_id=flow_id, node_id=node_id)\n        return cls(external_fetcher.get_result())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.__name__","title":"<code>__name__</code>  <code>property</code>","text":"<p>The name of the table.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.cols_idx","title":"<code>cols_idx</code>  <code>property</code>","text":"<p>A dictionary mapping column names to their integer index.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.data_frame","title":"<code>data_frame</code>  <code>property</code> <code>writable</code>","text":"<p>The underlying Polars DataFrame or LazyFrame.</p> <p>This property provides access to the Polars object that backs the FlowDataEngine. It handles lazy-loading from external sources if necessary.</p> <p>Returns:</p> Type Description <code>LazyFrame | DataFrame | None</code> <p>The active Polars <code>DataFrame</code> or <code>LazyFrame</code>.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.external_source","title":"<code>external_source</code>  <code>property</code>","text":"<p>The external data source, if any.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.has_errors","title":"<code>has_errors</code>  <code>property</code>","text":"<p>Checks if there are any errors.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.lazy","title":"<code>lazy</code>  <code>property</code> <code>writable</code>","text":"<p>Indicates if the DataFrame is in lazy mode.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.number_of_fields","title":"<code>number_of_fields</code>  <code>property</code>","text":"<p>The number of columns (fields) in the DataFrame.</p> <p>Returns:</p> Type Description <code>int</code> <p>The integer count of columns.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.schema","title":"<code>schema</code>  <code>property</code>","text":"<p>The schema of the DataFrame as a list of <code>FlowfileColumn</code> objects.</p> <p>This property lazily calculates the schema if it hasn't been determined yet.</p> <p>Returns:</p> Type Description <code>List[FlowfileColumn]</code> <p>A list of <code>FlowfileColumn</code> objects describing the schema.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.__call__","title":"<code>__call__()</code>","text":"<p>Makes the class instance callable, returning itself.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def __call__(self) -&gt; \"FlowDataEngine\":\n    \"\"\"Makes the class instance callable, returning itself.\"\"\"\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.__get_sample__","title":"<code>__get_sample__(n_rows=100, streamable=True)</code>","text":"<p>Internal method to get a sample of the data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def __get_sample__(self, n_rows: int = 100, streamable: bool = True) -&gt; \"FlowDataEngine\":\n    \"\"\"Internal method to get a sample of the data.\"\"\"\n    if not self.lazy:\n        df = self.data_frame.lazy()\n    else:\n        df = self.data_frame\n\n    if streamable:\n        try:\n            df = df.head(n_rows).collect()\n        except Exception as e:\n            logger.warning(f'Error in getting sample: {e}')\n            df = df.head(n_rows).collect(engine=\"auto\")\n    else:\n        df = self.collect()\n    return FlowDataEngine(df, number_of_records=len(df), schema=self.schema)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.__getitem__","title":"<code>__getitem__(item)</code>","text":"<p>Accesses a specific column or item from the DataFrame.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def __getitem__(self, item):\n    \"\"\"Accesses a specific column or item from the DataFrame.\"\"\"\n    return self.data_frame.select([item])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.__init__","title":"<code>__init__(raw_data=None, path_ref=None, name=None, optimize_memory=True, schema=None, number_of_records=None, calculate_schema_stats=False, streamable=True, number_of_records_callback=None, data_callback=None)</code>","text":"<p>Initializes the FlowDataEngine from various data sources.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data</code> <code>Union[List[Dict], List[Any], Dict[str, Any], ParquetFile, DataFrame, LazyFrame, RawData]</code> <p>The input data. Can be a list of dicts, a Polars DataFrame/LazyFrame, or a <code>RawData</code> schema object.</p> <code>None</code> <code>path_ref</code> <code>str</code> <p>A string path to a Parquet file.</p> <code>None</code> <code>name</code> <code>str</code> <p>An optional name for the data engine instance.</p> <code>None</code> <code>optimize_memory</code> <code>bool</code> <p>If True, prefers lazy operations to conserve memory.</p> <code>True</code> <code>schema</code> <code>List[FlowfileColumn] | List[str] | Schema</code> <p>An optional schema definition. Can be a list of <code>FlowfileColumn</code> objects, a list of column names, or a Polars <code>Schema</code>.</p> <code>None</code> <code>number_of_records</code> <code>int</code> <p>The number of records, if known.</p> <code>None</code> <code>calculate_schema_stats</code> <code>bool</code> <p>If True, computes detailed statistics for each column.</p> <code>False</code> <code>streamable</code> <code>bool</code> <p>If True, allows for streaming operations when possible.</p> <code>True</code> <code>number_of_records_callback</code> <code>Callable</code> <p>A callback function to retrieve the number of records.</p> <code>None</code> <code>data_callback</code> <code>Callable</code> <p>A callback function to retrieve the data.</p> <code>None</code> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def __init__(self,\n             raw_data: Union[List[Dict], List[Any], Dict[str, Any], 'ParquetFile', pl.DataFrame, pl.LazyFrame, input_schema.RawData] = None,\n             path_ref: str = None,\n             name: str = None,\n             optimize_memory: bool = True,\n             schema: List['FlowfileColumn'] | List[str] | pl.Schema = None,\n             number_of_records: int = None,\n             calculate_schema_stats: bool = False,\n             streamable: bool = True,\n             number_of_records_callback: Callable = None,\n             data_callback: Callable = None):\n    \"\"\"Initializes the FlowDataEngine from various data sources.\n\n    Args:\n        raw_data: The input data. Can be a list of dicts, a Polars DataFrame/LazyFrame,\n            or a `RawData` schema object.\n        path_ref: A string path to a Parquet file.\n        name: An optional name for the data engine instance.\n        optimize_memory: If True, prefers lazy operations to conserve memory.\n        schema: An optional schema definition. Can be a list of `FlowfileColumn` objects,\n            a list of column names, or a Polars `Schema`.\n        number_of_records: The number of records, if known.\n        calculate_schema_stats: If True, computes detailed statistics for each column.\n        streamable: If True, allows for streaming operations when possible.\n        number_of_records_callback: A callback function to retrieve the number of records.\n        data_callback: A callback function to retrieve the data.\n    \"\"\"\n    self._initialize_attributes(number_of_records_callback, data_callback, streamable)\n\n    if raw_data is not None:\n        self._handle_raw_data(raw_data, number_of_records, optimize_memory)\n    elif path_ref:\n        self._handle_path_ref(path_ref, optimize_memory)\n    else:\n        self.initialize_empty_fl()\n    self._finalize_initialization(name, optimize_memory, schema, calculate_schema_stats)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of records in the table.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of records in the table.\"\"\"\n    return self.number_of_records if self.number_of_records &gt;= 0 else self.get_number_of_records()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns a string representation of the FlowDataEngine.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Returns a string representation of the FlowDataEngine.\"\"\"\n    return f'flow data engine\\n{self.data_frame.__repr__()}'\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.add_new_values","title":"<code>add_new_values(values, col_name=None)</code>","text":"<p>Adds a new column with the provided values.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Iterable</code> <p>An iterable (e.g., list, tuple) of values to add as a new column.</p> required <code>col_name</code> <code>str</code> <p>The name for the new column. Defaults to 'new_values'.</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the added column.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def add_new_values(self, values: Iterable, col_name: str = None) -&gt; \"FlowDataEngine\":\n    \"\"\"Adds a new column with the provided values.\n\n    Args:\n        values: An iterable (e.g., list, tuple) of values to add as a new column.\n        col_name: The name for the new column. Defaults to 'new_values'.\n\n    Returns:\n        A new `FlowDataEngine` instance with the added column.\n    \"\"\"\n    if col_name is None:\n        col_name = 'new_values'\n    return FlowDataEngine(self.data_frame.with_columns(pl.Series(values).alias(col_name)))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.add_record_id","title":"<code>add_record_id(record_id_settings)</code>","text":"<p>Adds a record ID (row number) column to the DataFrame.</p> <p>Can generate a simple sequential ID or a grouped ID that resets for each group.</p> <p>Parameters:</p> Name Type Description Default <code>record_id_settings</code> <code>RecordIdInput</code> <p>A <code>RecordIdInput</code> object specifying the output column name, offset, and optional grouping columns.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the added record ID column.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def add_record_id(self, record_id_settings: transform_schemas.RecordIdInput) -&gt; \"FlowDataEngine\":\n    \"\"\"Adds a record ID (row number) column to the DataFrame.\n\n    Can generate a simple sequential ID or a grouped ID that resets for\n    each group.\n\n    Args:\n        record_id_settings: A `RecordIdInput` object specifying the output\n            column name, offset, and optional grouping columns.\n\n    Returns:\n        A new `FlowDataEngine` instance with the added record ID column.\n    \"\"\"\n    if record_id_settings.group_by and len(record_id_settings.group_by_columns) &gt; 0:\n        return self._add_grouped_record_id(record_id_settings)\n    return self._add_simple_record_id(record_id_settings)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.apply_flowfile_formula","title":"<code>apply_flowfile_formula(func, col_name, output_data_type=None)</code>","text":"<p>Applies a formula to create a new column or transform an existing one.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>str</code> <p>A string containing a Polars expression formula.</p> required <code>col_name</code> <code>str</code> <p>The name of the new or transformed column.</p> required <code>output_data_type</code> <code>DataType</code> <p>The desired Polars data type for the output column.</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the applied formula.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def apply_flowfile_formula(self, func: str, col_name: str,\n                           output_data_type: pl.DataType = None) -&gt; \"FlowDataEngine\":\n    \"\"\"Applies a formula to create a new column or transform an existing one.\n\n    Args:\n        func: A string containing a Polars expression formula.\n        col_name: The name of the new or transformed column.\n        output_data_type: The desired Polars data type for the output column.\n\n    Returns:\n        A new `FlowDataEngine` instance with the applied formula.\n    \"\"\"\n    parsed_func = to_expr(func)\n    if output_data_type is not None:\n        df2 = self.data_frame.with_columns(parsed_func.cast(output_data_type).alias(col_name))\n    else:\n        df2 = self.data_frame.with_columns(parsed_func.alias(col_name))\n\n    return FlowDataEngine(df2, number_of_records=self.number_of_records)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.apply_sql_formula","title":"<code>apply_sql_formula(func, col_name, output_data_type=None)</code>","text":"<p>Applies an SQL-style formula using <code>pl.sql_expr</code>.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>str</code> <p>A string containing an SQL expression.</p> required <code>col_name</code> <code>str</code> <p>The name of the new or transformed column.</p> required <code>output_data_type</code> <code>DataType</code> <p>The desired Polars data type for the output column.</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the applied formula.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def apply_sql_formula(self, func: str, col_name: str,\n                      output_data_type: pl.DataType = None) -&gt; \"FlowDataEngine\":\n    \"\"\"Applies an SQL-style formula using `pl.sql_expr`.\n\n    Args:\n        func: A string containing an SQL expression.\n        col_name: The name of the new or transformed column.\n        output_data_type: The desired Polars data type for the output column.\n\n    Returns:\n        A new `FlowDataEngine` instance with the applied formula.\n    \"\"\"\n    expr = to_expr(func)\n    if output_data_type not in (None, \"Auto\"):\n        df = self.data_frame.with_columns(expr.cast(output_data_type).alias(col_name))\n    else:\n        df = self.data_frame.with_columns(expr.alias(col_name))\n\n    return FlowDataEngine(df, number_of_records=self.number_of_records)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.assert_equal","title":"<code>assert_equal(other, ordered=True, strict_schema=False)</code>","text":"<p>Asserts that this DataFrame is equal to another.</p> <p>Useful for testing.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>FlowDataEngine</code> <p>The other <code>FlowDataEngine</code> to compare with.</p> required <code>ordered</code> <code>bool</code> <p>If True, the row order must be identical.</p> <code>True</code> <code>strict_schema</code> <code>bool</code> <p>If True, the data types of the schemas must be identical.</p> <code>False</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the DataFrames are not equal based on the specified criteria.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def assert_equal(self, other: \"FlowDataEngine\", ordered: bool = True, strict_schema: bool = False):\n    \"\"\"Asserts that this DataFrame is equal to another.\n\n    Useful for testing.\n\n    Args:\n        other: The other `FlowDataEngine` to compare with.\n        ordered: If True, the row order must be identical.\n        strict_schema: If True, the data types of the schemas must be identical.\n\n    Raises:\n        Exception: If the DataFrames are not equal based on the specified criteria.\n    \"\"\"\n    org_laziness = self.lazy, other.lazy\n    self.lazy = False\n    other.lazy = False\n    self.number_of_records = -1\n    other.number_of_records = -1\n    other = other.select_columns(self.columns)\n\n    if self.get_number_of_records_in_process() != other.get_number_of_records_in_process():\n        raise Exception('Number of records is not equal')\n\n    if self.columns != other.columns:\n        raise Exception('Schema is not equal')\n\n    if strict_schema:\n        assert self.data_frame.schema == other.data_frame.schema, 'Data types do not match'\n\n    if ordered:\n        self_lf = self.data_frame.sort(by=self.columns)\n        other_lf = other.data_frame.sort(by=other.columns)\n    else:\n        self_lf = self.data_frame\n        other_lf = other.data_frame\n\n    self.lazy, other.lazy = org_laziness\n    assert self_lf.equals(other_lf), 'Data is not equal'\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.cache","title":"<code>cache()</code>","text":"<p>Caches the current DataFrame to disk and updates the internal reference.</p> <p>This triggers a background process to write the current LazyFrame's result to a temporary file. Subsequent operations on this <code>FlowDataEngine</code> instance will read from the cached file, which can speed up downstream computations.</p> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>The same <code>FlowDataEngine</code> instance, now backed by the cached data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def cache(self) -&gt; \"FlowDataEngine\":\n    \"\"\"Caches the current DataFrame to disk and updates the internal reference.\n\n    This triggers a background process to write the current LazyFrame's result\n    to a temporary file. Subsequent operations on this `FlowDataEngine` instance\n    will read from the cached file, which can speed up downstream computations.\n\n    Returns:\n        The same `FlowDataEngine` instance, now backed by the cached data.\n    \"\"\"\n    edf = ExternalDfFetcher(lf=self.data_frame, file_ref=str(id(self)), wait_on_completion=False,\n                            flow_id=-1,\n                            node_id=-1)\n    logger.info('Caching data in background')\n    result = edf.get_result()\n    if isinstance(result, pl.LazyFrame):\n        logger.info('Data cached')\n        del self._data_frame\n        self.data_frame = result\n        logger.info('Data loaded from cache')\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.calculate_schema","title":"<code>calculate_schema()</code>","text":"<p>Calculates and returns the schema.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def calculate_schema(self):\n    \"\"\"Calculates and returns the schema.\"\"\"\n    self._calculate_schema_stats = True\n    return self.schema\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.change_column_types","title":"<code>change_column_types(transforms, calculate_schema=False)</code>","text":"<p>Changes the data type of one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>List[SelectInput]</code> <p>A list of <code>SelectInput</code> objects, where each object specifies the column and its new <code>polars_type</code>.</p> required <code>calculate_schema</code> <code>bool</code> <p>If True, recalculates the schema after the type change.</p> <code>False</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the updated column types.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def change_column_types(self, transforms: List[transform_schemas.SelectInput],\n                        calculate_schema: bool = False) -&gt; \"FlowDataEngine\":\n    \"\"\"Changes the data type of one or more columns.\n\n    Args:\n        transforms: A list of `SelectInput` objects, where each object specifies\n            the column and its new `polars_type`.\n        calculate_schema: If True, recalculates the schema after the type change.\n\n    Returns:\n        A new `FlowDataEngine` instance with the updated column types.\n    \"\"\"\n    dtypes = [dtype.base_type() for dtype in self.data_frame.collect_schema().dtypes()]\n    idx_mapping = list(\n        (transform.old_name, self.cols_idx.get(transform.old_name), getattr(pl, transform.polars_type))\n        for transform in transforms if transform.data_type is not None\n    )\n\n    actual_transforms = [c for c in idx_mapping if c[2] != dtypes[c[1]]]\n    transformations = [\n        utils.define_pl_col_transformation(col_name=transform[0], col_type=transform[2])\n        for transform in actual_transforms\n    ]\n\n    df = self.data_frame.with_columns(transformations)\n    return FlowDataEngine(\n        df,\n        number_of_records=self.number_of_records,\n        calculate_schema_stats=calculate_schema,\n        streamable=self._streamable\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.collect","title":"<code>collect(n_records=None)</code>","text":"<p>Collects the data and returns it as a Polars DataFrame.</p> <p>This method triggers the execution of the lazy query plan (if applicable) and returns the result. It supports streaming to optimize memory usage for large datasets.</p> <p>Parameters:</p> Name Type Description Default <code>n_records</code> <code>int</code> <p>The maximum number of records to collect. If None, all records are collected.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Polars <code>DataFrame</code> containing the collected data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def collect(self, n_records: int = None) -&gt; pl.DataFrame:\n    \"\"\"Collects the data and returns it as a Polars DataFrame.\n\n    This method triggers the execution of the lazy query plan (if applicable)\n    and returns the result. It supports streaming to optimize memory usage\n    for large datasets.\n\n    Args:\n        n_records: The maximum number of records to collect. If None, all\n            records are collected.\n\n    Returns:\n        A Polars `DataFrame` containing the collected data.\n    \"\"\"\n    if n_records is None:\n        logger.info(f'Fetching all data for Table object \"{id(self)}\". Settings: streaming={self._streamable}')\n    else:\n        logger.info(f'Fetching {n_records} record(s) for Table object \"{id(self)}\". '\n                    f'Settings: streaming={self._streamable}')\n\n    if not self.lazy:\n        return self.data_frame\n\n    try:\n        return self._collect_data(n_records)\n    except Exception as e:\n        self.errors = [e]\n        return self._handle_collection_error(n_records)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.collect_external","title":"<code>collect_external()</code>","text":"<p>Materializes data from a tracked external source.</p> <p>If the <code>FlowDataEngine</code> was created from an <code>ExternalDataSource</code>, this method will trigger the data retrieval, update the internal <code>_data_frame</code> to a <code>LazyFrame</code> of the collected data, and reset the schema to be re-evaluated.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def collect_external(self):\n    \"\"\"Materializes data from a tracked external source.\n\n    If the `FlowDataEngine` was created from an `ExternalDataSource`, this\n    method will trigger the data retrieval, update the internal `_data_frame`\n    to a `LazyFrame` of the collected data, and reset the schema to be\n    re-evaluated.\n    \"\"\"\n    if self._external_source is not None:\n        logger.info('Collecting external source')\n        if self.external_source.get_pl_df() is not None:\n            self.data_frame = self.external_source.get_pl_df().lazy()\n        else:\n            self.data_frame = pl.LazyFrame(list(self.external_source.get_iter()))\n        self._schema = None  # enforce reset schema\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.concat","title":"<code>concat(other)</code>","text":"<p>Concatenates this DataFrame with one or more other DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Iterable[FlowDataEngine] | FlowDataEngine</code> <p>A single <code>FlowDataEngine</code> or an iterable of them.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> containing the concatenated data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def concat(self, other: Iterable[\"FlowDataEngine\"] | \"FlowDataEngine\") -&gt; \"FlowDataEngine\":\n    \"\"\"Concatenates this DataFrame with one or more other DataFrames.\n\n    Args:\n        other: A single `FlowDataEngine` or an iterable of them.\n\n    Returns:\n        A new `FlowDataEngine` containing the concatenated data.\n    \"\"\"\n    if isinstance(other, FlowDataEngine):\n        other = [other]\n\n    dfs: List[pl.LazyFrame] | List[pl.DataFrame] = [self.data_frame] + [flt.data_frame for flt in other]\n    return FlowDataEngine(pl.concat(dfs, how='diagonal_relaxed'))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.count","title":"<code>count()</code>","text":"<p>Gets the total number of records.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def count(self) -&gt; int:\n    \"\"\"Gets the total number of records.\"\"\"\n    return self.get_number_of_records()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.create_from_external_source","title":"<code>create_from_external_source(external_source)</code>  <code>classmethod</code>","text":"<p>Creates a FlowDataEngine from an external data source.</p> <p>Parameters:</p> Name Type Description Default <code>external_source</code> <code>ExternalDataSource</code> <p>An object that conforms to the <code>ExternalDataSource</code> interface.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef create_from_external_source(cls, external_source: ExternalDataSource) -&gt; \"FlowDataEngine\":\n    \"\"\"Creates a FlowDataEngine from an external data source.\n\n    Args:\n        external_source: An object that conforms to the `ExternalDataSource`\n            interface.\n\n    Returns:\n        A new `FlowDataEngine` instance.\n    \"\"\"\n    if external_source.schema is not None:\n        ff = cls.create_from_schema(external_source.schema)\n    elif external_source.initial_data_getter is not None:\n        ff = cls(raw_data=external_source.initial_data_getter())\n    else:\n        ff = cls()\n    ff._external_source = external_source\n    return ff\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.create_from_path","title":"<code>create_from_path(received_table)</code>  <code>classmethod</code>","text":"<p>Creates a FlowDataEngine from a local file path.</p> <p>Supports various file types like CSV, Parquet, and Excel.</p> <p>Parameters:</p> Name Type Description Default <code>received_table</code> <code>ReceivedTableBase</code> <p>A <code>ReceivedTableBase</code> object containing the file path and format details.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with data from the file.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef create_from_path(cls, received_table: input_schema.ReceivedTableBase) -&gt; \"FlowDataEngine\":\n    \"\"\"Creates a FlowDataEngine from a local file path.\n\n    Supports various file types like CSV, Parquet, and Excel.\n\n    Args:\n        received_table: A `ReceivedTableBase` object containing the file path\n            and format details.\n\n    Returns:\n        A new `FlowDataEngine` instance with data from the file.\n    \"\"\"\n    received_table.set_absolute_filepath()\n    file_type_handlers = {\n        'csv': create_funcs.create_from_path_csv,\n        'parquet': create_funcs.create_from_path_parquet,\n        'excel': create_funcs.create_from_path_excel\n    }\n\n    handler = file_type_handlers.get(received_table.file_type)\n    if not handler:\n        raise Exception(f'Cannot create from {received_table.file_type}')\n\n    flow_file = cls(handler(received_table))\n    flow_file._org_path = received_table.abs_file_path\n    return flow_file\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.create_from_path_worker","title":"<code>create_from_path_worker(received_table, flow_id, node_id)</code>  <code>classmethod</code>","text":"<p>Creates a FlowDataEngine from a path in a worker process.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef create_from_path_worker(cls, received_table: input_schema.ReceivedTable, flow_id: int, node_id: int | str):\n    \"\"\"Creates a FlowDataEngine from a path in a worker process.\"\"\"\n    received_table.set_absolute_filepath()\n    external_fetcher = ExternalCreateFetcher(received_table=received_table,\n                                             file_type=received_table.file_type, flow_id=flow_id, node_id=node_id)\n    return cls(external_fetcher.get_result())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.create_from_schema","title":"<code>create_from_schema(schema)</code>  <code>classmethod</code>","text":"<p>Creates an empty FlowDataEngine from a schema definition.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>List[FlowfileColumn]</code> <p>A list of <code>FlowfileColumn</code> objects defining the schema.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new, empty <code>FlowDataEngine</code> instance with the specified schema.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef create_from_schema(cls, schema: List[FlowfileColumn]) -&gt; \"FlowDataEngine\":\n    \"\"\"Creates an empty FlowDataEngine from a schema definition.\n\n    Args:\n        schema: A list of `FlowfileColumn` objects defining the schema.\n\n    Returns:\n        A new, empty `FlowDataEngine` instance with the specified schema.\n    \"\"\"\n    pl_schema = []\n    for i, flow_file_column in enumerate(schema):\n        pl_schema.append((flow_file_column.name, cast_str_to_polars_type(flow_file_column.data_type)))\n        schema[i].col_index = i\n    df = pl.LazyFrame(schema=pl_schema)\n    return cls(df, schema=schema, calculate_schema_stats=False, number_of_records=0)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.create_from_sql","title":"<code>create_from_sql(sql, conn)</code>  <code>classmethod</code>","text":"<p>Creates a FlowDataEngine by executing a SQL query.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL query string to execute.</p> required <code>conn</code> <code>Any</code> <p>A database connection object or connection URI string.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the query result.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef create_from_sql(cls, sql: str, conn: Any) -&gt; \"FlowDataEngine\":\n    \"\"\"Creates a FlowDataEngine by executing a SQL query.\n\n    Args:\n        sql: The SQL query string to execute.\n        conn: A database connection object or connection URI string.\n\n    Returns:\n        A new `FlowDataEngine` instance with the query result.\n    \"\"\"\n    return cls(pl.read_sql(sql, conn))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.create_random","title":"<code>create_random(number_of_records=1000)</code>  <code>classmethod</code>","text":"<p>Creates a FlowDataEngine with randomly generated data.</p> <p>Useful for testing and examples.</p> <p>Parameters:</p> Name Type Description Default <code>number_of_records</code> <code>int</code> <p>The number of random records to generate.</p> <code>1000</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with fake data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef create_random(cls, number_of_records: int = 1000) -&gt; \"FlowDataEngine\":\n    \"\"\"Creates a FlowDataEngine with randomly generated data.\n\n    Useful for testing and examples.\n\n    Args:\n        number_of_records: The number of random records to generate.\n\n    Returns:\n        A new `FlowDataEngine` instance with fake data.\n    \"\"\"\n    return cls(create_fake_data(number_of_records))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.do_cross_join","title":"<code>do_cross_join(cross_join_input, auto_generate_selection, verify_integrity, other)</code>","text":"<p>Performs a cross join with another DataFrame.</p> <p>A cross join produces the Cartesian product of the two DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>cross_join_input</code> <code>CrossJoinInput</code> <p>A <code>CrossJoinInput</code> object specifying column selections.</p> required <code>auto_generate_selection</code> <code>bool</code> <p>If True, automatically renames columns to avoid conflicts.</p> required <code>verify_integrity</code> <code>bool</code> <p>If True, checks if the resulting join would be too large.</p> required <code>other</code> <code>FlowDataEngine</code> <p>The right <code>FlowDataEngine</code> to join with.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> with the result of the cross join.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If <code>verify_integrity</code> is True and the join would result in an excessively large number of records.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def do_cross_join(self, cross_join_input: transform_schemas.CrossJoinInput,\n                  auto_generate_selection: bool, verify_integrity: bool,\n                  other: \"FlowDataEngine\") -&gt; \"FlowDataEngine\":\n    \"\"\"Performs a cross join with another DataFrame.\n\n    A cross join produces the Cartesian product of the two DataFrames.\n\n    Args:\n        cross_join_input: A `CrossJoinInput` object specifying column selections.\n        auto_generate_selection: If True, automatically renames columns to avoid conflicts.\n        verify_integrity: If True, checks if the resulting join would be too large.\n        other: The right `FlowDataEngine` to join with.\n\n    Returns:\n        A new `FlowDataEngine` with the result of the cross join.\n\n    Raises:\n        Exception: If `verify_integrity` is True and the join would result in\n            an excessively large number of records.\n    \"\"\"\n\n    self.lazy = True\n\n    other.lazy = True\n\n    verify_join_select_integrity(cross_join_input, left_columns=self.columns, right_columns=other.columns)\n    right_select = [v.old_name for v in cross_join_input.right_select.renames\n                    if (v.keep or v.join_key) and v.is_available]\n    left_select = [v.old_name for v in cross_join_input.left_select.renames\n                   if (v.keep or v.join_key) and v.is_available]\n\n    left = self.data_frame.select(left_select).rename(cross_join_input.left_select.rename_table)\n    right = other.data_frame.select(right_select).rename(cross_join_input.right_select.rename_table)\n\n    joined_df = left.join(right, how='cross')\n\n    cols_to_delete_after = [col.new_name for col in\n                            cross_join_input.left_select.renames + cross_join_input.left_select.renames\n                            if col.join_key and not col.keep and col.is_available]\n\n    fl = FlowDataEngine(joined_df.drop(cols_to_delete_after), calculate_schema_stats=False, streamable=False)\n    return fl\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.do_filter","title":"<code>do_filter(predicate)</code>","text":"<p>Filters rows based on a predicate expression.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>str</code> <p>A string containing a Polars expression that evaluates to a boolean value.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance containing only the rows that match</p> <code>FlowDataEngine</code> <p>the predicate.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def do_filter(self, predicate: str) -&gt; \"FlowDataEngine\":\n    \"\"\"Filters rows based on a predicate expression.\n\n    Args:\n        predicate: A string containing a Polars expression that evaluates to\n            a boolean value.\n\n    Returns:\n        A new `FlowDataEngine` instance containing only the rows that match\n        the predicate.\n    \"\"\"\n    try:\n        f = to_expr(predicate)\n    except Exception as e:\n        logger.warning(f'Error in filter expression: {e}')\n        f = to_expr(\"False\")\n    df = self.data_frame.filter(f)\n    return FlowDataEngine(df, schema=self.schema, streamable=self._streamable)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.do_group_by","title":"<code>do_group_by(group_by_input, calculate_schema_stats=True)</code>","text":"<p>Performs a group-by operation on the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>group_by_input</code> <code>GroupByInput</code> <p>A <code>GroupByInput</code> object defining the grouping columns and aggregations.</p> required <code>calculate_schema_stats</code> <code>bool</code> <p>If True, calculates schema statistics for the resulting DataFrame.</p> <code>True</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the grouped and aggregated data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def do_group_by(self, group_by_input: transform_schemas.GroupByInput,\n                calculate_schema_stats: bool = True) -&gt; \"FlowDataEngine\":\n    \"\"\"Performs a group-by operation on the DataFrame.\n\n    Args:\n        group_by_input: A `GroupByInput` object defining the grouping columns\n            and aggregations.\n        calculate_schema_stats: If True, calculates schema statistics for the\n            resulting DataFrame.\n\n    Returns:\n        A new `FlowDataEngine` instance with the grouped and aggregated data.\n    \"\"\"\n    aggregations = [c for c in group_by_input.agg_cols if c.agg != 'groupby']\n    group_columns = [c for c in group_by_input.agg_cols if c.agg == 'groupby']\n\n    if len(group_columns) == 0:\n        return FlowDataEngine(\n            self.data_frame.select(\n                ac.agg_func(ac.old_name).alias(ac.new_name) for ac in aggregations\n            ),\n            calculate_schema_stats=calculate_schema_stats\n        )\n\n    df = self.data_frame.rename({c.old_name: c.new_name for c in group_columns})\n    group_by_columns = [n_c.new_name for n_c in group_columns]\n    return FlowDataEngine(\n        df.group_by(*group_by_columns).agg(\n            ac.agg_func(ac.old_name).alias(ac.new_name) for ac in aggregations\n        ),\n        calculate_schema_stats=calculate_schema_stats\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.do_pivot","title":"<code>do_pivot(pivot_input, node_logger=None)</code>","text":"<p>Converts the DataFrame from a long to a wide format, aggregating values.</p> <p>Parameters:</p> Name Type Description Default <code>pivot_input</code> <code>PivotInput</code> <p>A <code>PivotInput</code> object defining the index, pivot, and value columns, along with the aggregation logic.</p> required <code>node_logger</code> <code>NodeLogger</code> <p>An optional logger for reporting warnings, e.g., if the pivot column has too many unique values.</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new, pivoted <code>FlowDataEngine</code> instance.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def do_pivot(self, pivot_input: transform_schemas.PivotInput, node_logger: NodeLogger = None) -&gt; \"FlowDataEngine\":\n    \"\"\"Converts the DataFrame from a long to a wide format, aggregating values.\n\n    Args:\n        pivot_input: A `PivotInput` object defining the index, pivot, and value\n            columns, along with the aggregation logic.\n        node_logger: An optional logger for reporting warnings, e.g., if the\n            pivot column has too many unique values.\n\n    Returns:\n        A new, pivoted `FlowDataEngine` instance.\n    \"\"\"\n    # Get unique values for pivot columns\n    max_unique_vals = 200\n    new_cols_unique = fetch_unique_values(self.data_frame.select(pivot_input.pivot_column)\n                                          .unique()\n                                          .sort(pivot_input.pivot_column)\n                                          .limit(max_unique_vals).cast(pl.String))\n    if len(new_cols_unique) &gt;= max_unique_vals:\n        if node_logger:\n            node_logger.warning('Pivot column has too many unique values. Please consider using a different column.'\n                                f' Max unique values: {max_unique_vals}')\n\n    if len(pivot_input.index_columns) == 0:\n        no_index_cols = True\n        pivot_input.index_columns = ['__temp__']\n        ff = self.apply_flowfile_formula('1', col_name='__temp__')\n    else:\n        no_index_cols = False\n        ff = self\n\n    # Perform pivot operations\n    index_columns = pivot_input.get_index_columns()\n    grouped_ff = ff.do_group_by(pivot_input.get_group_by_input(), False)\n    pivot_column = pivot_input.get_pivot_column()\n\n    input_df = grouped_ff.data_frame.with_columns(\n        pivot_column.cast(pl.String).alias(pivot_input.pivot_column)\n    )\n    number_of_aggregations = len(pivot_input.aggregations)\n    df = (\n        input_df.select(\n            *index_columns,\n            pivot_column,\n            pivot_input.get_values_expr()\n        )\n        .group_by(*index_columns)\n        .agg([\n            (pl.col('vals').filter(pivot_column == new_col_value))\n            .first()\n            .alias(new_col_value)\n            for new_col_value in new_cols_unique\n        ])\n        .select(\n            *index_columns,\n            *[\n                pl.col(new_col).struct.field(agg).alias(f'{new_col + \"_\" + agg if number_of_aggregations &gt; 1 else new_col }')\n                for new_col in new_cols_unique\n                for agg in pivot_input.aggregations\n            ]\n        )\n    )\n\n    # Clean up temporary columns if needed\n    if no_index_cols:\n        df = df.drop('__temp__')\n        pivot_input.index_columns = []\n\n    return FlowDataEngine(df, calculate_schema_stats=False)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.do_select","title":"<code>do_select(select_inputs, keep_missing=True)</code>","text":"<p>Performs a complex column selection, renaming, and reordering operation.</p> <p>Parameters:</p> Name Type Description Default <code>select_inputs</code> <code>SelectInputs</code> <p>A <code>SelectInputs</code> object defining the desired transformations.</p> required <code>keep_missing</code> <code>bool</code> <p>If True, columns not specified in <code>select_inputs</code> are kept. If False, they are dropped.</p> <code>True</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> with the transformed selection.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def do_select(self, select_inputs: transform_schemas.SelectInputs,\n              keep_missing: bool = True) -&gt; \"FlowDataEngine\":\n    \"\"\"Performs a complex column selection, renaming, and reordering operation.\n\n    Args:\n        select_inputs: A `SelectInputs` object defining the desired transformations.\n        keep_missing: If True, columns not specified in `select_inputs` are kept.\n            If False, they are dropped.\n\n    Returns:\n        A new `FlowDataEngine` with the transformed selection.\n    \"\"\"\n    new_schema = deepcopy(self.schema)\n    renames = [r for r in select_inputs.renames if r.is_available]\n\n    if not keep_missing:\n        drop_cols = set(self.data_frame.collect_schema().names()) - set(r.old_name for r in renames).union(\n            set(r.old_name for r in renames if not r.keep))\n        keep_cols = []\n    else:\n        keep_cols = list(set(self.data_frame.collect_schema().names()) - set(r.old_name for r in renames))\n        drop_cols = set(r.old_name for r in renames if not r.keep)\n\n    if len(drop_cols) &gt; 0:\n        new_schema = [s for s in new_schema if s.name not in drop_cols]\n    new_schema_mapping = {v.name: v for v in new_schema}\n\n    available_renames = []\n    for rename in renames:\n        if (rename.new_name != rename.old_name or rename.new_name not in new_schema_mapping) and rename.keep:\n            schema_entry = new_schema_mapping.get(rename.old_name)\n            if schema_entry is not None:\n                available_renames.append(rename)\n                schema_entry.column_name = rename.new_name\n\n    rename_dict = {r.old_name: r.new_name for r in available_renames}\n    fl = self.select_columns(\n        list_select=[col_to_keep.old_name for col_to_keep in renames if col_to_keep.keep] + keep_cols)\n    fl = fl.change_column_types(transforms=[r for r in renames if r.keep])\n    ndf = fl.data_frame.rename(rename_dict)\n    renames.sort(key=lambda r: 0 if r.position is None else r.position)\n    sorted_cols = utils.match_order(ndf.collect_schema().names(),\n                                    [r.new_name for r in renames] + self.data_frame.collect_schema().names())\n    output_file = FlowDataEngine(ndf, number_of_records=self.number_of_records)\n    return output_file.reorganize_order(sorted_cols)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.do_sort","title":"<code>do_sort(sorts)</code>","text":"<p>Sorts the DataFrame by one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>sorts</code> <code>List[SortByInput]</code> <p>A list of <code>SortByInput</code> objects, each specifying a column and sort direction ('asc' or 'desc').</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the sorted data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def do_sort(self, sorts: List[transform_schemas.SortByInput]) -&gt; \"FlowDataEngine\":\n    \"\"\"Sorts the DataFrame by one or more columns.\n\n    Args:\n        sorts: A list of `SortByInput` objects, each specifying a column\n            and sort direction ('asc' or 'desc').\n\n    Returns:\n        A new `FlowDataEngine` instance with the sorted data.\n    \"\"\"\n    if not sorts:\n        return self\n\n    descending = [s.how == 'desc' or s.how.lower() == 'descending' for s in sorts]\n    df = self.data_frame.sort([sort_by.column for sort_by in sorts], descending=descending)\n    return FlowDataEngine(df, number_of_records=self.number_of_records, schema=self.schema)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.drop_columns","title":"<code>drop_columns(columns)</code>","text":"<p>Drops specified columns from the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>List[str]</code> <p>A list of column names to drop.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance without the dropped columns.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def drop_columns(self, columns: List[str]) -&gt; \"FlowDataEngine\":\n    \"\"\"Drops specified columns from the DataFrame.\n\n    Args:\n        columns: A list of column names to drop.\n\n    Returns:\n        A new `FlowDataEngine` instance without the dropped columns.\n    \"\"\"\n    cols_for_select = tuple(set(self.columns) - set(columns))\n    idx_to_keep = [self.cols_idx.get(c) for c in cols_for_select]\n    new_schema = [self.schema[i] for i in idx_to_keep]\n\n    return FlowDataEngine(\n        self.data_frame.select(cols_for_select),\n        number_of_records=self.number_of_records,\n        schema=new_schema\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.from_cloud_storage_obj","title":"<code>from_cloud_storage_obj(settings)</code>  <code>classmethod</code>","text":"<p>Creates a FlowDataEngine from an object in cloud storage.</p> <p>This method supports reading from various cloud storage providers like AWS S3, Azure Data Lake Storage, and Google Cloud Storage, with support for various authentication methods.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>CloudStorageReadSettingsInternal</code> <p>A <code>CloudStorageReadSettingsInternal</code> object containing connection details, file format, and read options.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance containing the data from cloud storage.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the storage type or file format is not supported.</p> <code>NotImplementedError</code> <p>If a requested file format like \"delta\" or \"iceberg\" is not yet implemented.</p> <code>Exception</code> <p>If reading from cloud storage fails.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef from_cloud_storage_obj(cls, settings: cloud_storage_schemas.CloudStorageReadSettingsInternal) -&gt; \"FlowDataEngine\":\n    \"\"\"Creates a FlowDataEngine from an object in cloud storage.\n\n    This method supports reading from various cloud storage providers like AWS S3,\n    Azure Data Lake Storage, and Google Cloud Storage, with support for\n    various authentication methods.\n\n    Args:\n        settings: A `CloudStorageReadSettingsInternal` object containing connection\n            details, file format, and read options.\n\n    Returns:\n        A new `FlowDataEngine` instance containing the data from cloud storage.\n\n    Raises:\n        ValueError: If the storage type or file format is not supported.\n        NotImplementedError: If a requested file format like \"delta\" or \"iceberg\"\n            is not yet implemented.\n        Exception: If reading from cloud storage fails.\n    \"\"\"\n    connection = settings.connection\n    read_settings = settings.read_settings\n\n    logger.info(f\"Reading from {connection.storage_type} storage: {read_settings.resource_path}\")\n    # Get storage options based on connection type\n    storage_options = CloudStorageReader.get_storage_options(connection)\n    # Get credential provider if needed\n    credential_provider = CloudStorageReader.get_credential_provider(connection)\n    if read_settings.file_format == \"parquet\":\n        return cls._read_parquet_from_cloud(\n            read_settings.resource_path,\n            storage_options,\n            credential_provider,\n            read_settings.scan_mode == \"directory\",\n        )\n    elif read_settings.file_format == \"delta\":\n        return cls._read_delta_from_cloud(\n            read_settings.resource_path,\n            storage_options,\n            credential_provider,\n            read_settings\n        )\n    elif read_settings.file_format == \"csv\":\n        return cls._read_csv_from_cloud(\n            read_settings.resource_path,\n            storage_options,\n            credential_provider,\n            read_settings\n        )\n    elif read_settings.file_format == \"json\":\n        return cls._read_json_from_cloud(\n            read_settings.resource_path,\n            storage_options,\n            credential_provider,\n            read_settings.scan_mode == \"directory\"\n        )\n    elif read_settings.file_format == \"iceberg\":\n        return cls._read_iceberg_from_cloud(\n            read_settings.resource_path,\n            storage_options,\n            credential_provider,\n            read_settings\n        )\n\n    elif read_settings.file_format in [\"delta\", \"iceberg\"]:\n        # These would require additional libraries\n        raise NotImplementedError(f\"File format {read_settings.file_format} not yet implemented\")\n    else:\n        raise ValueError(f\"Unsupported file format: {read_settings.file_format}\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.generate_enumerator","title":"<code>generate_enumerator(length=1000, output_name='output_column')</code>  <code>classmethod</code>","text":"<p>Generates a FlowDataEngine with a single column containing a sequence of integers.</p> <p>Parameters:</p> Name Type Description Default <code>length</code> <code>int</code> <p>The number of integers to generate in the sequence.</p> <code>1000</code> <code>output_name</code> <code>str</code> <p>The name of the output column.</p> <code>'output_column'</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>@classmethod\ndef generate_enumerator(cls, length: int = 1000, output_name: str = 'output_column') -&gt; \"FlowDataEngine\":\n    \"\"\"Generates a FlowDataEngine with a single column containing a sequence of integers.\n\n    Args:\n        length: The number of integers to generate in the sequence.\n        output_name: The name of the output column.\n\n    Returns:\n        A new `FlowDataEngine` instance.\n    \"\"\"\n    if length &gt; 10_000_000:\n        length = 10_000_000\n    return cls(pl.LazyFrame().select((pl.int_range(0, length, dtype=pl.UInt32)).alias(output_name)))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_estimated_file_size","title":"<code>get_estimated_file_size()</code>","text":"<p>Estimates the file size in bytes if the data originated from a local file.</p> <p>This relies on the original path being tracked during file ingestion.</p> <p>Returns:</p> Type Description <code>int</code> <p>The file size in bytes, or 0 if the original path is unknown.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_estimated_file_size(self) -&gt; int:\n    \"\"\"Estimates the file size in bytes if the data originated from a local file.\n\n    This relies on the original path being tracked during file ingestion.\n\n    Returns:\n        The file size in bytes, or 0 if the original path is unknown.\n    \"\"\"\n    if self._org_path is not None:\n        return os.path.getsize(self._org_path)\n    return 0\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_number_of_records","title":"<code>get_number_of_records(warn=False, force_calculate=False, calculate_in_worker_process=False)</code>","text":"<p>Gets the total number of records in the DataFrame.</p> <p>For lazy frames, this may trigger a full data scan, which can be expensive.</p> <p>Parameters:</p> Name Type Description Default <code>warn</code> <code>bool</code> <p>If True, logs a warning if a potentially expensive calculation is triggered.</p> <code>False</code> <code>force_calculate</code> <code>bool</code> <p>If True, forces recalculation even if a value is cached.</p> <code>False</code> <code>calculate_in_worker_process</code> <code>bool</code> <p>If True, offloads the calculation to a worker process.</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>The total number of records.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of records could not be determined.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_number_of_records(self, warn: bool = False, force_calculate: bool = False,\n                          calculate_in_worker_process: bool = False) -&gt; int:\n    \"\"\"Gets the total number of records in the DataFrame.\n\n    For lazy frames, this may trigger a full data scan, which can be expensive.\n\n    Args:\n        warn: If True, logs a warning if a potentially expensive calculation is triggered.\n        force_calculate: If True, forces recalculation even if a value is cached.\n        calculate_in_worker_process: If True, offloads the calculation to a worker process.\n\n    Returns:\n        The total number of records.\n\n    Raises:\n        ValueError: If the number of records could not be determined.\n    \"\"\"\n    if self.is_future and not self.is_collected:\n        return -1\n    if self.number_of_records is None or self.number_of_records &lt; 0 or force_calculate:\n        if self._number_of_records_callback is not None:\n            self._number_of_records_callback(self)\n\n        if self.lazy:\n            if calculate_in_worker_process:\n                try:\n                    self.number_of_records = self._calculate_number_of_records_in_worker()\n                    return self.number_of_records\n                except Exception as e:\n                    logger.error(f\"Error: {e}\")\n            if warn:\n                logger.warning('Calculating the number of records this can be expensive on a lazy frame')\n            try:\n                self.number_of_records = self.data_frame.select(pl.len()).collect(\n                    engine=\"streaming\" if self._streamable else \"auto\")[0, 0]\n            except Exception:\n                raise ValueError('Could not get number of records')\n        else:\n            self.number_of_records = self.data_frame.__len__()\n    return self.number_of_records\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_number_of_records_in_process","title":"<code>get_number_of_records_in_process(force_calculate=False)</code>","text":"<p>Get the number of records in the DataFrame in the local process.</p> <p>Parameters:</p> Name Type Description Default <code>force_calculate</code> <code>bool</code> <p>If True, forces recalculation even if a value is cached.</p> <code>False</code> <p>Returns:</p> Type Description <p>The total number of records.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_number_of_records_in_process(self, force_calculate: bool = False):\n    \"\"\"\n    Get the number of records in the DataFrame in the local process.\n\n    args:\n        force_calculate: If True, forces recalculation even if a value is cached.\n\n    Returns:\n        The total number of records.\n    \"\"\"\n    return self.get_number_of_records(force_calculate=force_calculate)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_output_sample","title":"<code>get_output_sample(n_rows=10)</code>","text":"<p>Gets a sample of the data as a list of dictionaries.</p> <p>This is typically used to display a preview of the data in a UI.</p> <p>Parameters:</p> Name Type Description Default <code>n_rows</code> <code>int</code> <p>The number of rows to sample.</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A list of dictionaries, where each dictionary represents a row.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_output_sample(self, n_rows: int = 10) -&gt; List[Dict]:\n    \"\"\"Gets a sample of the data as a list of dictionaries.\n\n    This is typically used to display a preview of the data in a UI.\n\n    Args:\n        n_rows: The number of rows to sample.\n\n    Returns:\n        A list of dictionaries, where each dictionary represents a row.\n    \"\"\"\n    if self.number_of_records &gt; n_rows or self.number_of_records &lt; 0:\n        df = self.collect(n_rows)\n    else:\n        df = self.collect()\n    return df.to_dicts()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_record_count","title":"<code>get_record_count()</code>","text":"<p>Returns a new FlowDataEngine with a single column 'number_of_records' containing the total number of records.</p> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_record_count(self) -&gt; \"FlowDataEngine\":\n    \"\"\"Returns a new FlowDataEngine with a single column 'number_of_records'\n    containing the total number of records.\n\n    Returns:\n        A new `FlowDataEngine` instance.\n    \"\"\"\n    return FlowDataEngine(self.data_frame.select(pl.len().alias('number_of_records')))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_sample","title":"<code>get_sample(n_rows=100, random=False, shuffle=False, seed=None, execution_location=None)</code>","text":"<p>Gets a sample of rows from the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>n_rows</code> <code>int</code> <p>The number of rows to sample.</p> <code>100</code> <code>random</code> <code>bool</code> <p>If True, performs random sampling. If False, takes the first n_rows.</p> <code>False</code> <code>shuffle</code> <code>bool</code> <p>If True (and <code>random</code> is True), shuffles the data before sampling.</p> <code>False</code> <code>seed</code> <code>int</code> <p>A random seed for reproducibility.</p> <code>None</code> <code>execution_location</code> <code>Optional[ExecutionLocationsLiteral]</code> <p>Location which is used to calculate the size of the dataframe</p> <code>None</code> <p>Returns:     A new <code>FlowDataEngine</code> instance containing the sampled data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_sample(self, n_rows: int = 100, random: bool = False, shuffle: bool = False,\n               seed: int = None, execution_location: Optional[ExecutionLocationsLiteral] = None) -&gt; \"FlowDataEngine\":\n    \"\"\"Gets a sample of rows from the DataFrame.\n\n    Args:\n        n_rows: The number of rows to sample.\n        random: If True, performs random sampling. If False, takes the first n_rows.\n        shuffle: If True (and `random` is True), shuffles the data before sampling.\n        seed: A random seed for reproducibility.\n        execution_location: Location which is used to calculate the size of the dataframe\n    Returns:\n        A new `FlowDataEngine` instance containing the sampled data.\n    \"\"\"\n    logging.info(f'Getting sample of {n_rows} rows')\n\n    if random:\n        if self.lazy and self.external_source is not None:\n            self.collect_external()\n\n        if self.lazy and shuffle:\n            sample_df = (self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\")\n                         .sample(n_rows, seed=seed, shuffle=shuffle))\n        elif shuffle:\n            sample_df = self.data_frame.sample(n_rows, seed=seed, shuffle=shuffle)\n        else:\n            if execution_location is None:\n                execution_location = get_global_execution_location()\n            n_rows = min(n_rows, self.get_number_of_records(\n                calculate_in_worker_process=execution_location == \"remote\")\n                         )\n\n            every_n_records = ceil(self.number_of_records / n_rows)\n            sample_df = self.data_frame.gather_every(every_n_records)\n    else:\n        if self.external_source:\n            self.collect(n_rows)\n        sample_df = self.data_frame.head(n_rows)\n\n    return FlowDataEngine(sample_df, schema=self.schema)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_schema_column","title":"<code>get_schema_column(col_name)</code>","text":"<p>Retrieves the schema information for a single column by its name.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to retrieve.</p> required <p>Returns:</p> Type Description <code>FlowfileColumn</code> <p>A <code>FlowfileColumn</code> object for the specified column, or <code>None</code> if not found.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_schema_column(self, col_name: str) -&gt; FlowfileColumn:\n    \"\"\"Retrieves the schema information for a single column by its name.\n\n    Args:\n        col_name: The name of the column to retrieve.\n\n    Returns:\n        A `FlowfileColumn` object for the specified column, or `None` if not found.\n    \"\"\"\n    for s in self.schema:\n        if s.name == col_name:\n            return s\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_select_inputs","title":"<code>get_select_inputs()</code>","text":"<p>Gets <code>SelectInput</code> specifications for all columns in the current schema.</p> <p>Returns:</p> Type Description <code>SelectInputs</code> <p>A <code>SelectInputs</code> object that can be used to configure selection or</p> <code>SelectInputs</code> <p>transformation operations.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_select_inputs(self) -&gt; transform_schemas.SelectInputs:\n    \"\"\"Gets `SelectInput` specifications for all columns in the current schema.\n\n    Returns:\n        A `SelectInputs` object that can be used to configure selection or\n        transformation operations.\n    \"\"\"\n    return transform_schemas.SelectInputs(\n        [transform_schemas.SelectInput(old_name=c.name, data_type=c.data_type) for c in self.schema]\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.get_subset","title":"<code>get_subset(n_rows=100)</code>","text":"<p>Gets the first <code>n_rows</code> from the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>n_rows</code> <code>int</code> <p>The number of rows to include in the subset.</p> <code>100</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance containing the subset of data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def get_subset(self, n_rows: int = 100) -&gt; \"FlowDataEngine\":\n    \"\"\"Gets the first `n_rows` from the DataFrame.\n\n    Args:\n        n_rows: The number of rows to include in the subset.\n\n    Returns:\n        A new `FlowDataEngine` instance containing the subset of data.\n    \"\"\"\n    if not self.lazy:\n        return FlowDataEngine(self.data_frame.head(n_rows), calculate_schema_stats=True)\n    else:\n        return FlowDataEngine(self.data_frame.head(n_rows), calculate_schema_stats=True)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.initialize_empty_fl","title":"<code>initialize_empty_fl()</code>","text":"<p>Initializes an empty LazyFrame.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def initialize_empty_fl(self):\n    \"\"\"Initializes an empty LazyFrame.\"\"\"\n    self.data_frame = pl.LazyFrame()\n    self.number_of_records = 0\n    self._lazy = True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.iter_batches","title":"<code>iter_batches(batch_size=1000, columns=None)</code>","text":"<p>Iterates over the DataFrame in batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The size of each batch.</p> <code>1000</code> <code>columns</code> <code>Union[List, Tuple, str]</code> <p>A list of column names to include in the batches. If None, all columns are included.</p> <code>None</code> <p>Yields:</p> Type Description <code>FlowDataEngine</code> <p>A <code>FlowDataEngine</code> instance for each batch.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def iter_batches(self, batch_size: int = 1000,\n                 columns: Union[List, Tuple, str] = None) -&gt; Generator[\"FlowDataEngine\", None, None]:\n    \"\"\"Iterates over the DataFrame in batches.\n\n    Args:\n        batch_size: The size of each batch.\n        columns: A list of column names to include in the batches. If None,\n            all columns are included.\n\n    Yields:\n        A `FlowDataEngine` instance for each batch.\n    \"\"\"\n    if columns:\n        self.data_frame = self.data_frame.select(columns)\n    self.lazy = False\n    batches = self.data_frame.iter_slices(batch_size)\n    for batch in batches:\n        yield FlowDataEngine(batch)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.join","title":"<code>join(join_input, auto_generate_selection, verify_integrity, other)</code>","text":"<p>Performs a standard SQL-style join with another DataFrame.</p> <p>Supports various join types like 'inner', 'left', 'right', 'outer', 'semi', and 'anti'.</p> <p>Parameters:</p> Name Type Description Default <code>join_input</code> <code>JoinInput</code> <p>A <code>JoinInput</code> object defining the join keys, join type, and column selections.</p> required <code>auto_generate_selection</code> <code>bool</code> <p>If True, automatically handles column renaming.</p> required <code>verify_integrity</code> <code>bool</code> <p>If True, performs checks to prevent excessively large joins.</p> required <code>other</code> <code>FlowDataEngine</code> <p>The right <code>FlowDataEngine</code> to join with.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> with the joined data.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the join configuration is invalid or if <code>verify_integrity</code> is True and the join is predicted to be too large.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def join(self, join_input: transform_schemas.JoinInput, auto_generate_selection: bool,\n         verify_integrity: bool, other: \"FlowDataEngine\") -&gt; \"FlowDataEngine\":\n    \"\"\"Performs a standard SQL-style join with another DataFrame.\n\n    Supports various join types like 'inner', 'left', 'right', 'outer', 'semi', and 'anti'.\n\n    Args:\n        join_input: A `JoinInput` object defining the join keys, join type,\n            and column selections.\n        auto_generate_selection: If True, automatically handles column renaming.\n        verify_integrity: If True, performs checks to prevent excessively large joins.\n        other: The right `FlowDataEngine` to join with.\n\n    Returns:\n        A new `FlowDataEngine` with the joined data.\n\n    Raises:\n        Exception: If the join configuration is invalid or if `verify_integrity`\n            is True and the join is predicted to be too large.\n    \"\"\"\n    ensure_right_unselect_for_semi_and_anti_joins(join_input)\n    verify_join_select_integrity(join_input, left_columns=self.columns, right_columns=other.columns)\n    if not verify_join_map_integrity(join_input, left_columns=self.schema, right_columns=other.schema):\n        raise Exception('Join is not valid by the data fields')\n    if auto_generate_selection:\n        join_input.auto_rename()\n    left = self.data_frame.select(get_select_columns(join_input.left_select.renames)).rename(join_input.left_select.rename_table)\n    right = other.data_frame.select(get_select_columns(join_input.right_select.renames)).rename(join_input.right_select.rename_table)\n    if verify_integrity and join_input.how != 'right':\n        n_records = get_join_count(left, right, left_on_keys=join_input.left_join_keys,\n                                   right_on_keys=join_input.right_join_keys, how=join_input.how)\n        if n_records &gt; 1_000_000_000:\n            raise Exception(\"Join will result in too many records, ending process\")\n    else:\n        n_records = -1\n    left, right, reverse_join_key_mapping = _handle_duplication_join_keys(left, right, join_input)\n    left, right = rename_df_table_for_join(left, right, join_input.get_join_key_renames())\n    if join_input.how == 'right':\n        joined_df = right.join(\n            other=left,\n            left_on=join_input.right_join_keys,\n            right_on=join_input.left_join_keys,\n            how=\"left\",\n            suffix=\"\").rename(reverse_join_key_mapping)\n    else:\n        joined_df = left.join(\n            other=right,\n            left_on=join_input.left_join_keys,\n            right_on=join_input.right_join_keys,\n            how=join_input.how,\n            suffix=\"\").rename(reverse_join_key_mapping)\n    left_cols_to_delete_after = [get_col_name_to_delete(col, 'left') for col in join_input.left_select.renames\n                                 if not col.keep\n                                 and col.is_available and col.join_key\n                                 ]\n    right_cols_to_delete_after = [get_col_name_to_delete(col, 'right') for col in join_input.right_select.renames\n                                  if not col.keep\n                                  and col.is_available and col.join_key\n                                  and join_input.how in (\"left\", \"right\", \"inner\", \"cross\", \"outer\")\n                                  ]\n    if len(right_cols_to_delete_after + left_cols_to_delete_after) &gt; 0:\n        joined_df = joined_df.drop(left_cols_to_delete_after + right_cols_to_delete_after)\n    undo_join_key_remapping = get_undo_rename_mapping_join(join_input)\n    joined_df = joined_df.rename(undo_join_key_remapping)\n\n    if verify_integrity:\n        return FlowDataEngine(joined_df, calculate_schema_stats=True,\n                              number_of_records=n_records, streamable=False)\n    else:\n        fl = FlowDataEngine(joined_df, calculate_schema_stats=False,\n                            number_of_records=0, streamable=False)\n        return fl\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.make_unique","title":"<code>make_unique(unique_input=None)</code>","text":"<p>Gets the unique rows from the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>unique_input</code> <code>UniqueInput</code> <p>A <code>UniqueInput</code> object specifying a subset of columns to consider for uniqueness and a strategy for keeping rows.</p> <code>None</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with unique rows.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def make_unique(self, unique_input: transform_schemas.UniqueInput = None) -&gt; \"FlowDataEngine\":\n    \"\"\"Gets the unique rows from the DataFrame.\n\n    Args:\n        unique_input: A `UniqueInput` object specifying a subset of columns\n            to consider for uniqueness and a strategy for keeping rows.\n\n    Returns:\n        A new `FlowDataEngine` instance with unique rows.\n    \"\"\"\n    if unique_input is None or unique_input.columns is None:\n        return FlowDataEngine(self.data_frame.unique())\n    return FlowDataEngine(self.data_frame.unique(unique_input.columns, keep=unique_input.strategy))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.output","title":"<code>output(output_fs, flow_id, node_id, execute_remote=True)</code>","text":"<p>Writes the DataFrame to an output file.</p> <p>Can execute the write operation locally or in a remote worker process.</p> <p>Parameters:</p> Name Type Description Default <code>output_fs</code> <code>OutputSettings</code> <p>An <code>OutputSettings</code> object with details about the output file.</p> required <code>flow_id</code> <code>int</code> <p>The flow ID for tracking.</p> required <code>node_id</code> <code>int | str</code> <p>The node ID for tracking.</p> required <code>execute_remote</code> <code>bool</code> <p>If True, executes the write in a worker process.</p> <code>True</code> <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>The same <code>FlowDataEngine</code> instance for chaining.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def output(self, output_fs: input_schema.OutputSettings, flow_id: int, node_id: int | str,\n           execute_remote: bool = True) -&gt; \"FlowDataEngine\":\n    \"\"\"Writes the DataFrame to an output file.\n\n    Can execute the write operation locally or in a remote worker process.\n\n    Args:\n        output_fs: An `OutputSettings` object with details about the output file.\n        flow_id: The flow ID for tracking.\n        node_id: The node ID for tracking.\n        execute_remote: If True, executes the write in a worker process.\n\n    Returns:\n        The same `FlowDataEngine` instance for chaining.\n    \"\"\"\n    logger.info('Starting to write output')\n    if execute_remote:\n        status = utils.write_output(\n            self.data_frame,\n            data_type=output_fs.file_type,\n            path=output_fs.abs_file_path,\n            write_mode=output_fs.write_mode,\n            sheet_name=output_fs.output_excel_table.sheet_name,\n            delimiter=output_fs.output_csv_table.delimiter,\n            flow_id=flow_id,\n            node_id=node_id\n        )\n        tracker = ExternalExecutorTracker(status)\n        tracker.get_result()\n        logger.info('Finished writing output')\n    else:\n        logger.info(\"Starting to write results locally\")\n        utils.local_write_output(\n            self.data_frame,\n            data_type=output_fs.file_type,\n            path=output_fs.abs_file_path,\n            write_mode=output_fs.write_mode,\n            sheet_name=output_fs.output_excel_table.sheet_name,\n            delimiter=output_fs.output_csv_table.delimiter,\n            flow_id=flow_id,\n            node_id=node_id,\n        )\n        logger.info(\"Finished writing output\")\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.reorganize_order","title":"<code>reorganize_order(column_order)</code>","text":"<p>Reorganizes columns into a specified order.</p> <p>Parameters:</p> Name Type Description Default <code>column_order</code> <code>List[str]</code> <p>A list of column names in the desired order.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the columns reordered.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def reorganize_order(self, column_order: List[str]) -&gt; \"FlowDataEngine\":\n    \"\"\"Reorganizes columns into a specified order.\n\n    Args:\n        column_order: A list of column names in the desired order.\n\n    Returns:\n        A new `FlowDataEngine` instance with the columns reordered.\n    \"\"\"\n    df = self.data_frame.select(column_order)\n    schema = sorted(self.schema, key=lambda x: column_order.index(x.column_name))\n    return FlowDataEngine(df, schema=schema, number_of_records=self.number_of_records)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.save","title":"<code>save(path, data_type='parquet')</code>","text":"<p>Saves the DataFrame to a file in a separate thread.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to save to.</p> required <code>data_type</code> <code>str</code> <p>The format to save in (e.g., 'parquet', 'csv').</p> <code>'parquet'</code> <p>Returns:</p> Type Description <code>Future</code> <p>A <code>loky.Future</code> object representing the asynchronous save operation.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def save(self, path: str, data_type: str = 'parquet') -&gt; Future:\n    \"\"\"Saves the DataFrame to a file in a separate thread.\n\n    Args:\n        path: The file path to save to.\n        data_type: The format to save in (e.g., 'parquet', 'csv').\n\n    Returns:\n        A `loky.Future` object representing the asynchronous save operation.\n    \"\"\"\n    estimated_size = deepcopy(self.get_estimated_file_size() * 4)\n    df = deepcopy(self.data_frame)\n    return write_threaded(_df=df, path=path, data_type=data_type, estimated_size=estimated_size)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.select_columns","title":"<code>select_columns(list_select)</code>","text":"<p>Selects a subset of columns from the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>list_select</code> <code>Union[List[str], Tuple[str], str]</code> <p>A list, tuple, or single string of column names to select.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance containing only the selected columns.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def select_columns(self, list_select: Union[List[str], Tuple[str], str]) -&gt; \"FlowDataEngine\":\n    \"\"\"Selects a subset of columns from the DataFrame.\n\n    Args:\n        list_select: A list, tuple, or single string of column names to select.\n\n    Returns:\n        A new `FlowDataEngine` instance containing only the selected columns.\n    \"\"\"\n    if isinstance(list_select, str):\n        list_select = [list_select]\n\n    idx_to_keep = [self.cols_idx.get(c) for c in list_select]\n    selects = [ls for ls, id_to_keep in zip(list_select, idx_to_keep) if id_to_keep is not None]\n    new_schema = [self.schema[i] for i in idx_to_keep if i is not None]\n\n    return FlowDataEngine(\n        self.data_frame.select(selects),\n        number_of_records=self.number_of_records,\n        schema=new_schema,\n        streamable=self._streamable\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.set_streamable","title":"<code>set_streamable(streamable=False)</code>","text":"<p>Sets whether DataFrame operations should be streamable.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def set_streamable(self, streamable: bool = False):\n    \"\"\"Sets whether DataFrame operations should be streamable.\"\"\"\n    self._streamable = streamable\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.solve_graph","title":"<code>solve_graph(graph_solver_input)</code>","text":"<p>Solves a graph problem represented by 'from' and 'to' columns.</p> <p>This is used for operations like finding connected components in a graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph_solver_input</code> <code>GraphSolverInput</code> <p>A <code>GraphSolverInput</code> object defining the source, destination, and output column names.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the solved graph data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def solve_graph(self, graph_solver_input: transform_schemas.GraphSolverInput) -&gt; \"FlowDataEngine\":\n    \"\"\"Solves a graph problem represented by 'from' and 'to' columns.\n\n    This is used for operations like finding connected components in a graph.\n\n    Args:\n        graph_solver_input: A `GraphSolverInput` object defining the source,\n            destination, and output column names.\n\n    Returns:\n        A new `FlowDataEngine` instance with the solved graph data.\n    \"\"\"\n    lf = self.data_frame.with_columns(\n        graph_solver(graph_solver_input.col_from, graph_solver_input.col_to)\n        .alias(graph_solver_input.output_column_name)\n    )\n    return FlowDataEngine(lf)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.split","title":"<code>split(split_input)</code>","text":"<p>Splits a column's text values into multiple rows based on a delimiter.</p> <p>This operation is often referred to as \"exploding\" the DataFrame, as it increases the number of rows.</p> <p>Parameters:</p> Name Type Description Default <code>split_input</code> <code>TextToRowsInput</code> <p>A <code>TextToRowsInput</code> object specifying the column to split, the delimiter, and the output column name.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new <code>FlowDataEngine</code> instance with the exploded rows.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def split(self, split_input: transform_schemas.TextToRowsInput) -&gt; \"FlowDataEngine\":\n    \"\"\"Splits a column's text values into multiple rows based on a delimiter.\n\n    This operation is often referred to as \"exploding\" the DataFrame, as it\n    increases the number of rows.\n\n    Args:\n        split_input: A `TextToRowsInput` object specifying the column to split,\n            the delimiter, and the output column name.\n\n    Returns:\n        A new `FlowDataEngine` instance with the exploded rows.\n    \"\"\"\n    output_column_name = (\n        split_input.output_column_name\n        if split_input.output_column_name\n        else split_input.column_to_split\n    )\n\n    split_value = (\n        split_input.split_fixed_value\n        if split_input.split_by_fixed_value\n        else pl.col(split_input.split_by_column)\n    )\n\n    df = (\n        self.data_frame.with_columns(\n            pl.col(split_input.column_to_split)\n            .str.split(by=split_value)\n            .alias(output_column_name)\n        )\n        .explode(output_column_name)\n    )\n\n    return FlowDataEngine(df)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.start_fuzzy_join","title":"<code>start_fuzzy_join(fuzzy_match_input, other, file_ref, flow_id=-1, node_id=-1)</code>","text":"<p>Starts a fuzzy join operation in a background process.</p> <p>This method prepares the data and initiates the fuzzy matching in a separate process, returning a tracker object immediately.</p> <p>Parameters:</p> Name Type Description Default <code>fuzzy_match_input</code> <code>FuzzyMatchInput</code> <p>A <code>FuzzyMatchInput</code> object with the matching parameters.</p> required <code>other</code> <code>FlowDataEngine</code> <p>The right <code>FlowDataEngine</code> to join with.</p> required <code>file_ref</code> <code>str</code> <p>A reference string for temporary files.</p> required <code>flow_id</code> <code>int</code> <p>The flow ID for tracking.</p> <code>-1</code> <code>node_id</code> <code>int | str</code> <p>The node ID for tracking.</p> <code>-1</code> <p>Returns:</p> Type Description <code>ExternalFuzzyMatchFetcher</code> <p>An <code>ExternalFuzzyMatchFetcher</code> object that can be used to track the</p> <code>ExternalFuzzyMatchFetcher</code> <p>progress and retrieve the result of the fuzzy join.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def start_fuzzy_join(self, fuzzy_match_input: transform_schemas.FuzzyMatchInput,\n                     other: \"FlowDataEngine\", file_ref: str, flow_id: int = -1,\n                     node_id: int | str = -1) -&gt; ExternalFuzzyMatchFetcher:\n    \"\"\"Starts a fuzzy join operation in a background process.\n\n    This method prepares the data and initiates the fuzzy matching in a\n    separate process, returning a tracker object immediately.\n\n    Args:\n        fuzzy_match_input: A `FuzzyMatchInput` object with the matching parameters.\n        other: The right `FlowDataEngine` to join with.\n        file_ref: A reference string for temporary files.\n        flow_id: The flow ID for tracking.\n        node_id: The node ID for tracking.\n\n    Returns:\n        An `ExternalFuzzyMatchFetcher` object that can be used to track the\n        progress and retrieve the result of the fuzzy join.\n    \"\"\"\n    left_df, right_df = prepare_for_fuzzy_match(left=self, right=other, fuzzy_match_input=fuzzy_match_input)\n    return ExternalFuzzyMatchFetcher(left_df, right_df,\n                                     fuzzy_maps=fuzzy_match_input.fuzzy_maps,\n                                     file_ref=file_ref + '_fm',\n                                     wait_on_completion=False,\n                                     flow_id=flow_id,\n                                     node_id=node_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.to_arrow","title":"<code>to_arrow()</code>","text":"<p>Converts the DataFrame to a PyArrow Table.</p> <p>This method triggers a <code>.collect()</code> call if the data is lazy, then converts the resulting eager DataFrame into a <code>pyarrow.Table</code>.</p> <p>Returns:</p> Type Description <code>Table</code> <p>A <code>pyarrow.Table</code> instance representing the data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def to_arrow(self) -&gt; PaTable:\n    \"\"\"Converts the DataFrame to a PyArrow Table.\n\n    This method triggers a `.collect()` call if the data is lazy,\n    then converts the resulting eager DataFrame into a `pyarrow.Table`.\n\n    Returns:\n        A `pyarrow.Table` instance representing the data.\n    \"\"\"\n    if self.lazy:\n        return self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\").to_arrow()\n    else:\n        return self.data_frame.to_arrow()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.to_cloud_storage_obj","title":"<code>to_cloud_storage_obj(settings)</code>","text":"<p>Writes the DataFrame to an object in cloud storage.</p> <p>This method supports writing to various cloud storage providers like AWS S3, Azure Data Lake Storage, and Google Cloud Storage.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>CloudStorageWriteSettingsInternal</code> <p>A <code>CloudStorageWriteSettingsInternal</code> object containing connection details, file format, and write options.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified file format is not supported for writing.</p> <code>NotImplementedError</code> <p>If the 'append' write mode is used with an unsupported format.</p> <code>Exception</code> <p>If the write operation to cloud storage fails for any reason.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def to_cloud_storage_obj(self, settings: cloud_storage_schemas.CloudStorageWriteSettingsInternal):\n    \"\"\"Writes the DataFrame to an object in cloud storage.\n\n    This method supports writing to various cloud storage providers like AWS S3,\n    Azure Data Lake Storage, and Google Cloud Storage.\n\n    Args:\n        settings: A `CloudStorageWriteSettingsInternal` object containing connection\n            details, file format, and write options.\n\n    Raises:\n        ValueError: If the specified file format is not supported for writing.\n        NotImplementedError: If the 'append' write mode is used with an unsupported format.\n        Exception: If the write operation to cloud storage fails for any reason.\n    \"\"\"\n    connection = settings.connection\n    write_settings = settings.write_settings\n\n    logger.info(f\"Writing to {connection.storage_type} storage: {write_settings.resource_path}\")\n\n    if write_settings.write_mode == 'append' and write_settings.file_format != \"delta\":\n        raise NotImplementedError(\"The 'append' write mode is not yet supported for this destination.\")\n    storage_options = CloudStorageReader.get_storage_options(connection)\n    credential_provider = CloudStorageReader.get_credential_provider(connection)\n    # Dispatch to the correct writer based on file format\n    if write_settings.file_format == \"parquet\":\n        self._write_parquet_to_cloud(\n            write_settings.resource_path,\n            storage_options,\n            credential_provider,\n            write_settings\n        )\n    elif write_settings.file_format == \"delta\":\n        self._write_delta_to_cloud(\n            write_settings.resource_path,\n            storage_options,\n            credential_provider,\n            write_settings\n        )\n    elif write_settings.file_format == \"csv\":\n        self._write_csv_to_cloud(\n            write_settings.resource_path,\n            storage_options,\n            credential_provider,\n            write_settings\n        )\n    elif write_settings.file_format == \"json\":\n        self._write_json_to_cloud(\n            write_settings.resource_path,\n            storage_options,\n            credential_provider,\n            write_settings\n        )\n    else:\n        raise ValueError(f\"Unsupported file format for writing: {write_settings.file_format}\")\n\n    logger.info(f\"Successfully wrote data to {write_settings.resource_path}\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.to_dict","title":"<code>to_dict()</code>","text":"<p>Converts the DataFrame to a Python dictionary of columns.</p> <p>Each key in the dictionary is a column name, and the corresponding value is a list of the data in that column.</p> <p>Returns:</p> Type Description <code>Dict[str, List]</code> <p>A dictionary mapping column names to lists of their values.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, List]:\n    \"\"\"Converts the DataFrame to a Python dictionary of columns.\n\n     Each key in the dictionary is a column name, and the corresponding value\n     is a list of the data in that column.\n\n     Returns:\n         A dictionary mapping column names to lists of their values.\n     \"\"\"\n    if self.lazy:\n        return self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\").to_dict(as_series=False)\n    else:\n        return self.data_frame.to_dict(as_series=False)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.to_pylist","title":"<code>to_pylist()</code>","text":"<p>Converts the DataFrame to a list of Python dictionaries.</p> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A list where each item is a dictionary representing a row.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def to_pylist(self) -&gt; List[Dict]:\n    \"\"\"Converts the DataFrame to a list of Python dictionaries.\n\n    Returns:\n        A list where each item is a dictionary representing a row.\n    \"\"\"\n    if self.lazy:\n        return self.data_frame.collect(engine=\"streaming\" if self._streamable else \"auto\").to_dicts()\n    return self.data_frame.to_dicts()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.to_raw_data","title":"<code>to_raw_data()</code>","text":"<p>Converts the DataFrame to a <code>RawData</code> schema object.</p> <p>Returns:</p> Type Description <code>RawData</code> <p>An <code>input_schema.RawData</code> object containing the schema and data.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def to_raw_data(self) -&gt; input_schema.RawData:\n    \"\"\"Converts the DataFrame to a `RawData` schema object.\n\n    Returns:\n        An `input_schema.RawData` object containing the schema and data.\n    \"\"\"\n    columns = [c.get_minimal_field_info() for c in self.schema]\n    data = list(self.to_dict().values())\n    return input_schema.RawData(columns=columns, data=data)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_data_engine.FlowDataEngine.unpivot","title":"<code>unpivot(unpivot_input)</code>","text":"<p>Converts the DataFrame from a wide to a long format.</p> <p>This is the inverse of a pivot operation, taking columns and transforming them into <code>variable</code> and <code>value</code> rows.</p> <p>Parameters:</p> Name Type Description Default <code>unpivot_input</code> <code>UnpivotInput</code> <p>An <code>UnpivotInput</code> object specifying which columns to unpivot and which to keep as index columns.</p> required <p>Returns:</p> Type Description <code>FlowDataEngine</code> <p>A new, unpivoted <code>FlowDataEngine</code> instance.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_data_engine.py</code> <pre><code>def unpivot(self, unpivot_input: transform_schemas.UnpivotInput) -&gt; \"FlowDataEngine\":\n    \"\"\"Converts the DataFrame from a wide to a long format.\n\n    This is the inverse of a pivot operation, taking columns and transforming\n    them into `variable` and `value` rows.\n\n    Args:\n        unpivot_input: An `UnpivotInput` object specifying which columns to\n            unpivot and which to keep as index columns.\n\n    Returns:\n        A new, unpivoted `FlowDataEngine` instance.\n    \"\"\"\n    lf = self.data_frame\n\n    if unpivot_input.data_type_selector_expr is not None:\n        result = lf.unpivot(\n            on=unpivot_input.data_type_selector_expr(),\n            index=unpivot_input.index_columns\n        )\n    elif unpivot_input.value_columns is not None:\n        result = lf.unpivot(\n            on=unpivot_input.value_columns,\n            index=unpivot_input.index_columns\n        )\n    else:\n        result = lf.unpivot()\n\n    return FlowDataEngine(result)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfilecolumn","title":"FlowfileColumn","text":""},{"location":"for-developers/python-api-reference.html#flowfilecolumn_1","title":"<code>FlowfileColumn</code>","text":"<p>The <code>FlowfileColumn</code> is a data class that holds the schema and rich metadata for a single column managed by the <code>FlowDataEngine</code>.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_file_column.main.FlowfileColumn","title":"<code>flowfile_core.flowfile.flow_data_engine.flow_file_column.main.FlowfileColumn</code>  <code>dataclass</code>","text":"<p>Methods:</p> Name Description <code>__repr__</code> <p>Provides a concise, developer-friendly representation of the object.</p> <code>__str__</code> <p>Provides a detailed, readable summary of the column's metadata.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_file_column/main.py</code> <pre><code>@dataclass\nclass FlowfileColumn:\n    column_name: str\n    data_type: str\n    size: int\n    max_value: str\n    min_value: str\n    col_index: int\n    number_of_empty_values: int\n    number_of_unique_values: int\n    example_values: str\n    data_type_group: ReadableDataTypeGroup\n    __sql_type: Optional[Any]\n    __is_unique: Optional[bool]\n    __nullable: Optional[bool]\n    __has_values: Optional[bool]\n    average_value: Optional[str]\n    __perc_unique: Optional[float]\n\n    def __init__(self, polars_type: PlType):\n        self.data_type = convert_pl_type_to_string(polars_type.pl_datatype)\n        self.size = polars_type.count - polars_type.null_count\n        self.max_value = polars_type.max\n        self.min_value = polars_type.min\n        self.number_of_unique_values = polars_type.n_unique\n        self.number_of_empty_values = polars_type.null_count\n        self.example_values = polars_type.examples\n        self.column_name = polars_type.column_name\n        self.average_value = polars_type.mean\n        self.col_index = polars_type.col_index\n        self.__has_values = None\n        self.__nullable = None\n        self.__is_unique = None\n        self.__sql_type = None\n        self.__perc_unique = None\n        self.data_type_group = self.get_readable_datatype_group()\n\n    def __repr__(self):\n        \"\"\"\n        Provides a concise, developer-friendly representation of the object.\n        Ideal for debugging and console inspection.\n        \"\"\"\n        return (f\"FlowfileColumn(name='{self.column_name}', \"\n                f\"type={self.data_type}, \"\n                f\"size={self.size}, \"\n                f\"nulls={self.number_of_empty_values})\")\n\n    def __str__(self):\n        \"\"\"\n        Provides a detailed, readable summary of the column's metadata.\n        It conditionally omits any attribute that is None, ensuring a clean output.\n        \"\"\"\n        # --- Header (Always Shown) ---\n        header = f\"&lt;FlowfileColumn: '{self.column_name}'&gt;\"\n        lines = []\n\n        # --- Core Attributes (Conditionally Shown) ---\n        if self.data_type is not None:\n            lines.append(f\"  Type: {self.data_type}\")\n        if self.size is not None:\n            lines.append(f\"  Non-Nulls: {self.size}\")\n\n        # Calculate and display nulls if possible\n        if self.size is not None and self.number_of_empty_values is not None:\n            total_entries = self.size + self.number_of_empty_values\n            if total_entries &gt; 0:\n                null_perc = (self.number_of_empty_values / total_entries) * 100\n                null_info = f\"{self.number_of_empty_values} ({null_perc:.1f}%)\"\n            else:\n                null_info = \"0 (0.0%)\"\n            lines.append(f\"  Nulls: {null_info}\")\n\n        if self.number_of_unique_values is not None:\n            lines.append(f\"  Unique: {self.number_of_unique_values}\")\n\n        # --- Conditional Stats Section ---\n        stats = []\n        if self.min_value is not None:\n            stats.append(f\"    Min: {self.min_value}\")\n        if self.max_value is not None:\n            stats.append(f\"    Max: {self.max_value}\")\n        if self.average_value is not None:\n            stats.append(f\"    Mean: {self.average_value}\")\n\n        if stats:\n            lines.append(\"  Stats:\")\n            lines.extend(stats)\n\n        # --- Conditional Examples Section ---\n        if self.example_values:\n            example_str = str(self.example_values)\n            # Truncate long example strings for cleaner display\n            if len(example_str) &gt; 70:\n                example_str = example_str[:67] + '...'\n            lines.append(f\"  Examples: {example_str}\")\n\n        return f\"{header}\\n\" + \"\\n\".join(lines)\n\n    @classmethod\n    def create_from_polars_type(cls, polars_type: PlType, **kwargs) -&gt; \"FlowfileColumn\":\n        for k, v in kwargs.items():\n            if hasattr(polars_type, k):\n                setattr(polars_type, k, v)\n        return cls(polars_type)\n\n    @classmethod\n    def from_input(cls, column_name: str, data_type: str, **kwargs) -&gt; \"FlowfileColumn\":\n        pl_type = cast_str_to_polars_type(data_type)\n        if pl_type is not None:\n            data_type = pl_type\n        return cls(PlType(column_name=column_name, pl_datatype=data_type, **kwargs))\n\n    @classmethod\n    def create_from_polars_dtype(cls, column_name: str, data_type: pl.DataType, **kwargs):\n        return cls(PlType(column_name=column_name, pl_datatype=data_type, **kwargs))\n\n    def get_minimal_field_info(self) -&gt; input_schema.MinimalFieldInfo:\n        return input_schema.MinimalFieldInfo(name=self.column_name, data_type=self.data_type)\n\n    @classmethod\n    def create_from_minimal_field_info(cls, minimal_field_info: input_schema.MinimalFieldInfo) -&gt; \"FlowfileColumn\":\n        return cls.from_input(column_name=minimal_field_info.name,\n                              data_type=minimal_field_info.data_type)\n\n    @property\n    def is_unique(self) -&gt; bool:\n        if self.__is_unique is None:\n            if self.has_values:\n                self.__is_unique = self.number_of_unique_values == self.number_of_filled_values\n            else:\n                self.__is_unique = False\n        return self.__is_unique\n\n    @property\n    def perc_unique(self) -&gt; float:\n        if self.__perc_unique is None:\n            self.__perc_unique = self.number_of_unique_values / self.number_of_filled_values\n        return self.__perc_unique\n\n    @property\n    def has_values(self) -&gt; bool:\n        if not self.__has_values:\n            self.__has_values = self.number_of_unique_values &gt; 0\n        return self.__has_values\n\n    @property\n    def number_of_filled_values(self):\n        return self.size\n\n    @property\n    def nullable(self):\n        if self.__nullable is None:\n            self.__nullable = self.number_of_empty_values &gt; 0\n        return self.__nullable\n\n    @property\n    def name(self):\n        return self.column_name\n\n    def get_column_repr(self):\n        return dict(name=self.name,\n                    size=self.size,\n                    data_type=str(self.data_type),\n                    has_values=self.has_values,\n                    is_unique=self.is_unique,\n                    max_value=str(self.max_value),\n                    min_value=str(self.min_value),\n                    number_of_unique_values=self.number_of_unique_values,\n                    number_of_filled_values=self.number_of_filled_values,\n                    number_of_empty_values=self.number_of_empty_values,\n                    average_size=self.average_value)\n\n    def generic_datatype(self) -&gt; DataTypeGroup:\n        if self.data_type in ('Utf8', 'VARCHAR', 'CHAR', 'NVARCHAR', 'String'):\n            return 'str'\n        elif self.data_type in ('fixed_decimal', 'decimal', 'float', 'integer', 'boolean', 'double', 'Int16', 'Int32',\n                                'Int64', 'Float32', 'Float64', 'Decimal', 'Binary', 'Boolean', 'Uint8', 'Uint16',\n                                'Uint32', 'Uint64'):\n            return 'numeric'\n        elif self.data_type in ('datetime', 'date', 'Date', 'Datetime', 'Time'):\n            return 'date'\n        else:\n            return 'str'\n\n    def get_readable_datatype_group(self) -&gt; ReadableDataTypeGroup:\n        if self.data_type in ('Utf8', 'VARCHAR', 'CHAR', 'NVARCHAR', 'String'):\n            return 'String'\n        elif self.data_type in ('fixed_decimal', 'decimal', 'float', 'integer', 'boolean', 'double', 'Int16', 'Int32',\n                                'Int64', 'Float32', 'Float64', 'Decimal', 'Binary', 'Boolean', 'Uint8', 'Uint16',\n                                'Uint32', 'Uint64'):\n            return 'Numeric'\n        elif self.data_type in ('datetime', 'date', 'Date', 'Datetime', 'Time'):\n            return 'Date'\n        else:\n            return 'Other'\n\n    def get_polars_type(self) -&gt; PlType:\n        pl_datatype = cast_str_to_polars_type(self.data_type)\n        pl_type = PlType(pl_datatype=pl_datatype, **self.__dict__)\n        return pl_type\n\n    def update_type_from_polars_type(self, pl_type: PlType):\n        self.data_type = str(pl_type.pl_datatype.base_type())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_file_column.main.FlowfileColumn.__repr__","title":"<code>__repr__()</code>","text":"<p>Provides a concise, developer-friendly representation of the object. Ideal for debugging and console inspection.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_file_column/main.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    Provides a concise, developer-friendly representation of the object.\n    Ideal for debugging and console inspection.\n    \"\"\"\n    return (f\"FlowfileColumn(name='{self.column_name}', \"\n            f\"type={self.data_type}, \"\n            f\"size={self.size}, \"\n            f\"nulls={self.number_of_empty_values})\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.flowfile.flow_data_engine.flow_file_column.main.FlowfileColumn.__str__","title":"<code>__str__()</code>","text":"<p>Provides a detailed, readable summary of the column's metadata. It conditionally omits any attribute that is None, ensuring a clean output.</p> Source code in <code>flowfile_core/flowfile_core/flowfile/flow_data_engine/flow_file_column/main.py</code> <pre><code>def __str__(self):\n    \"\"\"\n    Provides a detailed, readable summary of the column's metadata.\n    It conditionally omits any attribute that is None, ensuring a clean output.\n    \"\"\"\n    # --- Header (Always Shown) ---\n    header = f\"&lt;FlowfileColumn: '{self.column_name}'&gt;\"\n    lines = []\n\n    # --- Core Attributes (Conditionally Shown) ---\n    if self.data_type is not None:\n        lines.append(f\"  Type: {self.data_type}\")\n    if self.size is not None:\n        lines.append(f\"  Non-Nulls: {self.size}\")\n\n    # Calculate and display nulls if possible\n    if self.size is not None and self.number_of_empty_values is not None:\n        total_entries = self.size + self.number_of_empty_values\n        if total_entries &gt; 0:\n            null_perc = (self.number_of_empty_values / total_entries) * 100\n            null_info = f\"{self.number_of_empty_values} ({null_perc:.1f}%)\"\n        else:\n            null_info = \"0 (0.0%)\"\n        lines.append(f\"  Nulls: {null_info}\")\n\n    if self.number_of_unique_values is not None:\n        lines.append(f\"  Unique: {self.number_of_unique_values}\")\n\n    # --- Conditional Stats Section ---\n    stats = []\n    if self.min_value is not None:\n        stats.append(f\"    Min: {self.min_value}\")\n    if self.max_value is not None:\n        stats.append(f\"    Max: {self.max_value}\")\n    if self.average_value is not None:\n        stats.append(f\"    Mean: {self.average_value}\")\n\n    if stats:\n        lines.append(\"  Stats:\")\n        lines.extend(stats)\n\n    # --- Conditional Examples Section ---\n    if self.example_values:\n        example_str = str(self.example_values)\n        # Truncate long example strings for cleaner display\n        if len(example_str) &gt; 70:\n            example_str = example_str[:67] + '...'\n        lines.append(f\"  Examples: {example_str}\")\n\n    return f\"{header}\\n\" + \"\\n\".join(lines)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#data-modeling-schemas","title":"Data Modeling (Schemas)","text":"<p>This section documents the Pydantic models that define the structure of settings and data.</p>"},{"location":"for-developers/python-api-reference.html#schemas","title":"<code>schemas</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas","title":"<code>flowfile_core.schemas.schemas</code>","text":"<p>Classes:</p> Name Description <code>FlowGraphConfig</code> <p>Configuration model for a flow graph's basic properties.</p> <code>FlowInformation</code> <p>Represents the complete state of a flow, including settings, nodes, and connections.</p> <code>FlowSettings</code> <p>Extends FlowGraphConfig with additional operational settings for a flow.</p> <code>NodeDefault</code> <p>Defines default properties for a node type.</p> <code>NodeEdge</code> <p>Represents a connection (edge) between two nodes in the frontend.</p> <code>NodeInformation</code> <p>Stores the state and configuration of a specific node instance within a flow.</p> <code>NodeInput</code> <p>Represents a node as it is received from the frontend, including position.</p> <code>NodeTemplate</code> <p>Defines the template for a node type, specifying its UI and functional characteristics.</p> <code>RawLogInput</code> <p>Schema for a raw log message.</p> <code>VueFlowInput</code> <p>Represents the complete graph structure from the Vue-based frontend.</p> <p>Functions:</p> Name Description <code>get_global_execution_location</code> <p>Calculates the default execution location based on the global settings</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowGraphConfig","title":"<code>FlowGraphConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration model for a flow graph's basic properties.</p> <p>Attributes:</p> Name Type Description <code>flow_id</code> <code>int</code> <p>Unique identifier for the flow.</p> <code>description</code> <code>Optional[str]</code> <p>A description of the flow.</p> <code>save_location</code> <code>Optional[str]</code> <p>The location where the flow is saved.</p> <code>name</code> <code>str</code> <p>The name of the flow.</p> <code>path</code> <code>str</code> <p>The file path associated with the flow.</p> <code>execution_mode</code> <code>ExecutionModeLiteral</code> <p>The mode of execution ('Development' or 'Performance').</p> <code>execution_location</code> <code>ExecutionLocationsLiteral</code> <p>The location for execution ('local', 'remote').</p> <p>Methods:</p> Name Description <code>validate_and_set_execution_location</code> <p>Validates and sets the execution location.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class FlowGraphConfig(BaseModel):\n    \"\"\"\n    Configuration model for a flow graph's basic properties.\n\n    Attributes:\n        flow_id (int): Unique identifier for the flow.\n        description (Optional[str]): A description of the flow.\n        save_location (Optional[str]): The location where the flow is saved.\n        name (str): The name of the flow.\n        path (str): The file path associated with the flow.\n        execution_mode (ExecutionModeLiteral): The mode of execution ('Development' or 'Performance').\n        execution_location (ExecutionLocationsLiteral): The location for execution ('local', 'remote').\n    \"\"\"\n    flow_id: int = Field(default_factory=create_unique_id, description=\"Unique identifier for the flow.\")\n    description: Optional[str] = None\n    save_location: Optional[str] = None\n    name: str = ''\n    path: str = ''\n    execution_mode: ExecutionModeLiteral = 'Performance'\n    execution_location: ExecutionLocationsLiteral = Field(default_factory=get_global_execution_location)\n\n    @field_validator('execution_location', mode='before')\n    def validate_and_set_execution_location(cls, v: Optional[ExecutionLocationsLiteral]) -&gt; ExecutionLocationsLiteral:\n        \"\"\"\n        Validates and sets the execution location.\n        1.  **If `None` is provided**: It defaults to the location determined by global settings.\n        2.  **If a value is provided**: It checks if the value is compatible with the global\n            settings. If not (e.g., requesting 'remote' when only 'local' is possible),\n            it corrects the value to a compatible one.\n        \"\"\"\n        if v is None:\n            return get_global_execution_location()\n        if v == \"auto\":\n            return get_global_execution_location()\n\n        return get_prio_execution_location(v, get_global_execution_location())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowGraphConfig.validate_and_set_execution_location","title":"<code>validate_and_set_execution_location(v)</code>","text":"<p>Validates and sets the execution location. 1.  If <code>None</code> is provided: It defaults to the location determined by global settings. 2.  If a value is provided: It checks if the value is compatible with the global     settings. If not (e.g., requesting 'remote' when only 'local' is possible),     it corrects the value to a compatible one.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>@field_validator('execution_location', mode='before')\ndef validate_and_set_execution_location(cls, v: Optional[ExecutionLocationsLiteral]) -&gt; ExecutionLocationsLiteral:\n    \"\"\"\n    Validates and sets the execution location.\n    1.  **If `None` is provided**: It defaults to the location determined by global settings.\n    2.  **If a value is provided**: It checks if the value is compatible with the global\n        settings. If not (e.g., requesting 'remote' when only 'local' is possible),\n        it corrects the value to a compatible one.\n    \"\"\"\n    if v is None:\n        return get_global_execution_location()\n    if v == \"auto\":\n        return get_global_execution_location()\n\n    return get_prio_execution_location(v, get_global_execution_location())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowInformation","title":"<code>FlowInformation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the complete state of a flow, including settings, nodes, and connections.</p> <p>Attributes:</p> Name Type Description <code>flow_id</code> <code>int</code> <p>The unique ID of the flow.</p> <code>flow_name</code> <code>Optional[str]</code> <p>The name of the flow.</p> <code>flow_settings</code> <code>FlowSettings</code> <p>The settings for the flow.</p> <code>data</code> <code>Dict[int, NodeInformation]</code> <p>A dictionary mapping node IDs to their information.</p> <code>node_starts</code> <code>List[int]</code> <p>A list of starting node IDs.</p> <code>node_connections</code> <code>List[Tuple[int, int]]</code> <p>A list of tuples representing connections between nodes.</p> <p>Methods:</p> Name Description <code>ensure_string</code> <p>Validator to ensure the flow_name is always a string.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class FlowInformation(BaseModel):\n    \"\"\"\n    Represents the complete state of a flow, including settings, nodes, and connections.\n\n    Attributes:\n        flow_id (int): The unique ID of the flow.\n        flow_name (Optional[str]): The name of the flow.\n        flow_settings (FlowSettings): The settings for the flow.\n        data (Dict[int, NodeInformation]): A dictionary mapping node IDs to their information.\n        node_starts (List[int]): A list of starting node IDs.\n        node_connections (List[Tuple[int, int]]): A list of tuples representing connections between nodes.\n    \"\"\"\n    flow_id: int\n    flow_name: Optional[str] = ''\n    flow_settings: FlowSettings\n    data: Dict[int, NodeInformation] = {}\n    node_starts: List[int]\n    node_connections: List[Tuple[int, int]] = []\n\n    @field_validator('flow_name', mode=\"before\")\n    def ensure_string(cls, v):\n        \"\"\"\n        Validator to ensure the flow_name is always a string.\n        :param v: The value to validate.\n        :return: The value as a string, or an empty string if it's None.\n        \"\"\"\n        return str(v) if v is not None else ''\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowInformation.ensure_string","title":"<code>ensure_string(v)</code>","text":"<p>Validator to ensure the flow_name is always a string. :param v: The value to validate. :return: The value as a string, or an empty string if it's None.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>@field_validator('flow_name', mode=\"before\")\ndef ensure_string(cls, v):\n    \"\"\"\n    Validator to ensure the flow_name is always a string.\n    :param v: The value to validate.\n    :return: The value as a string, or an empty string if it's None.\n    \"\"\"\n    return str(v) if v is not None else ''\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowSettings","title":"<code>FlowSettings</code>","text":"<p>               Bases: <code>FlowGraphConfig</code></p> <p>Extends FlowGraphConfig with additional operational settings for a flow.</p> <p>Attributes:</p> Name Type Description <code>auto_save</code> <code>bool</code> <p>Flag to enable or disable automatic saving.</p> <code>modified_on</code> <code>Optional[float]</code> <p>Timestamp of the last modification.</p> <code>show_detailed_progress</code> <code>bool</code> <p>Flag to show detailed progress during execution.</p> <code>is_running</code> <code>bool</code> <p>Indicates if the flow is currently running.</p> <code>is_canceled</code> <code>bool</code> <p>Indicates if the flow execution has been canceled.</p> <p>Methods:</p> Name Description <code>from_flow_settings_input</code> <p>Creates a FlowSettings instance from a FlowGraphConfig instance.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class FlowSettings(FlowGraphConfig):\n    \"\"\"\n    Extends FlowGraphConfig with additional operational settings for a flow.\n\n    Attributes:\n        auto_save (bool): Flag to enable or disable automatic saving.\n        modified_on (Optional[float]): Timestamp of the last modification.\n        show_detailed_progress (bool): Flag to show detailed progress during execution.\n        is_running (bool): Indicates if the flow is currently running.\n        is_canceled (bool): Indicates if the flow execution has been canceled.\n    \"\"\"\n    auto_save: bool = False\n    modified_on: Optional[float] = None\n    show_detailed_progress: bool = True\n    is_running: bool = False\n    is_canceled: bool = False\n\n    @classmethod\n    def from_flow_settings_input(cls, flow_graph_config: FlowGraphConfig):\n        \"\"\"\n        Creates a FlowSettings instance from a FlowGraphConfig instance.\n\n        :param flow_graph_config: The base flow graph configuration.\n        :return: A new instance of FlowSettings with data from flow_graph_config.\n        \"\"\"\n        return cls.model_validate(flow_graph_config.model_dump())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.FlowSettings.from_flow_settings_input","title":"<code>from_flow_settings_input(flow_graph_config)</code>  <code>classmethod</code>","text":"<p>Creates a FlowSettings instance from a FlowGraphConfig instance.</p> <p>:param flow_graph_config: The base flow graph configuration. :return: A new instance of FlowSettings with data from flow_graph_config.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>@classmethod\ndef from_flow_settings_input(cls, flow_graph_config: FlowGraphConfig):\n    \"\"\"\n    Creates a FlowSettings instance from a FlowGraphConfig instance.\n\n    :param flow_graph_config: The base flow graph configuration.\n    :return: A new instance of FlowSettings with data from flow_graph_config.\n    \"\"\"\n    return cls.model_validate(flow_graph_config.model_dump())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.NodeDefault","title":"<code>NodeDefault</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines default properties for a node type.</p> <p>Attributes:</p> Name Type Description <code>node_name</code> <code>str</code> <p>The name of the node.</p> <code>node_type</code> <code>NodeTypeLiteral</code> <p>The functional type of the node ('input', 'output', 'process').</p> <code>transform_type</code> <code>TransformTypeLiteral</code> <p>The data transformation behavior ('narrow', 'wide', 'other').</p> <code>has_default_settings</code> <code>Optional[Any]</code> <p>Indicates if the node has predefined default settings.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class NodeDefault(BaseModel):\n    \"\"\"\n    Defines default properties for a node type.\n\n    Attributes:\n        node_name (str): The name of the node.\n        node_type (NodeTypeLiteral): The functional type of the node ('input', 'output', 'process').\n        transform_type (TransformTypeLiteral): The data transformation behavior ('narrow', 'wide', 'other').\n        has_default_settings (Optional[Any]): Indicates if the node has predefined default settings.\n    \"\"\"\n    node_name: str\n    node_type: NodeTypeLiteral\n    transform_type: TransformTypeLiteral\n    has_default_settings: Optional[Any] = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.NodeEdge","title":"<code>NodeEdge</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a connection (edge) between two nodes in the frontend.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>A unique identifier for the edge.</p> <code>source</code> <code>str</code> <p>The ID of the source node.</p> <code>target</code> <code>str</code> <p>The ID of the target node.</p> <code>targetHandle</code> <code>str</code> <p>The specific input handle on the target node.</p> <code>sourceHandle</code> <code>str</code> <p>The specific output handle on the source node.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class NodeEdge(BaseModel):\n    \"\"\"\n    Represents a connection (edge) between two nodes in the frontend.\n\n    Attributes:\n        id (str): A unique identifier for the edge.\n        source (str): The ID of the source node.\n        target (str): The ID of the target node.\n        targetHandle (str): The specific input handle on the target node.\n        sourceHandle (str): The specific output handle on the source node.\n    \"\"\"\n    model_config = ConfigDict(coerce_numbers_to_str=True)\n    id: str\n    source: str\n    target: str\n    targetHandle: str\n    sourceHandle: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.NodeInformation","title":"<code>NodeInformation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Stores the state and configuration of a specific node instance within a flow.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[int]</code> <p>The unique ID of the node instance.</p> <code>type</code> <code>Optional[str]</code> <p>The type of the node (e.g., 'join', 'filter').</p> <code>is_setup</code> <code>Optional[bool]</code> <p>Whether the node has been configured.</p> <code>description</code> <code>Optional[str]</code> <p>A user-provided description.</p> <code>x_position</code> <code>Optional[int]</code> <p>The x-coordinate on the canvas.</p> <code>y_position</code> <code>Optional[int]</code> <p>The y-coordinate on the canvas.</p> <code>left_input_id</code> <code>Optional[int]</code> <p>The ID of the node connected to the left input.</p> <code>right_input_id</code> <code>Optional[int]</code> <p>The ID of the node connected to the right input.</p> <code>input_ids</code> <code>Optional[List[int]]</code> <p>A list of IDs for main input nodes.</p> <code>outputs</code> <code>Optional[List[int]]</code> <p>A list of IDs for nodes this node outputs to.</p> <code>setting_input</code> <code>Optional[Any]</code> <p>The specific settings for this node instance.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class NodeInformation(BaseModel):\n    \"\"\"\n    Stores the state and configuration of a specific node instance within a flow.\n\n    Attributes:\n        id (Optional[int]): The unique ID of the node instance.\n        type (Optional[str]): The type of the node (e.g., 'join', 'filter').\n        is_setup (Optional[bool]): Whether the node has been configured.\n        description (Optional[str]): A user-provided description.\n        x_position (Optional[int]): The x-coordinate on the canvas.\n        y_position (Optional[int]): The y-coordinate on the canvas.\n        left_input_id (Optional[int]): The ID of the node connected to the left input.\n        right_input_id (Optional[int]): The ID of the node connected to the right input.\n        input_ids (Optional[List[int]]): A list of IDs for main input nodes.\n        outputs (Optional[List[int]]): A list of IDs for nodes this node outputs to.\n        setting_input (Optional[Any]): The specific settings for this node instance.\n    \"\"\"\n    id: Optional[int] = None\n    type: Optional[str] = None\n    is_setup: Optional[bool] = None\n    description: Optional[str] = ''\n    x_position: Optional[int] = 0\n    y_position: Optional[int] = 0\n    left_input_id: Optional[int] = None\n    right_input_id: Optional[int] = None\n    input_ids: Optional[List[int]] = [-1]\n    outputs: Optional[List[int]] = [-1]\n    setting_input: Optional[Any] = None\n\n    @property\n    def data(self) -&gt; Any:\n        \"\"\"\n        Property to access the node's specific settings.\n        :return: The settings of the node.\n        \"\"\"\n        return self.setting_input\n\n    @property\n    def main_input_ids(self) -&gt; Optional[List[int]]:\n        \"\"\"\n        Property to access the main input node IDs.\n        :return: A list of main input node IDs.\n        \"\"\"\n        return self.input_ids\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.NodeInformation.data","title":"<code>data</code>  <code>property</code>","text":"<p>Property to access the node's specific settings. :return: The settings of the node.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.NodeInformation.main_input_ids","title":"<code>main_input_ids</code>  <code>property</code>","text":"<p>Property to access the main input node IDs. :return: A list of main input node IDs.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.NodeInput","title":"<code>NodeInput</code>","text":"<p>               Bases: <code>NodeTemplate</code></p> <p>Represents a node as it is received from the frontend, including position.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>The unique ID of the node instance.</p> <code>pos_x</code> <code>float</code> <p>The x-coordinate on the canvas.</p> <code>pos_y</code> <code>float</code> <p>The y-coordinate on the canvas.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class NodeInput(NodeTemplate):\n    \"\"\"\n    Represents a node as it is received from the frontend, including position.\n\n    Attributes:\n        id (int): The unique ID of the node instance.\n        pos_x (float): The x-coordinate on the canvas.\n        pos_y (float): The y-coordinate on the canvas.\n    \"\"\"\n    id: int\n    pos_x: float\n    pos_y: float\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.NodeTemplate","title":"<code>NodeTemplate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the template for a node type, specifying its UI and functional characteristics.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The display name of the node.</p> <code>item</code> <code>str</code> <p>The unique identifier for the node type.</p> <code>input</code> <code>int</code> <p>The number of required input connections.</p> <code>output</code> <code>int</code> <p>The number of output connections.</p> <code>image</code> <code>str</code> <p>The filename of the icon for the node.</p> <code>multi</code> <code>bool</code> <p>Whether the node accepts multiple main input connections.</p> <code>node_group</code> <code>str</code> <p>The category group the node belongs to (e.g., 'input', 'transform').</p> <code>prod_ready</code> <code>bool</code> <p>Whether the node is considered production-ready.</p> <code>can_be_start</code> <code>bool</code> <p>Whether the node can be a starting point in a flow.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class NodeTemplate(BaseModel):\n    \"\"\"\n    Defines the template for a node type, specifying its UI and functional characteristics.\n\n    Attributes:\n        name (str): The display name of the node.\n        item (str): The unique identifier for the node type.\n        input (int): The number of required input connections.\n        output (int): The number of output connections.\n        image (str): The filename of the icon for the node.\n        multi (bool): Whether the node accepts multiple main input connections.\n        node_group (str): The category group the node belongs to (e.g., 'input', 'transform').\n        prod_ready (bool): Whether the node is considered production-ready.\n        can_be_start (bool): Whether the node can be a starting point in a flow.\n    \"\"\"\n    name: str\n    item: str\n    input: int\n    output: int\n    image: str\n    multi: bool = False\n    node_type: NodeTypeLiteral\n    transform_type: TransformTypeLiteral\n    node_group: str\n    prod_ready: bool = True\n    can_be_start: bool = False\n    drawer_title: str = \"Node title\"\n    drawer_intro: str = \"Drawer into\"\n    custom_node: Optional[bool] = False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.RawLogInput","title":"<code>RawLogInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for a raw log message.</p> <p>Attributes:</p> Name Type Description <code>flowfile_flow_id</code> <code>int</code> <p>The ID of the flow that generated the log.</p> <code>log_message</code> <code>str</code> <p>The content of the log message.</p> <code>log_type</code> <code>Literal['INFO', 'ERROR']</code> <p>The type of log.</p> <code>extra</code> <code>Optional[dict]</code> <p>Extra context data for the log.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class RawLogInput(BaseModel):\n    \"\"\"\n    Schema for a raw log message.\n\n    Attributes:\n        flowfile_flow_id (int): The ID of the flow that generated the log.\n        log_message (str): The content of the log message.\n        log_type (Literal[\"INFO\", \"ERROR\"]): The type of log.\n        extra (Optional[dict]): Extra context data for the log.\n    \"\"\"\n    flowfile_flow_id: int\n    log_message: str\n    log_type: Literal[\"INFO\", \"ERROR\"]\n    extra: Optional[dict] = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.VueFlowInput","title":"<code>VueFlowInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the complete graph structure from the Vue-based frontend.</p> <p>Attributes:</p> Name Type Description <code>node_edges</code> <code>List[NodeEdge]</code> <p>A list of all edges in the graph.</p> <code>node_inputs</code> <code>List[NodeInput]</code> <p>A list of all nodes in the graph.</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>class VueFlowInput(BaseModel):\n    \"\"\"\n\n    Represents the complete graph structure from the Vue-based frontend.\n\n    Attributes:\n        node_edges (List[NodeEdge]): A list of all edges in the graph.\n        node_inputs (List[NodeInput]): A list of all nodes in the graph.\n    \"\"\"\n    node_edges: List[NodeEdge]\n    node_inputs: List[NodeInput]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.schemas.get_global_execution_location","title":"<code>get_global_execution_location()</code>","text":"<p>Calculates the default execution location based on the global settings Returns</p> <p>ExecutionLocationsLiteral where the current</p> Source code in <code>flowfile_core/flowfile_core/schemas/schemas.py</code> <pre><code>def get_global_execution_location() -&gt; ExecutionLocationsLiteral:\n    \"\"\"\n    Calculates the default execution location based on the global settings\n    Returns\n    -------\n    ExecutionLocationsLiteral where the current\n    \"\"\"\n    if OFFLOAD_TO_WORKER:\n        return \"remote\"\n    return \"local\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#input_schema","title":"<code>input_schema</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema","title":"<code>flowfile_core.schemas.input_schema</code>","text":"<p>Classes:</p> Name Description <code>DatabaseConnection</code> <p>Defines the connection parameters for a database.</p> <code>DatabaseSettings</code> <p>Defines settings for reading from a database, either via table or query.</p> <code>DatabaseWriteSettings</code> <p>Defines settings for writing data to a database table.</p> <code>ExternalSource</code> <p>Base model for data coming from a predefined external source.</p> <code>FullDatabaseConnection</code> <p>A complete database connection model including the secret password.</p> <code>FullDatabaseConnectionInterface</code> <p>A database connection model intended for UI display, omitting the password.</p> <code>MinimalFieldInfo</code> <p>Represents the most basic information about a data field (column).</p> <code>NewDirectory</code> <p>Defines the information required to create a new directory.</p> <code>NodeBase</code> <p>Base model for all nodes in a FlowGraph. Contains common metadata.</p> <code>NodeCloudStorageReader</code> <p>Settings for a node that reads from a cloud storage service (S3, GCS, etc.).</p> <code>NodeCloudStorageWriter</code> <p>Settings for a node that writes to a cloud storage service.</p> <code>NodeConnection</code> <p>Represents a connection (edge) between two nodes in the graph.</p> <code>NodeCrossJoin</code> <p>Settings for a node that performs a cross join.</p> <code>NodeDatabaseReader</code> <p>Settings for a node that reads from a database.</p> <code>NodeDatabaseWriter</code> <p>Settings for a node that writes data to a database.</p> <code>NodeDatasource</code> <p>Base settings for a node that acts as a data source.</p> <code>NodeDescription</code> <p>A simple model for updating a node's description text.</p> <code>NodeExploreData</code> <p>Settings for a node that provides an interactive data exploration interface.</p> <code>NodeExternalSource</code> <p>Settings for a node that connects to a registered external data source.</p> <code>NodeFilter</code> <p>Settings for a node that filters rows based on a condition.</p> <code>NodeFormula</code> <p>Settings for a node that applies a formula to create/modify a column.</p> <code>NodeFuzzyMatch</code> <p>Settings for a node that performs a fuzzy join based on string similarity.</p> <code>NodeGraphSolver</code> <p>Settings for a node that solves graph-based problems (e.g., connected components).</p> <code>NodeGroupBy</code> <p>Settings for a node that performs a group-by and aggregation operation.</p> <code>NodeInputConnection</code> <p>Represents the input side of a connection between two nodes.</p> <code>NodeJoin</code> <p>Settings for a node that performs a standard SQL-style join.</p> <code>NodeManualInput</code> <p>Settings for a node that allows direct data entry in the UI.</p> <code>NodeMultiInput</code> <p>A base model for any node that takes multiple data inputs.</p> <code>NodeOutput</code> <p>Settings for a node that writes its input to a file.</p> <code>NodeOutputConnection</code> <p>Represents the output side of a connection between two nodes.</p> <code>NodePivot</code> <p>Settings for a node that pivots data from a long to a wide format.</p> <code>NodePolarsCode</code> <p>Settings for a node that executes arbitrary user-provided Polars code.</p> <code>NodePromise</code> <p>A placeholder node for an operation that has not yet been configured.</p> <code>NodeRead</code> <p>Settings for a node that reads data from a file.</p> <code>NodeRecordCount</code> <p>Settings for a node that counts the number of records.</p> <code>NodeRecordId</code> <p>Settings for a node that adds a unique record ID column.</p> <code>NodeSample</code> <p>Settings for a node that samples a subset of the data.</p> <code>NodeSelect</code> <p>Settings for a node that selects, renames, and reorders columns.</p> <code>NodeSingleInput</code> <p>A base model for any node that takes a single data input.</p> <code>NodeSort</code> <p>Settings for a node that sorts the data by one or more columns.</p> <code>NodeTextToRows</code> <p>Settings for a node that splits a text column into multiple rows.</p> <code>NodeUnion</code> <p>Settings for a node that concatenates multiple data inputs.</p> <code>NodeUnique</code> <p>Settings for a node that returns the unique rows from the data.</p> <code>NodeUnpivot</code> <p>Settings for a node that unpivots data from a wide to a long format.</p> <code>OutputCsvTable</code> <p>Defines settings for writing a CSV file.</p> <code>OutputExcelTable</code> <p>Defines settings for writing an Excel file.</p> <code>OutputParquetTable</code> <p>Defines settings for writing a Parquet file.</p> <code>OutputSettings</code> <p>Defines the complete settings for an output node.</p> <code>RawData</code> <p>Represents data in a raw, columnar format for manual input.</p> <code>ReceivedCsvTable</code> <p>Defines settings for reading a CSV file.</p> <code>ReceivedExcelTable</code> <p>Defines settings for reading an Excel file.</p> <code>ReceivedJsonTable</code> <p>Defines settings for reading a JSON file (inherits from CSV settings).</p> <code>ReceivedParquetTable</code> <p>Defines settings for reading a Parquet file.</p> <code>ReceivedTable</code> <p>A comprehensive model that can represent any type of received table.</p> <code>ReceivedTableBase</code> <p>Base model for defining a table received from an external source.</p> <code>RemoveItem</code> <p>Represents a single item to be removed from a directory or list.</p> <code>RemoveItemsInput</code> <p>Defines a list of items to be removed.</p> <code>SampleUsers</code> <p>Settings for generating a sample dataset of users.</p> <code>UserDefinedNode</code> <p>Settings for a node that contains the user defined node information</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.DatabaseConnection","title":"<code>DatabaseConnection</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the connection parameters for a database.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class DatabaseConnection(BaseModel):\n    \"\"\"Defines the connection parameters for a database.\"\"\"\n    database_type: str = \"postgresql\"\n    username: Optional[str] = None\n    password_ref: Optional[SecretRef] = None\n    host: Optional[str] = None\n    port: Optional[int] = None\n    database: Optional[str] = None\n    url: Optional[str] = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.DatabaseSettings","title":"<code>DatabaseSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for reading from a database, either via table or query.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class DatabaseSettings(BaseModel):\n    \"\"\"Defines settings for reading from a database, either via table or query.\"\"\"\n    connection_mode: Optional[Literal['inline', 'reference']] = 'inline'\n    database_connection: Optional[DatabaseConnection] = None\n    database_connection_name: Optional[str] = None\n    schema_name: Optional[str] = None\n    table_name: Optional[str] = None\n    query: Optional[str] = None\n    query_mode: Literal['query', 'table', 'reference'] = 'table'\n\n    @model_validator(mode='after')\n    def validate_table_or_query(self):\n        # Validate that either table_name or query is provided\n        if (not self.table_name and not self.query) and self.query_mode == 'inline':\n            raise ValueError(\"Either 'table_name' or 'query' must be provided\")\n\n        # Validate correct connection information based on connection_mode\n        if self.connection_mode == 'inline' and self.database_connection is None:\n            raise ValueError(\"When 'connection_mode' is 'inline', 'database_connection' must be provided\")\n\n        if self.connection_mode == 'reference' and not self.database_connection_name:\n            raise ValueError(\"When 'connection_mode' is 'reference', 'database_connection_name' must be provided\")\n\n        return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.DatabaseWriteSettings","title":"<code>DatabaseWriteSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for writing data to a database table.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class DatabaseWriteSettings(BaseModel):\n    \"\"\"Defines settings for writing data to a database table.\"\"\"\n    connection_mode: Optional[Literal['inline', 'reference']] = 'inline'\n    database_connection: Optional[DatabaseConnection] = None\n    database_connection_name: Optional[str] = None\n    table_name: str\n    schema_name: Optional[str] = None\n    if_exists: Optional[Literal['append', 'replace', 'fail']] = 'append'\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ExternalSource","title":"<code>ExternalSource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model for data coming from a predefined external source.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class ExternalSource(BaseModel):\n    \"\"\"Base model for data coming from a predefined external source.\"\"\"\n    orientation: str = 'row'\n    fields: Optional[List[MinimalFieldInfo]] = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.FullDatabaseConnection","title":"<code>FullDatabaseConnection</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A complete database connection model including the secret password.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class FullDatabaseConnection(BaseModel):\n    \"\"\"A complete database connection model including the secret password.\"\"\"\n    connection_name: str\n    database_type: str = \"postgresql\"\n    username: str\n    password: SecretStr\n    host: Optional[str] = None\n    port: Optional[int] = None\n    database: Optional[str] = None\n    ssl_enabled: Optional[bool] = False\n    url: Optional[str] = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.FullDatabaseConnectionInterface","title":"<code>FullDatabaseConnectionInterface</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A database connection model intended for UI display, omitting the password.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class FullDatabaseConnectionInterface(BaseModel):\n    \"\"\"A database connection model intended for UI display, omitting the password.\"\"\"\n    connection_name: str\n    database_type: str = \"postgresql\"\n    username: str\n    host: Optional[str] = None\n    port: Optional[int] = None\n    database: Optional[str] = None\n    ssl_enabled: Optional[bool] = False\n    url: Optional[str] = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.MinimalFieldInfo","title":"<code>MinimalFieldInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the most basic information about a data field (column).</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class MinimalFieldInfo(BaseModel):\n    \"\"\"Represents the most basic information about a data field (column).\"\"\"\n    name: str\n    data_type: str = \"String\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NewDirectory","title":"<code>NewDirectory</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the information required to create a new directory.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NewDirectory(BaseModel):\n    \"\"\"Defines the information required to create a new directory.\"\"\"\n    source_path: str\n    dir_name: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeBase","title":"<code>NodeBase</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model for all nodes in a FlowGraph. Contains common metadata.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeBase(BaseModel):\n    \"\"\"Base model for all nodes in a FlowGraph. Contains common metadata.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    flow_id: int\n    node_id: int\n    cache_results: Optional[bool] = False\n    pos_x: Optional[float] = 0\n    pos_y: Optional[float] = 0\n    is_setup: Optional[bool] = True\n    description: Optional[str] = ''\n    user_id: Optional[int] = None\n    is_flow_output: Optional[bool] = False\n    is_user_defined: Optional[bool] = False  # Indicator if the node is a user defined node\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeCloudStorageReader","title":"<code>NodeCloudStorageReader</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>Settings for a node that reads from a cloud storage service (S3, GCS, etc.).</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeCloudStorageReader(NodeBase):\n    \"\"\"Settings for a node that reads from a cloud storage service (S3, GCS, etc.).\"\"\"\n    cloud_storage_settings: CloudStorageReadSettings\n    fields: Optional[List[MinimalFieldInfo]] = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeCloudStorageWriter","title":"<code>NodeCloudStorageWriter</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that writes to a cloud storage service.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeCloudStorageWriter(NodeSingleInput):\n    \"\"\"Settings for a node that writes to a cloud storage service.\"\"\"\n    cloud_storage_settings: CloudStorageWriteSettings\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeConnection","title":"<code>NodeConnection</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a connection (edge) between two nodes in the graph.</p> <p>Methods:</p> Name Description <code>create_from_simple_input</code> <p>Creates a standard connection between two nodes.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeConnection(BaseModel):\n    \"\"\"Represents a connection (edge) between two nodes in the graph.\"\"\"\n    input_connection: NodeInputConnection\n    output_connection: NodeOutputConnection\n\n    @classmethod\n    def create_from_simple_input(cls, from_id: int, to_id: int, input_type: InputType = \"input-0\"):\n        \"\"\"Creates a standard connection between two nodes.\"\"\"\n        match input_type:\n            case \"main\": connection_class: InputConnectionClass = \"input-0\"\n            case \"right\": connection_class: InputConnectionClass = \"input-1\"\n            case \"left\": connection_class: InputConnectionClass = \"input-2\"\n            case _: connection_class: InputConnectionClass = \"input-0\"\n        node_input = NodeInputConnection(node_id=to_id, connection_class=connection_class)\n        node_output = NodeOutputConnection(node_id=from_id, connection_class='output-0')\n        return cls(input_connection=node_input, output_connection=node_output)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeConnection.create_from_simple_input","title":"<code>create_from_simple_input(from_id, to_id, input_type='input-0')</code>  <code>classmethod</code>","text":"<p>Creates a standard connection between two nodes.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@classmethod\ndef create_from_simple_input(cls, from_id: int, to_id: int, input_type: InputType = \"input-0\"):\n    \"\"\"Creates a standard connection between two nodes.\"\"\"\n    match input_type:\n        case \"main\": connection_class: InputConnectionClass = \"input-0\"\n        case \"right\": connection_class: InputConnectionClass = \"input-1\"\n        case \"left\": connection_class: InputConnectionClass = \"input-2\"\n        case _: connection_class: InputConnectionClass = \"input-0\"\n    node_input = NodeInputConnection(node_id=to_id, connection_class=connection_class)\n    node_output = NodeOutputConnection(node_id=from_id, connection_class='output-0')\n    return cls(input_connection=node_input, output_connection=node_output)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeCrossJoin","title":"<code>NodeCrossJoin</code>","text":"<p>               Bases: <code>NodeMultiInput</code></p> <p>Settings for a node that performs a cross join.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeCrossJoin(NodeMultiInput):\n    \"\"\"Settings for a node that performs a cross join.\"\"\"\n    auto_generate_selection: bool = True\n    verify_integrity: bool = True\n    cross_join_input: transform_schema.CrossJoinInput\n    auto_keep_all: bool = True\n    auto_keep_right: bool = True\n    auto_keep_left: bool = True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeDatabaseReader","title":"<code>NodeDatabaseReader</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>Settings for a node that reads from a database.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeDatabaseReader(NodeBase):\n    \"\"\"Settings for a node that reads from a database.\"\"\"\n    database_settings: DatabaseSettings\n    fields: Optional[List[MinimalFieldInfo]] = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeDatabaseWriter","title":"<code>NodeDatabaseWriter</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that writes data to a database.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeDatabaseWriter(NodeSingleInput):\n    \"\"\"Settings for a node that writes data to a database.\"\"\"\n    database_write_settings: DatabaseWriteSettings\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeDatasource","title":"<code>NodeDatasource</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>Base settings for a node that acts as a data source.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeDatasource(NodeBase):\n    \"\"\"Base settings for a node that acts as a data source.\"\"\"\n    file_ref: str = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeDescription","title":"<code>NodeDescription</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A simple model for updating a node's description text.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeDescription(BaseModel):\n    \"\"\"A simple model for updating a node's description text.\"\"\"\n    description: str = ''\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeExploreData","title":"<code>NodeExploreData</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>Settings for a node that provides an interactive data exploration interface.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeExploreData(NodeBase):\n    \"\"\"Settings for a node that provides an interactive data exploration interface.\"\"\"\n    graphic_walker_input: Optional[gs_schemas.GraphicWalkerInput] = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeExternalSource","title":"<code>NodeExternalSource</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>Settings for a node that connects to a registered external data source.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeExternalSource(NodeBase):\n    \"\"\"Settings for a node that connects to a registered external data source.\"\"\"\n    identifier: str\n    source_settings: SampleUsers\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeFilter","title":"<code>NodeFilter</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that filters rows based on a condition.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeFilter(NodeSingleInput):\n    \"\"\"Settings for a node that filters rows based on a condition.\"\"\"\n    filter_input: transform_schema.FilterInput\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeFormula","title":"<code>NodeFormula</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that applies a formula to create/modify a column.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeFormula(NodeSingleInput):\n    \"\"\"Settings for a node that applies a formula to create/modify a column.\"\"\"\n    function: transform_schema.FunctionInput = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeFuzzyMatch","title":"<code>NodeFuzzyMatch</code>","text":"<p>               Bases: <code>NodeJoin</code></p> <p>Settings for a node that performs a fuzzy join based on string similarity.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeFuzzyMatch(NodeJoin):\n    \"\"\"Settings for a node that performs a fuzzy join based on string similarity.\"\"\"\n    join_input: transform_schema.FuzzyMatchInput\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeGraphSolver","title":"<code>NodeGraphSolver</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that solves graph-based problems (e.g., connected components).</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeGraphSolver(NodeSingleInput):\n    \"\"\"Settings for a node that solves graph-based problems (e.g., connected components).\"\"\"\n    graph_solver_input: transform_schema.GraphSolverInput\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeGroupBy","title":"<code>NodeGroupBy</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that performs a group-by and aggregation operation.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeGroupBy(NodeSingleInput):\n    \"\"\"Settings for a node that performs a group-by and aggregation operation.\"\"\"\n    groupby_input: transform_schema.GroupByInput = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeInputConnection","title":"<code>NodeInputConnection</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the input side of a connection between two nodes.</p> <p>Methods:</p> Name Description <code>get_node_input_connection_type</code> <p>Determines the semantic type of the input (e.g., for a join).</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeInputConnection(BaseModel):\n    \"\"\"Represents the input side of a connection between two nodes.\"\"\"\n    node_id: int\n    connection_class: InputConnectionClass\n\n    def get_node_input_connection_type(self) -&gt; Literal['main', 'right', 'left']:\n        \"\"\"Determines the semantic type of the input (e.g., for a join).\"\"\"\n        match self.connection_class:\n            case 'input-0': return 'main'\n            case 'input-1': return 'right'\n            case 'input-2': return 'left'\n            case _: raise ValueError(f\"Unexpected connection_class: {self.connection_class}\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeInputConnection.get_node_input_connection_type","title":"<code>get_node_input_connection_type()</code>","text":"<p>Determines the semantic type of the input (e.g., for a join).</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def get_node_input_connection_type(self) -&gt; Literal['main', 'right', 'left']:\n    \"\"\"Determines the semantic type of the input (e.g., for a join).\"\"\"\n    match self.connection_class:\n        case 'input-0': return 'main'\n        case 'input-1': return 'right'\n        case 'input-2': return 'left'\n        case _: raise ValueError(f\"Unexpected connection_class: {self.connection_class}\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeJoin","title":"<code>NodeJoin</code>","text":"<p>               Bases: <code>NodeMultiInput</code></p> <p>Settings for a node that performs a standard SQL-style join.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeJoin(NodeMultiInput):\n    \"\"\"Settings for a node that performs a standard SQL-style join.\"\"\"\n    auto_generate_selection: bool = True\n    verify_integrity: bool = True\n    join_input: transform_schema.JoinInput\n    auto_keep_all: bool = True\n    auto_keep_right: bool = True\n    auto_keep_left: bool = True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeManualInput","title":"<code>NodeManualInput</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>Settings for a node that allows direct data entry in the UI.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeManualInput(NodeBase):\n    \"\"\"Settings for a node that allows direct data entry in the UI.\"\"\"\n    raw_data_format: Optional[RawData] = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeMultiInput","title":"<code>NodeMultiInput</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>A base model for any node that takes multiple data inputs.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeMultiInput(NodeBase):\n    \"\"\"A base model for any node that takes multiple data inputs.\"\"\"\n    depending_on_ids: Optional[List[int]] = [-1]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeOutput","title":"<code>NodeOutput</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that writes its input to a file.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeOutput(NodeSingleInput):\n    \"\"\"Settings for a node that writes its input to a file.\"\"\"\n    output_settings: OutputSettings\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeOutputConnection","title":"<code>NodeOutputConnection</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the output side of a connection between two nodes.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeOutputConnection(BaseModel):\n    \"\"\"Represents the output side of a connection between two nodes.\"\"\"\n    node_id: int\n    connection_class: OutputConnectionClass\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodePivot","title":"<code>NodePivot</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that pivots data from a long to a wide format.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodePivot(NodeSingleInput):\n    \"\"\"Settings for a node that pivots data from a long to a wide format.\"\"\"\n    pivot_input: transform_schema.PivotInput = None\n    output_fields: Optional[List[MinimalFieldInfo]] = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodePolarsCode","title":"<code>NodePolarsCode</code>","text":"<p>               Bases: <code>NodeMultiInput</code></p> <p>Settings for a node that executes arbitrary user-provided Polars code.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodePolarsCode(NodeMultiInput):\n    \"\"\"Settings for a node that executes arbitrary user-provided Polars code.\"\"\"\n    polars_code_input: transform_schema.PolarsCodeInput\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodePromise","title":"<code>NodePromise</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>A placeholder node for an operation that has not yet been configured.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodePromise(NodeBase):\n    \"\"\"A placeholder node for an operation that has not yet been configured.\"\"\"\n    is_setup: bool = False\n    node_type: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeRead","title":"<code>NodeRead</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>Settings for a node that reads data from a file.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeRead(NodeBase):\n    \"\"\"Settings for a node that reads data from a file.\"\"\"\n    received_file: ReceivedTable\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeRecordCount","title":"<code>NodeRecordCount</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that counts the number of records.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeRecordCount(NodeSingleInput):\n    \"\"\"Settings for a node that counts the number of records.\"\"\"\n    pass\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeRecordId","title":"<code>NodeRecordId</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that adds a unique record ID column.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeRecordId(NodeSingleInput):\n    \"\"\"Settings for a node that adds a unique record ID column.\"\"\"\n    record_id_input: transform_schema.RecordIdInput\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeSample","title":"<code>NodeSample</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that samples a subset of the data.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeSample(NodeSingleInput):\n    \"\"\"Settings for a node that samples a subset of the data.\"\"\"\n    sample_size: int = 1000\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeSelect","title":"<code>NodeSelect</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that selects, renames, and reorders columns.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeSelect(NodeSingleInput):\n    \"\"\"Settings for a node that selects, renames, and reorders columns.\"\"\"\n    keep_missing: bool = True\n    select_input: List[transform_schema.SelectInput] = Field(default_factory=list)\n    sorted_by: Optional[Literal['none', 'asc', 'desc']] = 'none'\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeSingleInput","title":"<code>NodeSingleInput</code>","text":"<p>               Bases: <code>NodeBase</code></p> <p>A base model for any node that takes a single data input.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeSingleInput(NodeBase):\n    \"\"\"A base model for any node that takes a single data input.\"\"\"\n    depending_on_id: Optional[int] = -1\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeSort","title":"<code>NodeSort</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that sorts the data by one or more columns.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeSort(NodeSingleInput):\n    \"\"\"Settings for a node that sorts the data by one or more columns.\"\"\"\n    sort_input: List[transform_schema.SortByInput] = Field(default_factory=list)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeTextToRows","title":"<code>NodeTextToRows</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that splits a text column into multiple rows.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeTextToRows(NodeSingleInput):\n    \"\"\"Settings for a node that splits a text column into multiple rows.\"\"\"\n    text_to_rows_input: transform_schema.TextToRowsInput\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeUnion","title":"<code>NodeUnion</code>","text":"<p>               Bases: <code>NodeMultiInput</code></p> <p>Settings for a node that concatenates multiple data inputs.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeUnion(NodeMultiInput):\n    \"\"\"Settings for a node that concatenates multiple data inputs.\"\"\"\n    union_input: transform_schema.UnionInput = Field(default_factory=transform_schema.UnionInput)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeUnique","title":"<code>NodeUnique</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that returns the unique rows from the data.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeUnique(NodeSingleInput):\n    \"\"\"Settings for a node that returns the unique rows from the data.\"\"\"\n    unique_input: transform_schema.UniqueInput\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.NodeUnpivot","title":"<code>NodeUnpivot</code>","text":"<p>               Bases: <code>NodeSingleInput</code></p> <p>Settings for a node that unpivots data from a wide to a long format.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class NodeUnpivot(NodeSingleInput):\n    \"\"\"Settings for a node that unpivots data from a wide to a long format.\"\"\"\n    unpivot_input: transform_schema.UnpivotInput = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputCsvTable","title":"<code>OutputCsvTable</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for writing a CSV file.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class OutputCsvTable(BaseModel):\n    \"\"\"Defines settings for writing a CSV file.\"\"\"\n    file_type: str = 'csv'\n    delimiter: str = ','\n    encoding: str = 'utf-8'\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputExcelTable","title":"<code>OutputExcelTable</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for writing an Excel file.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class OutputExcelTable(BaseModel):\n    \"\"\"Defines settings for writing an Excel file.\"\"\"\n    file_type: str = 'excel'\n    sheet_name: str = 'Sheet1'\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputParquetTable","title":"<code>OutputParquetTable</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines settings for writing a Parquet file.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class OutputParquetTable(BaseModel):\n    \"\"\"Defines settings for writing a Parquet file.\"\"\"\n    file_type: str = 'parquet'\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputSettings","title":"<code>OutputSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the complete settings for an output node.</p> <p>Methods:</p> Name Description <code>populate_abs_file_path</code> <p>Ensures the absolute file path is populated after validation.</p> <code>set_absolute_filepath</code> <p>Resolves the output directory and name into an absolute path.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class OutputSettings(BaseModel):\n    \"\"\"Defines the complete settings for an output node.\"\"\"\n    name: str\n    directory: str\n    file_type: str\n    fields: Optional[List[str]] = Field(default_factory=list)\n    write_mode: str = 'overwrite'\n    output_csv_table: Optional[OutputCsvTable] = Field(default_factory=OutputCsvTable)\n    output_parquet_table: OutputParquetTable = Field(default_factory=OutputParquetTable)\n    output_excel_table: OutputExcelTable = Field(default_factory=OutputExcelTable)\n    abs_file_path: Optional[str] = None\n\n    def set_absolute_filepath(self):\n        \"\"\"Resolves the output directory and name into an absolute path.\"\"\"\n        base_path = Path(self.directory)\n        if not base_path.is_absolute():\n            base_path = Path.cwd() / base_path\n        if self.name and self.name not in base_path.name:\n            base_path = base_path / self.name\n        self.abs_file_path = str(base_path.resolve())\n\n    @model_validator(mode='after')\n    def populate_abs_file_path(self):\n        \"\"\"Ensures the absolute file path is populated after validation.\"\"\"\n        self.set_absolute_filepath()\n        return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputSettings.populate_abs_file_path","title":"<code>populate_abs_file_path()</code>","text":"<p>Ensures the absolute file path is populated after validation.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@model_validator(mode='after')\ndef populate_abs_file_path(self):\n    \"\"\"Ensures the absolute file path is populated after validation.\"\"\"\n    self.set_absolute_filepath()\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.OutputSettings.set_absolute_filepath","title":"<code>set_absolute_filepath()</code>","text":"<p>Resolves the output directory and name into an absolute path.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def set_absolute_filepath(self):\n    \"\"\"Resolves the output directory and name into an absolute path.\"\"\"\n    base_path = Path(self.directory)\n    if not base_path.is_absolute():\n        base_path = Path.cwd() / base_path\n    if self.name and self.name not in base_path.name:\n        base_path = base_path / self.name\n    self.abs_file_path = str(base_path.resolve())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.RawData","title":"<code>RawData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents data in a raw, columnar format for manual input.</p> <p>Methods:</p> Name Description <code>from_pylist</code> <p>Creates a RawData object from a list of Python dictionaries.</p> <code>to_pylist</code> <p>Converts the RawData object back into a list of Python dictionaries.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class RawData(BaseModel):\n    \"\"\"Represents data in a raw, columnar format for manual input.\"\"\"\n    columns: List[MinimalFieldInfo] = None\n    data: List[List]\n\n    @classmethod\n    def from_pylist(cls, pylist: List[dict]):\n        \"\"\"Creates a RawData object from a list of Python dictionaries.\"\"\"\n        if len(pylist) == 0:\n            return cls(columns=[], data=[])\n        pylist = ensure_similarity_dicts(pylist)\n        values = [standardize_col_dtype([vv for vv in c]) for c in\n                  zip(*(r.values() for r in pylist))]\n        data_types = (pl.DataType.from_python(type(next((v for v in column_values), None))) for column_values in values)\n        columns = [MinimalFieldInfo(name=c, data_type=str(next(data_types))) for c in pylist[0].keys()]\n        return cls(columns=columns, data=values)\n\n    def to_pylist(self) -&gt; List[dict]:\n        \"\"\"Converts the RawData object back into a list of Python dictionaries.\"\"\"\n        return [{c.name: self.data[ci][ri] for ci, c in enumerate(self.columns)} for ri in range(len(self.data[0]))]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.RawData.from_pylist","title":"<code>from_pylist(pylist)</code>  <code>classmethod</code>","text":"<p>Creates a RawData object from a list of Python dictionaries.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@classmethod\ndef from_pylist(cls, pylist: List[dict]):\n    \"\"\"Creates a RawData object from a list of Python dictionaries.\"\"\"\n    if len(pylist) == 0:\n        return cls(columns=[], data=[])\n    pylist = ensure_similarity_dicts(pylist)\n    values = [standardize_col_dtype([vv for vv in c]) for c in\n              zip(*(r.values() for r in pylist))]\n    data_types = (pl.DataType.from_python(type(next((v for v in column_values), None))) for column_values in values)\n    columns = [MinimalFieldInfo(name=c, data_type=str(next(data_types))) for c in pylist[0].keys()]\n    return cls(columns=columns, data=values)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.RawData.to_pylist","title":"<code>to_pylist()</code>","text":"<p>Converts the RawData object back into a list of Python dictionaries.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def to_pylist(self) -&gt; List[dict]:\n    \"\"\"Converts the RawData object back into a list of Python dictionaries.\"\"\"\n    return [{c.name: self.data[ci][ri] for ci, c in enumerate(self.columns)} for ri in range(len(self.data[0]))]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedCsvTable","title":"<code>ReceivedCsvTable</code>","text":"<p>               Bases: <code>ReceivedTableBase</code></p> <p>Defines settings for reading a CSV file.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class ReceivedCsvTable(ReceivedTableBase):\n    \"\"\"Defines settings for reading a CSV file.\"\"\"\n    file_type: str = 'csv'\n    reference: str = ''\n    starting_from_line: int = 0\n    delimiter: str = ','\n    has_headers: bool = True\n    encoding: Optional[str] = 'utf-8'\n    parquet_ref: Optional[str] = None\n    row_delimiter: str = '\\n'\n    quote_char: str = '\"'\n    infer_schema_length: int = 10_000\n    truncate_ragged_lines: bool = False\n    ignore_errors: bool = False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedExcelTable","title":"<code>ReceivedExcelTable</code>","text":"<p>               Bases: <code>ReceivedTableBase</code></p> <p>Defines settings for reading an Excel file.</p> <p>Methods:</p> Name Description <code>validate_range_values</code> <p>Validates that the Excel cell range is logical.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class ReceivedExcelTable(ReceivedTableBase):\n    \"\"\"Defines settings for reading an Excel file.\"\"\"\n    sheet_name: Optional[str] = None\n    start_row: int = 0\n    start_column: int = 0\n    end_row: int = 0\n    end_column: int = 0\n    has_headers: bool = True\n    type_inference: bool = False\n\n    def validate_range_values(self):\n        \"\"\"Validates that the Excel cell range is logical.\"\"\"\n        for attribute in [self.start_row, self.start_column, self.end_row, self.end_column]:\n            if not isinstance(attribute, int) or attribute &lt; 0:\n                raise ValueError(\"Row and column indices must be non-negative integers\")\n        if (self.end_row &gt; 0 and self.start_row &gt; self.end_row) or \\\n           (self.end_column &gt; 0 and self.start_column &gt; self.end_column):\n            raise ValueError(\"Start row/column must not be greater than end row/column\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedExcelTable.validate_range_values","title":"<code>validate_range_values()</code>","text":"<p>Validates that the Excel cell range is logical.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def validate_range_values(self):\n    \"\"\"Validates that the Excel cell range is logical.\"\"\"\n    for attribute in [self.start_row, self.start_column, self.end_row, self.end_column]:\n        if not isinstance(attribute, int) or attribute &lt; 0:\n            raise ValueError(\"Row and column indices must be non-negative integers\")\n    if (self.end_row &gt; 0 and self.start_row &gt; self.end_row) or \\\n       (self.end_column &gt; 0 and self.start_column &gt; self.end_column):\n        raise ValueError(\"Start row/column must not be greater than end row/column\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedJsonTable","title":"<code>ReceivedJsonTable</code>","text":"<p>               Bases: <code>ReceivedCsvTable</code></p> <p>Defines settings for reading a JSON file (inherits from CSV settings).</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class ReceivedJsonTable(ReceivedCsvTable):\n    \"\"\"Defines settings for reading a JSON file (inherits from CSV settings).\"\"\"\n    pass\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedParquetTable","title":"<code>ReceivedParquetTable</code>","text":"<p>               Bases: <code>ReceivedTableBase</code></p> <p>Defines settings for reading a Parquet file.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class ReceivedParquetTable(ReceivedTableBase):\n    \"\"\"Defines settings for reading a Parquet file.\"\"\"\n    file_type: str = 'parquet'\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedTable","title":"<code>ReceivedTable</code>","text":"<p>               Bases: <code>ReceivedExcelTable</code>, <code>ReceivedCsvTable</code>, <code>ReceivedParquetTable</code></p> <p>A comprehensive model that can represent any type of received table.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class ReceivedTable(ReceivedExcelTable, ReceivedCsvTable, ReceivedParquetTable):\n    \"\"\"A comprehensive model that can represent any type of received table.\"\"\"\n    ...\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedTableBase","title":"<code>ReceivedTableBase</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model for defining a table received from an external source.</p> <p>Methods:</p> Name Description <code>create_from_path</code> <p>Creates an instance from a file path string.</p> <code>populate_abs_file_path</code> <p>Ensures the absolute file path is populated after validation.</p> <code>set_absolute_filepath</code> <p>Resolves the path to an absolute file path.</p> <p>Attributes:</p> Name Type Description <code>file_path</code> <code>str</code> <p>Constructs the full file path from the directory and name.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class ReceivedTableBase(BaseModel):\n    \"\"\"Base model for defining a table received from an external source.\"\"\"\n    id: Optional[int] = None\n    name: Optional[str]\n    path: str  # This can be an absolute or relative path\n    directory: Optional[str] = None\n    analysis_file_available: bool = False\n    status: Optional[str] = None\n    file_type: Optional[str] = None\n    fields: List[MinimalFieldInfo] = Field(default_factory=list)\n    abs_file_path: Optional[str] = None\n\n    @classmethod\n    def create_from_path(cls, path: str):\n        \"\"\"Creates an instance from a file path string.\"\"\"\n        filename = Path(path).name\n        return cls(name=filename, path=path)\n\n    @property\n    def file_path(self) -&gt; str:\n        \"\"\"Constructs the full file path from the directory and name.\"\"\"\n        if not self.name in self.path:\n            return os.path.join(self.path, self.name)\n        else:\n            return self.path\n\n    def set_absolute_filepath(self):\n        \"\"\"Resolves the path to an absolute file path.\"\"\"\n        base_path = Path(self.path).expanduser()\n        if not base_path.is_absolute():\n            base_path = Path.cwd() / base_path\n        if self.name and self.name not in base_path.name:\n            base_path = base_path / self.name\n        self.abs_file_path = str(base_path.resolve())\n\n    @model_validator(mode='after')\n    def populate_abs_file_path(self):\n        \"\"\"Ensures the absolute file path is populated after validation.\"\"\"\n        if not self.abs_file_path:\n            self.set_absolute_filepath()\n        return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedTableBase.file_path","title":"<code>file_path</code>  <code>property</code>","text":"<p>Constructs the full file path from the directory and name.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedTableBase.create_from_path","title":"<code>create_from_path(path)</code>  <code>classmethod</code>","text":"<p>Creates an instance from a file path string.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@classmethod\ndef create_from_path(cls, path: str):\n    \"\"\"Creates an instance from a file path string.\"\"\"\n    filename = Path(path).name\n    return cls(name=filename, path=path)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedTableBase.populate_abs_file_path","title":"<code>populate_abs_file_path()</code>","text":"<p>Ensures the absolute file path is populated after validation.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>@model_validator(mode='after')\ndef populate_abs_file_path(self):\n    \"\"\"Ensures the absolute file path is populated after validation.\"\"\"\n    if not self.abs_file_path:\n        self.set_absolute_filepath()\n    return self\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.ReceivedTableBase.set_absolute_filepath","title":"<code>set_absolute_filepath()</code>","text":"<p>Resolves the path to an absolute file path.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>def set_absolute_filepath(self):\n    \"\"\"Resolves the path to an absolute file path.\"\"\"\n    base_path = Path(self.path).expanduser()\n    if not base_path.is_absolute():\n        base_path = Path.cwd() / base_path\n    if self.name and self.name not in base_path.name:\n        base_path = base_path / self.name\n    self.abs_file_path = str(base_path.resolve())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.RemoveItem","title":"<code>RemoveItem</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single item to be removed from a directory or list.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class RemoveItem(BaseModel):\n    \"\"\"Represents a single item to be removed from a directory or list.\"\"\"\n    path: str\n    id: int = -1\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.RemoveItemsInput","title":"<code>RemoveItemsInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines a list of items to be removed.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class RemoveItemsInput(BaseModel):\n    \"\"\"Defines a list of items to be removed.\"\"\"\n    paths: List[RemoveItem]\n    source_path: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.SampleUsers","title":"<code>SampleUsers</code>","text":"<p>               Bases: <code>ExternalSource</code></p> <p>Settings for generating a sample dataset of users.</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class SampleUsers(ExternalSource):\n    \"\"\"Settings for generating a sample dataset of users.\"\"\"\n    SAMPLE_USERS: bool\n    class_name: str = \"sample_users\"\n    size: int = 100\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.input_schema.UserDefinedNode","title":"<code>UserDefinedNode</code>","text":"<p>               Bases: <code>NodeMultiInput</code></p> <p>Settings for a node that contains the user defined node information</p> Source code in <code>flowfile_core/flowfile_core/schemas/input_schema.py</code> <pre><code>class UserDefinedNode(NodeMultiInput):\n    \"\"\"Settings for a node that contains the user defined node information\"\"\"\n    settings: Any\n</code></pre>"},{"location":"for-developers/python-api-reference.html#transform_schema","title":"<code>transform_schema</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema","title":"<code>flowfile_core.schemas.transform_schema</code>","text":"<p>Classes:</p> Name Description <code>AggColl</code> <p>A data class that represents a single aggregation operation for a group by operation.</p> <code>BasicFilter</code> <p>Defines a simple, single-condition filter (e.g., 'column' 'equals' 'value').</p> <code>CrossJoinInput</code> <p>Defines the settings for a cross join operation, including column selections for both inputs.</p> <code>FieldInput</code> <p>Represents a single field with its name and data type, typically for defining an output column.</p> <code>FilterInput</code> <p>Defines the settings for a filter operation, supporting basic or advanced (expression-based) modes.</p> <code>FullJoinKeyResponse</code> <p>Holds the join key rename responses for both sides of a join.</p> <code>FunctionInput</code> <p>Defines a formula to be applied, including the output field information.</p> <code>FuzzyMatchInput</code> <p>Extends <code>JoinInput</code> with settings specific to fuzzy matching, such as the matching algorithm and threshold.</p> <code>GraphSolverInput</code> <p>Defines settings for a graph-solving operation (e.g., finding connected components).</p> <code>GroupByInput</code> <p>A data class that represents the input for a group by operation.</p> <code>JoinInput</code> <p>Defines the settings for a standard SQL-style join, including keys, strategy, and selections.</p> <code>JoinInputs</code> <p>Extends <code>SelectInputs</code> with functionality specific to join operations, like handling join keys.</p> <code>JoinKeyRename</code> <p>Represents the renaming of a join key from its original to a temporary name.</p> <code>JoinKeyRenameResponse</code> <p>Contains a list of join key renames for one side of a join.</p> <code>JoinMap</code> <p>Defines a single mapping between a left and right column for a join key.</p> <code>JoinSelectMixin</code> <p>A mixin providing common methods for join-like operations that involve left and right inputs.</p> <code>PivotInput</code> <p>Defines the settings for a pivot (long-to-wide) operation.</p> <code>PolarsCodeInput</code> <p>A simple container for a string of user-provided Polars code to be executed.</p> <code>RecordIdInput</code> <p>Defines settings for adding a record ID (row number) column to the data.</p> <code>SelectInput</code> <p>Defines how a single column should be selected, renamed, or type-cast.</p> <code>SelectInputs</code> <p>A container for a list of <code>SelectInput</code> objects, providing helper methods for managing selections.</p> <code>SortByInput</code> <p>Defines a single sort condition on a column, including the direction.</p> <code>TextToRowsInput</code> <p>Defines settings for splitting a text column into multiple rows based on a delimiter.</p> <code>UnionInput</code> <p>Defines settings for a union (concatenation) operation.</p> <code>UniqueInput</code> <p>Defines settings for a uniqueness operation, specifying columns and which row to keep.</p> <code>UnpivotInput</code> <p>Defines settings for an unpivot (wide-to-long) operation.</p> <p>Functions:</p> Name Description <code>construct_join_key_name</code> <p>Creates a temporary, unique name for a join key column.</p> <code>get_func_type_mapping</code> <p>Infers the output data type of common aggregation functions.</p> <code>string_concat</code> <p>A simple wrapper to concatenate string columns in Polars.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.AggColl","title":"<code>AggColl</code>  <code>dataclass</code>","text":"<p>A data class that represents a single aggregation operation for a group by operation.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.AggColl--attributes","title":"Attributes","text":"<p>old_name : str     The name of the column in the original DataFrame to be aggregated.</p> Any <p>The aggregation function to use. This can be a string representing a built-in function or a custom function.</p> Optional[str] <p>The name of the resulting aggregated column in the output DataFrame. If not provided, it will default to the old_name appended with the aggregation function.</p> Optional[str] <p>The type of the output values of the aggregation. If not provided, it is inferred from the aggregation function using the <code>get_func_type_mapping</code> function.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.AggColl--example","title":"Example","text":"<p>agg_col = AggColl(     old_name='col1',     agg='sum',     new_name='sum_col1',     output_type='float' )</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes an aggregation column with its source, function, and new name.</p> <p>Attributes:</p> Name Type Description <code>agg_func</code> <p>Returns the corresponding Polars aggregation function from the <code>agg</code> string.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass AggColl:\n    \"\"\"\n    A data class that represents a single aggregation operation for a group by operation.\n\n    Attributes\n    ----------\n    old_name : str\n        The name of the column in the original DataFrame to be aggregated.\n\n    agg : Any\n        The aggregation function to use. This can be a string representing a built-in function or a custom function.\n\n    new_name : Optional[str]\n        The name of the resulting aggregated column in the output DataFrame. If not provided, it will default to the\n        old_name appended with the aggregation function.\n\n    output_type : Optional[str]\n        The type of the output values of the aggregation. If not provided, it is inferred from the aggregation function\n        using the `get_func_type_mapping` function.\n\n    Example\n    --------\n    agg_col = AggColl(\n        old_name='col1',\n        agg='sum',\n        new_name='sum_col1',\n        output_type='float'\n    )\n    \"\"\"\n    old_name: str\n    agg: str\n    new_name: Optional[str]\n    output_type: Optional[str] = None\n\n    def __init__(self, old_name: str, agg: str, new_name: str = None, output_type: str = None):\n        \"\"\"Initializes an aggregation column with its source, function, and new name.\"\"\"\n        self.old_name = str(old_name)\n        if agg != 'groupby':\n            self.new_name = new_name if new_name is not None else self.old_name + \"_\" + agg\n        else:\n            self.new_name = new_name if new_name is not None else self.old_name\n        self.output_type = output_type if output_type is not None else get_func_type_mapping(agg)\n        self.agg = agg\n\n    @property\n    def agg_func(self):\n        \"\"\"Returns the corresponding Polars aggregation function from the `agg` string.\"\"\"\n        if self.agg == 'groupby':\n            return self.agg\n        elif self.agg == 'concat':\n            return string_concat\n        else:\n            return getattr(pl, self.agg) if isinstance(self.agg, str) else self.agg\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.AggColl.agg_func","title":"<code>agg_func</code>  <code>property</code>","text":"<p>Returns the corresponding Polars aggregation function from the <code>agg</code> string.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.AggColl.__init__","title":"<code>__init__(old_name, agg, new_name=None, output_type=None)</code>","text":"<p>Initializes an aggregation column with its source, function, and new name.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def __init__(self, old_name: str, agg: str, new_name: str = None, output_type: str = None):\n    \"\"\"Initializes an aggregation column with its source, function, and new name.\"\"\"\n    self.old_name = str(old_name)\n    if agg != 'groupby':\n        self.new_name = new_name if new_name is not None else self.old_name + \"_\" + agg\n    else:\n        self.new_name = new_name if new_name is not None else self.old_name\n    self.output_type = output_type if output_type is not None else get_func_type_mapping(agg)\n    self.agg = agg\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.BasicFilter","title":"<code>BasicFilter</code>  <code>dataclass</code>","text":"<p>Defines a simple, single-condition filter (e.g., 'column' 'equals' 'value').</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass BasicFilter:\n    \"\"\"Defines a simple, single-condition filter (e.g., 'column' 'equals' 'value').\"\"\"\n    field: str = ''\n    filter_type: str = ''\n    filter_value: str = ''\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInput","title":"<code>CrossJoinInput</code>  <code>dataclass</code>","text":"<p>               Bases: <code>JoinSelectMixin</code></p> <p>Defines the settings for a cross join operation, including column selections for both inputs.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the CrossJoinInput with selections for left and right tables.</p> <code>auto_rename</code> <p>Automatically renames columns on the right side to prevent naming conflicts.</p> <p>Attributes:</p> Name Type Description <code>overlapping_records</code> <p>Finds column names that would conflict after the join.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass CrossJoinInput(JoinSelectMixin):\n    \"\"\"Defines the settings for a cross join operation, including column selections for both inputs.\"\"\"\n    left_select: SelectInputs = None\n    right_select: SelectInputs = None\n\n    def __init__(self, left_select: List[SelectInput] | List[str],\n                 right_select: List[SelectInput] | List[str]):\n        \"\"\"Initializes the CrossJoinInput with selections for left and right tables.\"\"\"\n        self.left_select = self.parse_select(left_select)\n        self.right_select = self.parse_select(right_select)\n\n    @property\n    def overlapping_records(self):\n        \"\"\"Finds column names that would conflict after the join.\"\"\"\n        return self.left_select.new_cols &amp; self.right_select.new_cols\n\n    def auto_rename(self):\n        \"\"\"Automatically renames columns on the right side to prevent naming conflicts.\"\"\"\n        overlapping_records = self.overlapping_records\n        while len(overlapping_records) &gt; 0:\n            for right_col in self.right_select.renames:\n                if right_col.new_name in overlapping_records:\n                    right_col.new_name = 'right_' + right_col.new_name\n            overlapping_records = self.overlapping_records\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInput.overlapping_records","title":"<code>overlapping_records</code>  <code>property</code>","text":"<p>Finds column names that would conflict after the join.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInput.__init__","title":"<code>__init__(left_select, right_select)</code>","text":"<p>Initializes the CrossJoinInput with selections for left and right tables.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def __init__(self, left_select: List[SelectInput] | List[str],\n             right_select: List[SelectInput] | List[str]):\n    \"\"\"Initializes the CrossJoinInput with selections for left and right tables.\"\"\"\n    self.left_select = self.parse_select(left_select)\n    self.right_select = self.parse_select(right_select)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.CrossJoinInput.auto_rename","title":"<code>auto_rename()</code>","text":"<p>Automatically renames columns on the right side to prevent naming conflicts.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def auto_rename(self):\n    \"\"\"Automatically renames columns on the right side to prevent naming conflicts.\"\"\"\n    overlapping_records = self.overlapping_records\n    while len(overlapping_records) &gt; 0:\n        for right_col in self.right_select.renames:\n            if right_col.new_name in overlapping_records:\n                right_col.new_name = 'right_' + right_col.new_name\n        overlapping_records = self.overlapping_records\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FieldInput","title":"<code>FieldInput</code>  <code>dataclass</code>","text":"<p>Represents a single field with its name and data type, typically for defining an output column.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass FieldInput:\n    \"\"\"Represents a single field with its name and data type, typically for defining an output column.\"\"\"\n    name: str\n    data_type: Optional[str] = None\n\n    def __init__(self, name: str, data_type: str = None):\n        self.name = name\n        self.data_type = data_type\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FilterInput","title":"<code>FilterInput</code>  <code>dataclass</code>","text":"<p>Defines the settings for a filter operation, supporting basic or advanced (expression-based) modes.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass FilterInput:\n    \"\"\"Defines the settings for a filter operation, supporting basic or advanced (expression-based) modes.\"\"\"\n    advanced_filter: str = ''\n    basic_filter: BasicFilter = None\n    filter_type: str = 'basic'\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FullJoinKeyResponse","title":"<code>FullJoinKeyResponse</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Holds the join key rename responses for both sides of a join.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class FullJoinKeyResponse(NamedTuple):\n    \"\"\"Holds the join key rename responses for both sides of a join.\"\"\"\n    left: JoinKeyRenameResponse\n    right: JoinKeyRenameResponse\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FunctionInput","title":"<code>FunctionInput</code>  <code>dataclass</code>","text":"<p>Defines a formula to be applied, including the output field information.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass FunctionInput:\n    \"\"\"Defines a formula to be applied, including the output field information.\"\"\"\n    field: FieldInput\n    function: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInput","title":"<code>FuzzyMatchInput</code>  <code>dataclass</code>","text":"<p>               Bases: <code>JoinInput</code></p> <p>Extends <code>JoinInput</code> with settings specific to fuzzy matching, such as the matching algorithm and threshold.</p> <p>Attributes:</p> Name Type Description <code>fuzzy_maps</code> <code>List[FuzzyMapping]</code> <p>Returns the final fuzzy mappings after applying all column renames.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass FuzzyMatchInput(JoinInput):\n    \"\"\"Extends `JoinInput` with settings specific to fuzzy matching, such as the matching algorithm and threshold.\"\"\"\n    join_mapping: List[FuzzyMapping]\n    aggregate_output: bool = False\n\n    @staticmethod\n    def parse_fuzz_mapping(fuzz_mapping: List[FuzzyMapping] | Tuple[str, str] | str) -&gt; List[FuzzyMapping]:\n        if isinstance(fuzz_mapping, (tuple, list)):\n            assert len(fuzz_mapping) &gt; 0\n            if all(isinstance(fm, dict) for fm in fuzz_mapping):\n                fuzz_mapping = [FuzzyMapping(**fm) for fm in fuzz_mapping]\n\n            if not isinstance(fuzz_mapping[0], FuzzyMapping):\n                assert len(fuzz_mapping) &lt;= 2\n                if len(fuzz_mapping) == 2:\n                    assert isinstance(fuzz_mapping[0], str) and isinstance(fuzz_mapping[1], str)\n                    fuzz_mapping = [FuzzyMapping(*fuzz_mapping)]\n                elif isinstance(fuzz_mapping[0], str):\n                    fuzz_mapping = [FuzzyMapping(fuzz_mapping[0], fuzz_mapping[0])]\n        elif isinstance(fuzz_mapping, str):\n            fuzz_mapping = [FuzzyMapping(fuzz_mapping, fuzz_mapping)]\n        elif isinstance(fuzz_mapping, FuzzyMapping):\n            fuzz_mapping = [fuzz_mapping]\n        else:\n            raise Exception('No valid join mapping as input')\n        return fuzz_mapping\n\n    def __init__(self, join_mapping: List[FuzzyMapping] | Tuple[str, str] | str, left_select: List[SelectInput] | List[str],\n                 right_select: List[SelectInput] | List[str], aggregate_output: bool = False, how: JoinStrategy = 'inner'):\n        self.join_mapping = self.parse_fuzz_mapping(join_mapping)\n        self.left_select = self.parse_select(left_select)\n        self.right_select = self.parse_select(right_select)\n        self.how = how\n        for jm in self.join_mapping:\n\n            if jm.right_col not in {v.old_name for v in self.right_select.renames}:\n                self.right_select.append(SelectInput(jm.right_col, keep=False, join_key=True))\n            if jm.left_col not in {v.old_name for v in self.left_select.renames}:\n                self.left_select.append(SelectInput(jm.left_col, keep=False, join_key=True))\n        [setattr(v, \"join_key\", v.old_name in self._left_join_keys) for v in self.left_select.renames]\n        [setattr(v, \"join_key\", v.old_name in self._right_join_keys) for v in self.right_select.renames]\n        self.aggregate_output = aggregate_output\n\n    @property\n    def overlapping_records(self):\n        return self.left_select.new_cols &amp; self.right_select.new_cols\n\n    @property\n    def fuzzy_maps(self) -&gt; List[FuzzyMapping]:\n        \"\"\"Returns the final fuzzy mappings after applying all column renames.\"\"\"\n        new_mappings = []\n        left_rename_table, right_rename_table = self.left_select.rename_table, self.right_select.rename_table\n        for org_fuzzy_map in self.join_mapping:\n            right_col = right_rename_table.get(org_fuzzy_map.right_col)\n            left_col = left_rename_table.get(org_fuzzy_map.left_col)\n            if right_col != org_fuzzy_map.right_col or left_col != org_fuzzy_map.left_col:\n                new_mapping = deepcopy(org_fuzzy_map)\n                new_mapping.left_col = left_col\n                new_mapping.right_col = right_col\n                new_mappings.append(new_mapping)\n            else:\n                new_mappings.append(org_fuzzy_map)\n        return new_mappings\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.FuzzyMatchInput.fuzzy_maps","title":"<code>fuzzy_maps</code>  <code>property</code>","text":"<p>Returns the final fuzzy mappings after applying all column renames.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.GraphSolverInput","title":"<code>GraphSolverInput</code>  <code>dataclass</code>","text":"<p>Defines settings for a graph-solving operation (e.g., finding connected components).</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass GraphSolverInput:\n    \"\"\"Defines settings for a graph-solving operation (e.g., finding connected components).\"\"\"\n    col_from: str\n    col_to: str\n    output_column_name: Optional[str] = 'graph_group'\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.GroupByInput","title":"<code>GroupByInput</code>  <code>dataclass</code>","text":"<p>A data class that represents the input for a group by operation.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.GroupByInput--attributes","title":"Attributes","text":"<p>group_columns : List[str]     A list of column names to group the DataFrame by. These column(s) will be set as the DataFrame index.</p> List[AggColl] <p>A list of <code>AggColl</code> objects that specify the aggregation operations to perform on the DataFrame columns after grouping. Each <code>AggColl</code> object should specify the column to be aggregated and the aggregation function to use.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.GroupByInput--example","title":"Example","text":"<p>group_by_input = GroupByInput(     agg_cols=[AggColl(old_name='ix', agg='groupby'), AggColl(old_name='groups', agg='groupby'), AggColl(old_name='col1', agg='sum'), AggColl(old_name='col2', agg='mean')] )</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass GroupByInput:\n    \"\"\"\n    A data class that represents the input for a group by operation.\n\n    Attributes\n    ----------\n    group_columns : List[str]\n        A list of column names to group the DataFrame by. These column(s) will be set as the DataFrame index.\n\n    agg_cols : List[AggColl]\n        A list of `AggColl` objects that specify the aggregation operations to perform on the DataFrame columns\n        after grouping. Each `AggColl` object should specify the column to be aggregated and the aggregation\n        function to use.\n\n    Example\n    --------\n    group_by_input = GroupByInput(\n        agg_cols=[AggColl(old_name='ix', agg='groupby'), AggColl(old_name='groups', agg='groupby'), AggColl(old_name='col1', agg='sum'), AggColl(old_name='col2', agg='mean')]\n    )\n    \"\"\"\n    agg_cols: List[AggColl]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInput","title":"<code>JoinInput</code>  <code>dataclass</code>","text":"<p>               Bases: <code>JoinSelectMixin</code></p> <p>Defines the settings for a standard SQL-style join, including keys, strategy, and selections.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the JoinInput with keys, selections, and join strategy.</p> <code>auto_rename</code> <p>Automatically renames columns on the right side to prevent naming conflicts.</p> <code>get_join_key_renames</code> <p>Gets the temporary rename mappings for the join keys on both sides.</p> <code>parse_join_mapping</code> <p>Parses various input formats for join keys into a standardized list of <code>JoinMap</code> objects.</p> <code>set_join_keys</code> <p>Marks the <code>SelectInput</code> objects corresponding to join keys.</p> <p>Attributes:</p> Name Type Description <code>left_join_keys</code> <code>List[str]</code> <p>Returns an ordered list of the left-side join key column names to be used in the join.</p> <code>right_join_keys</code> <code>List[str]</code> <p>Returns an ordered list of the right-side join key column names to be used in the join.</p> <code>used_join_mapping</code> <code>List[JoinMap]</code> <p>Returns the final join mapping after applying all renames and transformations.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass JoinInput(JoinSelectMixin):\n    \"\"\"Defines the settings for a standard SQL-style join, including keys, strategy, and selections.\"\"\"\n    join_mapping: List[JoinMap]\n    left_select: JoinInputs = None\n    right_select: JoinInputs = None\n    how: JoinStrategy = 'inner'\n\n    @staticmethod\n    def parse_join_mapping(join_mapping: any) -&gt; List[JoinMap]:\n        \"\"\"Parses various input formats for join keys into a standardized list of `JoinMap` objects.\"\"\"\n        if isinstance(join_mapping, (tuple, list)):\n            assert len(join_mapping) &gt; 0\n            if all(isinstance(jm, dict) for jm in join_mapping):\n                join_mapping = [JoinMap(**jm) for jm in join_mapping]\n\n            if not isinstance(join_mapping[0], JoinMap):\n                assert len(join_mapping) &lt;= 2\n                if len(join_mapping) == 2:\n                    assert isinstance(join_mapping[0], str) and isinstance(join_mapping[1], str)\n                    join_mapping = [JoinMap(*join_mapping)]\n                elif isinstance(join_mapping[0], str):\n                    join_mapping = [JoinMap(join_mapping[0], join_mapping[0])]\n        elif isinstance(join_mapping, str):\n            join_mapping = [JoinMap(join_mapping, join_mapping)]\n        else:\n            raise Exception('No valid join mapping as input')\n        return join_mapping\n\n    def __init__(self, join_mapping: List[JoinMap] | Tuple[str, str] | str,\n                 left_select: List[SelectInput] | List[str],\n                 right_select: List[SelectInput] | List[str],\n                 how: JoinStrategy = 'inner'):\n        \"\"\"Initializes the JoinInput with keys, selections, and join strategy.\"\"\"\n        self.join_mapping = self.parse_join_mapping(join_mapping)\n        self.left_select = self.parse_select(left_select)\n        self.right_select = self.parse_select(right_select)\n        self.set_join_keys()\n        self.how = how\n\n    def set_join_keys(self):\n        \"\"\"Marks the `SelectInput` objects corresponding to join keys.\"\"\"\n        [setattr(v, \"join_key\", v.old_name in self._left_join_keys) for v in self.left_select.renames]\n        [setattr(v, \"join_key\", v.old_name in self._right_join_keys) for v in self.right_select.renames]\n\n    def get_join_key_renames(self, filter_drop: bool = False) -&gt; FullJoinKeyResponse:\n        \"\"\"Gets the temporary rename mappings for the join keys on both sides.\"\"\"\n        return FullJoinKeyResponse(self.left_select.get_join_key_renames(side=\"left\", filter_drop=filter_drop),\n                                   self.right_select.get_join_key_renames(side=\"right\", filter_drop=filter_drop))\n\n    def get_names_for_table_rename(self) -&gt; List[JoinMap]:\n        new_mappings: List[JoinMap] = []\n        left_rename_table, right_rename_table = self.left_select.rename_table, self.right_select.rename_table\n        for join_map in self.join_mapping:\n            new_mappings.append(JoinMap(left_rename_table.get(join_map.left_col, join_map.left_col),\n                                        right_rename_table.get(join_map.right_col, join_map.right_col)\n                                        )\n                                )\n        return new_mappings\n\n    @property\n    def _left_join_keys(self) -&gt; Set:\n        \"\"\"Returns a set of the left-side join key column names.\"\"\"\n        return set(jm.left_col for jm in self.join_mapping)\n\n    @property\n    def _right_join_keys(self) -&gt; Set:\n        \"\"\"Returns a set of the right-side join key column names.\"\"\"\n        return set(jm.right_col for jm in self.join_mapping)\n\n    @property\n    def left_join_keys(self) -&gt; List[str]:\n        \"\"\"Returns an ordered list of the left-side join key column names to be used in the join.\"\"\"\n        return [jm.left_col for jm in self.used_join_mapping]\n\n    @property\n    def right_join_keys(self) -&gt; List[str]:\n        \"\"\"Returns an ordered list of the right-side join key column names to be used in the join.\"\"\"\n        return [jm.right_col for jm in self.used_join_mapping]\n\n    @property\n    def overlapping_records(self):\n        if self.how in ('left', 'right', 'inner'):\n            return self.left_select.new_cols &amp; self.right_select.new_cols\n        else:\n            return self.left_select.new_cols &amp; self.right_select.new_cols\n\n    def auto_rename(self):\n        \"\"\"Automatically renames columns on the right side to prevent naming conflicts.\"\"\"\n        self.set_join_keys()\n        overlapping_records = self.overlapping_records\n        while len(overlapping_records) &gt; 0:\n            for right_col in self.right_select.renames:\n                if right_col.new_name in overlapping_records:\n                    right_col.new_name = right_col.new_name + '_right'\n            overlapping_records = self.overlapping_records\n\n    @property\n    def used_join_mapping(self) -&gt; List[JoinMap]:\n        \"\"\"Returns the final join mapping after applying all renames and transformations.\"\"\"\n        new_mappings: List[JoinMap] = []\n        left_rename_table, right_rename_table = self.left_select.rename_table, self.right_select.rename_table\n        left_join_rename_mapping: Dict[str, str] = self.left_select.get_join_key_rename_mapping(\"left\")\n        right_join_rename_mapping: Dict[str, str] = self.right_select.get_join_key_rename_mapping(\"right\")\n        for join_map in self.join_mapping:\n            # del self.right_select.rename_table, self.left_select.rename_table\n            new_mappings.append(JoinMap(left_join_rename_mapping.get(left_rename_table.get(join_map.left_col, join_map.left_col)),\n                                        right_join_rename_mapping.get(right_rename_table.get(join_map.right_col, join_map.right_col))\n                                        )\n                                )\n        return new_mappings\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInput.left_join_keys","title":"<code>left_join_keys</code>  <code>property</code>","text":"<p>Returns an ordered list of the left-side join key column names to be used in the join.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInput.right_join_keys","title":"<code>right_join_keys</code>  <code>property</code>","text":"<p>Returns an ordered list of the right-side join key column names to be used in the join.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInput.used_join_mapping","title":"<code>used_join_mapping</code>  <code>property</code>","text":"<p>Returns the final join mapping after applying all renames and transformations.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInput.__init__","title":"<code>__init__(join_mapping, left_select, right_select, how='inner')</code>","text":"<p>Initializes the JoinInput with keys, selections, and join strategy.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def __init__(self, join_mapping: List[JoinMap] | Tuple[str, str] | str,\n             left_select: List[SelectInput] | List[str],\n             right_select: List[SelectInput] | List[str],\n             how: JoinStrategy = 'inner'):\n    \"\"\"Initializes the JoinInput with keys, selections, and join strategy.\"\"\"\n    self.join_mapping = self.parse_join_mapping(join_mapping)\n    self.left_select = self.parse_select(left_select)\n    self.right_select = self.parse_select(right_select)\n    self.set_join_keys()\n    self.how = how\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInput.auto_rename","title":"<code>auto_rename()</code>","text":"<p>Automatically renames columns on the right side to prevent naming conflicts.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def auto_rename(self):\n    \"\"\"Automatically renames columns on the right side to prevent naming conflicts.\"\"\"\n    self.set_join_keys()\n    overlapping_records = self.overlapping_records\n    while len(overlapping_records) &gt; 0:\n        for right_col in self.right_select.renames:\n            if right_col.new_name in overlapping_records:\n                right_col.new_name = right_col.new_name + '_right'\n        overlapping_records = self.overlapping_records\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInput.get_join_key_renames","title":"<code>get_join_key_renames(filter_drop=False)</code>","text":"<p>Gets the temporary rename mappings for the join keys on both sides.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_join_key_renames(self, filter_drop: bool = False) -&gt; FullJoinKeyResponse:\n    \"\"\"Gets the temporary rename mappings for the join keys on both sides.\"\"\"\n    return FullJoinKeyResponse(self.left_select.get_join_key_renames(side=\"left\", filter_drop=filter_drop),\n                               self.right_select.get_join_key_renames(side=\"right\", filter_drop=filter_drop))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInput.parse_join_mapping","title":"<code>parse_join_mapping(join_mapping)</code>  <code>staticmethod</code>","text":"<p>Parses various input formats for join keys into a standardized list of <code>JoinMap</code> objects.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@staticmethod\ndef parse_join_mapping(join_mapping: any) -&gt; List[JoinMap]:\n    \"\"\"Parses various input formats for join keys into a standardized list of `JoinMap` objects.\"\"\"\n    if isinstance(join_mapping, (tuple, list)):\n        assert len(join_mapping) &gt; 0\n        if all(isinstance(jm, dict) for jm in join_mapping):\n            join_mapping = [JoinMap(**jm) for jm in join_mapping]\n\n        if not isinstance(join_mapping[0], JoinMap):\n            assert len(join_mapping) &lt;= 2\n            if len(join_mapping) == 2:\n                assert isinstance(join_mapping[0], str) and isinstance(join_mapping[1], str)\n                join_mapping = [JoinMap(*join_mapping)]\n            elif isinstance(join_mapping[0], str):\n                join_mapping = [JoinMap(join_mapping[0], join_mapping[0])]\n    elif isinstance(join_mapping, str):\n        join_mapping = [JoinMap(join_mapping, join_mapping)]\n    else:\n        raise Exception('No valid join mapping as input')\n    return join_mapping\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInput.set_join_keys","title":"<code>set_join_keys()</code>","text":"<p>Marks the <code>SelectInput</code> objects corresponding to join keys.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def set_join_keys(self):\n    \"\"\"Marks the `SelectInput` objects corresponding to join keys.\"\"\"\n    [setattr(v, \"join_key\", v.old_name in self._left_join_keys) for v in self.left_select.renames]\n    [setattr(v, \"join_key\", v.old_name in self._right_join_keys) for v in self.right_select.renames]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputs","title":"<code>JoinInputs</code>","text":"<p>               Bases: <code>SelectInputs</code></p> <p>Extends <code>SelectInputs</code> with functionality specific to join operations, like handling join keys.</p> <p>Methods:</p> Name Description <code>get_join_key_rename_mapping</code> <p>Returns a dictionary mapping original join key names to their temporary names.</p> <code>get_join_key_renames</code> <p>Gets the temporary rename mapping for all join keys on one side of a join.</p> <p>Attributes:</p> Name Type Description <code>join_key_selects</code> <code>List[SelectInput]</code> <p>Returns only the <code>SelectInput</code> objects that are marked as join keys.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class JoinInputs(SelectInputs):\n    \"\"\"Extends `SelectInputs` with functionality specific to join operations, like handling join keys.\"\"\"\n\n    def __init__(self, renames: List[SelectInput]):\n        self.renames = renames\n\n    @property\n    def join_key_selects(self) -&gt; List[SelectInput]:\n        \"\"\"Returns only the `SelectInput` objects that are marked as join keys.\"\"\"\n        return [v for v in self.renames if v.join_key]\n\n    def get_join_key_renames(self, side: SideLit, filter_drop: bool = False) -&gt; JoinKeyRenameResponse:\n        \"\"\"Gets the temporary rename mapping for all join keys on one side of a join.\"\"\"\n        return JoinKeyRenameResponse(\n            side,\n            [JoinKeyRename(jk.new_name,\n                           construct_join_key_name(side, jk.new_name))\n             for jk in self.join_key_selects if jk.keep or not filter_drop]\n        )\n\n    def get_join_key_rename_mapping(self, side: SideLit) -&gt; Dict[str, str]:\n        \"\"\"Returns a dictionary mapping original join key names to their temporary names.\"\"\"\n        return {jkr[0]: jkr[1] for jkr in self.get_join_key_renames(side)[1]}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputs.join_key_selects","title":"<code>join_key_selects</code>  <code>property</code>","text":"<p>Returns only the <code>SelectInput</code> objects that are marked as join keys.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputs.get_join_key_rename_mapping","title":"<code>get_join_key_rename_mapping(side)</code>","text":"<p>Returns a dictionary mapping original join key names to their temporary names.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_join_key_rename_mapping(self, side: SideLit) -&gt; Dict[str, str]:\n    \"\"\"Returns a dictionary mapping original join key names to their temporary names.\"\"\"\n    return {jkr[0]: jkr[1] for jkr in self.get_join_key_renames(side)[1]}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinInputs.get_join_key_renames","title":"<code>get_join_key_renames(side, filter_drop=False)</code>","text":"<p>Gets the temporary rename mapping for all join keys on one side of a join.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_join_key_renames(self, side: SideLit, filter_drop: bool = False) -&gt; JoinKeyRenameResponse:\n    \"\"\"Gets the temporary rename mapping for all join keys on one side of a join.\"\"\"\n    return JoinKeyRenameResponse(\n        side,\n        [JoinKeyRename(jk.new_name,\n                       construct_join_key_name(side, jk.new_name))\n         for jk in self.join_key_selects if jk.keep or not filter_drop]\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinKeyRename","title":"<code>JoinKeyRename</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Represents the renaming of a join key from its original to a temporary name.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class JoinKeyRename(NamedTuple):\n    \"\"\"Represents the renaming of a join key from its original to a temporary name.\"\"\"\n    original_name: str\n    temp_name: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinKeyRenameResponse","title":"<code>JoinKeyRenameResponse</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Contains a list of join key renames for one side of a join.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class JoinKeyRenameResponse(NamedTuple):\n    \"\"\"Contains a list of join key renames for one side of a join.\"\"\"\n    side: SideLit\n    join_key_renames: List[JoinKeyRename]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinMap","title":"<code>JoinMap</code>  <code>dataclass</code>","text":"<p>Defines a single mapping between a left and right column for a join key.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass JoinMap:\n    \"\"\"Defines a single mapping between a left and right column for a join key.\"\"\"\n    left_col: str\n    right_col: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinSelectMixin","title":"<code>JoinSelectMixin</code>","text":"<p>A mixin providing common methods for join-like operations that involve left and right inputs.</p> <p>Methods:</p> Name Description <code>add_new_select_column</code> <p>Adds a new column to the selection for either the left or right side.</p> <code>auto_generate_new_col_name</code> <p>Generates a new, non-conflicting column name by adding a suffix if necessary.</p> <code>parse_select</code> <p>Parses various input formats into a standardized <code>JoinInputs</code> object.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>class JoinSelectMixin:\n    \"\"\"A mixin providing common methods for join-like operations that involve left and right inputs.\"\"\"\n    left_select: JoinInputs = None\n    right_select: JoinInputs = None\n\n    @staticmethod\n    def parse_select(select: List[SelectInput] | List[str] | List[Dict]) -&gt; JoinInputs | None:\n        \"\"\"Parses various input formats into a standardized `JoinInputs` object.\"\"\"\n        if all(isinstance(c, SelectInput) for c in select):\n            return JoinInputs(select)\n        elif all(isinstance(c, dict) for c in select):\n            return JoinInputs([SelectInput(**c.__dict__) for c in select])\n        elif isinstance(select, dict):\n            renames = select.get('renames')\n            if renames:\n                return JoinInputs([SelectInput(**c) for c in renames])\n        elif all(isinstance(c, str) for c in select):\n            return JoinInputs([SelectInput(s, s) for s in select])\n\n    def auto_generate_new_col_name(self, old_col_name: str, side: str) -&gt; str:\n        \"\"\"Generates a new, non-conflicting column name by adding a suffix if necessary.\"\"\"\n        current_names = self.left_select.new_cols &amp; self.right_select.new_cols\n        if old_col_name not in current_names:\n            return old_col_name\n        while True:\n            if old_col_name not in current_names:\n                return old_col_name\n            old_col_name = f'{side}_{old_col_name}'\n\n    def add_new_select_column(self, select_input: SelectInput, side: str):\n        \"\"\"Adds a new column to the selection for either the left or right side.\"\"\"\n        selects = self.right_select if side == 'right' else self.left_select\n        select_input.new_name = self.auto_generate_new_col_name(select_input.old_name, side=side)\n        selects.__add__(select_input)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinSelectMixin.add_new_select_column","title":"<code>add_new_select_column(select_input, side)</code>","text":"<p>Adds a new column to the selection for either the left or right side.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def add_new_select_column(self, select_input: SelectInput, side: str):\n    \"\"\"Adds a new column to the selection for either the left or right side.\"\"\"\n    selects = self.right_select if side == 'right' else self.left_select\n    select_input.new_name = self.auto_generate_new_col_name(select_input.old_name, side=side)\n    selects.__add__(select_input)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinSelectMixin.auto_generate_new_col_name","title":"<code>auto_generate_new_col_name(old_col_name, side)</code>","text":"<p>Generates a new, non-conflicting column name by adding a suffix if necessary.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def auto_generate_new_col_name(self, old_col_name: str, side: str) -&gt; str:\n    \"\"\"Generates a new, non-conflicting column name by adding a suffix if necessary.\"\"\"\n    current_names = self.left_select.new_cols &amp; self.right_select.new_cols\n    if old_col_name not in current_names:\n        return old_col_name\n    while True:\n        if old_col_name not in current_names:\n            return old_col_name\n        old_col_name = f'{side}_{old_col_name}'\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.JoinSelectMixin.parse_select","title":"<code>parse_select(select)</code>  <code>staticmethod</code>","text":"<p>Parses various input formats into a standardized <code>JoinInputs</code> object.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@staticmethod\ndef parse_select(select: List[SelectInput] | List[str] | List[Dict]) -&gt; JoinInputs | None:\n    \"\"\"Parses various input formats into a standardized `JoinInputs` object.\"\"\"\n    if all(isinstance(c, SelectInput) for c in select):\n        return JoinInputs(select)\n    elif all(isinstance(c, dict) for c in select):\n        return JoinInputs([SelectInput(**c.__dict__) for c in select])\n    elif isinstance(select, dict):\n        renames = select.get('renames')\n        if renames:\n            return JoinInputs([SelectInput(**c) for c in renames])\n    elif all(isinstance(c, str) for c in select):\n        return JoinInputs([SelectInput(s, s) for s in select])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.PivotInput","title":"<code>PivotInput</code>  <code>dataclass</code>","text":"<p>Defines the settings for a pivot (long-to-wide) operation.</p> <p>Methods:</p> Name Description <code>get_group_by_input</code> <p>Constructs the <code>GroupByInput</code> needed for the pre-aggregation step of the pivot.</p> <code>get_pivot_column</code> <p>Returns the pivot column as a Polars column expression.</p> <code>get_values_expr</code> <p>Creates the struct expression used to gather the values for pivoting.</p> <p>Attributes:</p> Name Type Description <code>grouped_columns</code> <code>List[str]</code> <p>Returns the list of columns to be used for the initial grouping stage of the pivot.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass PivotInput:\n    \"\"\"Defines the settings for a pivot (long-to-wide) operation.\"\"\"\n    index_columns: List[str]\n    pivot_column: str\n    value_col: str\n    aggregations: List[str]\n\n    @property\n    def grouped_columns(self) -&gt; List[str]:\n        \"\"\"Returns the list of columns to be used for the initial grouping stage of the pivot.\"\"\"\n        return self.index_columns + [self.pivot_column]\n\n    def get_group_by_input(self) -&gt; GroupByInput:\n        \"\"\"Constructs the `GroupByInput` needed for the pre-aggregation step of the pivot.\"\"\"\n        group_by_cols = [AggColl(c, 'groupby') for c in self.grouped_columns]\n        agg_cols = [AggColl(self.value_col, agg=aggregation, new_name=aggregation) for aggregation in self.aggregations]\n        return GroupByInput(group_by_cols+agg_cols)\n\n    def get_index_columns(self) -&gt; List[pl.col]:\n        return [pl.col(c) for c in self.index_columns]\n\n    def get_pivot_column(self) -&gt; pl.Expr:\n        \"\"\"Returns the pivot column as a Polars column expression.\"\"\"\n        return pl.col(self.pivot_column)\n\n    def get_values_expr(self) -&gt; pl.Expr:\n        \"\"\"Creates the struct expression used to gather the values for pivoting.\"\"\"\n        return pl.struct([pl.col(c) for c in self.aggregations]).alias('vals')\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.PivotInput.grouped_columns","title":"<code>grouped_columns</code>  <code>property</code>","text":"<p>Returns the list of columns to be used for the initial grouping stage of the pivot.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.PivotInput.get_group_by_input","title":"<code>get_group_by_input()</code>","text":"<p>Constructs the <code>GroupByInput</code> needed for the pre-aggregation step of the pivot.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_group_by_input(self) -&gt; GroupByInput:\n    \"\"\"Constructs the `GroupByInput` needed for the pre-aggregation step of the pivot.\"\"\"\n    group_by_cols = [AggColl(c, 'groupby') for c in self.grouped_columns]\n    agg_cols = [AggColl(self.value_col, agg=aggregation, new_name=aggregation) for aggregation in self.aggregations]\n    return GroupByInput(group_by_cols+agg_cols)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.PivotInput.get_pivot_column","title":"<code>get_pivot_column()</code>","text":"<p>Returns the pivot column as a Polars column expression.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_pivot_column(self) -&gt; pl.Expr:\n    \"\"\"Returns the pivot column as a Polars column expression.\"\"\"\n    return pl.col(self.pivot_column)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.PivotInput.get_values_expr","title":"<code>get_values_expr()</code>","text":"<p>Creates the struct expression used to gather the values for pivoting.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_values_expr(self) -&gt; pl.Expr:\n    \"\"\"Creates the struct expression used to gather the values for pivoting.\"\"\"\n    return pl.struct([pl.col(c) for c in self.aggregations]).alias('vals')\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.PolarsCodeInput","title":"<code>PolarsCodeInput</code>  <code>dataclass</code>","text":"<p>A simple container for a string of user-provided Polars code to be executed.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass PolarsCodeInput:\n    \"\"\"A simple container for a string of user-provided Polars code to be executed.\"\"\"\n    polars_code: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.RecordIdInput","title":"<code>RecordIdInput</code>  <code>dataclass</code>","text":"<p>Defines settings for adding a record ID (row number) column to the data.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass RecordIdInput:\n    \"\"\"Defines settings for adding a record ID (row number) column to the data.\"\"\"\n    output_column_name: str = 'record_id'\n    offset: int = 1\n    group_by: Optional[bool] = False\n    group_by_columns: Optional[List[str]] = field(default_factory=list)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInput","title":"<code>SelectInput</code>  <code>dataclass</code>","text":"<p>Defines how a single column should be selected, renamed, or type-cast.</p> <p>This is a core building block for any operation that involves column manipulation. It holds all the configuration for a single field in a selection operation.</p> <p>Attributes:</p> Name Type Description <code>polars_type</code> <code>str</code> <p>Translates a user-friendly type name to a Polars data type string.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass SelectInput:\n    \"\"\"Defines how a single column should be selected, renamed, or type-cast.\n\n    This is a core building block for any operation that involves column manipulation.\n    It holds all the configuration for a single field in a selection operation.\n    \"\"\"\n    old_name: str\n    original_position: Optional[int] = None\n    new_name: Optional[str] = None\n    data_type: Optional[str] = None\n    data_type_change: Optional[bool] = False\n    join_key: Optional[bool] = False\n    is_altered: Optional[bool] = False\n    position: Optional[int] = None\n    is_available: Optional[bool] = True\n    keep: Optional[bool] = True\n\n    def __hash__(self):\n        return hash(self.old_name)\n\n    def __init__(self, old_name: str, new_name: str = None, keep: bool = True, data_type: str = None,\n                 data_type_change: bool = False, join_key: bool = False, is_altered: bool = False,\n                 is_available: bool = True, position: int = None):\n        self.old_name = old_name\n        if new_name is None:\n            new_name = old_name\n        self.new_name = new_name\n        self.keep = keep\n        self.data_type = data_type\n        self.data_type_change = data_type_change\n        self.join_key = join_key\n        self.is_altered = is_altered\n        self.is_available = is_available\n        self.position = position\n\n    @property\n    def polars_type(self) -&gt; str:\n        \"\"\"Translates a user-friendly type name to a Polars data type string.\"\"\"\n        if self.data_type.lower() == 'string':\n            return 'Utf8'\n        elif self.data_type.lower() == 'integer':\n            return 'Int64'\n        elif self.data_type.lower() == 'double':\n            return 'Float64'\n        return self.data_type\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInput.polars_type","title":"<code>polars_type</code>  <code>property</code>","text":"<p>Translates a user-friendly type name to a Polars data type string.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs","title":"<code>SelectInputs</code>  <code>dataclass</code>","text":"<p>A container for a list of <code>SelectInput</code> objects, providing helper methods for managing selections.</p> <p>Methods:</p> Name Description <code>__add__</code> <p>Allows adding a SelectInput using the '+' operator.</p> <code>append</code> <p>Appends a new SelectInput to the list of renames.</p> <code>create_from_list</code> <p>Creates a SelectInputs object from a simple list of column names.</p> <code>create_from_pl_df</code> <p>Creates a SelectInputs object from a Polars DataFrame's columns.</p> <code>get_select_cols</code> <p>Gets a list of original column names to select from the source DataFrame.</p> <code>has_drop_cols</code> <p>Checks if any column is marked to be dropped from the selection.</p> <code>remove_select_input</code> <p>Removes a SelectInput from the list based on its original name.</p> <code>unselect_field</code> <p>Marks a field to be dropped from the final selection by setting <code>keep</code> to False.</p> <p>Attributes:</p> Name Type Description <code>drop_columns</code> <code>List[SelectInput]</code> <p>Returns a list of column names that are marked to be dropped from the selection.</p> <code>new_cols</code> <code>Set</code> <p>Returns a set of new (renamed) column names to be kept in the selection.</p> <code>old_cols</code> <code>Set</code> <p>Returns a set of original column names to be kept in the selection.</p> <code>rename_table</code> <p>Generates a dictionary for use in Polars' <code>.rename()</code> method.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass SelectInputs:\n    \"\"\"A container for a list of `SelectInput` objects, providing helper methods for managing selections.\"\"\"\n    renames: List[SelectInput]\n\n    @property\n    def old_cols(self) -&gt; Set:\n        \"\"\"Returns a set of original column names to be kept in the selection.\"\"\"\n        return set(v.old_name for v in self.renames if v.keep)\n\n    @property\n    def new_cols(self) -&gt; Set:\n        \"\"\"Returns a set of new (renamed) column names to be kept in the selection.\"\"\"\n        return set(v.new_name for v in self.renames if v.keep)\n\n    @property\n    def rename_table(self):\n        \"\"\"Generates a dictionary for use in Polars' `.rename()` method.\"\"\"\n        return {v.old_name: v.new_name for v in self.renames if v.is_available and (v.keep or v.join_key)}\n\n    def get_select_cols(self, include_join_key: bool = True):\n        \"\"\"Gets a list of original column names to select from the source DataFrame.\"\"\"\n        return [v.old_name for v in self.renames if v.keep or (v.join_key and include_join_key)]\n\n    def has_drop_cols(self) -&gt; bool:\n        \"\"\"Checks if any column is marked to be dropped from the selection.\"\"\"\n        return any(not v.keep for v in self.renames)\n\n    @property\n    def drop_columns(self) -&gt; List[SelectInput]:\n        \"\"\"Returns a list of column names that are marked to be dropped from the selection.\"\"\"\n        return [v for v in self.renames if not v.keep and v.is_available]\n\n    @property\n    def non_jk_drop_columns(self) -&gt; List[SelectInput]:\n        return [v for v in self.renames if not v.keep and v.is_available and not v.join_key]\n\n    def __add__(self, other: \"SelectInput\"):\n        \"\"\"Allows adding a SelectInput using the '+' operator.\"\"\"\n        self.renames.append(other)\n\n    def append(self, other: \"SelectInput\"):\n        \"\"\"Appends a new SelectInput to the list of renames.\"\"\"\n        self.renames.append(other)\n\n    def remove_select_input(self, old_key: str):\n        \"\"\"Removes a SelectInput from the list based on its original name.\"\"\"\n        self.renames = [rename for rename in self.renames if rename.old_name != old_key]\n\n    def unselect_field(self, old_key: str):\n        \"\"\"Marks a field to be dropped from the final selection by setting `keep` to False.\"\"\"\n        for rename in self.renames:\n            if old_key == rename.old_name:\n                rename.keep = False\n\n    @classmethod\n    def create_from_list(cls, col_list: List[str]):\n        \"\"\"Creates a SelectInputs object from a simple list of column names.\"\"\"\n        return cls([SelectInput(c) for c in col_list])\n\n    @classmethod\n    def create_from_pl_df(cls, df: pl.DataFrame | pl.LazyFrame):\n        \"\"\"Creates a SelectInputs object from a Polars DataFrame's columns.\"\"\"\n        return cls([SelectInput(c) for c in df.columns])\n\n    def get_select_input_on_old_name(self, old_name: str) -&gt; SelectInput | None:\n        return next((v for v in self.renames if v.old_name == old_name), None)\n\n    def get_select_input_on_new_name(self, old_name: str) -&gt; SelectInput | None:\n        return next((v for v in self.renames if v.new_name == old_name), None)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.drop_columns","title":"<code>drop_columns</code>  <code>property</code>","text":"<p>Returns a list of column names that are marked to be dropped from the selection.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.new_cols","title":"<code>new_cols</code>  <code>property</code>","text":"<p>Returns a set of new (renamed) column names to be kept in the selection.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.old_cols","title":"<code>old_cols</code>  <code>property</code>","text":"<p>Returns a set of original column names to be kept in the selection.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.rename_table","title":"<code>rename_table</code>  <code>property</code>","text":"<p>Generates a dictionary for use in Polars' <code>.rename()</code> method.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.__add__","title":"<code>__add__(other)</code>","text":"<p>Allows adding a SelectInput using the '+' operator.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def __add__(self, other: \"SelectInput\"):\n    \"\"\"Allows adding a SelectInput using the '+' operator.\"\"\"\n    self.renames.append(other)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.append","title":"<code>append(other)</code>","text":"<p>Appends a new SelectInput to the list of renames.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def append(self, other: \"SelectInput\"):\n    \"\"\"Appends a new SelectInput to the list of renames.\"\"\"\n    self.renames.append(other)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.create_from_list","title":"<code>create_from_list(col_list)</code>  <code>classmethod</code>","text":"<p>Creates a SelectInputs object from a simple list of column names.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@classmethod\ndef create_from_list(cls, col_list: List[str]):\n    \"\"\"Creates a SelectInputs object from a simple list of column names.\"\"\"\n    return cls([SelectInput(c) for c in col_list])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.create_from_pl_df","title":"<code>create_from_pl_df(df)</code>  <code>classmethod</code>","text":"<p>Creates a SelectInputs object from a Polars DataFrame's columns.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@classmethod\ndef create_from_pl_df(cls, df: pl.DataFrame | pl.LazyFrame):\n    \"\"\"Creates a SelectInputs object from a Polars DataFrame's columns.\"\"\"\n    return cls([SelectInput(c) for c in df.columns])\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.get_select_cols","title":"<code>get_select_cols(include_join_key=True)</code>","text":"<p>Gets a list of original column names to select from the source DataFrame.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_select_cols(self, include_join_key: bool = True):\n    \"\"\"Gets a list of original column names to select from the source DataFrame.\"\"\"\n    return [v.old_name for v in self.renames if v.keep or (v.join_key and include_join_key)]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.has_drop_cols","title":"<code>has_drop_cols()</code>","text":"<p>Checks if any column is marked to be dropped from the selection.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def has_drop_cols(self) -&gt; bool:\n    \"\"\"Checks if any column is marked to be dropped from the selection.\"\"\"\n    return any(not v.keep for v in self.renames)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.remove_select_input","title":"<code>remove_select_input(old_key)</code>","text":"<p>Removes a SelectInput from the list based on its original name.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def remove_select_input(self, old_key: str):\n    \"\"\"Removes a SelectInput from the list based on its original name.\"\"\"\n    self.renames = [rename for rename in self.renames if rename.old_name != old_key]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SelectInputs.unselect_field","title":"<code>unselect_field(old_key)</code>","text":"<p>Marks a field to be dropped from the final selection by setting <code>keep</code> to False.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def unselect_field(self, old_key: str):\n    \"\"\"Marks a field to be dropped from the final selection by setting `keep` to False.\"\"\"\n    for rename in self.renames:\n        if old_key == rename.old_name:\n            rename.keep = False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.SortByInput","title":"<code>SortByInput</code>  <code>dataclass</code>","text":"<p>Defines a single sort condition on a column, including the direction.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass SortByInput:\n    \"\"\"Defines a single sort condition on a column, including the direction.\"\"\"\n    column: str\n    how: str = 'asc'\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.TextToRowsInput","title":"<code>TextToRowsInput</code>  <code>dataclass</code>","text":"<p>Defines settings for splitting a text column into multiple rows based on a delimiter.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass TextToRowsInput:\n    \"\"\"Defines settings for splitting a text column into multiple rows based on a delimiter.\"\"\"\n    column_to_split: str\n    output_column_name: Optional[str] = None\n    split_by_fixed_value: Optional[bool] = True\n    split_fixed_value: Optional[str] = ','\n    split_by_column: Optional[str] = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.UnionInput","title":"<code>UnionInput</code>  <code>dataclass</code>","text":"<p>Defines settings for a union (concatenation) operation.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass UnionInput:\n    \"\"\"Defines settings for a union (concatenation) operation.\"\"\"\n    mode: Literal['selective', 'relaxed'] = 'relaxed'\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.UniqueInput","title":"<code>UniqueInput</code>  <code>dataclass</code>","text":"<p>Defines settings for a uniqueness operation, specifying columns and which row to keep.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass UniqueInput:\n    \"\"\"Defines settings for a uniqueness operation, specifying columns and which row to keep.\"\"\"\n    columns: Optional[List[str]] = None\n    strategy: Literal[\"first\", \"last\", \"any\", \"none\"] = \"any\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.UnpivotInput","title":"<code>UnpivotInput</code>  <code>dataclass</code>","text":"<p>Defines settings for an unpivot (wide-to-long) operation.</p> <p>Methods:</p> Name Description <code>__post_init__</code> <p>Ensures that list attributes are initialized correctly if they are None.</p> <p>Attributes:</p> Name Type Description <code>data_type_selector_expr</code> <code>Optional[Callable]</code> <p>Returns a Polars selector function based on the <code>data_type_selector</code> string.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>@dataclass\nclass UnpivotInput:\n    \"\"\"Defines settings for an unpivot (wide-to-long) operation.\"\"\"\n    index_columns: Optional[List[str]] = field(default_factory=list)\n    value_columns: Optional[List[str]] = field(default_factory=list)\n    data_type_selector: Optional[Literal['float', 'all', 'date', 'numeric', 'string']] = None\n    data_type_selector_mode: Optional[Literal['data_type', 'column']] = 'column'\n\n    def __post_init__(self):\n        \"\"\"Ensures that list attributes are initialized correctly if they are None.\"\"\"\n        if self.index_columns is None:\n            self.index_columns = []\n        if self.value_columns is None:\n            self.value_columns = []\n        if self.data_type_selector_mode is None:\n            self.data_type_selector_mode = 'column'\n\n    @property\n    def data_type_selector_expr(self) -&gt; Optional[Callable]:\n        \"\"\"Returns a Polars selector function based on the `data_type_selector` string.\"\"\"\n        if self.data_type_selector_mode == 'data_type':\n            if self.data_type_selector is not None:\n                try:\n                    return getattr(selectors, self.data_type_selector)\n                except Exception as e:\n                    print(f'Could not find the selector: {self.data_type_selector}')\n                    return selectors.all\n            return selectors.all\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.UnpivotInput.data_type_selector_expr","title":"<code>data_type_selector_expr</code>  <code>property</code>","text":"<p>Returns a Polars selector function based on the <code>data_type_selector</code> string.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.UnpivotInput.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Ensures that list attributes are initialized correctly if they are None.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Ensures that list attributes are initialized correctly if they are None.\"\"\"\n    if self.index_columns is None:\n        self.index_columns = []\n    if self.value_columns is None:\n        self.value_columns = []\n    if self.data_type_selector_mode is None:\n        self.data_type_selector_mode = 'column'\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.construct_join_key_name","title":"<code>construct_join_key_name(side, column_name)</code>","text":"<p>Creates a temporary, unique name for a join key column.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def construct_join_key_name(side: SideLit, column_name: str) -&gt; str:\n    \"\"\"Creates a temporary, unique name for a join key column.\"\"\"\n    return \"_FLOWFILE_JOIN_KEY_\" + side.upper() + \"_\" + column_name\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.get_func_type_mapping","title":"<code>get_func_type_mapping(func)</code>","text":"<p>Infers the output data type of common aggregation functions.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def get_func_type_mapping(func: str):\n    \"\"\"Infers the output data type of common aggregation functions.\"\"\"\n    if func in [\"mean\", \"avg\", \"median\", \"std\", \"var\"]:\n        return \"Float64\"\n    elif func in ['min', 'max', 'first', 'last', \"cumsum\", \"sum\"]:\n        return None\n    elif func in ['count', 'n_unique']:\n        return \"Int64\"\n    elif func in ['concat']:\n        return \"Utf8\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.transform_schema.string_concat","title":"<code>string_concat(*column)</code>","text":"<p>A simple wrapper to concatenate string columns in Polars.</p> Source code in <code>flowfile_core/flowfile_core/schemas/transform_schema.py</code> <pre><code>def string_concat(*column: str):\n    \"\"\"A simple wrapper to concatenate string columns in Polars.\"\"\"\n    return pl.col(column).cast(pl.Utf8).str.concat(delimiter=',')\n</code></pre>"},{"location":"for-developers/python-api-reference.html#cloud_storage_schemas","title":"<code>cloud_storage_schemas</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas","title":"<code>flowfile_core.schemas.cloud_storage_schemas</code>","text":"<p>Cloud storage connection schemas for S3, ADLS, and other cloud providers.</p> <p>Classes:</p> Name Description <code>AuthSettingsInput</code> <p>The information needed for the user to provide the details that are needed to provide how to connect to the</p> <code>CloudStorageReadSettings</code> <p>Settings for reading from cloud storage</p> <code>CloudStorageSettings</code> <p>Settings for cloud storage nodes in the visual designer</p> <code>CloudStorageWriteSettings</code> <p>Settings for writing to cloud storage</p> <code>CloudStorageWriteSettingsWorkerInterface</code> <p>Settings for writing to cloud storage in worker context</p> <code>FullCloudStorageConnection</code> <p>Internal model with decrypted secrets</p> <code>FullCloudStorageConnectionInterface</code> <p>API response model - no secrets exposed</p> <code>FullCloudStorageConnectionWorkerInterface</code> <p>Internal model with decrypted secrets</p> <code>WriteSettingsWorkerInterface</code> <p>Settings for writing to cloud storage</p> <p>Functions:</p> Name Description <code>encrypt_for_worker</code> <p>Encrypts a secret value for use in worker contexts.</p> <code>get_cloud_storage_write_settings_worker_interface</code> <p>Convert to a worker interface model with hashed secrets.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.AuthSettingsInput","title":"<code>AuthSettingsInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The information needed for the user to provide the details that are needed to provide how to connect to the  Cloud provider</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class AuthSettingsInput(BaseModel):\n    \"\"\"\n    The information needed for the user to provide the details that are needed to provide how to connect to the\n     Cloud provider\n    \"\"\"\n    storage_type: CloudStorageType\n    auth_method: AuthMethod\n    connection_name: Optional[str] = \"None\"  # This is the reference to the item we will fetch that contains the data\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.CloudStorageReadSettings","title":"<code>CloudStorageReadSettings</code>","text":"<p>               Bases: <code>CloudStorageSettings</code></p> <p>Settings for reading from cloud storage</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class CloudStorageReadSettings(CloudStorageSettings):\n    \"\"\"Settings for reading from cloud storage\"\"\"\n\n    scan_mode: Literal[\"single_file\", \"directory\"] = \"single_file\"\n    file_format: Literal[\"csv\", \"parquet\", \"json\", \"delta\", \"iceberg\"] = \"parquet\"\n    # CSV specific options\n    csv_has_header: Optional[bool] = True\n    csv_delimiter: Optional[str] = \",\"\n    csv_encoding: Optional[str] = \"utf8\"\n    # Deltalake specific settings\n    delta_version: Optional[int] = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.CloudStorageSettings","title":"<code>CloudStorageSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Settings for cloud storage nodes in the visual designer</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class CloudStorageSettings(BaseModel):\n    \"\"\"Settings for cloud storage nodes in the visual designer\"\"\"\n\n    auth_mode: AuthMethod = \"auto\"\n    connection_name: Optional[str] = None  # Required only for 'reference' mode\n    resource_path: str  # s3://bucket/path/to/file.csv\n\n    @field_validator(\"auth_mode\", mode=\"after\")\n    def validate_auth_requirements(cls, v, values):\n        data = values.data\n        if v == \"reference\" and not data.get(\"connection_name\"):\n            raise ValueError(\"connection_name required when using reference mode\")\n        return v\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.CloudStorageWriteSettings","title":"<code>CloudStorageWriteSettings</code>","text":"<p>               Bases: <code>CloudStorageSettings</code>, <code>WriteSettingsWorkerInterface</code></p> <p>Settings for writing to cloud storage</p> <p>Methods:</p> Name Description <code>get_write_setting_worker_interface</code> <p>Convert to a worker interface model without secrets.</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class CloudStorageWriteSettings(CloudStorageSettings, WriteSettingsWorkerInterface):\n    \"\"\"Settings for writing to cloud storage\"\"\"\n    pass\n\n    def get_write_setting_worker_interface(self) -&gt; WriteSettingsWorkerInterface:\n        \"\"\"\n        Convert to a worker interface model without secrets.\n        \"\"\"\n        return WriteSettingsWorkerInterface(\n            resource_path=self.resource_path,\n            write_mode=self.write_mode,\n            file_format=self.file_format,\n            parquet_compression=self.parquet_compression,\n            csv_delimiter=self.csv_delimiter,\n            csv_encoding=self.csv_encoding\n        )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.CloudStorageWriteSettings.get_write_setting_worker_interface","title":"<code>get_write_setting_worker_interface()</code>","text":"<p>Convert to a worker interface model without secrets.</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>def get_write_setting_worker_interface(self) -&gt; WriteSettingsWorkerInterface:\n    \"\"\"\n    Convert to a worker interface model without secrets.\n    \"\"\"\n    return WriteSettingsWorkerInterface(\n        resource_path=self.resource_path,\n        write_mode=self.write_mode,\n        file_format=self.file_format,\n        parquet_compression=self.parquet_compression,\n        csv_delimiter=self.csv_delimiter,\n        csv_encoding=self.csv_encoding\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.CloudStorageWriteSettingsWorkerInterface","title":"<code>CloudStorageWriteSettingsWorkerInterface</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Settings for writing to cloud storage in worker context</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class CloudStorageWriteSettingsWorkerInterface(BaseModel):\n    \"\"\"Settings for writing to cloud storage in worker context\"\"\"\n    operation: str\n    write_settings: WriteSettingsWorkerInterface\n    connection: FullCloudStorageConnectionWorkerInterface\n    flowfile_flow_id: int = 1\n    flowfile_node_id: int | str = -1\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.FullCloudStorageConnection","title":"<code>FullCloudStorageConnection</code>","text":"<p>               Bases: <code>AuthSettingsInput</code></p> <p>Internal model with decrypted secrets</p> <p>Methods:</p> Name Description <code>get_worker_interface</code> <p>Convert to a public interface model without secrets.</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class FullCloudStorageConnection(AuthSettingsInput):\n    \"\"\"Internal model with decrypted secrets\"\"\"\n\n    # AWS S3\n    aws_region: Optional[str] = None\n    aws_access_key_id: Optional[str] = None\n    aws_secret_access_key: Optional[SecretStr] = None\n    aws_role_arn: Optional[str] = None\n    aws_allow_unsafe_html: Optional[bool] = None\n    aws_session_token: Optional[SecretStr] = None\n\n    # Azure ADLS\n    azure_account_name: Optional[str] = None\n    azure_account_key: Optional[SecretStr] = None\n    azure_tenant_id: Optional[str] = None\n    azure_client_id: Optional[str] = None\n    azure_client_secret: Optional[SecretStr] = None\n\n    # Common\n    endpoint_url: Optional[str] = None\n    verify_ssl: bool = True\n\n    def get_worker_interface(self) -&gt; \"FullCloudStorageConnectionWorkerInterface\":\n        \"\"\"\n        Convert to a public interface model without secrets.\n        \"\"\"\n        return FullCloudStorageConnectionWorkerInterface(\n            storage_type=self.storage_type,\n            auth_method=self.auth_method,\n            connection_name=self.connection_name,\n            aws_allow_unsafe_html=self.aws_allow_unsafe_html,\n            aws_secret_access_key=encrypt_for_worker(self.aws_secret_access_key),\n            aws_region=self.aws_region,\n            aws_access_key_id=self.aws_access_key_id,\n            aws_role_arn=self.aws_role_arn,\n            aws_session_token=encrypt_for_worker(self.aws_session_token),\n            azure_account_name=self.azure_account_name,\n            azure_tenant_id=self.azure_tenant_id,\n            azure_account_key=encrypt_for_worker(self.azure_account_key),\n            azure_client_id=self.azure_client_id,\n            azure_client_secret=encrypt_for_worker(self.azure_client_secret),\n            endpoint_url=self.endpoint_url,\n            verify_ssl=self.verify_ssl\n        )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.FullCloudStorageConnection.get_worker_interface","title":"<code>get_worker_interface()</code>","text":"<p>Convert to a public interface model without secrets.</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>def get_worker_interface(self) -&gt; \"FullCloudStorageConnectionWorkerInterface\":\n    \"\"\"\n    Convert to a public interface model without secrets.\n    \"\"\"\n    return FullCloudStorageConnectionWorkerInterface(\n        storage_type=self.storage_type,\n        auth_method=self.auth_method,\n        connection_name=self.connection_name,\n        aws_allow_unsafe_html=self.aws_allow_unsafe_html,\n        aws_secret_access_key=encrypt_for_worker(self.aws_secret_access_key),\n        aws_region=self.aws_region,\n        aws_access_key_id=self.aws_access_key_id,\n        aws_role_arn=self.aws_role_arn,\n        aws_session_token=encrypt_for_worker(self.aws_session_token),\n        azure_account_name=self.azure_account_name,\n        azure_tenant_id=self.azure_tenant_id,\n        azure_account_key=encrypt_for_worker(self.azure_account_key),\n        azure_client_id=self.azure_client_id,\n        azure_client_secret=encrypt_for_worker(self.azure_client_secret),\n        endpoint_url=self.endpoint_url,\n        verify_ssl=self.verify_ssl\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.FullCloudStorageConnectionInterface","title":"<code>FullCloudStorageConnectionInterface</code>","text":"<p>               Bases: <code>AuthSettingsInput</code></p> <p>API response model - no secrets exposed</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class FullCloudStorageConnectionInterface(AuthSettingsInput):\n    \"\"\"API response model - no secrets exposed\"\"\"\n\n    # Public fields only\n    aws_allow_unsafe_html: Optional[bool] = None\n    aws_region: Optional[str] = None\n    aws_access_key_id: Optional[str] = None\n    aws_role_arn: Optional[str] = None\n    azure_account_name: Optional[str] = None\n    azure_tenant_id: Optional[str] = None\n    azure_client_id: Optional[str] = None\n    endpoint_url: Optional[str] = None\n    verify_ssl: bool = True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.FullCloudStorageConnectionWorkerInterface","title":"<code>FullCloudStorageConnectionWorkerInterface</code>","text":"<p>               Bases: <code>AuthSettingsInput</code></p> <p>Internal model with decrypted secrets</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class FullCloudStorageConnectionWorkerInterface(AuthSettingsInput):\n    \"\"\"Internal model with decrypted secrets\"\"\"\n\n    # AWS S3\n    aws_region: Optional[str] = None\n    aws_access_key_id: Optional[str] = None\n    aws_secret_access_key: Optional[str] = None\n    aws_role_arn: Optional[str] = None\n    aws_allow_unsafe_html: Optional[bool] = None\n    aws_session_token: Optional[str] = None\n\n    # Azure ADLS\n    azure_account_name: Optional[str] = None\n    azure_account_key: Optional[str] = None\n    azure_tenant_id: Optional[str] = None\n    azure_client_id: Optional[str] = None\n    azure_client_secret: Optional[str] = None\n\n    # Common\n    endpoint_url: Optional[str] = None\n    verify_ssl: bool = True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.WriteSettingsWorkerInterface","title":"<code>WriteSettingsWorkerInterface</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Settings for writing to cloud storage</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>class WriteSettingsWorkerInterface(BaseModel):\n    \"\"\"Settings for writing to cloud storage\"\"\"\n    resource_path: str  # s3://bucket/path/to/file.csv\n\n    write_mode: Literal[\"overwrite\", \"append\"] = \"overwrite\"\n    file_format: Literal[\"csv\", \"parquet\", \"json\", \"delta\"] = \"parquet\"\n\n    parquet_compression: Literal[\"snappy\", \"gzip\", \"brotli\", \"lz4\", \"zstd\"] = \"snappy\"\n\n    csv_delimiter: str = \",\"\n    csv_encoding: str = \"utf8\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.encrypt_for_worker","title":"<code>encrypt_for_worker(secret_value)</code>","text":"<p>Encrypts a secret value for use in worker contexts. This is a placeholder function that simulates encryption. In practice, you would use a secure encryption method.</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>def encrypt_for_worker(secret_value: SecretStr|None) -&gt; str|None:\n    \"\"\"\n    Encrypts a secret value for use in worker contexts.\n    This is a placeholder function that simulates encryption.\n    In practice, you would use a secure encryption method.\n    \"\"\"\n    if secret_value is not None:\n        return encrypt_secret(secret_value.get_secret_value())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.cloud_storage_schemas.get_cloud_storage_write_settings_worker_interface","title":"<code>get_cloud_storage_write_settings_worker_interface(write_settings, connection, lf, flowfile_flow_id=1, flowfile_node_id=-1)</code>","text":"<p>Convert to a worker interface model with hashed secrets.</p> Source code in <code>flowfile_core/flowfile_core/schemas/cloud_storage_schemas.py</code> <pre><code>def get_cloud_storage_write_settings_worker_interface(\n        write_settings: CloudStorageWriteSettings,\n        connection: FullCloudStorageConnection,\n        lf: pl.LazyFrame,\n        flowfile_flow_id: int = 1,\n        flowfile_node_id: int | str = -1,\n        ) -&gt; CloudStorageWriteSettingsWorkerInterface:\n    \"\"\"\n    Convert to a worker interface model with hashed secrets.\n    \"\"\"\n    operation = base64.b64encode(lf.serialize()).decode()\n\n    return CloudStorageWriteSettingsWorkerInterface(\n        operation=operation,\n        write_settings=write_settings.get_write_setting_worker_interface(),\n        connection=connection.get_worker_interface(),\n        flowfile_flow_id=flowfile_flow_id,  # Default value, can be overridden\n        flowfile_node_id=flowfile_node_id  # Default value, can be overridden\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#output_model","title":"<code>output_model</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model","title":"<code>flowfile_core.schemas.output_model</code>","text":"<p>Classes:</p> Name Description <code>BaseItem</code> <p>A base model for any item in a file system, like a file or directory.</p> <code>ExpressionRef</code> <p>A reference to a single Polars expression, including its name and docstring.</p> <code>ExpressionsOverview</code> <p>Represents a categorized list of available Polars expressions.</p> <code>FileColumn</code> <p>Represents detailed schema and statistics for a single column (field).</p> <code>InstantFuncResult</code> <p>Represents the result of a function that is expected to execute instantly.</p> <code>ItemInfo</code> <p>Provides detailed information about a single item in an output directory.</p> <code>NodeData</code> <p>A comprehensive model holding the complete state and data for a single node.</p> <code>NodeResult</code> <p>Represents the execution result of a single node in a FlowGraph run.</p> <code>OutputDir</code> <p>Represents the contents of a single output directory.</p> <code>OutputFile</code> <p>Represents a single file in an output directory, extending BaseItem.</p> <code>OutputFiles</code> <p>Represents a collection of files, typically within a directory.</p> <code>OutputTree</code> <p>Represents a directory tree, including subdirectories.</p> <code>RunInformation</code> <p>Contains summary information about a complete FlowGraph execution.</p> <code>TableExample</code> <p>Represents a preview of a table, including schema and sample data.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.BaseItem","title":"<code>BaseItem</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A base model for any item in a file system, like a file or directory.</p> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class BaseItem(BaseModel):\n    \"\"\"A base model for any item in a file system, like a file or directory.\"\"\"\n    name: str\n    path: str\n    size: Optional[int] = None\n    creation_date: Optional[datetime] = None\n    access_date: Optional[datetime] = None\n    modification_date: Optional[datetime] = None\n    source_path: Optional[str] = None\n    number_of_items: int = -1\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.ExpressionRef","title":"<code>ExpressionRef</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A reference to a single Polars expression, including its name and docstring.</p> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class ExpressionRef(BaseModel):\n    \"\"\"A reference to a single Polars expression, including its name and docstring.\"\"\"\n    name: str\n    doc: Optional[str]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.ExpressionsOverview","title":"<code>ExpressionsOverview</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a categorized list of available Polars expressions.</p> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class ExpressionsOverview(BaseModel):\n    \"\"\"Represents a categorized list of available Polars expressions.\"\"\"\n    expression_type: str\n    expressions: List[ExpressionRef]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.FileColumn","title":"<code>FileColumn</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents detailed schema and statistics for a single column (field).</p> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class FileColumn(BaseModel):\n    \"\"\"Represents detailed schema and statistics for a single column (field).\"\"\"\n    name: str\n    data_type: str\n    is_unique: bool\n    max_value: str\n    min_value: str\n    number_of_empty_values: int\n    number_of_filled_values: int\n    number_of_unique_values: int\n    size: int\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.InstantFuncResult","title":"<code>InstantFuncResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the result of a function that is expected to execute instantly.</p> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class InstantFuncResult(BaseModel):\n    \"\"\"Represents the result of a function that is expected to execute instantly.\"\"\"\n    success: Optional[bool] = None\n    result: str\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.ItemInfo","title":"<code>ItemInfo</code>","text":"<p>               Bases: <code>OutputFile</code></p> <p>Provides detailed information about a single item in an output directory.</p> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class ItemInfo(OutputFile):\n    \"\"\"Provides detailed information about a single item in an output directory.\"\"\"\n    id: int = -1\n    type: str\n    analysis_file_available: bool = False\n    analysis_file_location: str = None\n    analysis_file_error: str = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.NodeData","title":"<code>NodeData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A comprehensive model holding the complete state and data for a single node.</p> <p>This includes its input/output data previews, settings, and run status.</p> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class NodeData(BaseModel):\n    \"\"\"A comprehensive model holding the complete state and data for a single node.\n\n    This includes its input/output data previews, settings, and run status.\n    \"\"\"\n    flow_id: int\n    node_id: int\n    flow_type: str\n    left_input: Optional[TableExample] = None\n    right_input: Optional[TableExample] = None\n    main_input: Optional[TableExample] = None\n    main_output: Optional[TableExample] = None\n    left_output: Optional[TableExample] = None\n    right_output: Optional[TableExample] = None\n    has_run: bool = False\n    is_cached: bool = False\n    setting_input: Any = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.NodeResult","title":"<code>NodeResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the execution result of a single node in a FlowGraph run.</p> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class NodeResult(BaseModel):\n    \"\"\"Represents the execution result of a single node in a FlowGraph run.\"\"\"\n    node_id: int\n    node_name: Optional[str] = None\n    start_timestamp: float = Field(default_factory=time.time)\n    end_timestamp: float = 0\n    success: Optional[bool] = None\n    error: str = ''\n    run_time: int = -1\n    is_running: bool = True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.OutputDir","title":"<code>OutputDir</code>","text":"<p>               Bases: <code>BaseItem</code></p> <p>Represents the contents of a single output directory.</p> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class OutputDir(BaseItem):\n    \"\"\"Represents the contents of a single output directory.\"\"\"\n    all_items: List[str]\n    items: List[ItemInfo]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.OutputFile","title":"<code>OutputFile</code>","text":"<p>               Bases: <code>BaseItem</code></p> <p>Represents a single file in an output directory, extending BaseItem.</p> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class OutputFile(BaseItem):\n    \"\"\"Represents a single file in an output directory, extending BaseItem.\"\"\"\n    ext: Optional[str] = None\n    mimetype: Optional[str] = None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.OutputFiles","title":"<code>OutputFiles</code>","text":"<p>               Bases: <code>BaseItem</code></p> <p>Represents a collection of files, typically within a directory.</p> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class OutputFiles(BaseItem):\n    \"\"\"Represents a collection of files, typically within a directory.\"\"\"\n    files: List[OutputFile] = Field(default_factory=list)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.OutputTree","title":"<code>OutputTree</code>","text":"<p>               Bases: <code>OutputFiles</code></p> <p>Represents a directory tree, including subdirectories.</p> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class OutputTree(OutputFiles):\n    \"\"\"Represents a directory tree, including subdirectories.\"\"\"\n    directories: List[OutputFiles] = Field(default_factory=list)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.RunInformation","title":"<code>RunInformation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Contains summary information about a complete FlowGraph execution.</p> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class RunInformation(BaseModel):\n    \"\"\"Contains summary information about a complete FlowGraph execution.\"\"\"\n    flow_id: int\n    start_time: Optional[datetime] = Field(default_factory=datetime.now)\n    end_time: Optional[datetime] = None\n    success: Optional[bool] = None\n    nodes_completed: int = 0\n    number_of_nodes: int = 0\n    node_step_result: List[NodeResult]\n    run_type: Literal[\"fetch_one\", \"full_run\"]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.schemas.output_model.TableExample","title":"<code>TableExample</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a preview of a table, including schema and sample data.</p> Source code in <code>flowfile_core/flowfile_core/schemas/output_model.py</code> <pre><code>class TableExample(BaseModel):\n    \"\"\"Represents a preview of a table, including schema and sample data.\"\"\"\n    node_id: int\n    number_of_records: int\n    number_of_columns: int\n    name: str\n    table_schema: List[FileColumn]\n    columns: List[str]\n    data: Optional[List[Dict]] = {}\n    has_example_data: bool = False\n    has_run_with_current_setup: bool = False\n</code></pre>"},{"location":"for-developers/python-api-reference.html#web-api","title":"Web API","text":"<p>This section documents the FastAPI routes that expose <code>flowfile-core</code>'s functionality over HTTP.</p>"},{"location":"for-developers/python-api-reference.html#routes","title":"<code>routes</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes","title":"<code>flowfile_core.routes.routes</code>","text":"<p>Main API router and endpoint definitions for the Flowfile application.</p> <p>This module sets up the FastAPI router, defines all the API endpoints for interacting with flows, nodes, files, and other core components of the application. It handles the logic for creating, reading, updating, and deleting these resources.</p> <p>Functions:</p> Name Description <code>add_generic_settings</code> <p>A generic endpoint to update the settings of any node.</p> <code>add_node</code> <p>Adds a new, unconfigured node (a \"promise\") to the flow graph.</p> <code>cancel_flow</code> <p>Cancels a currently running flow execution.</p> <code>close_flow</code> <p>Closes an active flow session.</p> <code>connect_node</code> <p>Creates a connection (edge) between two nodes in the flow graph.</p> <code>copy_node</code> <p>Copies an existing node's settings to a new node promise.</p> <code>create_db_connection</code> <p>Creates and securely stores a new database connection.</p> <code>create_directory</code> <p>Creates a new directory at the specified path.</p> <code>create_flow</code> <p>Creates a new, empty flow file at the specified path and registers a session for it.</p> <code>delete_db_connection</code> <p>Deletes a stored database connection.</p> <code>delete_node</code> <p>Deletes a node from the flow graph.</p> <code>delete_node_connection</code> <p>Deletes a connection (edge) between two nodes.</p> <code>get_active_flow_file_sessions</code> <p>Retrieves a list of all currently active flow sessions.</p> <code>get_current_directory_contents</code> <p>Gets the contents of the file explorer's current directory.</p> <code>get_current_files</code> <p>Gets the contents of the file explorer's current directory.</p> <code>get_current_path</code> <p>Returns the current absolute path of the file explorer.</p> <code>get_db_connections</code> <p>Retrieves all stored database connections for the current user (without passwords).</p> <code>get_description_node</code> <p>Retrieves the description text for a specific node.</p> <code>get_directory_contents</code> <p>Gets the contents of an arbitrary directory path.</p> <code>get_downstream_node_ids</code> <p>Gets a list of all node IDs that are downstream dependencies of a given node.</p> <code>get_excel_sheet_names</code> <p>Retrieves the sheet names from an Excel file.</p> <code>get_expression_doc</code> <p>Retrieves documentation for available Polars expressions.</p> <code>get_expressions</code> <p>Retrieves a list of all available Flowfile expression names.</p> <code>get_flow</code> <p>Retrieves the settings for a specific flow.</p> <code>get_flow_frontend_data</code> <p>Retrieves the data needed to render the flow graph in the frontend.</p> <code>get_flow_settings</code> <p>Retrieves the main settings for a flow.</p> <code>get_generated_code</code> <p>Generates and returns a Python script with Polars code representing the flow.</p> <code>get_graphic_walker_input</code> <p>Gets the data and configuration for the Graphic Walker data exploration tool.</p> <code>get_instant_function_result</code> <p>Executes a simple, instant function on a node's data and returns the result.</p> <code>get_list_of_saved_flows</code> <p>Scans a directory for saved flow files (<code>.flowfile</code>).</p> <code>get_local_files</code> <p>Retrieves a list of files from a specified local directory.</p> <code>get_node</code> <p>Retrieves the complete state and data preview for a single node.</p> <code>get_node_list</code> <p>Retrieves the list of all available node types and their templates.</p> <code>get_node_model</code> <p>(Internal) Retrieves a node's Pydantic model from the input_schema module by its name.</p> <code>get_run_status</code> <p>Retrieves the run status information for a specific flow.</p> <code>get_table_example</code> <p>Retrieves a data preview (schema and sample rows) for a node's output.</p> <code>get_vue_flow_data</code> <p>Retrieves the flow data formatted for the Vue-based frontend.</p> <code>import_saved_flow</code> <p>Imports a flow from a saved <code>.flowfile</code> and registers it as a new session.</p> <code>navigate_into_directory</code> <p>Navigates the file explorer into a specified subdirectory.</p> <code>navigate_to_directory</code> <p>Navigates the file explorer to an absolute directory path.</p> <code>navigate_up</code> <p>Navigates the file explorer one directory level up.</p> <code>register_flow</code> <p>Registers a new flow session with the application.</p> <code>run_flow</code> <p>Executes a flow in a background task.</p> <code>save_flow</code> <p>Saves the current state of a flow to a <code>.flowfile</code>.</p> <code>trigger_fetch_node_data</code> <p>Fetches and refreshes the data for a specific node.</p> <code>update_description_node</code> <p>Updates the description text for a specific node.</p> <code>update_flow_settings</code> <p>Updates the main settings for a flow.</p> <code>upload_file</code> <p>Uploads a file to the server's 'uploads' directory.</p> <code>validate_db_settings</code> <p>Validates that a connection can be made to a database with the given settings.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.add_generic_settings","title":"<code>add_generic_settings(input_data, node_type, current_user=Depends(get_current_active_user))</code>","text":"<p>A generic endpoint to update the settings of any node.</p> <p>This endpoint dynamically determines the correct Pydantic model and update function based on the <code>node_type</code> parameter.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/update_settings/', tags=['transform'])\ndef add_generic_settings(input_data: Dict[str, Any], node_type: str, current_user=Depends(get_current_active_user)):\n    \"\"\"A generic endpoint to update the settings of any node.\n\n    This endpoint dynamically determines the correct Pydantic model and update\n    function based on the `node_type` parameter.\n    \"\"\"\n    input_data['user_id'] = current_user.id\n    node_type = camel_case_to_snake_case(node_type)\n    flow_id = int(input_data.get('flow_id'))\n    logger.info(f'Updating the data for flow: {flow_id}, node {input_data[\"node_id\"]}')\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow.flow_settings.is_running:\n        raise HTTPException(422, 'Flow is running')\n    if flow is None:\n        raise HTTPException(404, 'could not find the flow')\n    add_func = getattr(flow, 'add_' + node_type)\n    parsed_input = None\n    setting_name_ref = 'node' + node_type.replace('_', '')\n\n    if add_func is None:\n        raise HTTPException(404, 'could not find the function')\n    try:\n        ref = get_node_model(setting_name_ref)\n        if ref:\n            parsed_input = ref(**input_data)\n    except Exception as e:\n        raise HTTPException(421, str(e))\n    if parsed_input is None:\n        raise HTTPException(404, 'could not find the interface')\n    try:\n        add_func(parsed_input)\n    except Exception as e:\n        logger.error(e)\n        raise HTTPException(419, str(f'error: {e}'))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.add_node","title":"<code>add_node(flow_id, node_id, node_type, pos_x=0, pos_y=0)</code>","text":"<p>Adds a new, unconfigured node (a \"promise\") to the flow graph.</p> <p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>int</code> <p>The ID of the flow to add the node to.</p> required <code>node_id</code> <code>int</code> <p>The client-generated ID for the new node.</p> required <code>node_type</code> <code>str</code> <p>The type of the node to add (e.g., 'filter', 'join').</p> required <code>pos_x</code> <code>int</code> <p>The X coordinate for the node's position in the UI.</p> <code>0</code> <code>pos_y</code> <code>int</code> <p>The Y coordinate for the node's position in the UI.</p> <code>0</code> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/editor/add_node/', tags=['editor'])\ndef add_node(flow_id: int, node_id: int, node_type: str, pos_x: int = 0, pos_y: int = 0):\n    \"\"\"Adds a new, unconfigured node (a \"promise\") to the flow graph.\n\n    Args:\n        flow_id: The ID of the flow to add the node to.\n        node_id: The client-generated ID for the new node.\n        node_type: The type of the node to add (e.g., 'filter', 'join').\n        pos_x: The X coordinate for the node's position in the UI.\n        pos_y: The Y coordinate for the node's position in the UI.\n    \"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    logger.info(f'Adding a promise for {node_type}')\n    if flow.flow_settings.is_running:\n        raise HTTPException(422, 'Flow is running')\n    node = flow.get_node(node_id)\n    if node is not None:\n        flow.delete_node(node_id)\n    node_promise = input_schema.NodePromise(flow_id=flow_id, node_id=node_id, cache_results=False, pos_x=pos_x,\n                                            pos_y=pos_y,\n                                            node_type=node_type)\n    if node_type == 'explore_data':\n        flow.add_initial_node_analysis(node_promise)\n        return\n    else:\n        logger.info(\"Adding node\")\n        flow.add_node_promise(node_promise)\n\n    if check_if_has_default_setting(node_type):\n        logger.info(f'Found standard settings for {node_type}, trying to upload them')\n        setting_name_ref = 'node' + node_type.replace('_', '')\n        node_model = get_node_model(setting_name_ref)\n        add_func = getattr(flow, 'add_' + node_type)\n        initial_settings = node_model(flow_id=flow_id, node_id=node_id, cache_results=False,\n                                      pos_x=pos_x, pos_y=pos_y, node_type=node_type)\n        add_func(initial_settings)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.cancel_flow","title":"<code>cancel_flow(flow_id)</code>","text":"<p>Cancels a currently running flow execution.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/flow/cancel/', tags=['editor'])\ndef cancel_flow(flow_id: int):\n    \"\"\"Cancels a currently running flow execution.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if not flow.flow_settings.is_running:\n        raise HTTPException(422, 'Flow is not running')\n    flow.cancel()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.close_flow","title":"<code>close_flow(flow_id)</code>","text":"<p>Closes an active flow session.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/editor/close_flow/', tags=['editor'])\ndef close_flow(flow_id: int) -&gt; None:\n    \"\"\"Closes an active flow session.\"\"\"\n    flow_file_handler.delete_flow(flow_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.connect_node","title":"<code>connect_node(flow_id, node_connection)</code>","text":"<p>Creates a connection (edge) between two nodes in the flow graph.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/editor/connect_node/', tags=['editor'])\ndef connect_node(flow_id: int, node_connection: input_schema.NodeConnection):\n    \"\"\"Creates a connection (edge) between two nodes in the flow graph.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        logger.info('could not find the flow')\n        raise HTTPException(404, 'could not find the flow')\n    if flow.flow_settings.is_running:\n        raise HTTPException(422, 'Flow is running')\n    add_connection(flow, node_connection)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.copy_node","title":"<code>copy_node(node_id_to_copy_from, flow_id_to_copy_from, node_promise)</code>","text":"<p>Copies an existing node's settings to a new node promise.</p> <p>Parameters:</p> Name Type Description Default <code>node_id_to_copy_from</code> <code>int</code> <p>The ID of the node to copy the settings from.</p> required <code>flow_id_to_copy_from</code> <code>int</code> <p>The ID of the flow containing the source node.</p> required <code>node_promise</code> <code>NodePromise</code> <p>A <code>NodePromise</code> representing the new node to be created.</p> required Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/editor/copy_node', tags=['editor'])\ndef copy_node(node_id_to_copy_from: int, flow_id_to_copy_from: int, node_promise: input_schema.NodePromise):\n    \"\"\"Copies an existing node's settings to a new node promise.\n\n    Args:\n        node_id_to_copy_from: The ID of the node to copy the settings from.\n        flow_id_to_copy_from: The ID of the flow containing the source node.\n        node_promise: A `NodePromise` representing the new node to be created.\n    \"\"\"\n    try:\n        flow_to_copy_from = flow_file_handler.get_flow(flow_id_to_copy_from)\n        flow = (flow_to_copy_from\n                if flow_id_to_copy_from == node_promise.flow_id\n                else flow_file_handler.get_flow(node_promise.flow_id)\n                )\n        node_to_copy = flow_to_copy_from.get_node(node_id_to_copy_from)\n        logger.info(f\"Copying data {node_promise.node_type}\")\n\n        if flow.flow_settings.is_running:\n            raise HTTPException(422, \"Flow is running\")\n\n        if flow.get_node(node_promise.node_id) is not None:\n            flow.delete_node(node_promise.node_id)\n\n        if node_promise.node_type == \"explore_data\":\n            flow.add_initial_node_analysis(node_promise)\n            return\n\n        flow.copy_node(node_promise, node_to_copy.setting_input, node_to_copy.node_type)\n\n    except Exception as e:\n        logger.error(e)\n        raise HTTPException(422, str(e))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.create_db_connection","title":"<code>create_db_connection(input_connection, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>","text":"<p>Creates and securely stores a new database connection.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/db_connection_lib\", tags=['db_connections'])\ndef create_db_connection(input_connection: input_schema.FullDatabaseConnection,\n                         current_user=Depends(get_current_active_user),\n                         db: Session = Depends(get_db)\n                         ):\n    \"\"\"Creates and securely stores a new database connection.\"\"\"\n    logger.info(f'Creating database connection {input_connection.connection_name}')\n    try:\n        store_database_connection(db, input_connection, current_user.id)\n    except ValueError:\n        raise HTTPException(422, 'Connection name already exists')\n    except Exception as e:\n        logger.error(e)\n        raise HTTPException(422, str(e))\n    return {\"message\": \"Database connection created successfully\"}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.create_directory","title":"<code>create_directory(new_directory)</code>","text":"<p>Creates a new directory at the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>new_directory</code> <code>NewDirectory</code> <p>An <code>input_schema.NewDirectory</code> object with the path and name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the directory was created successfully.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/files/create_directory', response_model=output_model.OutputDir, tags=['file manager'])\ndef create_directory(new_directory: input_schema.NewDirectory) -&gt; bool:\n    \"\"\"Creates a new directory at the specified path.\n\n    Args:\n        new_directory: An `input_schema.NewDirectory` object with the path and name.\n\n    Returns:\n        `True` if the directory was created successfully.\n    \"\"\"\n    result, error = create_dir(new_directory)\n    if result:\n        return True\n    else:\n        raise error\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.create_flow","title":"<code>create_flow(flow_path=None, name=None)</code>","text":"<p>Creates a new, empty flow file at the specified path and registers a session for it.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/editor/create_flow/', tags=['editor'])\ndef create_flow(flow_path: str = None, name: str = None):\n    \"\"\"Creates a new, empty flow file at the specified path and registers a session for it.\"\"\"\n    if flow_path is not None and name is None:\n        name = Path(flow_path).stem\n    elif flow_path is not None and name is not None:\n        if name not in flow_path and flow_path.endswith(\".flowfile\"):\n            raise HTTPException(422, 'The name must be part of the flow path when a full path is provided')\n        elif name in flow_path and not flow_path.endswith(\".flowfile\"):\n            flow_path = str(Path(flow_path) / (name + \".flowfile\"))\n        elif name not in flow_path and name.endswith(\".flowfile\"):\n            flow_path = str(Path(flow_path) / name)\n        elif name not in flow_path and not name.endswith(\".flowfile\"):\n            flow_path = str(Path(flow_path) / (name + \".flowfile\"))\n    if flow_path is not None:\n        flow_path_ref = Path(flow_path)\n        if not flow_path_ref.parent.exists():\n            raise HTTPException(422, 'The directory does not exist')\n    return flow_file_handler.add_flow(name=name, flow_path=flow_path)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.delete_db_connection","title":"<code>delete_db_connection(connection_name, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>","text":"<p>Deletes a stored database connection.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.delete('/db_connection_lib', tags=['db_connections'])\ndef delete_db_connection(connection_name: str,\n                         current_user=Depends(get_current_active_user),\n                         db: Session = Depends(get_db)\n                         ):\n    \"\"\"Deletes a stored database connection.\"\"\"\n    logger.info(f'Deleting database connection {connection_name}')\n    db_connection = get_database_connection(db, connection_name, current_user.id)\n    if db_connection is None:\n        raise HTTPException(404, 'Database connection not found')\n    delete_database_connection(db, connection_name, current_user.id)\n    return {\"message\": \"Database connection deleted successfully\"}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.delete_node","title":"<code>delete_node(flow_id, node_id)</code>","text":"<p>Deletes a node from the flow graph.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/editor/delete_node/', tags=['editor'])\ndef delete_node(flow_id: Optional[int], node_id: int):\n    \"\"\"Deletes a node from the flow graph.\"\"\"\n    logger.info('Deleting node')\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow.flow_settings.is_running:\n        raise HTTPException(422, 'Flow is running')\n    flow.delete_node(node_id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.delete_node_connection","title":"<code>delete_node_connection(flow_id, node_connection=None)</code>","text":"<p>Deletes a connection (edge) between two nodes.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/editor/delete_connection/', tags=['editor'])\ndef delete_node_connection(flow_id: int, node_connection: input_schema.NodeConnection = None):\n    \"\"\"Deletes a connection (edge) between two nodes.\"\"\"\n    flow_id = int(flow_id)\n    logger.info(\n        f'Deleting connection node {node_connection.output_connection.node_id} to node {node_connection.input_connection.node_id}')\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow.flow_settings.is_running:\n        raise HTTPException(422, 'Flow is running')\n    delete_connection(flow, node_connection)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_active_flow_file_sessions","title":"<code>get_active_flow_file_sessions()</code>  <code>async</code>","text":"<p>Retrieves a list of all currently active flow sessions.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/active_flowfile_sessions/', response_model=List[schemas.FlowSettings])\nasync def get_active_flow_file_sessions() -&gt; List[schemas.FlowSettings]:\n    \"\"\"Retrieves a list of all currently active flow sessions.\"\"\"\n    return [flf.flow_settings for flf in flow_file_handler.flowfile_flows]\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_current_directory_contents","title":"<code>get_current_directory_contents(file_types=None, include_hidden=False)</code>  <code>async</code>","text":"<p>Gets the contents of the file explorer's current directory.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/files/current_directory_contents/', response_model=List[FileInfo], tags=['file manager'])\nasync def get_current_directory_contents(file_types: List[str] = None, include_hidden: bool = False) -&gt; List[FileInfo]:\n    \"\"\"Gets the contents of the file explorer's current directory.\"\"\"\n    return file_explorer.list_contents(file_types=file_types, show_hidden=include_hidden)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_current_files","title":"<code>get_current_files()</code>  <code>async</code>","text":"<p>Gets the contents of the file explorer's current directory.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/files/tree/', response_model=List[FileInfo], tags=['file manager'])\nasync def get_current_files() -&gt; List[FileInfo]:\n    \"\"\"Gets the contents of the file explorer's current directory.\"\"\"\n    f = file_explorer.list_contents()\n    return f\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_current_path","title":"<code>get_current_path()</code>  <code>async</code>","text":"<p>Returns the current absolute path of the file explorer.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/files/current_path/', response_model=str, tags=['file manager'])\nasync def get_current_path() -&gt; str:\n    \"\"\"Returns the current absolute path of the file explorer.\"\"\"\n    return str(file_explorer.current_path)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_db_connections","title":"<code>get_db_connections(db=Depends(get_db), current_user=Depends(get_current_active_user))</code>","text":"<p>Retrieves all stored database connections for the current user (without passwords).</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/db_connection_lib', tags=['db_connections'],\n            response_model=List[input_schema.FullDatabaseConnectionInterface])\ndef get_db_connections(\n        db: Session = Depends(get_db),\n        current_user=Depends(get_current_active_user)) -&gt; List[input_schema.FullDatabaseConnectionInterface]:\n    \"\"\"Retrieves all stored database connections for the current user (without passwords).\"\"\"\n    return get_all_database_connections_interface(db, current_user.id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_description_node","title":"<code>get_description_node(flow_id, node_id)</code>","text":"<p>Retrieves the description text for a specific node.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/node/description', tags=['editor'])\ndef get_description_node(flow_id: int, node_id: int):\n    \"\"\"Retrieves the description text for a specific node.\"\"\"\n    try:\n        node = flow_file_handler.get_flow(flow_id).get_node(node_id)\n    except:\n        raise HTTPException(404, 'Could not find the node')\n    if node is None:\n        raise HTTPException(404, 'Could not find the node')\n    return node.setting_input.description\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_directory_contents","title":"<code>get_directory_contents(directory, file_types=None, include_hidden=False)</code>  <code>async</code>","text":"<p>Gets the contents of an arbitrary directory path.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The absolute path to the directory.</p> required <code>file_types</code> <code>List[str]</code> <p>An optional list of file extensions to filter by.</p> <code>None</code> <code>include_hidden</code> <code>bool</code> <p>If True, includes hidden files and directories.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[FileInfo]</code> <p>A list of <code>FileInfo</code> objects representing the directory's contents.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/files/directory_contents/', response_model=List[FileInfo], tags=['file manager'])\nasync def get_directory_contents(directory: str, file_types: List[str] = None,\n                                 include_hidden: bool = False) -&gt; List[FileInfo]:\n    \"\"\"Gets the contents of an arbitrary directory path.\n\n    Args:\n        directory: The absolute path to the directory.\n        file_types: An optional list of file extensions to filter by.\n        include_hidden: If True, includes hidden files and directories.\n\n    Returns:\n        A list of `FileInfo` objects representing the directory's contents.\n    \"\"\"\n    directory_explorer = SecureFileExplorer(directory, storage.user_data_directory)\n    try:\n        return directory_explorer.list_contents(show_hidden=include_hidden, file_types=file_types)\n    except Exception as e:\n        logger.error(e)\n        HTTPException(404, 'Could not access the directory')\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_downstream_node_ids","title":"<code>get_downstream_node_ids(flow_id, node_id)</code>  <code>async</code>","text":"<p>Gets a list of all node IDs that are downstream dependencies of a given node.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/node/downstream_node_ids', response_model=List[int], tags=['editor'])\nasync def get_downstream_node_ids(flow_id: int, node_id: int) -&gt; List[int]:\n    \"\"\"Gets a list of all node IDs that are downstream dependencies of a given node.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    node = flow.get_node(node_id)\n    return list(node.get_all_dependent_node_ids())\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_excel_sheet_names","title":"<code>get_excel_sheet_names(path)</code>  <code>async</code>","text":"<p>Retrieves the sheet names from an Excel file.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/api/get_xlsx_sheet_names', tags=['excel_reader'], response_model=List[str])\nasync def get_excel_sheet_names(path: str) -&gt; List[str] | None:\n    \"\"\"Retrieves the sheet names from an Excel file.\"\"\"\n    sheet_names = excel_file_manager.get_sheet_names(path)\n    if sheet_names:\n        return sheet_names\n    else:\n        raise HTTPException(404, 'File not found')\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_expression_doc","title":"<code>get_expression_doc()</code>","text":"<p>Retrieves documentation for available Polars expressions.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/editor/expression_doc', tags=['editor'], response_model=List[output_model.ExpressionsOverview])\ndef get_expression_doc() -&gt; List[output_model.ExpressionsOverview]:\n    \"\"\"Retrieves documentation for available Polars expressions.\"\"\"\n    return get_expression_overview()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_expressions","title":"<code>get_expressions()</code>","text":"<p>Retrieves a list of all available Flowfile expression names.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/editor/expressions', tags=['editor'], response_model=List[str])\ndef get_expressions() -&gt; List[str]:\n    \"\"\"Retrieves a list of all available Flowfile expression names.\"\"\"\n    return get_all_expressions()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_flow","title":"<code>get_flow(flow_id)</code>","text":"<p>Retrieves the settings for a specific flow.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/editor/flow', tags=['editor'], response_model=schemas.FlowSettings)\ndef get_flow(flow_id: int):\n    \"\"\"Retrieves the settings for a specific flow.\"\"\"\n    flow_id = int(flow_id)\n    result = get_flow_settings(flow_id)\n    return result\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_flow_frontend_data","title":"<code>get_flow_frontend_data(flow_id=1)</code>","text":"<p>Retrieves the data needed to render the flow graph in the frontend.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/flow_data', tags=['manager'])\ndef get_flow_frontend_data(flow_id: Optional[int] = 1):\n    \"\"\"Retrieves the data needed to render the flow graph in the frontend.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, 'could not find the flow')\n    return flow.get_frontend_data()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_flow_settings","title":"<code>get_flow_settings(flow_id=1)</code>","text":"<p>Retrieves the main settings for a flow.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/flow_settings', tags=['manager'], response_model=schemas.FlowSettings)\ndef get_flow_settings(flow_id: Optional[int] = 1) -&gt; schemas.FlowSettings:\n    \"\"\"Retrieves the main settings for a flow.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, 'could not find the flow')\n    return flow.flow_settings\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_generated_code","title":"<code>get_generated_code(flow_id)</code>","text":"<p>Generates and returns a Python script with Polars code representing the flow.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get(\"/editor/code_to_polars\", tags=[], response_model=str)\ndef get_generated_code(flow_id: int) -&gt; str:\n    \"\"\"Generates and returns a Python script with Polars code representing the flow.\"\"\"\n    flow_id = int(flow_id)\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, 'could not find the flow')\n    return export_flow_to_polars(flow)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_graphic_walker_input","title":"<code>get_graphic_walker_input(flow_id, node_id)</code>","text":"<p>Gets the data and configuration for the Graphic Walker data exploration tool.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/analysis_data/graphic_walker_input', tags=['analysis'], response_model=input_schema.NodeExploreData)\ndef get_graphic_walker_input(flow_id: int, node_id: int):\n    \"\"\"Gets the data and configuration for the Graphic Walker data exploration tool.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    node = flow.get_node(node_id)\n    if node.results.analysis_data_generator is None:\n        logger.error('The data is not refreshed and available for analysis')\n        raise HTTPException(422, 'The data is not refreshed and available for analysis')\n    return AnalyticsProcessor.process_graphic_walker_input(node)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_instant_function_result","title":"<code>get_instant_function_result(flow_id, node_id, func_string)</code>  <code>async</code>","text":"<p>Executes a simple, instant function on a node's data and returns the result.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/custom_functions/instant_result', tags=[])\nasync def get_instant_function_result(flow_id: int, node_id: int, func_string: str):\n    \"\"\"Executes a simple, instant function on a node's data and returns the result.\"\"\"\n    try:\n        node = flow_file_handler.get_node(flow_id, node_id)\n        result = await asyncio.to_thread(get_instant_func_results, node, func_string)\n        return result\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_list_of_saved_flows","title":"<code>get_list_of_saved_flows(path)</code>","text":"<p>Scans a directory for saved flow files (<code>.flowfile</code>).</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/files/available_flow_files', tags=['editor'], response_model=List[FileInfo])\ndef get_list_of_saved_flows(path: str):\n    \"\"\"Scans a directory for saved flow files (`.flowfile`).\"\"\"\n    try:\n        return get_files_from_directory(path, types=['flowfile'])\n    except:\n        return []\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_local_files","title":"<code>get_local_files(directory)</code>  <code>async</code>","text":"<p>Retrieves a list of files from a specified local directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The absolute path of the directory to scan.</p> required <p>Returns:</p> Type Description <code>List[FileInfo]</code> <p>A list of <code>FileInfo</code> objects for each item in the directory.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if the directory does not exist.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/files/files_in_local_directory/', response_model=List[FileInfo], tags=['file manager'])\nasync def get_local_files(directory: str) -&gt; List[FileInfo]:\n    \"\"\"Retrieves a list of files from a specified local directory.\n\n    Args:\n        directory: The absolute path of the directory to scan.\n\n    Returns:\n        A list of `FileInfo` objects for each item in the directory.\n\n    Raises:\n        HTTPException: 404 if the directory does not exist.\n    \"\"\"\n    files = get_files_from_directory(directory)\n    if files is None:\n        raise HTTPException(404, 'Directory does not exist')\n    return files\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_node","title":"<code>get_node(flow_id, node_id, get_data=False)</code>","text":"<p>Retrieves the complete state and data preview for a single node.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/node', response_model=output_model.NodeData, tags=['editor'])\ndef get_node(flow_id: int, node_id: int, get_data: bool = False):\n    \"\"\"Retrieves the complete state and data preview for a single node.\"\"\"\n    logging.info(f'Getting node {node_id} from flow {flow_id}')\n    flow = flow_file_handler.get_flow(flow_id)\n    node = flow.get_node(node_id)\n    if node is None:\n        raise HTTPException(422, 'Not found')\n    v = node.get_node_data(flow_id=flow.flow_id, include_example=get_data)\n    return v\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_node_list","title":"<code>get_node_list()</code>","text":"<p>Retrieves the list of all available node types and their templates.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/node_list', response_model=List[schemas.NodeTemplate])\ndef get_node_list() -&gt; List[schemas.NodeTemplate]:\n    \"\"\"Retrieves the list of all available node types and their templates.\"\"\"\n    return nodes_list\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_node_model","title":"<code>get_node_model(setting_name_ref)</code>","text":"<p>(Internal) Retrieves a node's Pydantic model from the input_schema module by its name.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>def get_node_model(setting_name_ref: str):\n    \"\"\"(Internal) Retrieves a node's Pydantic model from the input_schema module by its name.\"\"\"\n    logger.info(\"Getting node model for: \" + setting_name_ref)\n    for ref_name, ref in inspect.getmodule(input_schema).__dict__.items():\n        if ref_name.lower() == setting_name_ref:\n            return ref\n    logger.error(f\"Could not find node model for: {setting_name_ref}\")\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_run_status","title":"<code>get_run_status(flow_id, response)</code>","text":"<p>Retrieves the run status information for a specific flow.</p> <p>Returns a 202 Accepted status while the flow is running, and 200 OK when finished.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/flow/run_status/', tags=['editor'],\n            response_model=output_model.RunInformation)\ndef get_run_status(flow_id: int, response: Response):\n    \"\"\"Retrieves the run status information for a specific flow.\n\n    Returns a 202 Accepted status while the flow is running, and 200 OK when finished.\n    \"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if not flow:\n        raise HTTPException(status_code=404, detail=\"Flow not found\")\n    if flow.latest_run_info is None:\n        raise HTTPException(status_code=404, detail=\"No run information available\")\n    if flow.flow_settings.is_running:\n        response.status_code = status.HTTP_202_ACCEPTED\n    else:\n        response.status_code = status.HTTP_200_OK\n    return flow.get_run_info()\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_table_example","title":"<code>get_table_example(flow_id, node_id)</code>","text":"<p>Retrieves a data preview (schema and sample rows) for a node's output.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/node/data', response_model=output_model.TableExample, tags=['editor'])\ndef get_table_example(flow_id: int, node_id: int):\n    \"\"\"Retrieves a data preview (schema and sample rows) for a node's output.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    node = flow.get_node(node_id)\n    return node.get_table_example(True)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.get_vue_flow_data","title":"<code>get_vue_flow_data(flow_id)</code>","text":"<p>Retrieves the flow data formatted for the Vue-based frontend.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/flow_data/v2', tags=['manager'])\ndef get_vue_flow_data(flow_id: int) -&gt; schemas.VueFlowInput:\n    \"\"\"Retrieves the flow data formatted for the Vue-based frontend.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if flow is None:\n        raise HTTPException(404, 'could not find the flow')\n    data = flow.get_vue_flow_input()\n    return data\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.import_saved_flow","title":"<code>import_saved_flow(flow_path)</code>","text":"<p>Imports a flow from a saved <code>.flowfile</code> and registers it as a new session.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/import_flow/', tags=['editor'], response_model=int)\ndef import_saved_flow(flow_path: str) -&gt; int:\n    \"\"\"Imports a flow from a saved `.flowfile` and registers it as a new session.\"\"\"\n    flow_path = Path(flow_path)\n    if not flow_path.exists():\n        raise HTTPException(404, 'File not found')\n    return flow_file_handler.import_flow(flow_path)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.navigate_into_directory","title":"<code>navigate_into_directory(directory_name)</code>  <code>async</code>","text":"<p>Navigates the file explorer into a specified subdirectory.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/files/navigate_into/', response_model=str, tags=['file manager'])\nasync def navigate_into_directory(directory_name: str) -&gt; str:\n    \"\"\"Navigates the file explorer into a specified subdirectory.\"\"\"\n    file_explorer.navigate_into(directory_name)\n    return str(file_explorer.current_path)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.navigate_to_directory","title":"<code>navigate_to_directory(directory_name)</code>  <code>async</code>","text":"<p>Navigates the file explorer to an absolute directory path.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/files/navigate_to/', tags=['file manager'])\nasync def navigate_to_directory(directory_name: str) -&gt; str:\n    \"\"\"Navigates the file explorer to an absolute directory path.\"\"\"\n    file_explorer.navigate_to(directory_name)\n    return str(file_explorer.current_path)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.navigate_up","title":"<code>navigate_up()</code>  <code>async</code>","text":"<p>Navigates the file explorer one directory level up.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/files/navigate_up/', response_model=str, tags=['file manager'])\nasync def navigate_up() -&gt; str:\n    \"\"\"Navigates the file explorer one directory level up.\"\"\"\n    file_explorer.navigate_up()\n    return str(file_explorer.current_path)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.register_flow","title":"<code>register_flow(flow_data)</code>","text":"<p>Registers a new flow session with the application.</p> <p>Parameters:</p> Name Type Description Default <code>flow_data</code> <code>FlowSettings</code> <p>The <code>FlowSettings</code> for the new flow.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The ID of the newly registered flow.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/flow/register/', tags=['editor'])\ndef register_flow(flow_data: schemas.FlowSettings) -&gt; int:\n    \"\"\"Registers a new flow session with the application.\n\n    Args:\n        flow_data: The `FlowSettings` for the new flow.\n\n    Returns:\n        The ID of the newly registered flow.\n    \"\"\"\n    return flow_file_handler.register_flow(flow_data)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.run_flow","title":"<code>run_flow(flow_id, background_tasks)</code>  <code>async</code>","text":"<p>Executes a flow in a background task.</p> <p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>int</code> <p>The ID of the flow to execute.</p> required <code>background_tasks</code> <code>BackgroundTasks</code> <p>FastAPI's background task runner.</p> required <p>Returns:</p> Type Description <code>JSONResponse</code> <p>A JSON response indicating that the flow has started.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/flow/run/', tags=['editor'])\nasync def run_flow(flow_id: int, background_tasks: BackgroundTasks) -&gt; JSONResponse:\n    \"\"\"Executes a flow in a background task.\n\n    Args:\n        flow_id: The ID of the flow to execute.\n        background_tasks: FastAPI's background task runner.\n\n    Returns:\n        A JSON response indicating that the flow has started.\n    \"\"\"\n    logger.info('starting to run...')\n    flow = flow_file_handler.get_flow(flow_id)\n    lock = get_flow_run_lock(flow_id)\n    async with lock:\n        if flow.flow_settings.is_running:\n            raise HTTPException(422, 'Flow is already running')\n        background_tasks.add_task(flow.run_graph)\n    return JSONResponse(content={\"message\": \"Data started\", \"flow_id\": flow_id}, status_code=status.HTTP_200_OK)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.save_flow","title":"<code>save_flow(flow_id, flow_path=None)</code>","text":"<p>Saves the current state of a flow to a <code>.flowfile</code>.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.get('/save_flow', tags=['editor'])\ndef save_flow(flow_id: int, flow_path: str = None):\n    \"\"\"Saves the current state of a flow to a `.flowfile`.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    flow.save_flow(flow_path=flow_path)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.trigger_fetch_node_data","title":"<code>trigger_fetch_node_data(flow_id, node_id, background_tasks)</code>  <code>async</code>","text":"<p>Fetches and refreshes the data for a specific node.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/node/trigger_fetch_data\", tags=['editor'])\nasync def trigger_fetch_node_data(flow_id: int, node_id: int, background_tasks: BackgroundTasks):\n    \"\"\"Fetches and refreshes the data for a specific node.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    lock = get_flow_run_lock(flow_id)\n    async with lock:\n        if flow.flow_settings.is_running:\n            raise HTTPException(422, 'Flow is already running')\n        try:\n            flow.validate_if_node_can_be_fetched(node_id)\n        except Exception as e:\n            raise HTTPException(422, str(e))\n        background_tasks.add_task(flow.trigger_fetch_node, node_id)\n    return JSONResponse(content={\"message\": \"Data started\",\n                                 \"flow_id\": flow_id,\n                                 \"node_id\": node_id}, status_code=status.HTTP_200_OK)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.update_description_node","title":"<code>update_description_node(flow_id, node_id, description=Body(...))</code>","text":"<p>Updates the description text for a specific node.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/node/description/', tags=['editor'])\ndef update_description_node(flow_id: int, node_id: int, description: str = Body(...)):\n    \"\"\"Updates the description text for a specific node.\"\"\"\n    try:\n        node = flow_file_handler.get_flow(flow_id).get_node(node_id)\n    except:\n        raise HTTPException(404, 'Could not find the node')\n    node.setting_input.description = description\n    return True\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.update_flow_settings","title":"<code>update_flow_settings(flow_settings)</code>","text":"<p>Updates the main settings for a flow.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post('/flow_settings', tags=['manager'])\ndef update_flow_settings(flow_settings: schemas.FlowSettings):\n    \"\"\"Updates the main settings for a flow.\"\"\"\n    flow = flow_file_handler.get_flow(flow_settings.flow_id)\n    if flow is None:\n        raise HTTPException(404, 'could not find the flow')\n    flow.flow_settings = flow_settings\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.upload_file","title":"<code>upload_file(file=File(...))</code>  <code>async</code>","text":"<p>Uploads a file to the server's 'uploads' directory.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>UploadFile</code> <p>The file to be uploaded.</p> <code>File(...)</code> <p>Returns:</p> Type Description <code>JSONResponse</code> <p>A JSON response containing the filename and the path where it was saved.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/upload/\")\nasync def upload_file(file: UploadFile = File(...)) -&gt; JSONResponse:\n    \"\"\"Uploads a file to the server's 'uploads' directory.\n\n    Args:\n        file: The file to be uploaded.\n\n    Returns:\n        A JSON response containing the filename and the path where it was saved.\n    \"\"\"\n    file_location = f\"uploads/{file.filename}\"\n    with open(file_location, \"wb+\") as file_object:\n        file_object.write(file.file.read())\n    return JSONResponse(content={\"filename\": file.filename, \"filepath\": file_location})\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.routes.validate_db_settings","title":"<code>validate_db_settings(database_settings, current_user=Depends(get_current_active_user))</code>  <code>async</code>","text":"<p>Validates that a connection can be made to a database with the given settings.</p> Source code in <code>flowfile_core/flowfile_core/routes/routes.py</code> <pre><code>@router.post(\"/validate_db_settings\")\nasync def validate_db_settings(\n        database_settings: input_schema.DatabaseSettings,\n        current_user=Depends(get_current_active_user)\n):\n    \"\"\"Validates that a connection can be made to a database with the given settings.\"\"\"\n    # Validate the query settings\n    try:\n        sql_source = create_sql_source_from_db_settings(database_settings, user_id=current_user.id)\n        sql_source.validate()\n        return {\"message\": \"Query settings are valid\"}\n    except Exception as e:\n        raise HTTPException(status_code=422, detail=str(e))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#auth","title":"<code>auth</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.auth","title":"<code>flowfile_core.routes.auth</code>","text":""},{"location":"for-developers/python-api-reference.html#cloud_connections","title":"<code>cloud_connections</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.cloud_connections","title":"<code>flowfile_core.routes.cloud_connections</code>","text":"<p>Functions:</p> Name Description <code>create_cloud_storage_connection</code> <p>Create a new cloud storage connection.</p> <code>delete_cloud_connection_with_connection_name</code> <p>Delete a cloud connection.</p> <code>get_cloud_connections</code> <p>Get all cloud storage connections for the current user.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.cloud_connections.create_cloud_storage_connection","title":"<code>create_cloud_storage_connection(input_connection, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>","text":"<p>Create a new cloud storage connection. Parameters     input_connection: FullCloudStorageConnection schema containing connection details     current_user: User obtained from Depends(get_current_active_user)     db: Session obtained from Depends(get_db) Returns     Dict with a success message</p> Source code in <code>flowfile_core/flowfile_core/routes/cloud_connections.py</code> <pre><code>@router.post(\"/cloud_connection\", tags=['cloud_connections'])\ndef create_cloud_storage_connection(input_connection: FullCloudStorageConnection,\n                                    current_user=Depends(get_current_active_user),\n                                    db: Session = Depends(get_db)\n                                    ):\n    \"\"\"\n    Create a new cloud storage connection.\n    Parameters\n        input_connection: FullCloudStorageConnection schema containing connection details\n        current_user: User obtained from Depends(get_current_active_user)\n        db: Session obtained from Depends(get_db)\n    Returns\n        Dict with a success message\n    \"\"\"\n    logger.info(f'Create cloud connection {input_connection.connection_name}')\n    try:\n        store_cloud_connection(db, input_connection, current_user.id)\n    except ValueError:\n        raise HTTPException(422, 'Connection name already exists')\n    except Exception as e:\n        logger.error(e)\n        raise HTTPException(422, str(e))\n    return {\"message\": \"Cloud connection created successfully\"}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.cloud_connections.delete_cloud_connection_with_connection_name","title":"<code>delete_cloud_connection_with_connection_name(connection_name, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>","text":"<p>Delete a cloud connection.</p> Source code in <code>flowfile_core/flowfile_core/routes/cloud_connections.py</code> <pre><code>@router.delete('/cloud_connection', tags=['cloud_connections'])\ndef delete_cloud_connection_with_connection_name(connection_name: str,\n                                                 current_user=Depends(get_current_active_user),\n                                                 db: Session = Depends(get_db)\n                                                 ):\n    \"\"\"\n    Delete a cloud connection.\n    \"\"\"\n    logger.info(f'Deleting cloud connection {connection_name}')\n    cloud_storage_connection = get_cloud_connection_schema(db, connection_name, current_user.id)\n    if cloud_storage_connection is None:\n        raise HTTPException(404, 'Cloud connection connection not found')\n    delete_cloud_connection(db, connection_name, current_user.id)\n    return {\"message\": \"Cloud connection deleted successfully\"}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.cloud_connections.get_cloud_connections","title":"<code>get_cloud_connections(db=Depends(get_db), current_user=Depends(get_current_active_user))</code>","text":"<p>Get all cloud storage connections for the current user. Parameters     db: Session obtained from Depends(get_db)     current_user: User obtained from Depends(get_current_active_user)</p> <p>Returns     List[FullCloudStorageConnectionInterface]</p> Source code in <code>flowfile_core/flowfile_core/routes/cloud_connections.py</code> <pre><code>@router.get('/cloud_connections', tags=['cloud_connection'],\n            response_model=List[FullCloudStorageConnectionInterface])\ndef get_cloud_connections(\n        db: Session = Depends(get_db),\n        current_user=Depends(get_current_active_user)) -&gt; List[FullCloudStorageConnectionInterface]:\n    \"\"\"\n    Get all cloud storage connections for the current user.\n    Parameters\n        db: Session obtained from Depends(get_db)\n        current_user: User obtained from Depends(get_current_active_user)\n\n    Returns\n        List[FullCloudStorageConnectionInterface]\n    \"\"\"\n    return get_all_cloud_connections_interface(db, current_user.id)\n</code></pre>"},{"location":"for-developers/python-api-reference.html#logs","title":"<code>logs</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.logs","title":"<code>flowfile_core.routes.logs</code>","text":"<p>Functions:</p> Name Description <code>add_log</code> <p>Adds a log message to the log file for a given flow_id.</p> <code>add_raw_log</code> <p>Adds a log message to the log file for a given flow_id.</p> <code>format_sse_message</code> <p>Format the data as a proper SSE message</p> <code>stream_logs</code> <p>Streams logs for a given flow_id using Server-Sent Events.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.logs.add_log","title":"<code>add_log(flow_id, log_message)</code>  <code>async</code>","text":"<p>Adds a log message to the log file for a given flow_id.</p> Source code in <code>flowfile_core/flowfile_core/routes/logs.py</code> <pre><code>@router.post(\"/logs/{flow_id}\", tags=['flow_logging'])\nasync def add_log(flow_id: int, log_message: str):\n    \"\"\"Adds a log message to the log file for a given flow_id.\"\"\"\n    flow = flow_file_handler.get_flow(flow_id)\n    if not flow:\n        raise HTTPException(status_code=404, detail=\"Flow not found\")\n    flow.flow_logger.info(log_message)\n    return {\"message\": \"Log added successfully\"}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.logs.add_raw_log","title":"<code>add_raw_log(raw_log_input)</code>  <code>async</code>","text":"<p>Adds a log message to the log file for a given flow_id.</p> Source code in <code>flowfile_core/flowfile_core/routes/logs.py</code> <pre><code>@router.post(\"/raw_logs\", tags=['flow_logging'])\nasync def add_raw_log(raw_log_input: schemas.RawLogInput):\n    \"\"\"Adds a log message to the log file for a given flow_id.\"\"\"\n    logger.info('Adding raw logs')\n    flow = flow_file_handler.get_flow(raw_log_input.flowfile_flow_id)\n    if not flow:\n        raise HTTPException(status_code=404, detail=\"Flow not found\")\n    flow.flow_logger.get_log_filepath()\n    flow_logger = flow.flow_logger\n    flow_logger.get_log_filepath()\n    if raw_log_input.log_type == 'INFO':\n        flow_logger.info(raw_log_input.log_message,\n                         extra=raw_log_input.extra)\n    elif raw_log_input.log_type == 'ERROR':\n        flow_logger.error(raw_log_input.log_message,\n                          extra=raw_log_input.extra)\n    return {\"message\": \"Log added successfully\"}\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.logs.format_sse_message","title":"<code>format_sse_message(data)</code>  <code>async</code>","text":"<p>Format the data as a proper SSE message</p> Source code in <code>flowfile_core/flowfile_core/routes/logs.py</code> <pre><code>async def format_sse_message(data: str) -&gt; str:\n    \"\"\"Format the data as a proper SSE message\"\"\"\n    return f\"data: {json.dumps(data)}\\n\\n\"\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.logs.stream_logs","title":"<code>stream_logs(flow_id, idle_timeout=300, current_user=Depends(get_current_user_from_query))</code>  <code>async</code>","text":"<p>Streams logs for a given flow_id using Server-Sent Events. Requires authentication via token in query parameter. The connection will close gracefully if the server shuts down.</p> Source code in <code>flowfile_core/flowfile_core/routes/logs.py</code> <pre><code>@router.get(\"/logs/{flow_id}\", tags=['flow_logging'])\nasync def stream_logs(\n    flow_id: int,\n    idle_timeout: int = 300,\n    current_user=Depends(get_current_user_from_query)\n):\n    \"\"\"\n    Streams logs for a given flow_id using Server-Sent Events.\n    Requires authentication via token in query parameter.\n    The connection will close gracefully if the server shuts down.\n    \"\"\"\n    logger.info(f\"Starting log stream for flow_id: {flow_id} by user: {current_user.username}\")\n    await asyncio.sleep(.3)\n    flow = flow_file_handler.get_flow(flow_id)\n    logger.info('Streaming logs')\n    if not flow:\n        raise HTTPException(status_code=404, detail=\"Flow not found\")\n\n    log_file_path = flow.flow_logger.get_log_filepath()\n    if not Path(log_file_path).exists():\n        raise HTTPException(status_code=404, detail=\"Log file not found\")\n\n    class RunningState:\n        def __init__(self):\n            self.has_started = False\n\n        def is_running(self):\n            if flow.flow_settings.is_running:\n                self.has_started = True\n            return flow.flow_settings.is_running or not self.has_started\n\n    running_state = RunningState()\n\n    return StreamingResponse(\n        stream_log_file(log_file_path, running_state.is_running, idle_timeout),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"Content-Type\": \"text/event-stream\",\n        }\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#public","title":"<code>public</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.public","title":"<code>flowfile_core.routes.public</code>","text":"<p>Functions:</p> Name Description <code>docs_redirect</code> <p>Redirects to the documentation page.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.public.docs_redirect","title":"<code>docs_redirect()</code>  <code>async</code>","text":"<p>Redirects to the documentation page.</p> Source code in <code>flowfile_core/flowfile_core/routes/public.py</code> <pre><code>@router.get(\"/\", tags=['admin'])\nasync def docs_redirect():\n    \"\"\" Redirects to the documentation page.\"\"\"\n    return RedirectResponse(url='/docs')\n</code></pre>"},{"location":"for-developers/python-api-reference.html#secrets","title":"<code>secrets</code>","text":""},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.secrets","title":"<code>flowfile_core.routes.secrets</code>","text":"<p>Manages CRUD (Create, Read, Update, Delete) operations for secrets.</p> <p>This router provides secure endpoints for creating, retrieving, and deleting sensitive credentials for the authenticated user. Secrets are encrypted before being stored and are associated with the user's ID.</p> <p>Functions:</p> Name Description <code>create_secret</code> <p>Creates a new secret for the authenticated user.</p> <code>delete_secret</code> <p>Deletes a secret by name for the authenticated user.</p> <code>get_secret</code> <p>Retrieves a specific secret by name for the authenticated user.</p> <code>get_secrets</code> <p>Retrieves all secret names for the currently authenticated user.</p>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.secrets.create_secret","title":"<code>create_secret(secret, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>  <code>async</code>","text":"<p>Creates a new secret for the authenticated user.</p> <p>The secret value is encrypted before being stored in the database. A secret name must be unique for a given user.</p> <p>Parameters:</p> Name Type Description Default <code>secret</code> <code>SecretInput</code> <p>A <code>SecretInput</code> object containing the name and plaintext value of the secret.</p> required <code>current_user</code> <p>The authenticated user object, injected by FastAPI.</p> <code>Depends(get_current_active_user)</code> <code>db</code> <code>Session</code> <p>The database session, injected by FastAPI.</p> <code>Depends(get_db)</code> <p>Raises:</p> Type Description <code>HTTPException</code> <p>400 if a secret with the same name already exists for the user.</p> <p>Returns:</p> Type Description <code>Secret</code> <p>A <code>Secret</code> object containing the name and the encrypted value.</p> Source code in <code>flowfile_core/flowfile_core/routes/secrets.py</code> <pre><code>@router.post(\"/secrets\", response_model=Secret)\nasync def create_secret(secret: SecretInput, current_user=Depends(get_current_active_user),\n                        db: Session = Depends(get_db)) -&gt; Secret:\n    \"\"\"Creates a new secret for the authenticated user.\n\n    The secret value is encrypted before being stored in the database. A secret\n    name must be unique for a given user.\n\n    Args:\n        secret: A `SecretInput` object containing the name and plaintext value of the secret.\n        current_user: The authenticated user object, injected by FastAPI.\n        db: The database session, injected by FastAPI.\n\n    Raises:\n        HTTPException: 400 if a secret with the same name already exists for the user.\n\n    Returns:\n        A `Secret` object containing the name and the *encrypted* value.\n    \"\"\"\n    # Get user ID\n    user_id = 1 if os.environ.get(\"FLOWFILE_MODE\") == \"electron\" else current_user.id\n\n    existing_secret = db.query(db_models.Secret).filter(\n        db_models.Secret.user_id == user_id,\n        db_models.Secret.name == secret.name\n    ).first()\n\n    if existing_secret:\n        raise HTTPException(status_code=400, detail=\"Secret with this name already exists\")\n\n    # The store_secret function handles encryption and DB storage\n    stored_secret = store_secret(db, secret, user_id)\n    return Secret(name=stored_secret.name, value=stored_secret.encrypted_value, user_id=str(user_id))\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.secrets.delete_secret","title":"<code>delete_secret(secret_name, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>  <code>async</code>","text":"<p>Deletes a secret by name for the authenticated user.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>The name of the secret to delete.</p> required <code>current_user</code> <p>The authenticated user object, injected by FastAPI.</p> <code>Depends(get_current_active_user)</code> <code>db</code> <code>Session</code> <p>The database session, injected by FastAPI.</p> <code>Depends(get_db)</code> <p>Returns:</p> Type Description <code>None</code> <p>An empty response with a 204 No Content status code upon success.</p> Source code in <code>flowfile_core/flowfile_core/routes/secrets.py</code> <pre><code>@router.delete(\"/secrets/{secret_name}\", status_code=204)\nasync def delete_secret(secret_name: str, current_user=Depends(get_current_active_user),\n                        db: Session = Depends(get_db)) -&gt; None:\n    \"\"\"Deletes a secret by name for the authenticated user.\n\n    Args:\n        secret_name: The name of the secret to delete.\n        current_user: The authenticated user object, injected by FastAPI.\n        db: The database session, injected by FastAPI.\n\n    Returns:\n        An empty response with a 204 No Content status code upon success.\n    \"\"\"\n    # Get user ID\n    user_id = 1 if os.environ.get(\"FLOWFILE_MODE\") == \"electron\" else current_user.id\n    delete_secret_action(db, secret_name, user_id)\n    return None\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.secrets.get_secret","title":"<code>get_secret(secret_name, current_user=Depends(get_current_active_user), db=Depends(get_db))</code>  <code>async</code>","text":"<p>Retrieves a specific secret by name for the authenticated user.</p> <p>Note: This endpoint returns the secret name and metadata but does not expose the decrypted secret value.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>The name of the secret to retrieve.</p> required <code>current_user</code> <p>The authenticated user object, injected by FastAPI.</p> <code>Depends(get_current_active_user)</code> <code>db</code> <code>Session</code> <p>The database session, injected by FastAPI.</p> <code>Depends(get_db)</code> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if the secret is not found.</p> <p>Returns:</p> Type Description <code>Secret</code> <p>A <code>Secret</code> object containing the name and encrypted value.</p> Source code in <code>flowfile_core/flowfile_core/routes/secrets.py</code> <pre><code>@router.get(\"/secrets/{secret_name}\", response_model=Secret)\nasync def get_secret(secret_name: str,\n                     current_user=Depends(get_current_active_user), db: Session = Depends(get_db)) -&gt; Secret:\n    \"\"\"Retrieves a specific secret by name for the authenticated user.\n\n    Note: This endpoint returns the secret name and metadata but does not\n    expose the decrypted secret value.\n\n    Args:\n        secret_name: The name of the secret to retrieve.\n        current_user: The authenticated user object, injected by FastAPI.\n        db: The database session, injected by FastAPI.\n\n    Raises:\n        HTTPException: 404 if the secret is not found.\n\n    Returns:\n        A `Secret` object containing the name and encrypted value.\n    \"\"\"\n    # Get user ID\n    user_id = 1 if os.environ.get(\"FLOWFILE_MODE\") == \"electron\" else current_user.id\n\n    # Get secret from database\n    db_secret = db.query(db_models.Secret).filter(\n        db_models.Secret.user_id == user_id,\n        db_models.Secret.name == secret_name\n    ).first()\n\n    if not db_secret:\n        raise HTTPException(status_code=404, detail=\"Secret not found\")\n\n    return Secret(\n        name=db_secret.name,\n        value=db_secret.encrypted_value,\n        user_id=str(db_secret.user_id)\n    )\n</code></pre>"},{"location":"for-developers/python-api-reference.html#flowfile_core.routes.secrets.get_secrets","title":"<code>get_secrets(current_user=Depends(get_current_active_user), db=Depends(get_db))</code>  <code>async</code>","text":"<p>Retrieves all secret names for the currently authenticated user.</p> <p>Note: This endpoint returns the secret names and metadata but does not expose the decrypted secret values.</p> <p>Parameters:</p> Name Type Description Default <code>current_user</code> <p>The authenticated user object, injected by FastAPI.</p> <code>Depends(get_current_active_user)</code> <code>db</code> <code>Session</code> <p>The database session, injected by FastAPI.</p> <code>Depends(get_db)</code> <p>Returns:</p> Type Description <p>A list of <code>Secret</code> objects, each containing the name and encrypted value.</p> Source code in <code>flowfile_core/flowfile_core/routes/secrets.py</code> <pre><code>@router.get(\"/secrets\", response_model=List[Secret])\nasync def get_secrets(current_user=Depends(get_current_active_user), db: Session = Depends(get_db)):\n    \"\"\"Retrieves all secret names for the currently authenticated user.\n\n    Note: This endpoint returns the secret names and metadata but does not\n    expose the decrypted secret values.\n\n    Args:\n        current_user: The authenticated user object, injected by FastAPI.\n        db: The database session, injected by FastAPI.\n\n    Returns:\n        A list of `Secret` objects, each containing the name and encrypted value.\n    \"\"\"\n    user_id = current_user.id\n\n    # Get secrets from database\n    db_secrets = db.query(db_models.Secret).filter(db_models.Secret.user_id == user_id).all()\n\n    # Prepare response model (without decrypting)\n    secrets = []\n    for db_secret in db_secrets:\n        secrets.append(Secret(\n            name=db_secret.name,\n            value=db_secret.encrypted_value,\n            user_id=str(db_secret.user_id)\n        ))\n\n    return secrets\n</code></pre>"},{"location":"users/index.html","title":"Flowfile User Guides","text":"<p>Welcome to Flowfile! Whether you prefer visual drag-and-drop or writing code, we've got you covered.</p>"},{"location":"users/index.html#choose-your-path","title":"Choose Your Path","text":""},{"location":"users/index.html#visual-editor","title":"\ud83c\udfa8 Visual Editor","text":"<p>Perfect for analysts and business users who want to build data pipelines visually.</p> <p>You'll learn:</p> <ul> <li>Drag and drop nodes to build flows</li> <li>Configure transformations with forms</li> <li>Connect to databases and cloud storage</li> <li>Export your flows as Python code</li> </ul> <p>Get Started with Visual Editor \u2192</p>"},{"location":"users/index.html#python-api","title":"\ud83d\udc0d Python API","text":"<p>Perfect for developers and data scientists who prefer code.</p> <p>You'll learn:</p> <ul> <li>Build pipelines with Polars-compatible API</li> <li>Seamlessly integrate with existing code</li> <li>Visualize your code as flow graphs</li> <li>Use advanced features and optimizations</li> </ul> <p>Get Started with Python \u2192</p>"},{"location":"users/index.html#the-best-of-both-worlds","title":"The Best of Both Worlds","text":"<p>The beauty of Flowfile is that you don't have to choose. You can:</p> <ul> <li>Write code and visualize it instantly with <code>open_graph_in_editor()</code></li> <li>Build visually and export as Python code</li> <li>Switch between visual and code at any time</li> <li>Collaborate across technical and non-technical teams</li> </ul>"},{"location":"users/index.html#quick-examples","title":"Quick Examples","text":""},{"location":"users/index.html#visual-approach","title":"Visual Approach","text":"<ol> <li>Drag a \"Read Data\" node onto canvas</li> <li>Add a \"Filter\" node and connect them</li> <li>Configure filter conditions in the form</li> <li>Run and see results instantly</li> </ol>"},{"location":"users/index.html#code-approach","title":"Code Approach","text":"<pre><code>import flowfile as ff\n\ndf = ff.read_csv(\"data.csv\")\nresult = df.filter(ff.col(\"amount\") &gt; 100)\nff.open_graph_in_editor(result.flow_graph)  # See it visually!\n</code></pre>"},{"location":"users/index.html#where-to-start","title":"Where to Start?","text":"<ul> <li>New to Flowfile? Start with our Quick Start Guide</li> <li>Coming from Excel/Tableau? Try the Visual Editor</li> <li>Know Python/Pandas/Polars? Jump into the Python API</li> <li>Want to see real examples? Check out our tutorials in either section</li> </ul> <p>Remember: Every visual flow can become code, and every code pipeline can be visualized. Choose what feels natural and switch whenever you want!</p>"},{"location":"users/python-api/index.html","title":"Python API","text":"<p>Build data pipelines programmatically with Flowfile's Polars-compatible API.</p> <p>If You Know Polars, You Know Flowfile</p> <p>Our API is designed to be a seamless extension of Polars. The majority of the methods are identical, so you can leverage your existing knowledge to be productive from day one. The main additions are features that connect your code to the broader Flowfile ecosystem, like cloud integrations and UI visualization.</p>"},{"location":"users/python-api/index.html#who-this-is-for","title":"Who This Is For","text":"<ul> <li>Python developers who prefer code over drag-and-drop</li> <li>Data scientists familiar with Polars or Pandas</li> <li>Engineers building automated data pipelines</li> <li>Anyone who needs version control and programmatic pipeline generation</li> </ul>"},{"location":"users/python-api/index.html#quick-example","title":"Quick Example","text":"<pre><code>import flowfile as ff\n\ndf = ff.read_csv(\"sales.csv\")\nresult = df.filter(ff.col(\"amount\") &gt; 100).group_by(\"region\").agg(\n    ff.col(\"amount\").sum()\n)\n\n# Visualize your pipeline\nff.open_graph_in_editor(result.flow_graph)\n</code></pre>"},{"location":"users/python-api/index.html#documentation","title":"Documentation","text":""},{"location":"users/python-api/index.html#quick-start","title":"Quick Start","text":"<p>Get up and running in 5 minutes with your first pipeline.</p>"},{"location":"users/python-api/index.html#core-concepts","title":"Core Concepts","text":"<ul> <li>FlowFrame and FlowGraph - Fundamental building blocks</li> <li>Formula Syntax - Flowfile's Excel-like expressions</li> </ul>"},{"location":"users/python-api/index.html#api-reference","title":"API Reference","text":"<ul> <li>Reading Data</li> <li>Writing Data</li> <li>Data Types</li> <li>DataFrame Operations</li> <li>Aggregations</li> <li>Joins</li> <li>Cloud Storage</li> <li>Visual UI Integration</li> </ul>"},{"location":"users/python-api/index.html#tutorials","title":"Tutorials","text":"<ul> <li>Building Flows with Code</li> </ul>"},{"location":"users/python-api/index.html#for-contributors","title":"For Contributors","text":"<p>Want to understand how Flowfile works internally or contribute to the project? See the Developer Documentation for architecture details and internal API reference.</p> <p>Prefer visual workflows? Check out the Visual Editor Guide.</p>"},{"location":"users/python-api/quickstart.html","title":"Python API Quick Start","text":"<p>Get up and running with Flowfile's Python API in 5 minutes.</p>"},{"location":"users/python-api/quickstart.html#installation","title":"Installation","text":"<pre><code>pip install flowfile\n</code></pre>"},{"location":"users/python-api/quickstart.html#your-first-pipeline","title":"Your First Pipeline","text":"<pre><code>import flowfile as ff\n\n# Load data\ndf = ff.read_csv(\"sales.csv\", description=\"Load sales data\")\n\n# Transform\nresult = (\n    df\n    .filter(ff.col(\"amount\") &gt; 100, description=\"Filter large sales\")\n    .with_columns([\n        (ff.col(\"amount\") * 1.1).alias(\"amount_with_tax\")\n    ], description=\"Add tax calculation\")\n    .group_by(\"region\")\n    .agg([\n        ff.col(\"amount\").sum().alias(\"total_sales\"),\n        ff.col(\"amount\").mean().alias(\"avg_sale\")\n    ])\n)\n\n# Get results as Polars DataFrame\ndata = result.collect()\nprint(data)\n\n# Visualize in the UI\nff.open_graph_in_editor(result.flow_graph)\n</code></pre>"},{"location":"users/python-api/quickstart.html#key-concepts","title":"Key Concepts","text":""},{"location":"users/python-api/quickstart.html#flowframe","title":"FlowFrame","text":"<p>Your data container - like a Polars LazyFrame but tracks all operations:</p> <pre><code># Create from various sources\ndf = ff.FlowFrame({\"col1\": [1, 2, 3]})  # From dict\ndf = ff.read_csv(\"file.csv\")            # From CSV\ndf = ff.read_parquet(\"file.parquet\")    # From Parquet\n</code></pre>"},{"location":"users/python-api/quickstart.html#always-lazy","title":"Always Lazy","text":"<p>Operations don't execute until you call <code>.collect()</code>:</p> <pre><code># These operations just build the plan\ndf = ff.read_csv(\"huge_file.csv\")\ndf = df.filter(ff.col(\"status\") == \"active\")\ndf = df.select([\"id\", \"name\", \"amount\"])\n\n# Now it executes everything efficiently\nresult = df.collect()\n</code></pre>"},{"location":"users/python-api/quickstart.html#descriptions","title":"Descriptions","text":"<p>Document your pipeline as you build:</p> <pre><code>df = (\n    ff.read_csv(\"input.csv\", description=\"Raw customer data\")\n    .filter(ff.col(\"active\") == True, description=\"Keep active only\")\n    .drop_duplicates(description=\"Remove duplicates\")\n)\n</code></pre>"},{"location":"users/python-api/quickstart.html#common-operations","title":"Common Operations","text":""},{"location":"users/python-api/quickstart.html#filtering","title":"Filtering","text":"<pre><code># Polars style\ndf.filter(ff.col(\"age\") &gt; 21)\n\n# Flowfile formula style\ndf.filter(flowfile_formula=\"[age] &gt; 21 AND [status] = 'active'\")\n</code></pre>"},{"location":"users/python-api/quickstart.html#adding-columns","title":"Adding Columns","text":"<pre><code># Standard way\ndf.with_columns([\n    (ff.col(\"price\") * ff.col(\"quantity\")).alias(\"total\")\n])\n\n# Formula syntax\ndf.with_columns(\n    flowfile_formulas=[\"[price] * [quantity]\"],\n    output_column_names=[\"total\"]\n)\n</code></pre>"},{"location":"users/python-api/quickstart.html#grouping-aggregation","title":"Grouping &amp; Aggregation","text":"<pre><code>df.group_by(\"category\").agg([\n    ff.col(\"sales\").sum().alias(\"total_sales\"),\n    ff.col(\"sales\").mean().alias(\"avg_sales\"),\n    ff.col(\"id\").count().alias(\"count\")\n])\n</code></pre>"},{"location":"users/python-api/quickstart.html#joining","title":"Joining","text":"<pre><code>customers = ff.read_csv(\"customers.csv\")\norders = ff.read_csv(\"orders.csv\")\n\nresult = customers.join(\n    orders,\n    left_on=\"customer_id\",\n    right_on=\"cust_id\",\n    how=\"left\"\n)\n</code></pre>"},{"location":"users/python-api/quickstart.html#cloud-storage","title":"Cloud Storage","text":"<pre><code>from pydantic import SecretStr\n\n# Set up S3 connection\nff.create_cloud_storage_connection_if_not_exists(\n    ff.FullCloudStorageConnection(\n        connection_name=\"my-s3\",\n        storage_type=\"s3\",\n        auth_method=\"access_key\",\n        aws_region=\"us-east-1\",\n        aws_access_key_id=\"your-key\",\n        aws_secret_access_key=SecretStr(\"your-secret\")\n    )\n)\n\n# Read from S3\ndf = ff.scan_parquet_from_cloud_storage(\n    \"s3://bucket/data.parquet\",\n    connection_name=\"my-s3\"\n)\n\n# Write to S3\ndf.write_parquet_to_cloud_storage(\n    \"s3://bucket/output.parquet\",\n    connection_name=\"my-s3\"\n)\n</code></pre>"},{"location":"users/python-api/quickstart.html#visual-integration","title":"Visual Integration","text":""},{"location":"users/python-api/quickstart.html#open-in-editor","title":"Open in Editor","text":"<pre><code># Build pipeline in code\npipeline = ff.read_csv(\"data.csv\").filter(ff.col(\"value\") &gt; 100)\n\n# Open in visual editor\nff.open_graph_in_editor(pipeline.flow_graph)\n</code></pre>"},{"location":"users/python-api/quickstart.html#start-web-ui","title":"Start Web UI","text":"<pre><code># Launch the web interface\nff.start_web_ui()  # Opens browser automatically\n</code></pre>"},{"location":"users/python-api/quickstart.html#complete-example","title":"Complete Example","text":"<pre><code>import flowfile as ff\n\n# Build a complete ETL pipeline\npipeline = (\n    ff.read_csv(\"raw_sales.csv\", description=\"Load raw sales\")\n    .filter(ff.col(\"amount\") &gt; 0, description=\"Remove invalid\")\n    .with_columns([\n        ff.col(\"date\").str.strptime(ff.Date, \"%Y-%m-%d\"),\n        (ff.col(\"amount\") * ff.col(\"quantity\")).alias(\"total\")\n    ], description=\"Parse dates and calculate totals\")\n    .group_by([ff.col(\"date\").dt.year().alias(\"year\"), \"product\"])\n    .agg([\n        ff.col(\"total\").sum().alias(\"revenue\"),\n        ff.col(\"quantity\").sum().alias(\"units_sold\")\n    ])\n    .sort(\"revenue\", descending=True)\n)\n\n# Execute and get results\nresults = pipeline.collect()\nprint(results)\n\n# Visualize the pipeline\nff.open_graph_in_editor(pipeline.flow_graph)\n\n# Save results\npipeline.write_parquet(\"yearly_sales.parquet\")\n</code></pre>"},{"location":"users/python-api/quickstart.html#next-steps","title":"Next Steps","text":"<ul> <li>\ud83d\udcd6 Core Concepts - Understand FlowFrame and FlowGraph</li> <li>\ud83d\udcda API Reference - Detailed documentation</li> <li>\ud83c\udfaf Tutorials - Real-world examples</li> <li>\ud83d\udd04 Visual Integration - Working with the UI</li> </ul>"},{"location":"users/python-api/quickstart.html#tips","title":"Tips","text":"<ol> <li>Use descriptions - They appear in the visual editor</li> <li>Think lazy - Build your entire pipeline before collecting</li> <li>Leverage formulas - Use <code>[column]</code> syntax for simpler expressions</li> <li>Visualize often - <code>open_graph_in_editor()</code> helps debug</li> <li>Check schemas - Use <code>df.schema</code> to see structure without running</li> </ol> <p>Ready for more? Check out the full API reference or learn about core concepts. Or want to see another example? Checkout the quickstart guide!</p>"},{"location":"users/python-api/concepts/index.html","title":"Core Concepts","text":"<p>Understanding the key concepts behind Flowfile's Python API will help you build better pipelines.</p>"},{"location":"users/python-api/concepts/index.html#available-guides","title":"Available Guides","text":""},{"location":"users/python-api/concepts/index.html#flowframe-and-flowgraph","title":"FlowFrame and FlowGraph","text":"<p>The fundamental building blocks of Flowfile pipelines.</p> <p>You'll learn: - What FlowFrame is and how it differs from DataFrames - How FlowGraph tracks your operations - Why everything is lazy by default - How visual and code representations connect</p> <p>Key takeaways: - FlowFrame = Your data + its transformation history - FlowGraph = The complete pipeline blueprint - Every operation creates a node in the graph</p>"},{"location":"users/python-api/concepts/index.html#formula-syntax","title":"Formula Syntax","text":"<p>Flowfile's Excel-like formula syntax for expressions.</p> <p>You'll learn: - When to use <code>[column]</code> vs <code>ff.col(\"column\")</code> - Supported operations and functions - How formulas translate to Polars - Best practices for each syntax</p> <p>Key takeaways: - Formulas make simple operations more readable - Great for users coming from Excel/Tableau - Both syntaxes can be mixed in the same pipeline</p>"},{"location":"users/python-api/concepts/index.html#quick-overview","title":"Quick Overview","text":""},{"location":"users/python-api/concepts/index.html#flowframe-vs-dataframe","title":"FlowFrame vs DataFrame","text":"DataFrame (Pandas/Polars) FlowFrame (Flowfile) Holds data in memory Always lazy (data not loaded) Operations execute immediately Operations build a plan No operation history Full operation history in graph Can't visualize workflow Can open in visual editor"},{"location":"users/python-api/concepts/index.html#the-lazy-advantage","title":"The Lazy Advantage","text":"<pre><code># This doesn't load the 10GB file!\ndf = ff.read_csv(\"huge_file.csv\")\n\n# Still no data loaded - just building the plan\ndf = df.filter(ff.col(\"country\") == \"USA\")\ndf = df.select([\"id\", \"amount\"])\n\n# NOW it loads only what's needed\nresult = df.collect()  # Might only read 100MB!\n</code></pre>"},{"location":"users/python-api/concepts/index.html#visual-integration","title":"Visual Integration","text":"<p>Every FlowFrame knows its history:</p> <pre><code># Build a complex pipeline\npipeline = (\n    ff.read_csv(\"input.csv\")\n    .filter(ff.col(\"active\") == True)\n    .group_by(\"category\")\n    .agg(ff.col(\"revenue\").sum())\n)\n\n# See the entire pipeline visually\nff.open_graph_in_editor(pipeline.flow_graph)\n\n# The graph shows all 4 operations as connected nodes\n</code></pre>"},{"location":"users/python-api/concepts/index.html#why-these-concepts-matter","title":"Why These Concepts Matter","text":"<p>Understanding these concepts helps you:</p> <ol> <li>Write efficient code - Leverage lazy evaluation</li> <li>Debug effectively - Visualize your pipeline</li> <li>Collaborate better - Share visual representations</li> <li>Optimize performance - Understand what executes when</li> </ol>"},{"location":"users/python-api/concepts/index.html#learn-more","title":"Learn More","text":"<ul> <li>Deep dive: Read the full FlowFrame and FlowGraph guide</li> <li>Expressions: Master the Formula Syntax</li> <li>Practice: Try the tutorials</li> </ul> <p>These concepts are the foundation of Flowfile. Understanding them will make everything else click!</p>"},{"location":"users/python-api/concepts/design-concepts.html","title":"FlowFrame and FlowGraph Design Concepts","text":"<p>Understanding how FlowFrame and FlowGraph work together is key to mastering Flowfile. This guide explains the core design principles that make Flowfile both powerful and intuitive.</p> <p>Related Reading</p> <ul> <li>Practical Implementation: See these concepts in action in our Code to Flow guide</li> <li>Architecture Overview: Learn about the system design in Technical Architecture</li> <li>Visual Building: Compare with Building Flows visually</li> </ul>"},{"location":"users/python-api/concepts/design-concepts.html#flowframe-always-lazy-always-connected","title":"FlowFrame: Always Lazy, Always Connected","text":""},{"location":"users/python-api/concepts/design-concepts.html#what-is-flowframe","title":"What is FlowFrame?","text":"<p>FlowFrame is Flowfile's version of a Polars DataFrame with a crucial difference: it's always lazy and always connected to a graph.</p> <pre><code>import flowfile as ff\n\n# This creates a FlowFrame, not a regular DataFrame\ndf = ff.FlowFrame({\n    \"id\": [1, 2, 3, 4, 5],\n    \"amount\": [100, 250, 80, 300, 150],\n    \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\"]\n})\nprint(type(df))  # &lt;class 'FlowFrame'&gt;\nprint(type(df.data))  # &lt;class 'polars.LazyFrame'&gt;\n</code></pre>"},{"location":"users/python-api/concepts/design-concepts.html#key-properties-of-flowframe","title":"Key Properties of FlowFrame","text":""},{"location":"users/python-api/concepts/design-concepts.html#1-always-lazy-evaluation","title":"1. Always Lazy Evaluation","text":"<p>A <code>FlowFrame</code> never loads your actual data into memory until you explicitly call <code>.collect()</code>. This means you can build complex transformations on massive datasets without consuming memory:</p> <pre><code># None of this processes any data yet\ndf = (\n    ff.FlowFrame({\n        \"id\": [1, 2, 3, 4, 5],\n        \"amount\": [500, 1200, 800, 1500, 900], \n        \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\"]\n    })                                # Creates manual input node\n    .filter(ff.col(\"amount\") &gt; 1000)  # No filtering happens yet\n    .group_by(\"category\")             # No grouping happens yet\n    .agg(ff.col(\"amount\").sum())     # No aggregation happens yet\n)\n\n# Only now does the data get processed\nresult = df.collect()  # Everything executes at once, optimized\n</code></pre> <p>Performance Benefits</p> <p>This lazy evaluation is powered by Polars and explained in detail in our Technical Architecture guide. </p>"},{"location":"users/python-api/concepts/design-concepts.html#2-connected-to-a-dag-directed-acyclic-graph","title":"2. Connected to a DAG (Directed Acyclic Graph)","text":"<p>Every FlowFrame has a reference to a FlowGraph that tracks every operation as a node:</p> <pre><code>df = ff.FlowFrame({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"active\": [True, False, True]\n})\nprint(df.flow_graph)  # Shows the graph this FlowFrame belongs to\nprint(df.node_id)     # Shows which node in the graph this FlowFrame represents\n</code></pre> <p>For a deeper understanding of how this DAG works internally, see FlowGraph in the Developers Guide).</p>"},{"location":"users/python-api/concepts/design-concepts.html#3-linear-operation-tracking","title":"3. Linear Operation Tracking","text":"<p>Each operation creates a new node in the graph, even if you repeat the same operation:</p> <pre><code>df = ff.FlowFrame({\n    \"id\": [1, 2, 3, 4],\n    \"amount\": [50, 150, 75, 200],\n    \"status\": [\"active\", \"inactive\", \"active\", \"active\"]\n})\nprint(f\"Initial graph has {len(df.flow_graph.nodes)} nodes\")\n\n# First filter - creates node 1\ndf1 = df.filter(ff.col(\"amount\") &gt; 100)\nprint(f\"After first filter: {len(df1.flow_graph.nodes)} nodes\")\n\n# Second identical filter - creates node 2 (not reused!)\ndf2 = df.filter(ff.col(\"amount\") &gt; 100)  \nprint(f\"After second filter: {len(df2.flow_graph.nodes)} nodes\")\n\n# Both operations are tracked separately in the graph\n</code></pre>"},{"location":"users/python-api/concepts/design-concepts.html#flowgraph-the-pipelines-blueprint","title":"FlowGraph: The Pipeline's Blueprint","text":""},{"location":"users/python-api/concepts/design-concepts.html#what-is-flowgraph","title":"What is FlowGraph?","text":"<p>FlowGraph is the \"brain\" behind FlowFrame - it's a Directed Acyclic Graph (DAG) that tracks every step in your data transformation pipeline.</p> <pre><code># Access the graph from any FlowFrame\ndf = ff.FlowFrame({\n    \"product\": [\"Widget\", \"Gadget\", \"Tool\"],\n    \"price\": [10.99, 25.50, 8.75],\n    \"quantity\": [100, 50, 200]\n})\ngraph = df.flow_graph\n\nprint(f\"Graph ID: {graph.flow_id}\")\nprint(f\"Number of operations: {len(graph.nodes)}\")\nprint(f\"Node connections: {graph.node_connections}\")\n</code></pre>"},{"location":"users/python-api/concepts/design-concepts.html#visual-integration","title":"Visual Integration","text":""},{"location":"users/python-api/concepts/design-concepts.html#viewing-your-graph","title":"Viewing Your Graph","text":"<p>Every FlowFrame can show its complete operation history in the visual editor:</p> <pre><code># Build a pipeline\nresult = (\n    ff.FlowFrame({\n        \"region\": [\"North\", \"South\", \"North\", \"East\", \"South\"],\n        \"amount\": [1000, 0, 1500, 800, 1200],\n        \"product\": [\"A\", \"B\", \"A\", \"C\", \"B\"]\n    }, description=\"Load sales data\")\n    .filter(ff.col(\"amount\") &gt; 0, description=\"Remove invalid amounts\")\n    .group_by(\"region\", description=\"Group by sales region\")\n    .agg(ff.col(\"amount\").sum().alias(\"total_sales\"))\n)\n\n# Open the entire pipeline in the visual editor\nff.open_graph_in_editor(result.flow_graph)\n</code></pre> <p>Learn More</p> <p>See Visual UI Integration for details on launching and controlling the visual editor from Python.</p> <p>This opens a visual representation showing: - Each operation as a node - Data flow between operations - Descriptions you added for documentation - Schema changes at each step</p>"},{"location":"users/python-api/concepts/design-concepts.html#real-time-schema-prediction","title":"Real-time Schema Prediction","text":"<p>The DAG enables instant schema prediction without processing data:</p> <pre><code>df = ff.FlowFrame({\n    \"product\": [\"Widget\", \"Gadget\"],\n    \"price\": [10.50, 25.00],\n    \"quantity\": [2, 3]\n})\nprint(\"Original schema:\", df.schema)\n\n# Schema is predicted instantly, no data processed\ntransformed = df.with_columns([\n    (ff.col(\"price\") * ff.col(\"quantity\")).alias(\"total\")\n])\nprint(\"New schema:\", transformed.schema)  # Shows new 'total' column immediately\n</code></pre> <p>How Schema Prediction Works</p> <p>Learn about the closure pattern that enables this in The Magic of Closures.</p>"},{"location":"users/python-api/concepts/design-concepts.html#practical-implications","title":"Practical Implications","text":""},{"location":"users/python-api/concepts/design-concepts.html#memory-efficiency","title":"Memory Efficiency","text":"<p>Since FlowFrame is always lazy:</p> <pre><code># This can handle large datasets efficiently through lazy evaluation\nlarge_pipeline = (\n    ff.FlowFrame({\n        \"id\": list(range(10000)),\n        \"quality_score\": [0.1, 0.9, 0.8, 0.95] * 2500,  # Simulating large data\n        \"value\": list(range(10000)),\n        \"category\": [\"A\", \"B\", \"C\", \"D\"] * 2500\n    })\n    .filter(ff.col(\"quality_score\") &gt; 0.95)  # Reduces data early\n    .select([\"id\", \"value\", \"category\"])     # Reduces columns early  \n    .group_by(\"category\")\n    .agg(ff.col(\"value\").mean())\n)\n\n# Only processes what's needed when you collect\nresult = large_pipeline.collect()  # Optimized execution plan\n</code></pre> <p>Performance Guide</p> <p>For more on optimization strategies, see Execution Methods in our philosophy guide.</p>"},{"location":"users/python-api/concepts/design-concepts.html#graph-reuse-and-copying","title":"Graph Reuse and Copying","text":"<p>You can work with the same graph across multiple FlowFrames:</p> <pre><code># Start with common base\nbase = ff.FlowFrame({\n    \"region\": [\"North\", \"South\", \"East\"],\n    \"year\": [2024, 2024, 2023],\n    \"sales\": [1000, 1500, 800],\n    \"product\": [\"Widget\", \"Gadget\", \"Tool\"],\n    \"quantity\": [10, 15, 8]\n}).filter(ff.col(\"year\") == 2024)\n\n# Create different branches (same graph, different endpoints)\nsales_summary = base.group_by(\"region\").agg(ff.col(\"sales\").sum())\nproduct_summary = base.group_by(\"product\").agg(ff.col(\"quantity\").sum())\n\n# Both share the same underlying graph\nassert sales_summary.flow_graph is product_summary.flow_graph\n</code></pre>"},{"location":"users/python-api/concepts/design-concepts.html#best-practices","title":"Best Practices","text":""},{"location":"users/python-api/concepts/design-concepts.html#1-use-descriptions-for-complex-pipelines","title":"1. Use Descriptions for Complex Pipelines","text":"<pre><code>import flowfile as ff\npipeline = (\n    ff.FlowFrame({\n        \"customer_id\": [1, 2, 3, 4, 5],\n        \"status\": [\"active\", \"inactive\", \"active\", \"active\", \"inactive\"],\n        \"signup_date\": [\"2024-01-15\", \"2023-12-10\", \"2024-02-20\", \"2023-11-05\", \"2024-03-01\"],\n        \"customer_segment\": [\"premium\", \"basic\", \"premium\", \"basic\", \"premium\"],\n        \"revenue\": [1000, 500, 1500, 300, 2000]\n    }, description=\"Load raw customer data\")\n    .filter(ff.col(\"status\") == \"active\", description=\"Keep only active customers\")\n    .with_columns([\n        ff.col(\"signup_date\").str.strptime(ff.Date, \"%Y-%m-%d\").alias(\"signup_date\")\n    ], description=\"Parse signup dates\")\n    .group_by(\"customer_segment\", description=\"Aggregate by customer segment\")\n    .agg([\n        ff.col(\"revenue\").sum().alias(\"total_revenue\"),\n        ff.col(\"customer_id\").count().alias(\"customer_count\")\n    ], description=\"Calculate segment metrics\")\n)\n</code></pre>"},{"location":"users/python-api/concepts/design-concepts.html#2-visualize-during-development","title":"2. Visualize During Development","text":"<pre><code># Check your pipeline structure frequently\nff.open_graph_in_editor(pipeline.flow_graph)\n</code></pre> <p>Complete Examples</p> <ul> <li>Database Pipeline: See PostgreSQL Integration for a real-world ui example</li> <li>Cloud Pipeline: Check Cloud Connections for S3 workflows</li> <li>Export to Code: Learn how your pipelines convert to pure Python in Flow to Code</li> </ul>"},{"location":"users/python-api/concepts/design-concepts.html#summary","title":"Summary","text":"<p>FlowFrame and FlowGraph work together to provide:</p> <ul> <li>Lazy evaluation: No memory waste, optimal performance</li> <li>Complete lineage: Every operation is tracked and visualizable  </li> <li>Real-time feedback: Instant schema prediction and error detection</li> <li>Seamless integration: Switch between code and visual editing</li> <li>Polars compatibility: Very identical API with additional features</li> <li>Automatic adaptation: Complex operations automatically fall back to code nodes</li> </ul> <p>Understanding this design helps you build efficient, maintainable data pipelines that scale from quick analyses to production ETL workflows.</p>"},{"location":"users/python-api/concepts/design-concepts.html#related-documentation","title":"Related Documentation","text":"<ul> <li>FlowFrame Operations - Available transformations and methods</li> <li>Expressions - Column operations and formula syntax</li> <li>Joins - Combining datasets</li> <li>Aggregations - Group by and summarization</li> <li>Visual UI Integration - Working with the visual editor</li> <li>Developers guide - Core architecture and design philosophy</li> </ul>"},{"location":"users/python-api/concepts/expressions.html","title":"Expressions","text":"<p>Column expressions for data transformations. Flowfile follows the Polars expressions API, with additional features for Flowfile-formula syntax.</p>"},{"location":"users/python-api/concepts/expressions.html#flowfile-formula-syntax","title":"Flowfile formula syntax","text":"<p>Flowfile supports a simplified formula syntax for expressions, allowing you to use bracket notation for column references. This will render nicely in the Flowfile UI and was implemented to decrease the learning curve for users coming from Excel, PowerBI, Alteryx and Tableau. You can try it out here: Flowfile Formula Playground.</p> <p>For example, instead of using <code>ff.col(\"price\")</code>, you can use <code>[price]</code> in supported operations. Operations that support this syntax are documented below.</p> <ul> <li>ff.with_columns     You can use flowfile_formulas: Optional[List[str]] = None, output_column_names: Optional[List[str]] = None, to create new columns based on Flowfile-formula syntax. The number of columns in <code>output_column_names</code> must match the number of formulas in <code>flowfile_formulas</code>.</li> <li>ff.filter     You can use the flowfile_formula parameter to filter rows based on Flowfile Formula syntax.</li> </ul>"},{"location":"users/python-api/concepts/expressions.html#column-references","title":"Column References","text":"<pre><code>import flowfile as ff\n\n# Polars style\nff.col(\"price\")\n\n# Flowfile formula syntax (in supported operations)\n\"[price]\"  # Equivalent to ff.col(\"price\")\n</code></pre>"},{"location":"users/python-api/concepts/expressions.html#example-flowfile-formula-syntax","title":"Example Flowfile formula syntax","text":"<pre><code>import flowfile as ff\n\ndf = ff.FlowFrame({\n    \"product\": [\"Widget\", \"Gadget\", \"Tool\"],\n    \"price\": [10.50, 25.00, 8.75],\n    \"quantity\": [100, 50, 200],\n    \"discount\": [0.1, 0.15, 0.05]\n})\n\n# Using Flowfile formula syntax (Excel-like)\nresult = df.with_columns(\n    flowfile_formulas=[\n        \"[price] * [quantity]\",                    # Simple multiplication\n        \"[price] * (1 - [discount])\",              # With parentheses\n        \"if [quantity] &gt; 75 then 'High' else 'Low' endif\",      # Conditional\n        \"round([price] * [discount], 2)\"\n    ],\n    output_column_names=[\"revenue\", \"discounted_price\", \"volume_category\", \"discount_amount\"],\n    description=\"Calculate derived metrics\"\n)\n\n# Mix formulas with regular Polars expressions\noutput = df.with_columns([\n    ff.col(\"price\").round(0).alias(\"price_rounded\")  # Polars style\n]).with_columns(\n    flowfile_formulas=[\"[price_rounded] * [quantity]\"],  # Formula style\n    output_column_names=[\"estimated_revenue\"]\n)\n</code></pre>"},{"location":"users/python-api/concepts/expressions.html#arithmetic-operations","title":"Arithmetic Operations","text":"<pre><code># Standard expressions\ndf = df.with_columns([\n    (ff.col(\"price\") * ff.col(\"quantity\")).alias(\"revenue\"),\n    (ff.col(\"price\") * 1.1).alias(\"price_with_tax\"),\n    (ff.col(\"total\") / ff.col(\"count\")).alias(\"average\")\n])\n\n# Formula syntax\ndf = df.with_columns(\n    flowfile_formulas=[\n        \"[price] * [quantity]\",\n        \"[price] * 1.1\",\n        \"[total] / [count]\"\n    ],\n    output_column_names=[\"revenue\", \"price_with_tax\", \"average\"]\n)\n</code></pre> <p>Formula Syntax</p> <p>Use <code>[column_name]</code> in formula strings for simpler syntax when supported by the operation.</p> <p>Want to see more about the Flowfile python api? Check out the reference documentation.</p>"},{"location":"users/python-api/reference/index.html","title":"Python API Flowfile Reference","text":"<p>This section documents Flowfile's Python API, focusing on extensions and differences from Polars. For standard Polars operations, see the Polars documentation.</p>"},{"location":"users/python-api/reference/index.html#core-api","title":"Core API","text":""},{"location":"users/python-api/reference/index.html#data-inputoutput","title":"Data Input/Output","text":"<ul> <li>Reading Data - File formats and cloud storage</li> <li>Writing Data - Saving results</li> <li>Data Types - Supported data types</li> </ul>"},{"location":"users/python-api/reference/index.html#transformations","title":"Transformations","text":"<ul> <li>FlowFrame Operations - Filter, select, sort</li> <li>Aggregations - Group by and summarize</li> <li>Joins - Combining datasets</li> </ul>"},{"location":"users/python-api/reference/index.html#flowfile-specific-features","title":"Flowfile-Specific Features","text":"<ul> <li>Cloud Storage - S3 integration</li> <li>visualize pipelines - Working with the visual editor</li> </ul>"},{"location":"users/python-api/reference/index.html#key-extensions-to-polars","title":"Key Extensions to Polars","text":""},{"location":"users/python-api/reference/index.html#description-parameter","title":"Description Parameter","text":"<p>Every operation accepts <code>description</code> for visual documentation: <pre><code>df = df.filter(ff.col(\"active\") == True, description=\"Keep active records\")\n</code></pre></p>"},{"location":"users/python-api/reference/index.html#flowfile-formula-syntax","title":"Flowfile Formula Syntax","text":"<p>Alternative bracket-based syntax for expressions: <pre><code>df.filter(flowfile_formula=\"[price] &gt; 100 AND [quantity] &gt;= 10\")\n</code></pre> Read more about the formula syntax here: Flowfile Formula Syntax. Or try it out here: Flowfile Formula Playground</p>"},{"location":"users/python-api/reference/index.html#automatic-node-types","title":"Automatic Node Types","text":"<p>Operations map to UI nodes when possible, otherwise fall back to <code>polars_code</code>: <pre><code># Simple \u2192 UI node\ndf.group_by(\"category\").agg(ff.col(\"value\").sum())\n\n# Complex \u2192 polars_code node\ndf.group_by([ff.col(\"category\").str.to_uppercase()]).agg(ff.col(\"value\").sum())\n</code></pre></p>"},{"location":"users/python-api/reference/index.html#graph-access","title":"Graph Access","text":"<p>Inspect and visualize the pipeline DAG: <pre><code>ff.open_graph_in_editor(df.flow_graph)\n</code></pre></p>"},{"location":"users/python-api/reference/index.html#architecture-deep-dives","title":"Architecture Deep Dives","text":"<p>For understanding how Flowfile works internally:</p> <ul> <li>Core Architecture - FlowGraph, FlowNode, and FlowDataEngine internals</li> <li>Design Philosophy - The dual interface approach</li> </ul>"},{"location":"users/python-api/reference/index.html#getting-help","title":"Getting Help","text":"<ul> <li>Not finding a method? Check the Polars documentation - most methods work identically</li> <li>Need examples? See our tutorials</li> <li>Understanding concepts? Read about FlowFrame and FlowGraph</li> </ul> <p>This reference covers Flowfile-specific features. For standard Polars operations, see the Polars API Reference.</p>"},{"location":"users/python-api/reference/aggregations.html","title":"Aggregations","text":"<p>Group by and aggregate operations for summarizing data.</p>"},{"location":"users/python-api/reference/aggregations.html#basic-group-by","title":"Basic Group By","text":"<pre><code>import flowfile as ff\n\ndf = ff.FlowFrame({\n    \"category\": [\"A\", \"B\", \"A\", \"B\", \"A\"],\n    \"value\": [10, 20, 30, 40, 50],\n    \"quantity\": [1, 2, 3, 4, 5]\n})\n\n# Simple aggregation\nresult = df.group_by(\"category\").agg([\n    ff.col(\"value\").sum().alias(\"total_value\"),\n    ff.col(\"value\").mean().alias(\"avg_value\"),\n    ff.col(\"quantity\").count().alias(\"count\")\n])\n\n# With description\nresult = df.group_by(\"category\", description=\"Group by product category\").agg([\n    ff.col(\"value\").sum().alias(\"total_value\")\n])\n</code></pre>"},{"location":"users/python-api/reference/aggregations.html#multiple-grouping-columns","title":"Multiple Grouping Columns","text":"<pre><code>result = df.group_by([\"region\", \"category\"]).agg([\n    ff.col(\"sales\").sum().alias(\"total_sales\"),\n    ff.col(\"sales\").mean().alias(\"avg_sales\")\n])\n</code></pre>"},{"location":"users/python-api/reference/aggregations.html#complex-group-by","title":"Complex Group By","text":"<pre><code># Group by expression (creates polars_code node)\nresult = df.group_by([\n    ff.col(\"date\").dt.year().alias(\"year\")\n]).agg([\n    ff.col(\"amount\").sum()\n])\n\n# Dynamic aggregation\nresult = df.group_by(\"category\").agg([\n    ff.all().sum()  # Sum all numeric columns\n])\n</code></pre>"},{"location":"users/python-api/reference/aggregations.html#available-aggregations","title":"Available Aggregations","text":"Function Description <code>sum()</code> Sum of values <code>mean()</code> Average value <code>median()</code> Median value <code>min()</code> Minimum value <code>max()</code> Maximum value <code>count()</code> Count of non-null values <code>std()</code> Standard deviation <code>var()</code> Variance <code>first()</code> First value in group <code>last()</code> Last value in group <code>list()</code> Collect values into list"},{"location":"users/python-api/reference/aggregations.html#window-functions","title":"Window Functions","text":"<pre><code># Running calculations\ndf = df.with_columns([\n    ff.col(\"value\").cumsum().over(\"category\").alias(\"running_total\"),\n    ff.col(\"value\").rank().over(\"category\").alias(\"rank\")\n])\n</code></pre> <p>Node Type Selection</p> <p>Simple group_by operations create UI nodes. Complex expressions in group_by create <code>polars_code</code> nodes.</p> <p>\u2190 Previous: Expressions | Next: Joins \u2192</p>"},{"location":"users/python-api/reference/cloud-connections.html","title":"Cloud Connection Management","text":"<p>Flowfile provides secure, centralized management for cloud storage connections. Connections can be created through code or the UI\u2014both store credentials in an encrypted database.  On this page we will cover how to create and manage them in Python. If you want to learn how to create them in the UI,  check out the UI guide.</p>"},{"location":"users/python-api/reference/cloud-connections.html#creating-connections","title":"Creating Connections","text":""},{"location":"users/python-api/reference/cloud-connections.html#code-approach","title":"Code Approach","text":"<pre><code>import flowfile as ff\nfrom pydantic import SecretStr\n\n# Create a new S3 connection\nff.create_cloud_storage_connection(\n    ff.FullCloudStorageConnection(\n        connection_name=\"data-lake\",\n        storage_type=\"s3\",\n        auth_method=\"access_key\",\n        aws_region=\"us-east-1\",\n        aws_access_key_id=\"AKIAIOSFODNN7EXAMPLE\",\n        aws_secret_access_key=SecretStr(\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\")\n    )\n)\n</code></pre>"},{"location":"users/python-api/reference/cloud-connections.html#visual-editor-integration","title":"Visual Editor Integration","text":"<p>Connections created through code are immediately available in the Flowfile visual editor:</p> <pre><code># Create connection in code\nff.create_cloud_storage_connection(\n    ff.FullCloudStorageConnection(\n        connection_name=\"data-lake\",\n        # ... parameters\n    )\n)\n\n# This connection now appears in:\n# - Cloud Storage Reader node's connection dropdown\n# - Cloud Storage Writer node's connection dropdown\n# - Any other nodes that use cloud connections\n</code></pre> <p>Seamless Integration</p> <p>There's no difference between connections created via code or UI. Both are stored in the same encrypted database and are instantly available across all interfaces.</p>"},{"location":"users/python-api/reference/cloud-connections.html#connection-types","title":"Connection Types","text":""},{"location":"users/python-api/reference/cloud-connections.html#s3-connection-access-key","title":"S3 Connection (Access Key)","text":"<pre><code>ff.FullCloudStorageConnection(\n    connection_name=\"my-s3\",\n    storage_type=\"s3\",\n    auth_method=\"access_key\",\n    aws_region=\"us-east-1\",\n    aws_access_key_id=\"AKIAIOSFODNN7EXAMPLE\",\n    aws_secret_access_key=SecretStr(\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"),\n    endpoint_url=\"https://s3.amazonaws.com\"  # Optional, for custom endpoints\n)\n</code></pre> <pre><code>ff.FullCloudStorageConnection(\n    connection_name=\"my-s3-cli\",\n    storage_type=\"s3\",\n    auth_method=\"aws_cli\",  # Uses local AWS CLI credentials\n    aws_region=\"us-east-1\"\n)\n</code></pre>"},{"location":"users/python-api/reference/cloud-connections.html#managing-connections","title":"Managing Connections","text":""},{"location":"users/python-api/reference/cloud-connections.html#create-if-not-exists","title":"Create If Not Exists","text":"<p>Safely create connections without duplicates:</p> <pre><code># Only creates if \"data-lake\" doesn't exist\nff.create_cloud_storage_connection_if_not_exists(\n    ff.FullCloudStorageConnection(\n        connection_name=\"data-lake\",\n        storage_type=\"s3\",\n        # ... other parameters\n    )\n)\n</code></pre>"},{"location":"users/python-api/reference/cloud-connections.html#list-all-connections","title":"List All Connections","text":"<pre><code># Get all available connections for current user\nconnections = ff.get_all_available_cloud_storage_connections()\n\nfor conn in connections:\n    print(f\"Name: {conn.connection_name}\")\n    print(f\"Type: {conn.storage_type}\")\n    print(f\"Region: {conn.aws_region}\")\n</code></pre>"},{"location":"users/python-api/reference/cloud-connections.html#delete-connection","title":"Delete Connection","text":"<pre><code># Remove a connection by name\nff.del_cloud_storage_connection(\"old-connection\")\n</code></pre>"},{"location":"users/python-api/reference/cloud-connections.html#using-connections","title":"Using Connections","text":"<p>Once created, use connections in read/write operations:</p> <pre><code># Reading with connection\ndf = ff.scan_parquet_from_cloud_storage(\n    \"s3://bucket/data.parquet\",\n    connection_name=\"data-lake\"  # Use the connection name\n)\n\n# Writing with connection\ndf.write_parquet_to_cloud_storage(\n    \"s3://bucket/output.parquet\",\n    connection_name=\"data-lake\"\n)\n</code></pre>"},{"location":"users/python-api/reference/cloud-connections.html#security-features","title":"Security Features","text":""},{"location":"users/python-api/reference/cloud-connections.html#credential-encryption","title":"Credential Encryption","text":"<ul> <li>All credentials are encrypted before storage</li> <li>Secrets never appear in logs or error messages</li> <li>Use <code>SecretStr</code> wrapper for sensitive values</li> </ul>"},{"location":"users/python-api/reference/cloud-connections.html#user-isolation","title":"User Isolation","text":"<ul> <li>Connections are scoped to the current user</li> <li>Each user manages their own connections</li> <li>No cross-user credential access</li> </ul>"},{"location":"users/python-api/reference/cloud-connections.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"users/python-api/reference/cloud-connections.html#common-issues","title":"Common Issues","text":"Issue Solution \"Connection not found\" Ensure connection exists with <code>get_all_available_cloud_storage_connections()</code> \"Access denied\" Verify credentials and permissions \"Invalid endpoint\" Check <code>endpoint_url</code> for custom S3 services \"SSL verification failed\" Use <code>aws_allow_unsafe_html=True</code> for local/dev endpoints only"},{"location":"users/python-api/reference/cloud-connections.html#debug-connection","title":"Debug Connection","text":"<pre><code># List all connections to verify\nconns = ff.get_all_available_cloud_storage_connections()\nprint(f\"Available connections: {[c.connection_name for c in conns]}\")\n\n# Check specific connection details\nmy_conn = next((c for c in conns if c.connection_name == \"data-lake\"), None)\nif my_conn:\n    print(f\"Storage type: {my_conn.storage_type}\")\n    print(f\"Auth method: {my_conn.auth_method}\")\n    print(f\"Region: {my_conn.aws_region}\")\n</code></pre> <p>UI Integration</p> <p>All connections created via code are immediately available in the UI's connection dropdown when configuring nodes.</p> <p>\u2190 Previous: Joins | Next: visual Ui \u2192</p>"},{"location":"users/python-api/reference/data-types.html","title":"Data Types","text":"<p>Flowfile supports all Polars data types. This page covers the most commonly used types and conversions.</p>"},{"location":"users/python-api/reference/data-types.html#supported-types","title":"Supported Types","text":"Type Description Example <code>Int8</code>, <code>Int16</code>, <code>Int32</code>, <code>Int64</code> Signed integers <code>123</code> <code>UInt8</code>, <code>UInt16</code>, <code>UInt32</code>, <code>UInt64</code> Unsigned integers <code>456</code> <code>Float32</code>, <code>Float64</code> Floating point <code>12.34</code> <code>Boolean</code> True/False values <code>True</code> <code>Utf8</code> / <code>String</code> Text data <code>\"hello\"</code> <code>Date</code> Date without time <code>2024-01-15</code> <code>Datetime</code> Date with time <code>2024-01-15 14:30:00</code> <code>Time</code> Time without date <code>14:30:00</code> <code>Duration</code> Time delta <code>2 days</code> <code>List</code> Nested arrays <code>[1, 2, 3]</code> <code>Struct</code> Nested objects <code>{\"a\": 1, \"b\": 2}</code>"},{"location":"users/python-api/reference/data-types.html#type-casting","title":"Type Casting","text":"<pre><code>import flowfile as ff\n\ndf = ff.FlowFrame({\n    \"int_col\": [1, 2, 3],\n    \"str_col\": [\"10\", \"20\", \"30\"],\n    \"date_str\": [\"2024-01-01\", \"2024-01-02\", \"2024-01-03\"]\n})\n\n# Cast types\ndf = df.with_columns([\n    ff.col(\"int_col\").cast(ff.Float64).alias(\"float_col\"),\n    ff.col(\"str_col\").cast(ff.Int32).alias(\"parsed_int\"),\n    ff.col(\"date_str\").str.strptime(ff.Date, \"%Y-%m-%d\").alias(\"date_col\")\n])\n</code></pre>"},{"location":"users/python-api/reference/data-types.html#schema-inspection","title":"Schema Inspection","text":"<pre><code># Get schema without processing data\nprint(df.schema)\n# [Column(name='int_col', dtype=Int64), ...]\n\n# Check specific column type\nprint(df.schema[0].dtype)\n# Int64\n</code></pre> <p>\u2190 Previous: Writing data | Next: FlowFile Operations \u2192</p>"},{"location":"users/python-api/reference/flowframe-operations.html","title":"DataFrame Operations","text":"<p>Core operations for transforming data. All standard Polars operations are supported with additional Flowfile features.</p>"},{"location":"users/python-api/reference/flowframe-operations.html#filtering","title":"Filtering","text":"<pre><code>import flowfile as ff\n\ndf = ff.FlowFrame({\"price\": [10, 20, 30], \"qty\": [5, 0, 10]})\n\n# Standard Polars filter\ndf = df.filter(ff.col(\"price\") &gt; 15)\n\n# With description\ndf = df.filter(ff.col(\"price\") &gt; 15, description=\"Keep items over $15\")\n\n# Flowfile formula syntax\ndf = df.filter(flowfile_formula=\"[price] &gt; 15 AND [qty] &gt; 0\")\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#selecting-columns","title":"Selecting Columns","text":"<pre><code># Select specific columns\ndf = df.select([\"price\", \"qty\"])\n\n# Select with expressions\ndf = df.select([\n    ff.col(\"price\"),\n    ff.col(\"qty\").alias(\"quantity\")\n])\n\n# Exclude columns\ndf = df.select(ff.exclude(\"internal_id\"))\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#addingmodifying-columns","title":"Adding/Modifying Columns","text":"<pre><code># Standard with_columns\ndf = df.with_columns([\n    (ff.col(\"price\") * ff.col(\"qty\")).alias(\"total\")\n])\n\n# Flowfile formula syntax\ndf = df.with_columns(\n    flowfile_formulas=[\"[price] * [qty]\"],\n    output_column_names=[\"total\"],\n    description=\"Calculate line totals\"\n)\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#sorting","title":"Sorting","text":"<pre><code># Sort by column\ndf = df.sort(\"price\")\ndf = df.sort(\"price\", descending=True)\n\n# Multi-column sort\ndf = df.sort([\"category\", \"price\"], descending=[False, True])\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#unique-operations","title":"Unique Operations","text":"<pre><code># Get unique rows\ndf = df.unique()\n\n# Unique by specific columns\ndf = df.unique(subset=[\"product_id\"])\n\n# Drop duplicates (alias)\ndf = df.drop_duplicates(subset=[\"product_id\"])\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#string-operations","title":"String Operations","text":"<pre><code>df = df.with_columns([\n    ff.col(\"name\").str.to_uppercase().alias(\"name_upper\"),\n    ff.col(\"code\").str.slice(0, 3).alias(\"prefix\"),\n    ff.col(\"text\").str.contains(\"pattern\").alias(\"has_pattern\")\n])\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#conditional-logic","title":"Conditional Logic","text":"<pre><code># When/then/otherwise\ndf = df.with_columns([\n    ff.when(ff.col(\"price\") &gt; 100)\n    .then(ff.lit(\"Premium\"))\n    .when(ff.col(\"price\") &gt; 50)\n    .then(ff.lit(\"Standard\"))\n    .otherwise(ff.lit(\"Budget\"))\n    .alias(\"tier\")\n])\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#date-operations","title":"Date Operations","text":"<pre><code>df = df.with_columns([\n    ff.col(\"date\").dt.year().alias(\"year\"),\n    ff.col(\"date\").dt.month().alias(\"month\"),\n    ff.col(\"date\").dt.day().alias(\"day\"),\n    ff.col(\"date\").dt.weekday().alias(\"weekday\")\n])\n</code></pre>"},{"location":"users/python-api/reference/flowframe-operations.html#list-operations","title":"List Operations","text":"<pre><code>df = df.with_columns([\n    ff.col(\"tags\").list.len().alias(\"tag_count\"),\n    ff.col(\"values\").list.sum().alias(\"total\"),\n    ff.col(\"items\").list.first().alias(\"first_item\")\n])\n</code></pre> <p>Polars Compatibility</p> <p>All standard Polars DataFrame methods work identically. See Polars docs for complete reference.</p> <p>\u2190 Previous: Data Types | Next: Aggregations \u2192</p>"},{"location":"users/python-api/reference/joins.html","title":"Joins","text":"<p>Combining data from multiple FlowFrames.</p>"},{"location":"users/python-api/reference/joins.html#basic-join","title":"Basic Join","text":"<pre><code>import flowfile as ff\n\ncustomers = ff.FlowFrame({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n})\n\norders = ff.FlowFrame({\n    \"order_id\": [101, 102, 103],\n    \"customer_id\": [1, 2, 1],\n    \"amount\": [100, 200, 150]\n})\n\n# Inner join\nresult = customers.join(\n    orders,\n    left_on=\"id\",\n    right_on=\"customer_id\",\n    how=\"inner\",\n    description=\"Join customers with orders\"\n)\n</code></pre>"},{"location":"users/python-api/reference/joins.html#join-types","title":"Join Types","text":"<pre><code># Inner join (default)\ndf1.join(df2, on=\"key\", how=\"inner\")\n\n# Left join\ndf1.join(df2, on=\"key\", how=\"left\")\n\n# Outer join\ndf1.join(df2, on=\"key\", how=\"outer\")\n\n# Semi join (filter df1 by df2)\ndf1.join(df2, on=\"key\", how=\"semi\")\n\n# Anti join (exclude matches)\ndf1.join(df2, on=\"key\", how=\"anti\")\n</code></pre>"},{"location":"users/python-api/reference/joins.html#multiple-join-keys","title":"Multiple Join Keys","text":"<pre><code>result = df1.join(\n    df2,\n    on=[\"region\", \"year\"],  # Join on multiple columns\n    how=\"inner\"\n)\n\n# Different column names\nresult = df1.join(\n    df2,\n    left_on=[\"region_code\", \"period\"],\n    right_on=[\"region\", \"year\"],\n    how=\"left\"\n)\n</code></pre>"},{"location":"users/python-api/reference/joins.html#cross-join","title":"Cross Join","text":"<pre><code># Cartesian product\nresult = df1.join(df2, how=\"cross\")\n</code></pre>"},{"location":"users/python-api/reference/joins.html#unionconcatenation","title":"Union/Concatenation","text":"<pre><code># Vertical concatenation\ncombined = ff.concat([df1, df2, df3])\n\n# Union (removes duplicates)\nunion_df = df1.unique().vstack(df2.unique()).unique()\n\n# Diagonal concatenation (handles different schemas)\ncombined = ff.concat([df1, df2], how=\"diagonal\")\n</code></pre>"},{"location":"users/python-api/reference/joins.html#join-validation","title":"Join Validation","text":"<pre><code># Check for duplicates before joining\nif df2.select(\"customer_id\").n_unique() &lt; len(df2):\n    print(\"Warning: duplicate keys in right table\")\n\n# Validate join results\nresult = df1.join(df2, on=\"id\", how=\"left\")\nunmatched = result.filter(ff.col(\"amount\").is_null())\nprint(f\"Unmatched records: {len(unmatched)}\")\n</code></pre> <p>Unsupported Join Types</p> <p>Currently, <code>join_asof</code> and <code>join_where</code> are not supported in Flowfile. These operations will need to be implemented using alternative approaches or raw Polars code.</p> <p>\u2190 Previous: Aggregations | Next: Cloud Connection \u2192</p>"},{"location":"users/python-api/reference/reading-data.html","title":"Reading Data","text":"<p>Flowfile provides Polars-compatible readers with additional cloud storage integration and visual workflow features.</p> <p>Polars Compatibility</p> <p>All Flowfile readers accept the same parameters as Polars, plus optional <code>description</code> for visual documentation.</p>"},{"location":"users/python-api/reference/reading-data.html#local-file-reading","title":"Local File Reading","text":""},{"location":"users/python-api/reference/reading-data.html#csv-files","title":"CSV Files","text":"<pre><code>import flowfile as ff\n\n# Basic usage (same as Polars)\ndf = ff.read_csv(\"data.csv\")\n\n# With Flowfile description\ndf = ff.read_csv(\"data.csv\", description=\"Load customer data\")\n\n# Polars parameters work identically\ndf = ff.read_csv(\n    \"data.csv\",\n    separator=\",\",\n    has_header=True,\n    skip_rows=1,\n    n_rows=1000,\n    description=\"Sample first 1000 customer records\"\n)\n</code></pre> <p>Key Parameters (same as Polars):</p> <ul> <li><code>separator</code>: Field delimiter (default: <code>,</code>)</li> <li><code>has_header</code>: First row contains column names (default: <code>True</code>)</li> <li><code>skip_rows</code>: Skip rows at start of file</li> <li><code>n_rows</code>: Maximum rows to read</li> <li><code>encoding</code>: File encoding (default: <code>utf8</code>)</li> <li><code>null_values</code>: Values to treat as null</li> <li><code>schema_overrides</code>: Override column types</li> </ul>"},{"location":"users/python-api/reference/reading-data.html#parquet-files","title":"Parquet Files","text":"<pre><code># Basic usage\ndf = ff.read_parquet(\"data.parquet\")\n\n# With description\ndf = ff.read_parquet(\"sales_data.parquet\", description=\"Q4 sales results\")\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#scanning-vs-reading","title":"Scanning vs Reading","text":"<p>Flowfile provides both <code>read_*</code> and <code>scan_*</code> functions for Polars compatibility:</p> <pre><code># These are identical in Flowfile\ndf1 = ff.read_csv(\"data.csv\")\ndf2 = ff.scan_csv(\"data.csv\")  # Alias for read_csv\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#cloud-storage-reading","title":"Cloud Storage Reading","text":"<p>Flowfile extends Polars with specialized cloud storage readers that integrate with secure connection management.</p>"},{"location":"users/python-api/reference/reading-data.html#cloud-csv-reading","title":"Cloud CSV Reading","text":"<pre><code># Read from S3 with connection\ndf = ff.scan_csv_from_cloud_storage(\n    \"s3://my-bucket/data.csv\",\n    connection_name=\"my-aws-connection\",\n    delimiter=\",\",\n    has_header=True,\n    encoding=\"utf8\"\n)\n\n# Directory scanning (reads all CSV files)\ndf = ff.scan_csv_from_cloud_storage(\n    \"s3://my-bucket/csv-files/\",\n    connection_name=\"my-aws-connection\"\n)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#cloud-parquet-reading","title":"Cloud Parquet Reading","text":"<pre><code># Single file\ndf = ff.scan_parquet_from_cloud_storage(\n    \"s3://data-lake/sales.parquet\",\n    connection_name=\"data-lake-connection\"\n)\n\n# Directory of files\ndf = ff.scan_parquet_from_cloud_storage(\n    \"s3://data-lake/partitioned-data/\",\n    connection_name=\"data-lake-connection\",\n    scan_mode=\"directory\"\n)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#cloud-json-reading","title":"Cloud JSON Reading","text":"<pre><code>df = ff.scan_json_from_cloud_storage(\n    \"s3://my-bucket/data.json\",\n    connection_name=\"my-aws-connection\"\n)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#delta-lake-reading","title":"Delta Lake Reading","text":"<pre><code># Latest version\ndf = ff.scan_delta(\n    \"s3://data-lake/delta-table\",\n    connection_name=\"data-lake-connection\"\n)\n\n# Specific version (if supported)\ndf = ff.scan_delta(\n    \"s3://data-lake/delta-table\",\n    connection_name=\"data-lake-connection\"\n    # Note: version parameter support depends on implementation\n)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#connection-management","title":"Connection Management","text":"<p>Before reading from cloud storage, set up connections:</p> <pre><code>import flowfile as ff\nfrom pydantic import SecretStr\n\n# Create S3 connection\nff.create_cloud_storage_connection_if_not_exists(\n    ff.FullCloudStorageConnection(\n        connection_name=\"my-aws-connection\",\n        storage_type=\"s3\",\n        auth_method=\"access_key\",\n        aws_region=\"us-east-1\",\n        aws_access_key_id=\"your-access-key\",\n        aws_secret_access_key=SecretStr(\"your-secret-key\")\n    )\n)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#flowfile-specific-features","title":"Flowfile-Specific Features","text":""},{"location":"users/python-api/reference/reading-data.html#description-parameter","title":"Description Parameter","text":"<p>Every reader accepts an optional <code>description</code> for visual documentation:</p> <pre><code>df = ff.read_csv(\n    \"quarterly_sales.csv\",\n    description=\"Load Q4 2024 sales data for analysis\"\n)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#automatic-scan-mode-detection","title":"Automatic Scan Mode Detection","text":"<p>Cloud storage readers automatically detect scan mode:</p> <pre><code># Automatically detects single file\ndf = ff.scan_parquet_from_cloud_storage(\"s3://bucket/file.parquet\")\n\n# Automatically detects directory scan\ndf = ff.scan_parquet_from_cloud_storage(\"s3://bucket/folder/\")\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#integration-with-visual-ui","title":"Integration with Visual UI","text":"<p>All reading operations create nodes in the visual workflow:</p> <pre><code>df = ff.read_csv(\"data.csv\", description=\"Source data\")\n\n# Open in visual editor\nff.open_graph_in_editor(df.flow_graph)\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#examples","title":"Examples","text":""},{"location":"users/python-api/reference/reading-data.html#standard-data-pipeline","title":"Standard Data Pipeline","text":"<pre><code>import flowfile as ff\n\n# Read local file\ncustomers = ff.read_csv(\"customers.csv\", description=\"Customer master data\")\n\n# Read from cloud\norders = ff.scan_parquet_from_cloud_storage(\n    \"s3://data-warehouse/orders/\",\n    connection_name=\"warehouse\",\n    description=\"Order history from data warehouse\"\n)\n\n# Continue processing...\nresult = customers.join(orders, on=\"customer_id\")\n</code></pre>"},{"location":"users/python-api/reference/reading-data.html#multi-format-cloud-pipeline","title":"Multi-Format Cloud Pipeline","text":"<pre><code># Different formats from same connection\nconfig_data = ff.scan_json_from_cloud_storage(\n    \"s3://configs/settings.json\",\n    connection_name=\"app-data\"\n)\n\nsales_data = ff.scan_parquet_from_cloud_storage(\n    \"s3://analytics/sales/\",\n    connection_name=\"app-data\"\n)\n\ndelta_data = ff.scan_delta(\n    \"s3://warehouse/customer_dim\",\n    connection_name=\"app-data\"\n)\n</code></pre> <p>\u2190 Previous: Introduction | Next: Writing Data \u2192</p>"},{"location":"users/python-api/reference/visual-ui.html","title":"Visual UI Integration","text":"<p>Flowfile provides a web-based visual interface that can be launched directly from Python. This allows seamless transitions between code and visual pipeline development.</p>"},{"location":"users/python-api/reference/visual-ui.html#starting-the-web-ui","title":"Starting the Web UI","text":""},{"location":"users/python-api/reference/visual-ui.html#quick-start","title":"Quick Start","text":"<pre><code>import flowfile as ff\n\n# Start the web UI (opens browser automatically)\nff.start_web_ui()\n\n# Start without opening browser\nff.start_web_ui(open_browser=False)\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#command-line","title":"Command Line","text":"<pre><code># Start with default settings\nflowfile run ui\n\n# Start without opening browser\nflowfile run ui --no-browser\n</code></pre> <p>Unified Mode</p> <p>The web UI runs in \"unified mode\" - a single service that combines the Core API, Worker, and Web UI. No separate services or Docker required!</p>"},{"location":"users/python-api/reference/visual-ui.html#opening-pipelines-in-the-editor","title":"Opening Pipelines in the Editor","text":""},{"location":"users/python-api/reference/visual-ui.html#basic-usage","title":"Basic Usage","text":"<pre><code>import flowfile as ff\n\n# Build a pipeline in code\ndf = ff.FlowFrame({\n    \"product\": [\"Widget\", \"Gadget\", \"Tool\"],\n    \"price\": [19.99, 39.99, 15.99],\n    \"quantity\": [100, 50, 200]\n})\n\nresult = df.filter(ff.col(\"price\") &gt; 20).with_columns([\n    (ff.col(\"price\") * ff.col(\"quantity\")).alias(\"revenue\")\n])\n\n# Open in visual editor (auto-starts server if needed)\nff.open_graph_in_editor(result.flow_graph)\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#what-happens-behind-the-scenes","title":"What Happens Behind the Scenes","text":"<p>When you call <code>open_graph_in_editor()</code>:</p> <ol> <li>Saves the graph to a temporary <code>.flowfile</code> </li> <li>Checks if server is running at <code>http://localhost:63578</code></li> <li>Starts server if needed using <code>flowfile run ui --no-browser</code></li> <li>Imports the flow via API endpoint</li> <li>Opens browser tab at <code>http://localhost:63578/ui/flow/{id}</code></li> </ol>"},{"location":"users/python-api/reference/visual-ui.html#advanced-options","title":"Advanced Options","text":"<pre><code># Save to specific location instead of temp file\nff.open_graph_in_editor(\n    result.flow_graph,\n    storage_location=\"./my_pipeline.flowfile\"\n)\n\n# Don't automatically open browser\nff.open_graph_in_editor(\n    result.flow_graph,\n    automatically_open_browser=False\n)\n\n# Use custom module name (for development)\nff.open_graph_in_editor(\n    result.flow_graph,\n    module_name=\"my_custom_flowfile\"\n)\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#server-management","title":"Server Management","text":""},{"location":"users/python-api/reference/visual-ui.html#checking-server-status","title":"Checking Server Status","text":"<pre><code># All server management functions are in flowfile.api\nfrom flowfile.api import (\n    is_flowfile_running,\n    start_flowfile_server_process, \n    stop_flowfile_server_process,\n    get_auth_token\n)\n\nif is_flowfile_running():\n    print(\"Server is running\")\nelse:\n    print(\"Server is not running\")\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#manual-server-control","title":"Manual Server Control","text":"<pre><code>from flowfile.api import start_flowfile_server_process, stop_flowfile_server_process\n\n# Start server manually\nsuccess, single_mode = start_flowfile_server_process()\n\n# Stop server when done\nstop_flowfile_server_process()\n</code></pre> <p>Auto-cleanup</p> <p>The server process is automatically stopped when your Python script exits. No need to manually stop it unless you want to free resources earlier.</p>"},{"location":"users/python-api/reference/visual-ui.html#configuration","title":"Configuration","text":""},{"location":"users/python-api/reference/visual-ui.html#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>FLOWFILE_HOST</code> <code>127.0.0.1</code> Host to bind server to <code>FLOWFILE_PORT</code> <code>63578</code> Port for the server <code>FLOWFILE_MODULE_NAME</code> <code>flowfile</code> Module name to run"},{"location":"users/python-api/reference/visual-ui.html#urls-and-endpoints","title":"URLs and Endpoints","text":"<p>Once running, the following are available:</p> <ul> <li>Web UI: <code>http://localhost:63578/ui</code></li> <li>API Docs: <code>http://localhost:63578/docs</code></li> <li>Health Check: <code>http://localhost:63578/docs</code> (used to verify server is running)</li> </ul>"},{"location":"users/python-api/reference/visual-ui.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"users/python-api/reference/visual-ui.html#server-wont-start","title":"Server Won't Start","text":"<pre><code># Check if port is already in use\nimport socket\n\ndef is_port_in_use(port):\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        return s.connect_ex(('localhost', port)) == 0\n\nif is_port_in_use(63578):\n    print(\"Port 63578 is already in use\")\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#server-starts-but-ui-doesnt-open","title":"Server Starts but UI Doesn't Open","text":"<ul> <li>Manually navigate to <code>http://localhost:63578/ui</code></li> <li>Check server logs in terminal</li> <li>Verify no firewall blocking localhost connections</li> </ul>"},{"location":"users/python-api/reference/visual-ui.html#import-fails","title":"Import Fails","text":"<pre><code># Verify authentication is working\nfrom flowfile.api import get_auth_token\n\ntoken = get_auth_token()\nif token:\n    print(\"Auth successful\")\nelse:\n    print(\"Auth failed - check server logs\")\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#poetry-environment-issues","title":"Poetry Environment Issues","text":"<p>If using Poetry for development:</p> <pre><code># Force Poetry detection\nimport os\nos.environ[\"FORCE_POETRY\"] = \"1\"\n\n# Or specify Poetry path\nos.environ[\"POETRY_PATH\"] = \"/path/to/poetry\"\n\nff.open_graph_in_editor(df.flow_graph)\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#best-practices","title":"Best Practices","text":""},{"location":"users/python-api/reference/visual-ui.html#1-let-auto-start-handle-it","title":"1. Let Auto-start Handle It","text":"<pre><code># \u2705 Good: Let open_graph_in_editor start server\nff.open_graph_in_editor(df.flow_graph)\n\n# \u274c Avoid: Manual server management unless necessary\nff.start_web_ui()\ntime.sleep(5)\nff.open_graph_in_editor(df.flow_graph)\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#2-use-temporary-files","title":"2. Use Temporary Files","text":"<pre><code># \u2705 Good: Let Flowfile handle temp files\nff.open_graph_in_editor(df.flow_graph)\n\n# Only specify path if you need to keep the file\nff.open_graph_in_editor(\n    df.flow_graph,\n    storage_location=\"./important_pipeline.flowfile\"\n)\n</code></pre>"},{"location":"users/python-api/reference/visual-ui.html#3-single-server-instance","title":"3. Single Server Instance","text":"<p>The server is designed to be a singleton - multiple calls to <code>open_graph_in_editor()</code> will reuse the same server instance.</p> <pre><code># First call starts server\nff.open_graph_in_editor(pipeline1.flow_graph)\n\n# Subsequent calls reuse server\nff.open_graph_in_editor(pipeline2.flow_graph)  # No new server started\nff.open_graph_in_editor(pipeline3.flow_graph)  # Still same server\n</code></pre> <p>Where to Go Next</p> <ul> <li>Explore Visual Nodes: Learn the details of each node available in the Visual Editor.</li> <li>Convert Code to Visual: See how your code translates into a visual workflow in the Conversion Guide.</li> <li>Build with Code: Dive deeper into the code-first approach for building pipelines.</li> <li>Back to Index: Return to the main Python API Index.</li> </ul>"},{"location":"users/python-api/reference/writing-data.html","title":"Writing Data","text":"<p>Flowfile provides Polars-compatible writers with additional cloud storage integration and visual workflow features.</p> <p>Polars Compatibility</p> <p>Local file writers work identically to Polars, plus optional <code>description</code> for visual documentation.</p>"},{"location":"users/python-api/reference/writing-data.html#local-file-writing","title":"Local File Writing","text":""},{"location":"users/python-api/reference/writing-data.html#csv-files","title":"CSV Files","text":"<pre><code>import flowfile as ff\n\n# Basic usage (same as Polars)\ndf = ff.read_csv(\"input.csv\")\ndf.write_csv(\"output.csv\")\n\n# With Flowfile description\ndf.write_csv(\"processed_data.csv\", description=\"Save cleaned customer data\")\n\n# Polars parameters work identically\ndf.write_csv(\n    \"output.csv\",\n    separator=\";\",\n    encoding=\"utf-8\",\n    description=\"Export with semicolon delimiter\"\n)\n</code></pre> <p>Key Parameters (same as Polars):</p> <ul> <li><code>separator</code>: Field delimiter (default: <code>,</code>)</li> <li><code>encoding</code>: File encoding (default: <code>utf-8</code>)</li> </ul>"},{"location":"users/python-api/reference/writing-data.html#parquet-files","title":"Parquet Files","text":"<pre><code># Basic usage\ndf.write_parquet(\"output.parquet\")\n\n# With description and compression\ndf.write_parquet(\n    \"compressed_data.parquet\",\n    description=\"Save with high compression\",\n    compression=\"gzip\"\n)\n</code></pre>"},{"location":"users/python-api/reference/writing-data.html#cloud-storage-writing","title":"Cloud Storage Writing","text":"<p>Flowfile extends writing capabilities with specialized cloud storage writers that integrate with secure connection management.</p>"},{"location":"users/python-api/reference/writing-data.html#cloud-csv-writing","title":"Cloud CSV Writing","text":"<pre><code># Write to S3\ndf.write_csv_to_cloud_storage(\n    \"s3://my-bucket/output.csv\",\n    connection_name=\"my-aws-connection\",\n    delimiter=\",\",\n    encoding=\"utf8\",\n    description=\"Export processed data to S3\"\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>path</code>: Full S3 path including bucket and file name</li> <li><code>connection_name</code>: Name of configured cloud storage connection</li> <li><code>delimiter</code>: CSV field separator (default: <code>;</code>)</li> <li><code>encoding</code>: File encoding (<code>utf8</code> or <code>utf8-lossy</code>)</li> </ul>"},{"location":"users/python-api/reference/writing-data.html#cloud-parquet-writing","title":"Cloud Parquet Writing","text":"<pre><code># Write to S3 with compression\ndf.write_parquet_to_cloud_storage(\n    \"s3://data-lake/processed/results.parquet\",\n    connection_name=\"data-lake-connection\",\n    compression=\"snappy\",\n    description=\"Save analysis results to data lake\"\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>path</code>: Full S3 path for the output file</li> <li><code>connection_name</code>: Name of configured cloud storage connection  </li> <li><code>compression</code>: Compression algorithm (<code>snappy</code>, <code>gzip</code>, <code>brotli</code>, <code>lz4</code>, <code>zstd</code>)</li> </ul>"},{"location":"users/python-api/reference/writing-data.html#cloud-json-writing","title":"Cloud JSON Writing","text":"<pre><code># Write JSON to cloud storage\ndf.write_json_to_cloud_storage(\n    \"s3://api-data/export.json\", \n    connection_name=\"api-storage\",\n    description=\"Export for API consumption\"\n)\n</code></pre>"},{"location":"users/python-api/reference/writing-data.html#delta-lake-writing","title":"Delta Lake Writing","text":"<pre><code># Write Delta table (supports append mode)\ndf.write_delta(\n    \"s3://warehouse/customer_dim\",\n    connection_name=\"warehouse-connection\",\n    write_mode=\"overwrite\",\n    description=\"Update customer dimension table\"\n)\n\n# Append to existing Delta table\nnew_data.write_delta(\n    \"s3://warehouse/customer_dim\",\n    connection_name=\"warehouse-connection\", \n    write_mode=\"append\",\n    description=\"Add new customers to dimension\"\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>path</code>: S3 path for the Delta table</li> <li><code>connection_name</code>: Name of configured cloud storage connection</li> <li><code>write_mode</code>: <code>overwrite</code> (replace) or <code>append</code> (add to existing)</li> </ul>"},{"location":"users/python-api/reference/writing-data.html#write-modes","title":"Write Modes","text":""},{"location":"users/python-api/reference/writing-data.html#overwrite-vs-append","title":"Overwrite vs Append","text":"<pre><code># Overwrite existing data (default)\ndf.write_parquet_to_cloud_storage(\n    \"s3://bucket/data.parquet\",\n    connection_name=\"conn\",\n    write_mode=\"overwrite\"  # Default for most formats\n)\n\n# Append to existing (Delta Lake only)\ndf.write_delta(\n    \"s3://warehouse/events\",\n    connection_name=\"conn\",\n    write_mode=\"append\"\n)\n</code></pre> <p>Append Mode</p> <p>Currently only supported for Delta Lake format. Other formats always overwrite.</p>"},{"location":"users/python-api/reference/writing-data.html#connection-requirements","title":"Connection Requirements","text":"<p>All cloud storage writing requires a configured connection:</p> <pre><code>import flowfile as ff\nfrom pydantic import SecretStr\n\n# Set up connection before writing\nff.create_cloud_storage_connection_if_not_exists(\n    ff.FullCloudStorageConnection(\n        connection_name=\"data-lake\",\n        storage_type=\"s3\", \n        auth_method=\"access_key\",\n        aws_region=\"us-east-1\",\n        aws_access_key_id=\"your-key\",\n        aws_secret_access_key=SecretStr(\"your-secret\")\n    )\n)\n\n# Now you can write to cloud storage\ndf.write_parquet_to_cloud_storage(\n    \"s3://data-lake/output.parquet\",\n    connection_name=\"data-lake\"\n)\n</code></pre> <p>\u2190 Previous: Reading Data | Next: Data Types \u2192</p>"},{"location":"users/python-api/tutorials/index.html","title":"Python API Tutorials","text":"<p>Learn to build powerful data pipelines with code through practical, hands-on examples.</p>"},{"location":"users/python-api/tutorials/index.html#available-tutorials","title":"Available Tutorials","text":""},{"location":"users/python-api/tutorials/index.html#building-flows-with-code","title":"Building Flows with Code","text":"<p>The complete guide to creating data pipelines programmatically while maintaining visual compatibility.</p> <p>You'll learn: - Creating pipelines with the FlowFrame API - Using Polars-compatible operations - Automatically generating visual graphs - Switching between code and visual editing</p> <p>Perfect for: - Python developers new to Flowfile - Data scientists wanting reproducible pipelines - Anyone preferring code over drag-and-drop</p>"},{"location":"users/python-api/tutorials/index.html#coming-soon","title":"Coming Soon","text":""},{"location":"users/python-api/tutorials/index.html#data-pipeline-patterns","title":"Data Pipeline Patterns","text":"<p>Common patterns for ETL, data cleaning, and analysis.</p>"},{"location":"users/python-api/tutorials/index.html#performance-optimization","title":"Performance Optimization","text":"<p>Advanced techniques for handling large datasets efficiently.</p>"},{"location":"users/python-api/tutorials/index.html#integration-examples","title":"Integration Examples","text":"<p>Connecting Flowfile with pandas, scikit-learn, and other tools.</p>"},{"location":"users/python-api/tutorials/index.html#tutorial-style","title":"Tutorial Style","text":"<p>Our Python tutorials focus on: - Real-world examples - Practical use cases you'll actually encounter - Code-first approach - Everything done programmatically - Visual integration - How to leverage the UI when helpful - Best practices - Production-ready patterns</p>"},{"location":"users/python-api/tutorials/index.html#quick-examples","title":"Quick Examples","text":""},{"location":"users/python-api/tutorials/index.html#etl-pipeline","title":"ETL Pipeline","text":"<pre><code>import flowfile as ff\n\n# Extract\nraw_data = ff.read_csv(\"sales.csv\")\n\n# Transform\ntransformed = (\n    raw_data\n    .filter(ff.col(\"amount\") &gt; 0)\n    .with_columns([\n        ff.col(\"date\").str.strptime(ff.Date, \"%Y-%m-%d\")\n    ])\n    .group_by(\"region\")\n    .agg(ff.col(\"amount\").sum())\n)\n\n# Load\ntransformed.write_parquet(\"output.parquet\")\n</code></pre>"},{"location":"users/python-api/tutorials/index.html#data-validation","title":"Data Validation","text":"<pre><code># Check for data quality issues\ndf = ff.read_csv(\"input.csv\")\n\n# Find duplicates\nduplicates = df.group_by(\"id\").agg(\n    ff.count().alias(\"count\")\n).filter(ff.col(\"count\") &gt; 1)\n\n# Find nulls\nnull_counts = df.select([\n    ff.col(c).is_null().sum().alias(f\"{c}_nulls\")\n    for c in df.columns\n])\n</code></pre>"},{"location":"users/python-api/tutorials/index.html#resources","title":"Resources","text":"<ul> <li>API Reference - Complete method documentation</li> <li>Core Concepts - Understand the architecture</li> <li>Quick Start - Get running in 5 minutes</li> </ul> <p>Want more tutorials? Let us know what you'd like to see in our GitHub Discussions!</p>"},{"location":"users/python-api/tutorials/flowfile_frame_api.html","title":"Building Flows with code","text":"<p>The <code>flowfile_frame</code> module provides a powerful, Polars-like API that allows you to define and execute data transformation pipelines in Python while automatically generating a visual ETL graph. <sub><sup>[Source: readme.md]</sup></sub></p>"},{"location":"users/python-api/tutorials/flowfile_frame_api.html#overview","title":"Overview","text":"<p><code>flowfile_frame</code> is designed to bridge the gap between writing code and visual workflow design. It offers:</p> <ul> <li>A familiar API for those accustomed to Pandas or Polars.  </li> <li>Automatic generation of an ETL graph from your Python code.  </li> <li>The ability to visualize, save, and share your data pipelines in the Flowfile Designer UI.  </li> <li>The performance benefits of the Polars engine.  </li> </ul>"},{"location":"users/python-api/tutorials/flowfile_frame_api.html#installation","title":"Installation","text":"<p>The <code>flowfile_frame</code> module is included with the standard <code>flowfile</code> package.</p> <pre><code>pip install flowfile\n</code></pre>"},{"location":"users/python-api/tutorials/flowfile_frame_api.html#quick-start","title":"Quick Start","text":"<p>You can create a data pipeline programmatically and see the results:</p> <pre><code>import flowfile as ff\nfrom flowfile import col, open_graph_in_editor\n\ndf = ff.from_dict({\n    \"id\": [1, 2, 2],\n    \"value\": [10, 20, 15]\n})\n\nresult = df.filter(col(\"value\") &gt; 12, description=\"filter value &gt; 12\").with_columns(\n    (col(\"value\") * 10).alias(\"scaled_value\"), description=\"get a scaled value\"\n).group_by(col(\"id\")).agg(col(\"value\").sum().alias(\"sum_value\"),\n                        col(\"value\").max().alias(\"max_value\"),\n                        col(\"value\").min().alias(\"min_value\"))\ndf = result.collect()  # provides a polars dataframe\nopen_graph_in_editor(result.flow_graph)\n</code></pre> Generated Flow in Flowfile UI <p></p>"},{"location":"users/python-api/tutorials/flowfile_frame_api.html#visualizing-your-pipeline","title":"Visualizing Your Pipeline","text":"<p>One of the most powerful features of <code>flowfile_frame</code> is its ability to convert your code into a visual graph that can be opened in the Flowfile UI.</p> <p>You can build more advanced pipelines with conditional logic, grouping, and aggregation \u2014 and then instantly visualize them.</p> <pre><code>import flowfile as ff\nfrom flowfile import open_graph_in_editor\n\n# Create a more complex data pipeline\ndf = ff.from_dict({\n    \"id\": [1, 2, 3, 4, 5],\n    \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\"],\n    \"value\": [100, 200, 150, 300, 250]\n})\n\naggregated_df = (\n    df\n    .filter(ff.col(\"value\") &gt; 120, description='Filter on value greater then 120')\n    .with_columns([\n        (ff.col(\"value\") * 1.1).alias(\"adjusted_value\"),\n        ff.when(ff.col(\"category\") == \"A\").then(ff.lit(\"Premium\"))\n          .when(ff.col(\"category\") == \"B\").then(ff.lit(\"Standard\"))\n          .otherwise(ff.lit(\"Basic\")).alias(\"tier\")\n    ], description='Calculate the thier')\n    .group_by(\"tier\")\n    .agg([\n        ff.col(\"adjusted_value\").sum().alias(\"total_value\"),\n        ff.col(\"id\").count().alias(\"count\")\n    ])\n)\n\n# This will launch the Flowfile Designer UI and render your pipeline\nopen_graph_in_editor(aggregated_df.flow_graph)\n</code></pre> Generated Flow in Flowfile UI <p></p> <p>When you run <code>open_graph_in_editor(...)</code>, the Flowfile Designer UI will open and display a visual graph of your pipeline. You can:</p> <ul> <li>Inspect each transformation node</li> <li>Continue modifying your logic visually</li> <li>Share or export your pipeline</li> </ul>"},{"location":"users/python-api/tutorials/flowfile_frame_api.html#benefits-summary","title":"Benefits Summary","text":"<p>By combining the declarative power of a Polars-like API with Flowfile\u2019s interactive designer, <code>flowfile_frame</code> gives you:</p> <ul> <li>Code-first development with automatic visualization</li> <li>Zero-config ETL graph generation</li> <li>Easy debugging and collaboration</li> </ul>"},{"location":"users/visual-editor/index.html","title":"Visual Editor Guide","text":"<p>Build powerful data pipelines without writing code using Flowfile's intuitive drag-and-drop interface.</p>"},{"location":"users/visual-editor/index.html#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Build flows visually - Drag, drop, and connect nodes</li> <li>Transform data - Filter, aggregate, join, and more</li> <li>Connect to data sources - Databases, files, and cloud storage</li> <li>Preview results - See data at each step</li> <li>Export to code - Generate Python code from your visual flows</li> </ul>"},{"location":"users/visual-editor/index.html#getting-started","title":"Getting Started","text":""},{"location":"users/visual-editor/index.html#your-first-flow","title":"Your First Flow","text":"<ol> <li>Create a new flow - Click \"Create\" in the toolbar</li> <li>Add an input node - Drag a \"Read Data\" node from the left panel</li> <li>Configure the node - Click it and set file path in the right panel</li> <li>Add transformations - Connect filter, sort, or other nodes</li> <li>Run the flow - Click \"Run\" and see your results</li> </ol>"},{"location":"users/visual-editor/index.html#interface-overview","title":"Interface Overview","text":"<ul> <li>Left Panel: Node library organized by category</li> <li>Center Canvas: Build your flow here</li> <li>Right Panel: Configure selected nodes</li> <li>Bottom Panel: Preview data and logs</li> </ul>"},{"location":"users/visual-editor/index.html#core-concepts","title":"Core Concepts","text":""},{"location":"users/visual-editor/index.html#nodes","title":"Nodes","text":"<p>Each node represents a data operation:</p> <ul> <li>Input nodes - Load data from files, databases, APIs</li> <li>Transform nodes - Modify and clean your data</li> <li>Combine nodes - Join and merge datasets</li> <li>Aggregate nodes - Summarize and group data</li> <li>Output nodes - Save or export results</li> </ul>"},{"location":"users/visual-editor/index.html#connections","title":"Connections","text":"<p>Draw lines between nodes to define data flow. Data moves from top to bottom, left to right.</p>"},{"location":"users/visual-editor/index.html#execution","title":"Execution","text":"<ul> <li>Development mode - See data at every step (great for debugging)</li> <li>Performance mode - Optimized execution for large datasets</li> </ul>"},{"location":"users/visual-editor/index.html#learn-more","title":"Learn More","text":""},{"location":"users/visual-editor/index.html#more-resources","title":"More Resources","text":"<ul> <li>Building Flows - Detailed workflow guide</li> <li>Node Reference - Complete documentation of all nodes</li> </ul>"},{"location":"users/visual-editor/index.html#tutorials","title":"Tutorials","text":"<ul> <li>Connect to Databases - PostgreSQL, MySQL, and more</li> <li>Cloud Storage Setup - Work with S3 data</li> <li>Export to Python - Convert visual flows to code</li> </ul>"},{"location":"users/visual-editor/index.html#tips-for-success","title":"Tips for Success","text":"<ol> <li>Start simple - Build basic flows before adding complexity</li> <li>Use descriptions - Document nodes for your future self</li> <li>Preview often - Check data at each transformation</li> <li>Save regularly - Flows are saved as <code>.flowfile</code> files</li> <li>Try both modes - Development for testing, Performance for production</li> </ol>"},{"location":"users/visual-editor/index.html#visual-vs-code","title":"Visual vs Code","text":"<p>Wondering when to use visual vs Python? Here's a quick guide:</p> <p>Use Visual Editor when:</p> <ul> <li>Exploring new datasets</li> <li>Building one-off analyses</li> <li>Collaborating with non-technical users</li> <li>Creating documented workflows</li> <li>Learning data transformations</li> </ul> <p>Consider Python API when:</p> <ul> <li>Integrating with existing code</li> <li>Building programmatic pipelines</li> <li>Need version control</li> <li>Require advanced custom logic</li> <li>Automating workflows</li> </ul> <p>Remember, you can always switch between them!</p> <p>Ready to build? Start with Building Flows or explore the Node Reference.</p>"},{"location":"users/visual-editor/building-flows.html","title":"Building Flows","text":"<p>Flowfile allows you to create data pipelines visually by connecting nodes that represent different data operations. This guide will walk you through the process of creating and running flows.</p> <p>Looking for a quickstart overview?</p> <p>Check out our Quick Start Guide to get up and running in minutes.</p>"},{"location":"users/visual-editor/building-flows.html#interface-overview","title":"Interface Overview","text":"<p>The complete Flowfile interface showing:</p> <ul> <li>Left sidebar: Browse and select from available nodes </li> <li>Center canvas: Build your flow by arranging and connecting nodes</li> <li>Right sidebar: Configure node settings and parameters</li> <li>Bottom panel: Preview data at each step</li> </ul>"},{"location":"users/visual-editor/building-flows.html#creating-a-flow","title":"Creating a Flow","text":"<p> The Flowfile landing page when no flows are active, showing options to create a new flow or open an existing one </p>"},{"location":"users/visual-editor/building-flows.html#starting-a-new-flow","title":"Starting a New Flow","text":"<ol> <li>Click the Create button in the top toolbar</li> <li>A new empty canvas will open</li> <li>Save your flow at any time using the Save button</li> <li>Files are saved with the <code>.flowfile</code> extension</li> </ol>"},{"location":"users/visual-editor/building-flows.html#adding-nodes","title":"Adding Nodes","text":"<ol> <li>Browse nodes in the left sidebar, organized by category:<ul> <li>Input Sources (for loading data)</li> <li>Transformations (for modifying data)</li> <li>Combine Operations (for joining data)</li> <li>Aggregations (for summarizing data)</li> <li>Output Destinations (for saving data)</li> </ul> </li> <li>Drag any node onto the canvas</li> <li>Connect nodes to create a flow</li> </ol>"},{"location":"users/visual-editor/building-flows.html#configuring-nodes","title":"Configuring Nodes","text":""},{"location":"users/visual-editor/building-flows.html#node-settings","title":"Node Settings","text":"<p>Click any node on the canvas to open its settings in the right sidebar. Each node type has unique configuration options tailored to its function.</p> <p>For example, the \"Formula\" node shown here includes sections for:</p> <ul> <li>\ud83c\udf9b\ufe0f General: Add a custom description via general settings</li> <li>\u2699\ufe0f Performance tweaking: Define if the data needs to be cached for better performance via general settings</li> <li>\u2194\ufe0f Transformations: Define the formula to be applied on the incoming data </li> </ul> <p>The settings panel for a \"Formula\" node.</p>"},{"location":"users/visual-editor/building-flows.html#data-preview","title":"Data Preview","text":"<ol> <li>After configuration, each node shows the output schema of the action</li> <li>Click on the run button to execute the node</li> <li>The preview panel will show the output data</li> </ol>"},{"location":"users/visual-editor/building-flows.html#running-your-flow","title":"Running Your Flow","text":""},{"location":"users/visual-editor/building-flows.html#1-execution-options","title":"1. Execution Options","text":"<p>Choose your execution mode from the settings panel:</p> <ul> <li>Development: Lets you view the data in every step of the process, at the cost of performance</li> <li>Performance: Only executes steps needed for the output (e.g., writing data), allowing for query optimizations and better performance</li> </ul>"},{"location":"users/visual-editor/building-flows.html#2-running-the-flow","title":"2. Running the Flow","text":"<ol> <li>Click the Run button in the top toolbar</li> <li>Watch the execution progress:<ul> <li>\ud83d\udfe2 Green: Success</li> <li>\ud83d\udd34 Red: Error</li> <li>\ud83d\udfe1 Yellow: Warning</li> <li>\u26aa White: Not executed</li> </ul> </li> </ol>"},{"location":"users/visual-editor/building-flows.html#3-viewing-results","title":"3. Viewing Results","text":"<ol> <li>Click any node after execution to see its output data</li> <li>Review the results in the preview panel</li> <li>Check for any errors or warnings</li> <li>Export results using output nodes</li> </ol>"},{"location":"users/visual-editor/building-flows.html#example-flow","title":"Example Flow","text":"<p>Here's a typical flow that demonstrates common operations: </p>"},{"location":"users/visual-editor/building-flows.html#best-practices","title":"Best Practices","text":""},{"location":"users/visual-editor/building-flows.html#organization","title":"Organization","text":"<ul> <li>Give your flows clear, descriptive names</li> <li>Arrange nodes logically from left to right</li> <li>Group related operations together</li> <li>Use comments or node labels for documentation</li> </ul>"},{"location":"users/visual-editor/building-flows.html#development","title":"Development","text":"<ul> <li>Save your work frequently</li> <li>Test with a subset of data first</li> <li>Use the auto-run mode during development</li> <li>Break complex flows into smaller, manageable parts</li> </ul>"},{"location":"users/visual-editor/building-flows.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Check node configurations if errors occur</li> <li>Review data previews to understand transformations</li> <li>Ensure data types match between connected nodes</li> <li>Look for error messages in node status</li> </ul>"},{"location":"users/visual-editor/building-flows.html#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li> <p>Node Management:</p> <ul> <li>Double-click canvas to pan</li> <li>Use mouse wheel to zoom</li> <li>Hold Shift to select multiple nodes</li> <li>Right-click for context menu</li> <li>Right-click on the text to add notes</li> </ul> </li> <li> <p>Data Handling:</p> <ul> <li>Use sample nodes during development</li> <li>Preview data frequently</li> <li>Check column types early with select nodes</li> </ul> </li> </ul>"},{"location":"users/visual-editor/building-flows.html#want-to-see-another-example","title":"Want to see another example?","text":"<p>Checkout the quickstart guide!</p>"},{"location":"users/visual-editor/building-flows.html#next-steps","title":"Next Steps","text":"<p>After mastering basic flows, explore:</p> <ul> <li>Input sources</li> <li>Complex transformations</li> <li>Data aggregation techniques</li> <li>Advanced joining methods</li> <li>Output options</li> </ul>"},{"location":"users/visual-editor/nodes/index.html","title":"Nodes Overview","text":"<p>Flowfile's nodes are the building blocks of your data pipeline. Each node performs a specific operation, allowing you to load, transform, combine, summarize, and output data without writing code.  </p> <p>By connecting nodes together, you can create flexible and scalable workflows to process data efficiently.  </p>"},{"location":"users/visual-editor/nodes/index.html#node-categories","title":"Node Categories","text":"<p>Flowfile nodes are grouped into five main categories:  </p> <ul> <li>Input Nodes \u2013 Load data from files, databases, or external sources.  </li> <li>Transform Nodes \u2013 Modify, clean, and reshape your data.  </li> <li>Combine Nodes \u2013 Merge, join, or match multiple datasets.  </li> <li>Aggregate Nodes \u2013 Summarize, group, and compute metrics.  </li> <li>Output Nodes \u2013 Save data to files or explore it interactively.  </li> </ul>"},{"location":"users/visual-editor/nodes/index.html#how-nodes-work","title":"How Nodes Work","text":"<p>Each node is designed to be intuitive and consistent, ensuring a smooth workflow experience:  </p> <p>\u2714 Clear inputs and outputs \u2013 Easily understand what data flows into and out of each node. \u2714 Visual feedback \u2013 Monitor the data transformation process. \u2714 Preview capability \u2013 Inspect results before applying changes. \u2714 Error handling \u2013 Validate and debug data issues efficiently.  </p> <p>Flowfile\u2019s node-based approach makes data processing fast, flexible, and code-free, helping you build powerful data pipelines with ease.</p>"},{"location":"users/visual-editor/nodes/aggregate.html","title":"Aggregate Nodes","text":"<p>Aggregate nodes help you summarize and analyze your data by grouping and calculating statistics. These nodes are essential for creating summaries and transforming data structure.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#node-details","title":"Node Details","text":""},{"location":"users/visual-editor/nodes/aggregate.html#group-by","title":"Group By","text":"<p>The Group By node aggregates data based on selected columns, allowing calculations such as sums, averages, counts, and more.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#key-features","title":"Key Features","text":"<ul> <li>Group by one or more columns  </li> <li>Apply aggregation functions to other columns  </li> <li>Rename output columns  </li> </ul>"},{"location":"users/visual-editor/nodes/aggregate.html#usage","title":"Usage","text":"<ol> <li>Select one or more columns to group by.  </li> <li>Choose aggregation functions for other columns.  </li> <li>Set custom output column names if needed.  </li> </ol>"},{"location":"users/visual-editor/nodes/aggregate.html#configuration-options","title":"Configuration Options","text":"Parameter Description Group By Columns Columns used to define groups. Aggregations Functions like <code>sum</code>, <code>count</code>, <code>avg</code>, <code>min</code>, <code>max</code>. Output Column Name Custom name for the aggregated result (optional). <p>This node is essential for summarizing datasets and preparing structured outputs.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#pivot-data","title":"Pivot Data","text":"<p>The Pivot Data node converts data from a long format to a wide format by creating new columns based on unique values in a pivot column.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#key-features_1","title":"Key Features","text":"<ul> <li>Transform long format into wide format  </li> <li>Select multiple index columns  </li> <li>Aggregate values during pivoting  </li> </ul>"},{"location":"users/visual-editor/nodes/aggregate.html#usage_1","title":"Usage","text":"<ol> <li>Select index columns to retain in the final output.  </li> <li>Choose a pivot column whose unique values will become new columns.  </li> <li>Select a value column containing the data to fill the new columns.  </li> <li>Apply aggregation functions (e.g., <code>sum</code>, <code>count</code>, <code>avg</code>) if needed.  </li> </ol>"},{"location":"users/visual-editor/nodes/aggregate.html#configuration-options_1","title":"Configuration Options","text":"Parameter Description Index Columns Columns that define the groups in the final table. Pivot Column Unique values from this column become new column names. Value Column The column containing values to be placed in the new columns. Aggregations Functions applied when multiple values exist per pivot column entry. <p>This node is useful for restructuring datasets into a summary-friendly format.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#unpivot-data","title":"Unpivot Data","text":"<p>The Unpivot Data node transforms data from wide format to long format, making it easier for analysis and reporting.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#key-features_2","title":"Key Features","text":"<ul> <li>Convert multiple columns into key-value pairs  </li> <li>Select index columns to retain  </li> <li>Use dynamic data type selection  </li> </ul>"},{"location":"users/visual-editor/nodes/aggregate.html#usage_2","title":"Usage","text":"<ol> <li>Select index columns to keep unchanged.  </li> <li>Choose value columns to transform into key-value pairs.  </li> <li>(Optional) Enable dynamic data type selection to filter columns automatically.  </li> </ol>"},{"location":"users/visual-editor/nodes/aggregate.html#configuration-options_2","title":"Configuration Options","text":"Parameter Description Index Columns Columns that remain unchanged in the final structure. Value Columns Columns that will be unpivoted into key-value pairs. Data Type Selector Automatically select columns based on data type (e.g., <code>string</code>). Selection Mode Choose between <code>column</code> or <code>data_type</code> for unpivot selection. <p>This node helps in restructuring datasets, especially when working with reporting or analytical tools.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#count-records","title":"Count Records","text":"<p>The Count Records node calculates the total number of rows in the dataset.</p>"},{"location":"users/visual-editor/nodes/aggregate.html#key-features_3","title":"Key Features","text":"<ul> <li>Simple row count operation  </li> <li>No configuration required  </li> <li>Adds a new column <code>number_of_records</code> </li> </ul>"},{"location":"users/visual-editor/nodes/aggregate.html#usage_3","title":"Usage","text":"<ol> <li>Add the Count Records node to your workflow.  </li> <li>It will automatically count the total number of rows.  </li> </ol>"},{"location":"users/visual-editor/nodes/aggregate.html#configuration-options_3","title":"Configuration Options","text":"<p>This node has no additional settings\u2014it simply returns the record count.</p> <p>This transformation is useful for quick dataset validation and workflow monitoring.</p> <p>\u2190 Combine data | Next: Write data \u2192</p>"},{"location":"users/visual-editor/nodes/combine.html","title":"Combine Nodes","text":"<p>Combine nodes allow you to merge multiple datasets in different ways, enabling data integration and enrichment. These nodes help in aligning, linking, and structuring data from various sources to create a unified dataset.  </p> <p>Depending on the method used, datasets can be merged by matching values, stacking rows, finding similar records, generating all possible combinations, or grouping related elements in a network.  </p> <p>These transformations are essential for tasks like data preparation, consolidation, and relationship mapping across datasets.</p>"},{"location":"users/visual-editor/nodes/combine.html#node-details","title":"Node Details","text":""},{"location":"users/visual-editor/nodes/combine.html#join","title":"Join","text":"<p>The Join node merges two datasets based on matching values in selected columns.</p>"},{"location":"users/visual-editor/nodes/combine.html#key-features","title":"Key Features","text":"<ul> <li>Supports multiple join types: Inner, Left, Right, Outer </li> <li>Join on one or more columns </li> <li>Handles duplicate column names with automatic renaming  </li> </ul>"},{"location":"users/visual-editor/nodes/combine.html#usage","title":"Usage","text":"<ol> <li>Connect two input datasets (left and right).  </li> <li>Select join type (<code>inner</code>, <code>left</code>, <code>right</code>, <code>anti</code> or <code>outer</code>).  </li> <li>Choose columns to join on.  </li> <li>Select which columns to keep from each dataset.  </li> </ol>"},{"location":"users/visual-editor/nodes/combine.html#configuration-options","title":"Configuration Options","text":"Parameter Description Join Type Choose <code>inner</code>, <code>left</code>, <code>right</code>, <code>anti</code> or <code>outer</code> join. Join Columns Columns used to match records between datasets. <p>This node is useful for merging related datasets, such as combining customer data with orders or linking product details with inventory.</p>"},{"location":"users/visual-editor/nodes/combine.html#fuzzy-match","title":"Fuzzy Match","text":"<p>The Fuzzy Match node joins datasets based on similar values instead of exact matches, using various matching algorithms.</p>"},{"location":"users/visual-editor/nodes/combine.html#key-features_1","title":"Key Features","text":"<ul> <li>Supports fuzzy matching algorithms (e.g., Levenshtein)  </li> <li>Configurable similarity threshold </li> <li>Calculates match scores </li> <li>Joins datasets based on approximate values  </li> </ul>"},{"location":"users/visual-editor/nodes/combine.html#usage_1","title":"Usage","text":"<ol> <li>Connect two datasets (left and right).  </li> <li>Select columns to match on.  </li> <li>Choose a fuzzy matching algorithm.  </li> <li>Set a similarity threshold (e.g., 75%).  </li> </ol>"},{"location":"users/visual-editor/nodes/combine.html#configuration-options_1","title":"Configuration Options","text":"Parameter Description Join Columns Columns used for fuzzy matching. Fuzzy Algorithm Choose an algorithm (e.g., <code>Levenshtein</code>). Threshold Score Minimum similarity score for a match (0-100). <p>This node is useful for handling typos, name variations, and inconsistent formatting when merging datasets.</p>"},{"location":"users/visual-editor/nodes/combine.html#union-data","title":"Union Data","text":"<p>The Union Data node merges multiple datasets by stacking rows together.</p>"},{"location":"users/visual-editor/nodes/combine.html#key-features_2","title":"Key Features","text":"<ul> <li>Combines multiple datasets into one  </li> <li>Automatically aligns columns based on names  </li> <li>Uses diagonal relaxed mode, allowing flexible column matching  </li> </ul>"},{"location":"users/visual-editor/nodes/combine.html#usage_2","title":"Usage","text":"<ol> <li>Connect multiple input datasets.  </li> <li>The node will automatically align and stack the data.  </li> </ol> <p>This node is useful for combining similar datasets, such as monthly reports or regional data.</p>"},{"location":"users/visual-editor/nodes/combine.html#cross-join","title":"Cross Join","text":"<p>The Cross Join node creates all possible combinations between two datasets.</p>"},{"location":"users/visual-editor/nodes/combine.html#key-features_3","title":"Key Features","text":"<ul> <li>Generates a Cartesian product of two datasets  </li> <li>Automatically aligns columns  </li> <li>Handles duplicate column names  </li> </ul>"},{"location":"users/visual-editor/nodes/combine.html#usage_3","title":"Usage","text":"<ol> <li>Connect two datasets (left and right).</li> <li>Select the columns that you would like to keep and their output names</li> <li>The node will generate all possible row combinations.  </li> </ol> <p>This node is useful for creating test scenarios, generating all possible product combinations, or building comparison matrices.</p>"},{"location":"users/visual-editor/nodes/combine.html#graph-solver","title":"Graph Solver","text":"<p>The Graph Solver node groups related records based on connections in a graph-structured dataset.</p>"},{"location":"users/visual-editor/nodes/combine.html#key-features_4","title":"Key Features","text":"<ul> <li>Identifies connected components in graph-like data  </li> <li>Groups related nodes into the same category  </li> <li>Supports custom output column names </li> </ul>"},{"location":"users/visual-editor/nodes/combine.html#usage_4","title":"Usage","text":"<ol> <li>Select From and To columns to define relationships.  </li> <li>The node assigns a group identifier to connected nodes.  </li> </ol>"},{"location":"users/visual-editor/nodes/combine.html#configuration-options_2","title":"Configuration Options","text":"Parameter Description From Column Defines the starting point of each connection. To Column Defines the endpoint of each connection. Output Column Stores the assigned group identifier. <p>This node is useful for detecting dependencies, clustering related entities, and analyzing network connections.</p> <p>\u2190 Transform data | Next: Aggregate data \u2192</p>"},{"location":"users/visual-editor/nodes/input.html","title":"Input Nodes","text":"<p>Input nodes are the starting point for any data flow. Flowfile currently supports reading from local files, cloud storage (S3), and manual input.</p>"},{"location":"users/visual-editor/nodes/input.html#node-details","title":"Node Details","text":""},{"location":"users/visual-editor/nodes/input.html#read-data","title":"Read Data","text":"<p>The Read Data node allows you to load local data into your flow. It currently supports CSV, Excel, and Parquet file formats, each with specific configuration options.</p>"},{"location":"users/visual-editor/nodes/input.html#supported-formats","title":"Supported Formats:","text":"<ul> <li>CSV files (<code>.csv</code>)</li> <li>Excel files (<code>.xlsx</code>, <code>.xls</code>)</li> <li>Parquet files (<code>.parquet</code>)</li> </ul>"},{"location":"users/visual-editor/nodes/input.html#usage","title":"Usage:","text":"<ol> <li>Select your input file.  </li> <li>Configure any format-specific options.  </li> <li>Preview and confirm your data.  </li> </ol>"},{"location":"users/visual-editor/nodes/input.html#csv","title":"CSV","text":"<p>When a CSV file is selected, the following setup options are available:  </p> Parameter Description Has Headers Determines whether the first row is used as headers. If <code>\"yes\"</code>, the first row is treated as column names. If <code>\"no\"</code>, default column names like <code>\"Column 1, Column 2, ...\"</code> are assigned. Delimiter Specifies the character used to separate values (e.g., comma <code>,</code>, semicolon <code>;</code>, tab <code>\\t</code>). Encoding Defines the file encoding (e.g., <code>UTF-8</code>, <code>ISO-8859-1</code>). Quote Character Character used to enclose text fields, preventing delimiter conflicts (e.g., <code>\"</code>, <code>'</code>). New Line Delimiter Specifies how new lines are detected (e.g., <code>\\n</code>, <code>\\r\\n</code>). Schema Infer Length Determines how many rows are scanned to infer column types. Truncate Long Lines If enabled, long lines are truncated instead of causing errors. Ignore Errors If enabled, the process continues even if some rows cause errors."},{"location":"users/visual-editor/nodes/input.html#excel","title":"Excel","text":"<p>When an Excel file is selected, you can specify the sheet, select specific rows and columns, and configure headers and type inference options to tailor data loading to your needs.</p> Parameter Description Sheet Name The name of the sheet to be read. If not specified, the first sheet is used. Start Row The row index (zero-based) from which reading starts. Default is <code>0</code> (beginning of the sheet). Start Column The column index (zero-based) from which reading starts. Default is <code>0</code> (first column). End Row The row index (zero-based) at which reading stops. Default is <code>0</code> (read all rows). End Column The column index (zero-based) at which reading stops. Default is <code>0</code> (read all columns). Has Headers Determines whether the first row is treated as headers. If <code>true</code>, the first row is used as column names. If <code>false</code>, default column names are assigned. Type Inference If <code>true</code>, the engine attempts to infer data types. If <code>false</code>, data types are not automatically inferred."},{"location":"users/visual-editor/nodes/input.html#parquet","title":"Parquet","text":"<p>When a Parquet file is selected, no additional setup options are required. Parquet is a columnar storage format optimized for efficiency and performance. It retains schema information and data types, enabling faster reads and writes without manual configuration.</p>"},{"location":"users/visual-editor/nodes/input.html#cloud-storage-reader","title":"Cloud Storage Reader","text":"<p>The Cloud Storage Reader node allows you to read data directly from AWS S3.</p> Screenshot: Cloud Storage Reader Configuration <p></p>"},{"location":"users/visual-editor/nodes/input.html#connection-options","title":"Connection Options:","text":"<ul> <li>Use existing S3 connections configured in your workspace (see Manage Cloud Connections)</li> <li>Use local AWS CLI credentials or environment variables</li> </ul>"},{"location":"users/visual-editor/nodes/input.html#file-settings","title":"File Settings:","text":"Parameter Description File Path Path to the file or directory (e.g., <code>bucket-name/folder/file.csv</code>) File Format Supported formats: CSV, Parquet, JSON, Delta Lake Scan Mode Single file or directory scan (reads all matching files in a directory)"},{"location":"users/visual-editor/nodes/input.html#format-specific-options","title":"Format-Specific Options:","text":"<p>CSV Options: - Has Headers: First row contains column headers - Delimiter: Character separating values (default: <code>,</code>) - Encoding: File encoding (UTF-8 or UTF-8 Lossy)</p> <p>Delta Lake Options: - Version: Specify a specific version to read (optional, defaults to latest)</p>"},{"location":"users/visual-editor/nodes/input.html#manual-input","title":"Manual Input","text":"<p>The Manual Input node allows you to create data directly within Flowfile or paste data from your clipboard.</p> <p>\u2190 Node overview | Next: Transform data \u2192</p>"},{"location":"users/visual-editor/nodes/output.html","title":"Output Nodes","text":"<p>Output nodes represent the final steps in your data pipeline, allowing you to save your transformed data or explore it visually. These nodes help you deliver your results in the desired format or analyze them directly.</p>"},{"location":"users/visual-editor/nodes/output.html#node-details","title":"Node Details","text":""},{"location":"users/visual-editor/nodes/output.html#node-details_1","title":"Node Details","text":""},{"location":"users/visual-editor/nodes/output.html#write-data","title":"Write Data","text":"<p>The Write Data node allows you to save your processed data in different formats. It supports CSV, Excel, and Parquet, each with specific configuration options.  </p>"},{"location":"users/visual-editor/nodes/output.html#supported-formats","title":"Supported Formats","text":"<ul> <li>CSV files (<code>.csv</code>)  </li> <li>Excel files (<code>.xlsx</code>)  </li> <li>Parquet files (<code>.parquet</code>)  </li> </ul>"},{"location":"users/visual-editor/nodes/output.html#usage","title":"Usage","text":"<ol> <li>Configure the output file path.  </li> <li>Select the file format.  </li> <li>Set writing options (e.g., delimiter, compression).  </li> </ol>"},{"location":"users/visual-editor/nodes/output.html#csv","title":"CSV","text":"<p>When a CSV file is selected, the following setup options are available:  </p> Parameter Description Delimiter Specifies the character used to separate values (default: <code>,</code>). Encoding Defines the file encoding (default: <code>UTF-8</code>). Write Mode Determines how the file is saved (<code>overwrite</code>, <code>new file</code> or <code>append</code>)."},{"location":"users/visual-editor/nodes/output.html#excel","title":"Excel","text":"<p>When an Excel file is selected, additional configurations allow customizing the output.</p> Parameter Description Sheet Name Name of the sheet where data will be written (default: <code>Sheet1</code>). Write Mode Determines how the file is saved (<code>overwrite</code> or <code>new file</code>)."},{"location":"users/visual-editor/nodes/output.html#parquet","title":"Parquet","text":"<p>When a Parquet file is selected, no additional setup options are required. Parquet is a columnar storage format, optimized for efficient reading and writing.</p> Parameter Description Write Mode Determines how the file is saved (<code>overwrite</code> or <code>new file</code>)."},{"location":"users/visual-editor/nodes/output.html#general-configuration-options","title":"General Configuration Options","text":"Parameter Description File Path Directory and filename for the output file. File Format Selects the output format (<code>CSV</code>, <code>Excel</code>, <code>Parquet</code>). Overwrite Mode Controls whether to replace or append data. When <code>new file</code> is selected it will throw an error when the file already exists <p>This node ensures that your transformed data is saved in the correct format, ready for further use or analysis.</p>"},{"location":"users/visual-editor/nodes/output.html#cloud-storage-writer","title":"Cloud Storage Writer","text":"<p>The Cloud Storage Writer node allows you to save your processed data directly to cloud storage services like AWS S3.</p> Screenshot: Cloud Storage Writer Configuration <p></p>"},{"location":"users/visual-editor/nodes/output.html#connection-options","title":"Connection Options:","text":"<ul> <li>Use existing cloud storage connections configured in your workspace (see Manage Cloud Connections)</li> <li>Use local AWS CLI credentials or environment variables for authentication</li> </ul>"},{"location":"users/visual-editor/nodes/output.html#file-settings","title":"File Settings:","text":"Parameter Description File Path Full path including bucket/container and file name (e.g., <code>bucket-name/folder/output.parquet</code>) File Format Supported formats: CSV, Parquet, JSON, Delta Lake Write Mode <code>overwrite</code> (replace existing) or <code>append</code> (Delta Lake only)"},{"location":"users/visual-editor/nodes/output.html#format-specific-options","title":"Format-Specific Options:","text":"<p>CSV Options: - Delimiter: Character to separate values (default: <code>,</code>) - Encoding: File encoding (UTF-8 or UTF-8 Lossy)</p> <p>Parquet Options: - Compression: Choose from Snappy (default), Gzip, Brotli, LZ4, or Zstd</p> <p>Delta Lake Options: - Supports both <code>overwrite</code> and <code>append</code> write modes - Automatically handles schema evolution when appending</p> <p>Overwrite Mode</p> <p>When using <code>overwrite</code> mode, any existing file or data at the target path will be replaced. Make sure to verify the path before executing.</p> <p>Append Mode</p> <p>Available only for Delta Lake format.</p>"},{"location":"users/visual-editor/nodes/output.html#explore-data","title":"Explore Data","text":"<p>The Explore Data node provides interactive data exploration and analysis capabilities.</p>"},{"location":"users/visual-editor/nodes/transform.html","title":"Transform Nodes","text":"<p>Transform nodes modify and shape your data. These nodes handle everything from basic operations like filtering and sorting to more complex transformations like custom formulas and text manipulation.</p>"},{"location":"users/visual-editor/nodes/transform.html#node-details","title":"Node Details","text":""},{"location":"users/visual-editor/nodes/transform.html#add-record-id","title":"Add Record ID","text":"<p>The Add Record ID transformation generates a unique identifier for each record in your dataset. You can create a simple sequential ID or generate grouped IDs based on one or more columns.</p>"},{"location":"users/visual-editor/nodes/transform.html#usage","title":"Usage:","text":"<ol> <li>Add the Add Record ID node to your flow.  </li> <li>Configure the settings:</li> <li>Define the output column name.</li> <li>Set an optional offset for ID numbering.</li> <li>(Optional) Enable grouping and specify grouping columns.  </li> <li>Apply the transformation.  </li> </ol>"},{"location":"users/visual-editor/nodes/transform.html#configuration-options","title":"Configuration Options","text":"Parameter Description Output Column Name Name of the new column where the record ID will be stored. Default is <code>\"record_id\"</code>. Offset Starting value for the record ID. Default is <code>1</code>. Group By If <code>true</code>, record IDs are assigned within groups instead of sequentially across all records. Default is <code>false</code>. Group By Columns List of columns to group by when assigning record IDs. Only applies when Group By is enabled."},{"location":"users/visual-editor/nodes/transform.html#behavior","title":"Behavior","text":"<ul> <li>Sequential Record ID (Default)  </li> <li>A new column is added with a simple incremental ID starting from the defined offset.  </li> <li>Grouped Record ID </li> <li>When grouping is enabled, the record ID resets within each group based on the specified columns.  </li> </ul> <p>This transformation helps in creating unique keys, tracking row order, or structuring data for downstream processing.</p>"},{"location":"users/visual-editor/nodes/transform.html#formula","title":"Formula","text":"<p>The Formula node allows you to create new columns or modify existing ones using custom expressions. It supports a wide range of operations, including mathematical calculations, string manipulations, and conditional logic.</p>"},{"location":"users/visual-editor/nodes/transform.html#key-features","title":"Key Features","text":"<ul> <li>Create new columns dynamically  </li> <li>Modify existing columns using expressions  </li> <li>Perform mathematical operations (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>)  </li> <li>Apply string functions (<code>concat</code>, <code>uppercase</code>, <code>lowercase</code>)  </li> <li>Use conditional logic</li> <li>Use date time transformations</li> </ul>"},{"location":"users/visual-editor/nodes/transform.html#usage_1","title":"Usage","text":"<ol> <li>Drag the Formula node onto your canvas.  </li> <li>Connect input data.  </li> <li>Write your formula expression.  </li> <li>Preview the results.  </li> <li>Configure column names and data types.  </li> </ol>"},{"location":"users/visual-editor/nodes/transform.html#configuration-options_1","title":"Configuration Options","text":"Parameter Description Column Name The name of the new or modified column. Formula The expression used to compute values for the column. Data Type The expected data type of the resulting column (optional)."},{"location":"users/visual-editor/nodes/transform.html#behavior_1","title":"Behavior","text":"<ul> <li>If a new column is created, it will be added to the dataset.  </li> <li>If an existing column is modified, its values will be updated based on the formula.  </li> <li>If no data type is specified, the result defaults to <code>String</code>.  </li> </ul> <p>This transformation is useful for feature engineering, data cleaning, and enriching datasets with computed values.</p>"},{"location":"users/visual-editor/nodes/transform.html#select-data","title":"Select Data","text":"<p>The Select Data node allows you to choose which columns to keep, rename, and reorder. This transformation is useful for refining datasets, dropping unnecessary fields, and ensuring a structured column layout.</p>"},{"location":"users/visual-editor/nodes/transform.html#key-features_1","title":"Key Features","text":"<ul> <li>Select specific columns to retain in the dataset  </li> <li>Reorder columns by dragging them into the desired order or by ordering them alphabetically</li> <li>Rename columns to provide meaningful names  </li> <li>Automatically remove missing fields </li> </ul>"},{"location":"users/visual-editor/nodes/transform.html#configuration-options_2","title":"Configuration Options","text":"Parameter Description Column Selection Choose which columns to keep in the dataset. Reordering Drag and drop to change the column order. Rename Column Assign a new name to any selected column. Keep Missing Fields If enabled, columns that are missing from input data are retained in the selection list."},{"location":"users/visual-editor/nodes/transform.html#behavior_2","title":"Behavior","text":"<ul> <li>If a selected column is missing from the input, it is marked as unavailable.  </li> <li>Columns can be renamed without affecting their original data.  </li> <li>Changing the order affects how the columns appear in downstream processing.  </li> </ul> <p>This transformation ensures that datasets are structured efficiently before further analysis or processing.</p>"},{"location":"users/visual-editor/nodes/transform.html#filter-data","title":"Filter Data","text":"<p>The Filter Data node keeps only rows that match a specified condition. Enter a formula that evaluates to <code>true</code> or <code>false</code>, and only <code>true</code> rows remain.</p>"},{"location":"users/visual-editor/nodes/transform.html#key-features_2","title":"Key Features","text":"<ul> <li>Apply custom conditions to filter data  </li> <li>Use operators (<code>or</code>, <code>and</code>, <code>&lt;</code>,)  </li> <li>Support for string, numeric, and date filtering </li> </ul>"},{"location":"users/visual-editor/nodes/transform.html#usage_2","title":"Usage","text":"<ol> <li>Drag the Filter Data node onto your canvas.  </li> <li>Connect input data.  </li> <li>Enter a filter formula (e.g., <code>[City] = 'Amsterdam'</code>).  </li> <li>Apply the filter to keep matching rows.  </li> </ol>"},{"location":"users/visual-editor/nodes/transform.html#example-filters","title":"Example Filters","text":"Expression Description <code>[City] = 'Amsterdam'</code> Keep rows where <code>City</code> is \"Amsterdam\". <code>[Age] &gt; 30</code> Keep rows where <code>Age</code> is greater than 30. <code>[Country] = 'USA' &amp;&amp;[Sales] &gt; 100</code> Keep rows where <code>Country</code> is \"USA\" and <code>Sales</code> is over 100. <p>Use this node to refine datasets efficiently.</p>"},{"location":"users/visual-editor/nodes/transform.html#sort-data","title":"Sort Data","text":"<p>The Sort Data node orders your data based on one or more columns.</p>"},{"location":"users/visual-editor/nodes/transform.html#key-features_3","title":"Key Features","text":"<ul> <li>Sort by multiple columns  </li> <li>Choose ascending or descending order</li> </ul>"},{"location":"users/visual-editor/nodes/transform.html#usage_3","title":"Usage","text":"<ol> <li>Select columns to sort by.  </li> <li>Choose sort direction (ascending or descending) for each column.  </li> </ol>"},{"location":"users/visual-editor/nodes/transform.html#configuration-options_3","title":"Configuration Options","text":"Parameter Description Sort Columns Columns used to sort the dataset. Sort Order Set ascending (<code>asc</code>) or descending (<code>desc</code>). <p>This node ensures structured and ordered data for better analysis.</p>"},{"location":"users/visual-editor/nodes/transform.html#take-sample","title":"Take Sample","text":"<p>The Take Sample node lets you work with a subset of your data.</p>"},{"location":"users/visual-editor/nodes/transform.html#drop-duplicates","title":"Drop Duplicates","text":"<p>The Drop Duplicates node removes duplicate rows based on selected columns. Only the first occurrence is kept by default.</p>"},{"location":"users/visual-editor/nodes/transform.html#key-features_4","title":"Key Features","text":"<ul> <li>Remove duplicate rows  </li> <li>Select columns to check for duplicates  </li> </ul>"},{"location":"users/visual-editor/nodes/transform.html#usage_4","title":"Usage","text":"<ol> <li>Select columns to check for duplicates.  </li> <li>Choose whether to keep the first or last occurrence.  </li> </ol>"},{"location":"users/visual-editor/nodes/transform.html#configuration-options_4","title":"Configuration Options","text":"Parameter Description Columns Columns used to check for duplicates. <p>This node ensures a clean dataset by eliminating redundant rows.</p>"},{"location":"users/visual-editor/nodes/transform.html#text-to-rows","title":"Text to Rows","text":"<p>The Text to Rows node splits text from a selected column into multiple rows based on a delimiter.</p>"},{"location":"users/visual-editor/nodes/transform.html#key-features_5","title":"Key Features","text":"<ul> <li>Split a column into multiple rows  </li> <li>Use a fixed delimiter (e.g., <code>,</code>, <code>;</code>, <code>|</code>)  </li> <li>Split using values from another column  </li> </ul>"},{"location":"users/visual-editor/nodes/transform.html#usage_5","title":"Usage","text":"<ol> <li>Select the column to split.  </li> <li>Choose a delimiter or use another column for splitting.  </li> <li>(Optional) Set an output column name. </li> </ol>"},{"location":"users/visual-editor/nodes/transform.html#configuration-options_5","title":"Configuration Options","text":"Parameter Description Column to Split The column containing text to be split. Output Column Name Name of the new column after splitting (defaults to the original column). Split by Fixed Value If <code>true</code>, use a fixed delimiter (default: <code>,</code>). Delimiter The character used to split text (e.g., <code>,</code>, <code>|</code>, <code>;</code>). Split by Column Instead of a fixed delimiter, use values from another column. <p>This transformation helps normalize datasets by converting text lists into structured rows.</p>"},{"location":"users/visual-editor/nodes/transform.html#polars-code","title":"Polars Code","text":"<p>The Polars Code node allows you to write custom Polars DataFrame transformations directly in your workflow.</p>"},{"location":"users/visual-editor/nodes/transform.html#key-features_6","title":"Key Features","text":"<ul> <li>Write custom Polars expressions </li> <li>Apply advanced transformations not covered by standard nodes  </li> <li>Filter, aggregate, or modify data using Polars API </li> </ul>"},{"location":"users/visual-editor/nodes/transform.html#usage_6","title":"Usage","text":"<ol> <li>Write a single-line or multi-line Polars expression.  </li> <li>Use <code>input_df</code> as the DataFrame reference.  </li> <li>Assign results to <code>output_df</code> for multi-line operations.  </li> </ol>"},{"location":"users/visual-editor/nodes/transform.html#example-code","title":"Example Code","text":""},{"location":"users/visual-editor/nodes/transform.html#single-line-transformation","title":"Single-line transformation","text":"<pre><code>input_df.filter(pl.col('Age') &gt; 30)\n</code></pre>"},{"location":"users/visual-editor/nodes/transform.html#multi-line-transformation","title":"Multi-line transformation","text":"<pre><code>result = input_df.select(['Name', 'City'])\nfiltered = result.filter(pl.col('City') == 'Amsterdam')\noutput_df = filtered.with_columns(pl.col('Name').alias('Customer_Name')) # this will be the output of the node\n</code></pre> <p>\u2190 Read data | Next: Combine data \u2192</p>"},{"location":"users/visual-editor/tutorials/index.html","title":"Visual Editor Tutorials","text":"<p>Hands-on guides for building real-world data pipelines with the visual editor.</p>"},{"location":"users/visual-editor/tutorials/index.html#available-tutorials","title":"Available Tutorials","text":""},{"location":"users/visual-editor/tutorials/index.html#database-connectivity","title":"Database Connectivity","text":"<p>Learn how to connect to PostgreSQL databases, read data, apply transformations, and write results back. Perfect for building ETL pipelines with databases like Supabase.</p> <p>You'll learn:</p> <ul> <li>Setting up database connections</li> <li>Reading from tables</li> <li>Transforming data visually</li> <li>Writing results back to database</li> </ul>"},{"location":"users/visual-editor/tutorials/index.html#cloud-storage-setup","title":"Cloud Storage Setup","text":"<p>Connect to AWS S3 and other cloud storage services to read and write data directly from the cloud.</p> <p>You'll learn:</p> <ul> <li>Creating S3 connections</li> <li>Reading cloud data</li> <li>Configuring authentication</li> <li>Writing results to cloud storage</li> </ul>"},{"location":"users/visual-editor/tutorials/index.html#export-to-python-code","title":"Export to Python Code","text":"<p>Convert your visual pipelines into clean, executable Python code for deployment or version control.</p> <p>You'll learn: - How code generation works - Exporting visual flows - Understanding generated code - Deploying pipelines</p>"},{"location":"users/visual-editor/tutorials/index.html#tutorial-format","title":"Tutorial Format","text":"<p>Each tutorial includes: - Prerequisites - What you need before starting - Step-by-step instructions - With screenshots - Best practices - Tips for production use - Troubleshooting - Common issues and solutions</p>"},{"location":"users/visual-editor/tutorials/index.html#suggested-learning-path","title":"Suggested Learning Path","text":"<ol> <li>Start with Database Connectivity if you work with databases</li> <li>Move to Cloud Storage for S3 and cloud data</li> <li>Learn Export to Code when ready to deploy</li> </ol>"},{"location":"users/visual-editor/tutorials/index.html#coming-soon","title":"Coming Soon","text":"<ul> <li>Real-time data processing</li> <li>API integrations</li> <li>Advanced transformations</li> <li>Performance optimization</li> </ul> <p>Need help? Check the Node Reference for detailed documentation on each node type.</p>"},{"location":"users/visual-editor/tutorials/cloud-connections.html","title":"Manage S3 Connections","text":"<p>This guide walks you through creating AWS S3 connections in Flowfile to access your cloud data.</p>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#overview","title":"Overview","text":"<p>Cloud storage connections securely store your AWS credentials and configuration, allowing you to reuse them across multiple workflows without re-entering credentials.</p>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#steps-to-create-an-s3-connection","title":"Steps to Create an S3 Connection","text":""},{"location":"users/visual-editor/tutorials/cloud-connections.html#1-access-cloud-storage-connections","title":"1. Access Cloud Storage Connections","text":"<p>Click the Cloud icon in the left sidebar to access the Cloud Storage Connections page.</p> Screenshot: Cloud Storage Connections Page <p></p>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#2-add-new-connection","title":"2. Add New Connection","text":"<p>Click the \"+ Add Connection\" button to open the connection configuration dialog.</p> Screenshot: Add Connection Dialog <p></p>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#3-configure-connection-settings","title":"3. Configure Connection Settings","text":""},{"location":"users/visual-editor/tutorials/cloud-connections.html#basic-settings","title":"Basic Settings","text":"Field Description Connection Name A unique identifier for this connection (e.g., <code>my_s3_storage</code>) Storage Type Select AWS S3"},{"location":"users/visual-editor/tutorials/cloud-connections.html#authentication-methods","title":"Authentication Methods","text":"<p>Choose one of the following authentication methods:</p>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#access-key","title":"Access Key","text":"<ul> <li>AWS Access Key ID: Your AWS access key (e.g., <code>AKIAIOSFODNN7EXAMPLE</code>)</li> <li>AWS Secret Access Key: Your AWS secret access key</li> <li>AWS Region: The AWS region where your S3 buckets are located (e.g., <code>us-east-1</code>)</li> </ul>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#aws-cli","title":"AWS CLI","text":"<ul> <li>Uses credentials from your local AWS CLI configuration</li> <li>AWS Region: The AWS region where your S3 buckets are located</li> </ul>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#advanced-settings-optional","title":"Advanced Settings (Optional)","text":"Field Description Custom Endpoint URL For S3-compatible services (e.g., MinIO) Allow Unsafe HTML Enable if your S3 data contains HTML content Verify SSL Disable only for testing with self-signed certificates"},{"location":"users/visual-editor/tutorials/cloud-connections.html#4-save-connection","title":"4. Save Connection","text":"<p>Click \"Create Connection\" to save your configuration.</p>"},{"location":"users/visual-editor/tutorials/cloud-connections.html#using-s3-connections-in-workflows","title":"Using S3 Connections in Workflows","text":"<p>Once created, your S3 connection will appear in the Cloud Storage Reader and Writer node's connection dropdown. Simply:</p> <ol> <li>Add a Cloud Storage Reader node to your workflow</li> <li>Select your connection from the dropdown</li> <li>Enter the S3 path (e.g., <code>s3://my-bucket/data/file.csv</code>)</li> <li>Configure file format options</li> <li>Run your workflow</li> </ol>"},{"location":"users/visual-editor/tutorials/code-generator.html","title":"Building code with flows","text":"<p>Flowfile's Code Generator allows you to export your visually designed data pipelines as clean, executable Python code. This feature is designed to empower users who wish to inspect the underlying data transformation logic, integrate Flowfile pipelines into existing Python projects, or extend their workflows with custom scripts.</p> <p>The generated code is entirely self-contained and relies mostly on the Polars library, ensuring it can run independently of Flowfile. </p>"},{"location":"users/visual-editor/tutorials/code-generator.html#key-characteristics-of-the-generated-code","title":"Key Characteristics of the Generated Code","text":"<p>When you generate code from your Flowfile graph, you can expect the output to be:</p> <ul> <li>Standalone: The code functions independently, requiring only Polars and common Python libraries.</li> <li>Readable: The structure mirrors your visual flow, making it easy to understand the sequence of operations.</li> <li>Direct Translation: Each Flowfile node and its configured settings are directly translated into equivalent Polars operations.</li> <li>Ready for Integration: You can copy, modify, and embed this code into other Python applications or scripts.</li> </ul>"},{"location":"users/visual-editor/tutorials/code-generator.html#examples-of-generated-code","title":"Examples of Generated Code","text":"<p>Here are some simplified examples illustrating what the generated Polars code looks like for common Flowfile operations. These examples highlight how your visual workflow seamlessly translates into Python.</p>"},{"location":"users/visual-editor/tutorials/code-generator.html#example-1-reading-a-csv-and-selecting-columns","title":"Example 1: Reading a CSV and Selecting Columns","text":"<p>This example shows how a pipeline that reads a CSV file and then selects/renames specific columns translates into Polars code.</p> <p>Flowfile Pipeline:</p> <ol> <li>Read CSV (e.g., <code>customers.csv</code>)</li> <li>Select (e.g., keep <code>name</code> as <code>customer_name</code>, <code>age</code>)</li> </ol> Generated Polars Code <pre><code># Example 1: Reading a CSV and Selecting Columns\nimport polars as pl\n\ndef run_etl_pipeline():\n    \"\"\"\n    ETL Pipeline: Example CSV Read and Select\n    Generated from Flowfile\n    \"\"\"\n    df_1 = pl.scan_csv(\"/path/to/your/customers.csv\")\n    df_2 = df_1.select(\n        pl.col(\"name\").alias(\"customer_name\"),\n        pl.col(\"age\")\n    )\n    return df_2\n\nif __name__ == \"__main__\":\n    pipeline_output = run_etl_pipeline()\n</code></pre>"},{"location":"users/visual-editor/tutorials/code-generator.html#example-2-grouping-and-aggregating-data","title":"Example 2: Grouping and Aggregating Data","text":"<p>This example demonstrates the code generated for a pipeline that processes a dataset and performs a group by operation with aggregations.</p> <p>Flowfile Pipeline:</p> <ol> <li>Manual Input (sample sales data with <code>product</code> and <code>revenue</code>)</li> <li>Group By (e.g., group by <code>product</code>, sum <code>revenue</code> as <code>total_revenue</code>)</li> </ol> Generated Polars Code <pre><code># Example 2: Grouping and Aggregating Data\nimport polars as pl\n\ndef run_etl_pipeline():\n    \"\"\"\n    ETL Pipeline: Example Grouping and Aggregating\n    Generated from Flowfile\n    \"\"\"\n    # Simplified manual input example\n    df_1 = pl.LazyFrame(\n        {\n            \"product\": [\"A\", \"B\", \"A\", \"B\", \"C\"],\n            \"revenue\": [100.0, 200.0, 100.0, 200.0, 150.0],\n        }\n    )\n    df_2 = df_1.group_by([\"product\"]).agg([\n        pl.col(\"revenue\").sum().alias(\"total_revenue\"),\n    ])\n    return df_2\n\nif __name__ == \"__main__\":\n    pipeline_output = run_etl_pipeline()\n</code></pre>"},{"location":"users/visual-editor/tutorials/code-generator.html#example-3-custom-polars-code-execution","title":"Example 3: Custom Polars Code Execution","text":"<p>For advanced users, Flowfile offers a \"Polars Code\" node where you can write custom Polars expressions. Here's how that custom code is integrated into the generated script.</p> <p>Flowfile Pipeline:</p> <ol> <li>Manual Input (a basic DataFrame)</li> <li>Polars Code (a node containing custom Polars logic, e.g., adding a new column)</li> </ol> Generated Polars Code <pre><code># Example 3: Custom Polars Code Execution\nimport polars as pl\n\ndef run_etl_pipeline():\n    \"\"\"\n    ETL Pipeline: Custom Polars Code Example\n    Generated from Flowfile\n    \"\"\"\n    df_1 = pl.LazyFrame({\"value\": [1, 2, 3]})\n\n    # Custom Polars code as defined in the Flowfile node\n    def _custom_code_node_name(input_df: pl.LazyFrame):\n        return input_df.with_columns((pl.col('value') * 10).alias('scaled_value'))\n\n    df_2 = _custom_code_node_name(df_1)\n    return df_2\n\nif __name__ == \"__main__\":\n    pipeline_output = run_etl_pipeline()\n</code></pre> <p>These examples provide a clear overview of the type of high-quality, executable Python code produced by Flowfile's Code Generator.</p>"},{"location":"users/visual-editor/tutorials/database-connectivity.html","title":"How to Connect and Work with PostgreSQL Databases in Flowfile","text":""},{"location":"users/visual-editor/tutorials/database-connectivity.html#full-flow-overview","title":"Full flow overview","text":"<p>Flowfile's latest release introduces powerful database connectivity features that allow you to seamlessly integrate with PostgreSQL databases like Supabase. In this guide, I'll walk you through the entire process of connecting to a database, reading data, transforming it, and writing it back.</p>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#prerequisites","title":"Prerequisites","text":"<p>Before diving in, make sure you have:</p> <ul> <li>A Flowfile account (free tier works fine)</li> <li>A Supabase account (sign up here if needed)</li> <li>Sample data to work with (we're using the Sales Forecasting Dataset from Kaggle so you can easily follow along with the transformation examples)</li> </ul>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-1-set-up-your-supabase-database","title":"Step 1: Set Up Your Supabase Database","text":"<ol> <li>Create a new project in Supabase.</li> <li>Download the sample dataset from Kaggle.</li> <li>Create a new table in your Supabase project (e.g., <code>superstore_sales_data</code>).</li> <li>Import the dataset into your table (hint: use Supabase's built-in CSV import feature via the Table Editor).</li> <li>Note your database connection details (host, port, username, password).</li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-2-configure-your-database-connection-in-flowfile","title":"Step 2: Configure Your Database Connection in Flowfile","text":"<ol> <li>Open Flowfile and navigate to the database connection manager (often found under a \"Connections\" icon or within the main \"Settings\" area).</li> <li>Click \"Create New Connection\".</li> <li>Fill in your connection details:<ul> <li>Connection Name: <code>supa_base_connection</code> (or any name you prefer)</li> <li>Database Type: PostgreSQL</li> <li>Host: Your Supabase host (e.g., <code>aws-0-eu-central-1.pooler.supabase.com</code>)</li> <li>Port: <code>5432</code></li> <li>Database: <code>postgres</code></li> <li>Username: Your Supabase username</li> <li>Password: Your Supabase password</li> <li>Enable SSL: Check if required by your database (Supabase typically requires it).</li> </ul> </li> <li>Click \"Update Connection\" to save.</li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#connection-overview-in-flowfile","title":"Connection overview in Flowfile","text":""},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-3-create-a-new-data-flow","title":"Step 3: Create a New Data Flow","text":"<ol> <li>Click \"Create\" or \"New Flow\" to start a fresh workflow.</li> <li>Navigate to the \"Data actions\" panel on the left sidebar.</li> <li>Find the \"Read from Database\" node (look for the database icon).</li> <li>Drag and drop this node onto your canvas.</li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-4-configure-your-database-read-operation","title":"Step 4: Configure Your Database Read Operation","text":"<ol> <li>Click on the \"Read from Database\" node to open its settings panel.</li> <li>Select \"reference\" for Connection Mode (this tells Flowfile to use the connection you configured in Step 2).</li> <li>Choose your <code>supa_base_connection</code> from the Connection dropdown.</li> <li>Configure the table settings:<ul> <li>Schema: <code>public</code> (or your specific schema)</li> <li>Table: <code>superstore_sales_data</code></li> </ul> </li> <li>Click \"Validate Settings\" to ensure everything is working.</li> <li>You should see a green confirmation message: \"Query settings are valid\".</li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#node-settings-panel-showing-database-read-configuration","title":"Node Settings panel showing database read configuration","text":""},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-5-run-your-initial-flow","title":"Step 5: Run Your Initial Flow","text":"<ol> <li>Click the \"Run\" button in the top toolbar.</li> <li>Watch the flow execution in the log panel at the bottom.</li> <li>When completed, you'll see a success message and the number of records processed.</li> <li>You can now click on the node output dot to preview the data that was read from your database.</li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#flow-execution-logs-showing-successful-database-read-operation","title":"Flow execution logs showing successful database read operation","text":"<p>Ensure the image <code>initial_run.png</code> shows the successful run log/node status, not the configuration panel again.</p>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-6-add-data-transformations","title":"Step 6: Add Data Transformations","text":"<p>Now that you've successfully read data from your database, you can add transformation steps:</p> <ol> <li>Add transformation nodes from the \"Data actions\" panel to your workflow.<ul> <li>For example, creating time-to-ship metrics by category:<ul> <li>Add formula nodes to transform the <code>shipping_date</code> and <code>delivery_date</code> columns to a proper date type if needed.</li> <li>Add a \"Formula\" node to calculate shipping time (e.g., <code>delivery_date - shipping_date</code>). Name the new column <code>shipping_time_days</code>.</li> <li>Add a \"Group by\" node to aggregate by product category.</li> <li>In the \"Group by\" node, calculate <code>min</code>, <code>max</code>, and <code>median</code> of the <code>shipping_time_days</code> column.</li> </ul> </li> </ul> </li> <li>Connect these nodes in sequence by dragging from the output dot of one node to the input dot of the next.</li> <li>Configure each node with the specific transformations you need (refer to the Flowfile documentation for details on specific node configurations if needed).</li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#overview-of-connected-transformation-nodes-read-formula-group-by","title":"Overview of connected transformation nodes (Read -&gt; Formula -&gt; Group By)","text":""},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-7-add-a-write-to-database-node","title":"Step 7: Add a Write to Database Node","text":"<ol> <li>From the \"Data actions\" panel, find and drag the \"Write to Database\" node onto your canvas.</li> <li>Connect it to the output of the last transformation node (e.g., the \"Group by\" node).</li> <li>Configure the write operation in its settings panel:<ul> <li>Connection Mode: Select \"reference\".</li> <li>Connection: Choose your <code>supa_base_connection</code>.</li> <li>Schema: <code>public</code> (or your desired schema).</li> <li>Table: Enter a name for your new output table (e.g., <code>time_to_ship_per_category</code>).</li> <li>Write Mode: Select how to handle the table if it already exists:<ul> <li>\"Append\": Add new data to the table.</li> <li>\"Replace\": Delete the existing table and create a new one with the output data.</li> <li>\"Fail\": Abort the flow if the table already exists.</li> </ul> </li> </ul> </li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#setup-write-to-database-node-configuration-panel","title":"Setup write to database node configuration panel","text":"<p>Ensure the image <code>configure_write_db.png</code> actually shows the \"Write to Database\" node's configuration panel.</p>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#step-8-run-your-complete-workflow","title":"Step 8: Run Your Complete Workflow","text":"<ol> <li>Click \"Run\" to execute the full workflow from start to finish.</li> <li>The system will:<ul> <li>Read data from your source Supabase table (<code>superstore_sales_data</code>).</li> <li>Apply all the transformation steps (calculate shipping time, group by category).</li> <li>Write the aggregated results to your destination Supabase table (<code>time_to_ship_per_category</code>).</li> </ul> </li> <li>Check the logs to confirm successful execution, including messages about records read and written.</li> <li>Navigate to your Supabase project, open the SQL Editor or Table Editor, and check the <code>public.time_to_ship_per_category</code> table to see your newly created data!</li> </ol>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#overview-of-the-final-result-table-in-supabase","title":"Overview of the final result table in Supabase","text":"<p>Ensure the image <code>result.png</code> shows the data in the newly created Supabase table.</p>"},{"location":"users/visual-editor/tutorials/database-connectivity.html#conclusion","title":"Conclusion","text":"<p>Flowfile's database integration capabilities make it incredibly simple to build professional-grade data pipelines without writing code. By connecting to Supabase or other PostgreSQL databases, you can easily extract, transform, and load data in a visual, intuitive environment.</p> <p>Whether you're creating business dashboards, data warehousing solutions, or just exploring your data, the combination of Flowfile's visual workflow and Supabase's powerful PostgreSQL hosting gives you a robust platform for all your data needs.</p> <p>Feel free to experiment with different transformation nodes and workflow patterns to build increasingly sophisticated data pipelines!</p> <p>This guide is based on Flowfile v0.2.0, which introduced database connectivity features including PostgreSQL support, secure credential storage, and flexible connection management options.</p>"}]}